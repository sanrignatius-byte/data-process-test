{"query_id": "l1_1104.3913_1104.3913_fig_1_0", "query": "Why does the first constraint require x to have S(x)-T(x) outgoing flow units from the regions shown?", "answer": "The constraint ensures that flow f accounts for the difference between S and T values at each point x, represented by the spatial separation in the diagram where S and T occupy different regions divided by the horizontal line.", "doc_id": "1104.3913", "figure_id": "1104.3913_fig_1", "figure_number": 2, "image_path": "data/mineru_output/1104.3913/1104.3913/hybrid_auto/images/1104.3913_page0_fig0.jpg", "caption": "Figure 2: $S _ { 0 } = G _ { 0 } \\cap S$ , $T _ { 0 } = G _ { 0 } \\cap T$", "figure_type": "diagram", "visual_anchor": "S₀, S₁ labels on left shape and T₀, T₁ labels on right shape separated by dashed line", "text_evidence": "The first constraint requires that x has at least S(x) - T(x) outgoing units of flow in f.", "query_type": "comparison_explanation"}
{"query_id": "l1_1104.3913_1104.3913_fig_1_1", "query": "How does the over-representation of S members in G₀ shown above the dashed line affect treatment differences?", "answer": "Members of S being over-represented in G₀ and under-represented in G₁ causes the average treatment of S members to be very different from the average treatment of T members.", "doc_id": "1104.3913", "figure_id": "1104.3913_fig_1", "figure_number": 2, "image_path": "data/mineru_output/1104.3913/1104.3913/hybrid_auto/images/1104.3913_page0_fig0.jpg", "caption": "Figure 2: $S _ { 0 } = G _ { 0 } \\cap S$ , $T _ { 0 } = G _ { 0 } \\cap T$", "figure_type": "diagram", "visual_anchor": "G₀ label above dashed line and G₁ label below dashed line", "text_evidence": "members of S , on average, may therefore be very different from the treatment, on average, of members of T , since members of S are over-represented in G₀ and under-represented in G₁", "query_type": "comparison_explanation"}
{"query_id": "l1_1104.3913_1104.3913_fig_1_2", "query": "What concrete example does the spatial partition between S₀ and S₁ by the dashed line illustrate?", "answer": "The partition illustrates a cardinal question being examined, showing how the intersection of groups G₀ and G₁ with sets S and T creates distinct subregions (S₀, S₁, T₀, T₁) that demonstrate spatial or categorical separation.", "doc_id": "1104.3913", "figure_id": "1104.3913_fig_1", "figure_number": 2, "image_path": "data/mineru_output/1104.3913/1104.3913/hybrid_auto/images/1104.3913_page0_fig0.jpg", "caption": "Figure 2: $S _ { 0 } = G _ { 0 } \\cap S$ , $T _ { 0 } = G _ { 0 } \\cap T$", "figure_type": "diagram", "visual_anchor": "Horizontal dashed line separating S₀ above from S₁ below", "text_evidence": "This is a cardinal question, which we examine with a concrete example illustrated in Figure 2.", "query_type": "visual_definition"}
{"query_id": "l1_1306.5204_1306.5204_fig_1_3", "query": "Why does 'syria' dominate the tag cloud if researchers collected data using a boundary box with strong Asian bias?", "answer": "The Asian boundary box captured Middle Eastern tweets, where Syria-related content was highly active during this period. The prominence reflects regional social media activity patterns within the geographic collection parameters.", "doc_id": "1306.5204", "figure_id": "1306.5204_fig_1", "figure_number": 1, "image_path": "data/mineru_output/1306.5204/1306.5204/hybrid_auto/images/1306.5204_page0_fig0.jpg", "caption": "Figure 1: Tag cloud of top terms from each dataset.", "figure_type": "plot", "visual_anchor": "Large centered word 'syria' dominating the tag cloud", "text_evidence": "We start by grouping the locations of tweets by continent and can find a strong Asian bias due to the boundary box we used to collect the data from both sources, shown in Table 1.", "query_type": "anomaly_cause"}
{"query_id": "l1_1306.5204_1306.5204_fig_1_4", "query": "Does the presence of 'revolution' alongside city names like 'damascus' and 'homs' reflect the topic modeling applications researchers pursue with Twitter's Streaming API?", "answer": "Yes, the clustering of political terms with geographic locations demonstrates typical topic modeling patterns. Researchers use the Streaming API specifically for such topic modeling to understand content themes and user behavior.", "doc_id": "1306.5204", "figure_id": "1306.5204_fig_1", "figure_number": 1, "image_path": "data/mineru_output/1306.5204/1306.5204/hybrid_auto/images/1306.5204_page0_fig0.jpg", "caption": "Figure 1: Tag cloud of top terms from each dataset.", "figure_type": "plot", "visual_anchor": "Word 'revolution' appearing near 'damascus', 'homs', and 'aleppo'", "text_evidence": "Twitter's Streaming API has been used throughout the domain of social media and network analysis to generate understanding of how users behave on these platforms. It has been used to collect data for topic modeling (Hong and Davison 2010; Pozdnoukhov and Kaiser 2011)", "query_type": "comparison_explanation"}
{"query_id": "l1_1306.5204_1306.5204_fig_1_5", "query": "How does the mix of Arabic and English terms like 'bashar' and 'arab' illustrate the Streaming API's preset parameter matching?", "answer": "The bilingual content shows the API captured tweets matching geographic or keyword parameters related to Middle Eastern events. The preset parameters allowed collection of both Arabic and English tweets discussing the same regional topics.", "doc_id": "1306.5204", "figure_id": "1306.5204_fig_1", "figure_number": 1, "image_path": "data/mineru_output/1306.5204/1306.5204/hybrid_auto/images/1306.5204_page0_fig0.jpg", "caption": "Figure 1: Tag cloud of top terms from each dataset.", "figure_type": "plot", "visual_anchor": "Mixed Arabic script and English terms including 'bashar', 'arab', and Arabic characters", "text_evidence": "It provides a glance into its millions of users and billions of tweets through a 'Streaming API' which provides a sample of all tweets matching some parameters preset by the API user.", "query_type": "value_context"}
{"query_id": "l1_1306.5204_1306.5204_fig_3_6", "query": "Why does the dotted Streaming line drop below 10000 tweets after 12-22 despite matching parameters?", "answer": "The Streaming API collected only 528,592 tweets total compared to 1,280,344 from the Firehose during the same period with exactly the same parameters, indicating significant data loss in the Streaming API after the drop-off point.", "doc_id": "1306.5204", "figure_id": "1306.5204_fig_3", "figure_number": 2, "image_path": "data/mineru_output/1306.5204/1306.5204/hybrid_auto/images/1306.5204_page0_fig2.jpg", "caption": "Figure 2: Raw tweet counts for each day from both the Streaming API and the Firehose.", "figure_type": "plot", "visual_anchor": "Dotted blue Streaming line dropping from ~43000 to below 10000 tweets around date 12-22", "text_evidence": "During the time we collected 528,592 tweets from the Streaming API and 1,280,344 tweets from the Firehose.", "query_type": "anomaly_cause"}
{"query_id": "l1_1306.5204_1306.5204_fig_3_7", "query": "Does the solid red peak reaching 90000 tweets around 12-27 support TweetTracker's collection methodology?", "answer": "Yes, the Firehose peak demonstrates successful collection of the full dataset. TweetTracker collected from both sources using identical parameters, showing the Firehose captured significantly more data during high-volume periods.", "doc_id": "1306.5204", "figure_id": "1306.5204_fig_3", "figure_number": 2, "image_path": "data/mineru_output/1306.5204/1306.5204/hybrid_auto/images/1306.5204_page0_fig2.jpg", "caption": "Figure 2: Raw tweet counts for each day from both the Streaming API and the Firehose.", "figure_type": "plot", "visual_anchor": "Solid red Firehose line peaking at approximately 90000 tweets around date 12-27", "text_evidence": "we collected tweets from the Streaming API using TweetTracker (Kumar et al. 2011) with exactly the same parameters.", "query_type": "value_context"}
{"query_id": "l1_1306.5204_1306.5204_fig_3_8", "query": "How does the gap between red and blue lines relate to social media analysis reliability?", "answer": "The persistent gap demonstrates that researchers' reliance on the Streaming API is problematic, as it consistently undersamples compared to the Firehose. This affects topic modeling, network analysis, and statistical content analysis accuracy.", "doc_id": "1306.5204", "figure_id": "1306.5204_fig_3", "figure_number": 2, "image_path": "data/mineru_output/1306.5204/1306.5204/hybrid_auto/images/1306.5204_page0_fig2.jpg", "caption": "Figure 2: Raw tweet counts for each day from both the Streaming API and the Firehose.", "figure_type": "plot", "visual_anchor": "Vertical gap between solid red Firehose line (higher) and dotted blue Streaming line (lower) throughout the time period", "text_evidence": "It has been used to collect data for topic modeling (Hong and Davison 2010; Pozdnoukhov and Kaiser 2011), network analysis (Sofean and Smith 2012), and statistical analysis of content (Mathioudakis and Koudas 2010)", "query_type": "comparison_explanation"}
{"query_id": "l1_1306.5204_1306.5204_fig_4_9", "query": "Does the median coverage near 0.5 explain why December 29th was selected for analysis?", "answer": "Yes, December 29th was explicitly chosen because it represents the median coverage level shown in the box plot, making it useful for understanding typical Streaming API coverage characteristics.", "doc_id": "1306.5204", "figure_id": "1306.5204_fig_4", "figure_number": 3, "image_path": "data/mineru_output/1306.5204/1306.5204/hybrid_auto/images/1306.5204_page0_fig3.jpg", "caption": "Figure 3: Distribution of coverage for the Streaming data by day. Whiskers indicate extreme values.", "figure_type": "plot", "visual_anchor": "median line at approximately 0.5 in the box plot", "text_evidence": "we select five days with different levels of coverage as motivated by Figure 3: The minimum (December 27th), lower quartile (December 24th), median (December 29th), upper quartile (December 18th)", "query_type": "value_context"}
{"query_id": "l1_1306.5204_1306.5204_fig_4_10", "query": "Why does the box extend from roughly 0.3 to 0.75 when only 528,592 tweets were collected?", "answer": "The box spanning 0.3 to 0.75 represents the interquartile range of daily coverage proportions. Since 528,592 Streaming tweets were collected versus 1,280,344 Firehose tweets, the coverage varied substantially day-to-day within this range.", "doc_id": "1306.5204", "figure_id": "1306.5204_fig_4", "figure_number": 3, "image_path": "data/mineru_output/1306.5204/1306.5204/hybrid_auto/images/1306.5204_page0_fig3.jpg", "caption": "Figure 3: Distribution of coverage for the Streaming data by day. Whiskers indicate extreme values.", "figure_type": "plot", "visual_anchor": "box boundaries at approximately 0.3 (lower quartile) and 0.75 (upper quartile)", "text_evidence": "During the time we collected 528,592 tweets from the Streaming API and 1,280,344 tweets from the Firehose.", "query_type": "comparison_explanation"}
{"query_id": "l1_1306.5204_1306.5204_fig_4_11", "query": "What causes the upper whisker to reach approximately 0.9 despite investigating statistical properties of sampled data?", "answer": "The upper whisker at 0.9 indicates that on some days, the Streaming API captured nearly 90% of Firehose tweets, representing extreme coverage values. This variation is important for understanding how sampling affects statistical characteristics being investigated.", "doc_id": "1306.5204", "figure_id": "1306.5204_fig_4", "figure_number": 3, "image_path": "data/mineru_output/1306.5204/1306.5204/hybrid_auto/images/1306.5204_page0_fig3.jpg", "caption": "Figure 3: Distribution of coverage for the Streaming data by day. Whiskers indicate extreme values.", "figure_type": "plot", "visual_anchor": "upper whisker extending to approximately 0.9", "text_evidence": "We investigate the statistical properties of the two datasets with the intent of understanding how well the characteristics of the sampled data match those of the Firehose.", "query_type": "anomaly_cause"}
{"query_id": "l1_1306.5204_1306.5204_fig_5_12", "query": "Why does the solid black Max curve start near 0.8 while investigating how sampled data characteristics match the Firehose?", "answer": "The Max curve represents the highest rank correlation between top hashtags in Firehose and Streaming data. Starting at 0.8 indicates strong initial concordance in the most correlated ranking, which helps understand how well sampled data characteristics match the Firehose.", "doc_id": "1306.5204", "figure_id": "1306.5204_fig_5", "figure_number": 4, "image_path": "data/mineru_output/1306.5204/1306.5204/hybrid_auto/images/1306.5204_page0_fig4.jpg", "caption": "Figure 4: Relationship between $n$ - number of top hashtags, and the correlation coefficient, $\\tau _ { \\beta }$ .", "figure_type": "plot", "visual_anchor": "Solid black line labeled 'Max' starting around 0.8 at n near 0", "text_evidence": "We investigate the statistical properties of the two datasets with the intent of understanding how well the characteristics of the sampled data match those of the Firehose.", "query_type": "value_context"}
{"query_id": "l1_1306.5204_1306.5204_fig_5_13", "query": "Does the dotted red Median curve stabilizing near 0.55 at n=1000 reflect concordant versus discordant pairs in the rank correlation?", "answer": "Yes, the Median tau_β value near 0.55 represents the balance between concordant pairs (P_C) and discordant pairs (P_D) in the Kendall's tau calculation. This moderate positive correlation indicates reasonable agreement in hashtag rankings between datasets.", "doc_id": "1306.5204", "figure_id": "1306.5204_fig_5", "figure_number": 4, "image_path": "data/mineru_output/1306.5204/1306.5204/hybrid_auto/images/1306.5204_page0_fig4.jpg", "caption": "Figure 4: Relationship between $n$ - number of top hashtags, and the correlation coefficient, $\\tau _ { \\beta }$ .", "figure_type": "plot", "visual_anchor": "Dotted red line labeled 'Median' stabilizing around 0.55 at n=1000", "text_evidence": "where $P _ { C }$ is the set of concordant pairs, $P _ { D }$ is the set of discordant pairs, $T _ { F }$ is the set of ties in the Firehose data, but not in the Streaming data, $T _ { S }$ is the number of ties found in the Streaming data, but not in the Firehose, and $n$ is the number of pairs in total.", "query_type": "value_context"}
{"query_id": "l1_1306.5204_1306.5204_fig_5_14", "query": "Why do all five curves show sharp fluctuations before n=100 when comparing top hashtags at different coverage levels?", "answer": "The sharp initial fluctuations occur because with small n values, the rank correlation is highly sensitive to individual hashtag ranking differences. As n increases beyond 100, the correlation stabilizes because the statistical properties become more robust across different coverage levels.", "doc_id": "1306.5204", "figure_id": "1306.5204_fig_5", "figure_number": 4, "image_path": "data/mineru_output/1306.5204/1306.5204/hybrid_auto/images/1306.5204_page0_fig4.jpg", "caption": "Figure 4: Relationship between $n$ - number of top hashtags, and the correlation coefficient, $\\tau _ { \\beta }$ .", "figure_type": "plot", "visual_anchor": "Sharp fluctuations in all curves (Min, Q1, Median, Q3, Max) between n=0 and n=100", "text_evidence": "We begin first by comparing the top hashtags in the tweets for different levels of coverage using a rank correlation statistic.", "query_type": "anomaly_cause"}
{"query_id": "l1_1306.5204_1306.5204_fig_6_15", "query": "Why does the solid blue Min line drop to approximately 0.6 at n=1000 while computing concordant pairs PC?", "answer": "The Min line represents the minimum correlation across different coverage levels. As n increases, the minimum tau_β decreases because the correlation coefficient is calculated using concordant pairs (PC) and discordant pairs (PD), showing worst-case correlation degradation at higher hashtag counts.", "doc_id": "1306.5204", "figure_id": "1306.5204_fig_6", "figure_number": 5, "image_path": "data/mineru_output/1306.5204/1306.5204/hybrid_auto/images/1306.5204_page0_fig5.jpg", "caption": "Figure 5: Random sampling of Firehose data. Relationship between n - number of top hashtags, and $\\tau _ { \\beta }$ - the correlation coefficient for different levels of coverage.", "figure_type": "plot", "visual_anchor": "solid blue Min line reaching approximately 0.6 at n=1000", "text_evidence": "where $P _ { C }$ is the set of concordant pairs, $P _ { D }$ is the set of discordant pairs, $T _ { F }$ is the set of ties in the Firehose data, but not in the Streaming data", "query_type": "value_context"}
{"query_id": "l1_1306.5204_1306.5204_fig_6_16", "query": "Does the black Max line staying above 0.9 throughout align with tau_β ranging from perfect negative to perfect positive correlation?", "answer": "Yes, the Max line remaining above 0.9 indicates that the best-case correlation stays near perfect positive correlation (value of 1) across all n values, consistent with the tau_β range from -1 to 1.", "doc_id": "1306.5204", "figure_id": "1306.5204_fig_6", "figure_number": 5, "image_path": "data/mineru_output/1306.5204/1306.5204/hybrid_auto/images/1306.5204_page0_fig5.jpg", "caption": "Figure 5: Random sampling of Firehose data. Relationship between n - number of top hashtags, and $\\tau _ { \\beta }$ - the correlation coefficient for different levels of coverage.", "figure_type": "plot", "visual_anchor": "solid black Max line maintaining values above 0.9 across all n", "text_evidence": "The $\\tau _ { \\beta }$ value ranges from -1, perfect negative correlation, to 1, perfect positive correlation.", "query_type": "value_context"}
{"query_id": "l1_1306.5204_1306.5204_fig_6_17", "query": "How does the steeper decline of the blue Min curve compared to purple Q3 relate to understanding n versus correlation?", "answer": "The steeper Min decline demonstrates that minimum correlation degrades faster with increasing n than the third quartile, revealing the relationship between hashtag count and correlation variability that the analysis aims to understand.", "doc_id": "1306.5204", "figure_id": "1306.5204_fig_6", "figure_number": 5, "image_path": "data/mineru_output/1306.5204/1306.5204/hybrid_auto/images/1306.5204_page0_fig5.jpg", "caption": "Figure 5: Random sampling of Firehose data. Relationship between n - number of top hashtags, and $\\tau _ { \\beta }$ - the correlation coefficient for different levels of coverage.", "figure_type": "plot", "visual_anchor": "blue Min line declining more steeply than purple Q3 dash-dot line", "text_evidence": "To understand the relationship between $n$ and the resulting correlation, $\\tau _", "query_type": "comparison_explanation"}
{"query_id": "l1_1306.5204_1306.5204_fig_10_18", "query": "Why does the tallest blue bar near zero divergence support that matched topics have low average divergence?", "answer": "The histogram shows most matched topic pairs cluster near zero Jensen-Shannon divergence, with the tallest bar at the leftmost position, confirming that μ (average divergence) is low across matched pairs.", "doc_id": "1306.5204", "figure_id": "1306.5204_fig_10", "figure_number": 6, "image_path": "data/mineru_output/1306.5204/1306.5204/hybrid_auto/images/1306.5204_page0_fig9.jpg", "caption": "Figure 6: The Jensen-Shannon divergence of the matched topics at different levels of coverage. The $\\mathbf { X }$ -axis is the binned divergence. No divergence was $> 0 . 1 5$ . The y-axis is the count of each bin. $\\mu$ is the average divergence of the matched topics, $\\sigma$ is the standard deviation.   ", "figure_type": "plot", "visual_anchor": "tallest blue bar at leftmost position near 0.00 divergence", "text_evidence": "We compute the Jensen-Shannon divergence for each matched pair and plot a histogram of the values in Figure 6.", "query_type": "value_context"}
{"query_id": "l1_1306.5204_1306.5204_fig_10_19", "query": "Does the absence of histogram bars beyond 0.15 on the x-axis confirm the stated maximum divergence threshold?", "answer": "Yes, the histogram shows no blue bars extending beyond the 0.15 mark on the x-axis, directly confirming that no matched topic pair exceeded this divergence threshold.", "doc_id": "1306.5204", "figure_id": "1306.5204_fig_10", "figure_number": 6, "image_path": "data/mineru_output/1306.5204/1306.5204/hybrid_auto/images/1306.5204_page0_fig9.jpg", "caption": "Figure 6: The Jensen-Shannon divergence of the matched topics at different levels of coverage. The $\\mathbf { X }$ -axis is the binned divergence. No divergence was $> 0 . 1 5$ . The y-axis is the count of each bin. $\\mu$ is the average divergence of the matched topics, $\\sigma$ is the standard deviation.   ", "figure_type": "plot", "visual_anchor": "empty space on x-axis beyond 0.15 with no blue bars", "text_evidence": "The $\\mathbf { X }$ -axis is the binned divergence. No divergence was $> 0 . 1 5$ .", "query_type": "value_context"}
{"query_id": "l1_1306.5204_1306.5204_fig_10_20", "query": "How does the rapid height decrease of blue bars from left to right relate to standard deviation?", "answer": "The rapidly decreasing bar heights indicate most divergence values concentrate near zero with few outliers, suggesting a small standard deviation (σ) in the matched topic divergences.", "doc_id": "1306.5204", "figure_id": "1306.5204_fig_10", "figure_number": 6, "image_path": "data/mineru_output/1306.5204/1306.5204/hybrid_auto/images/1306.5204_page0_fig9.jpg", "caption": "Figure 6: The Jensen-Shannon divergence of the matched topics at different levels of coverage. The $\\mathbf { X }$ -axis is the binned divergence. No divergence was $> 0 . 1 5$ . The y-axis is the count of each bin. $\\mu$ is the average divergence of the matched topics, $\\sigma$ is the standard deviation.   ", "figure_type": "plot", "visual_anchor": "rapidly decreasing blue bar heights from left to right", "text_evidence": "$\\mu$ is the average divergence of the matched topics, $\\sigma$ is the standard deviation.", "query_type": "comparison_explanation"}
{"query_id": "l1_1306.5204_1306.5204_fig_14_21", "query": "Does the red vertical line's position relative to the blue curve's mean demonstrate the Streaming data divergence anomaly?", "answer": "Yes, the red vertical line representing Streaming data is positioned multiple standard deviations away from the blue curve's mean, indicating the Streaming data's average Jensen-Shannon divergence is significantly different from random samples.", "doc_id": "1306.5204", "figure_id": "1306.5204_fig_14", "figure_number": 7, "image_path": "data/mineru_output/1306.5204/1306.5204/hybrid_auto/images/1306.5204_page0_fig13.jpg", "caption": "Figure 7: The distribution of average Jensen-Shannon divergences in the random data (blue curve), with the single average obtained through the Streaming data (red, vertical line). $z$ indicates the number of standard deviations the Streaming data is from the mean of the random samples.", "figure_type": "plot", "visual_anchor": "red vertical line at approximately 0.015 compared to blue curve peak at 0.01", "text_evidence": "The distribution of average Jensen-Shannon divergences in the random data (blue curve), with the single average obtained through the Streaming data (red, vertical line). $z$ indicates the number of standard deviations the Streaming data is from the mean of the random samples.", "query_type": "anomaly_cause"}
{"query_id": "l1_1306.5204_1306.5204_fig_14_22", "query": "Why does analyzing the blue distribution curve justify focusing only on the largest component in retweet networks?", "answer": "The blue curve represents random baseline data for comparison, and since retweet networks create many small disconnected components with all smaller ones containing less than 1% of nodes, the main component analysis is justified.", "doc_id": "1306.5204", "figure_id": "1306.5204_fig_14", "figure_number": 7, "image_path": "data/mineru_output/1306.5204/1306.5204/hybrid_auto/images/1306.5204_page0_fig13.jpg", "caption": "Figure 7: The distribution of average Jensen-Shannon divergences in the random data (blue curve), with the single average obtained through the Streaming data (red, vertical line). $z$ indicates the number of standard deviations the Streaming data is from the mean of the random samples.", "figure_type": "plot", "visual_anchor": "blue distribution curve showing baseline random sample divergences", "text_evidence": "Since retweet networks create a lot of small disconnected components, we focus only on the size of the largest component. The size of the main component and the fact that all smaller components contain less than $1\\%$ of the nodes justify our focus on the main component for this data.", "query_type": "comparison_explanation"}
{"query_id": "l1_1306.5204_1306.5204_fig_14_23", "query": "How does the red line's deviation from the blue curve peak support calculating z-Scores in this experiment?", "answer": "The red line's position away from the blue curve's peak quantifies how many standard deviations the Streaming data diverges from random samples, which is precisely what z-Scores measure in this experimental setup.", "doc_id": "1306.5204", "figure_id": "1306.5204_fig_14", "figure_number": 7, "image_path": "data/mineru_output/1306.5204/1306.5204/hybrid_auto/images/1306.5204_page0_fig13.jpg", "caption": "Figure 7: The distribution of average Jensen-Shannon divergences in the random data (blue curve), with the single average obtained through the Streaming data (red, vertical line). $z$ indicates the number of standard deviations the Streaming data is from the mean of the random samples.", "figure_type": "plot", "visual_anchor": "red vertical line deviation from blue curve peak at 0.01", "text_evidence": "Results of this experiment, including $z$ -Scores are shown in Figure 7.", "query_type": "value_context"}
{"query_id": "l1_1403.7400_1403.7400_fig_1_24", "query": "Why does the spike reaching 4.5M on May 28 illustrate the denominator problem when analyzing who engaged?", "answer": "The peak shows how many people clicked or retweeted, but the denominator problem means we don't know how many people saw these hashtags and chose not to engage, making it impossible to calculate true engagement rates.", "doc_id": "1403.7400", "figure_id": "1403.7400_fig_1", "figure_number": 1, "image_path": "data/mineru_output/1403.7400/1403.7400/hybrid_auto/images/1403.7400_page0_fig0.jpg", "caption": "Figure 1: The frequency of top 20 hashtags associated with Gezi Protests. (Banko and Babacan, 2013)", "figure_type": "plot", "visual_anchor": "Peak at approximately 4.5M frequency on May 28", "text_evidence": "It's not enough to know how many people have \"liked\" a Facebook status update, clicked on a link, or \"retweeted\" a message without knowing how many people saw the item and chose not to take any action.", "query_type": "value_context"}
{"query_id": "l1_1403.7400_1403.7400_fig_1_25", "query": "Does the sharp drop from 4.5M to under 1M between May 28 and June 3 reflect Twitter's disproportionate focus?", "answer": "Yes, this dramatic decline in hashtag frequency exemplifies why Twitter data dominates research, as the platform provides accessible metrics that capture such rapid shifts in social movement activity.", "doc_id": "1403.7400", "figure_id": "1403.7400_fig_1", "figure_number": 1, "image_path": "data/mineru_output/1403.7400/1403.7400/hybrid_auto/images/1403.7400_page0_fig0.jpg", "caption": "Figure 1: The frequency of top 20 hashtags associated with Gezi Protests. (Banko and Babacan, 2013)", "figure_type": "plot", "visual_anchor": "Steep decline from 4.5M peak to below 1M between May 28 and June 3", "text_evidence": "While there are many social media platforms, big data research focuses disproportionately on Twitter. For example, ICWSM, perhaps the leading selective conference in the field of social media, had 72 full papers last year (2013), almost half of which presented data that was primarily or solely drawn from Twitter.", "query_type": "comparison_explanation"}
{"query_id": "l1_1403.7400_1403.7400_fig_1_26", "query": "How does the flat baseline below 1M after June 9 obscure understanding of sub-population characteristics?", "answer": "The low, stable frequency after June 9 shows reduced hashtag usage, but without knowing the characteristics of who saw versus who engaged with these protest hashtags, researchers cannot determine if the movement lost visibility or simply shifted audiences.", "doc_id": "1403.7400", "figure_id": "1403.7400_fig_1", "figure_number": 1, "image_path": "data/mineru_output/1403.7400/1403.7400/hybrid_auto/images/1403.7400_page0_fig0.jpg", "caption": "Figure 1: The frequency of top 20 hashtags associated with Gezi Protests. (Banko and Babacan, 2013)", "figure_type": "plot", "visual_anchor": "Flat baseline consistently below 1M from June 9 onwards", "text_evidence": "We rarely know the characteristics of the sub-population that sees the content even thoug", "query_type": "anomaly_cause"}
{"query_id": "l1_1403.7400_1403.7400_fig_3_27", "query": "Why do the two Turkish tweets shown side-by-side lack @ mentions despite making engagement algorithmically invisible?", "answer": "Subtweeting is the practice of making a tweet referring to a person algorithmically invisible to that person by not using @ mentions. This allows users to discuss someone without triggering algorithmic visibility or notifications.", "doc_id": "1403.7400", "figure_id": "1403.7400_fig_3", "figure_number": 3, "image_path": "data/mineru_output/1403.7400/1403.7400/hybrid_auto/images/1403.7400_page0_fig2.jpg", "caption": "Figure 3: Two peeople “subtweetting” each other  without mentionning names. Thee exchange was cclear enough, hoowever, to be re-- ported in nnewspapers.", "figure_type": "example", "visual_anchor": "Absence of @ mentions in both the left tweet by zekeriya öz and right tweet by Şemil Tayyar", "text_evidence": "Subtweeting is the practice of making a tweet referring to a person algorithmically invisible to that person—and conseque", "query_type": "visual_definition"}
{"query_id": "l1_1403.7400_1403.7400_fig_3_28", "query": "Do the Turkish exchanges between zekeriya öz and Şemil Tayyar demonstrate how big data analyses can miss engagement modes?", "answer": "Yes, these subtweets demonstrate practices that alter visibility to machine algorithms. Such exchanges blind big data analyses to this mode of activity and engagement despite being newsworthy human interactions.", "doc_id": "1403.7400", "figure_id": "1403.7400_fig_3", "figure_number": 3, "image_path": "data/mineru_output/1403.7400/1403.7400/hybrid_auto/images/1403.7400_page0_fig2.jpg", "caption": "Figure 3: Two peeople “subtweetting” each other  without mentionning names. Thee exchange was cclear enough, hoowever, to be re-- ported in nnewspapers.", "figure_type": "example", "visual_anchor": "The two Twitter posts without direct mentions but responding to each other", "text_evidence": "Social media users engage in practices that alter their visibility to machine algorithms, including subtweeting, discussing a person's tweets via \"screen captures,\" and hatelinking. All these practices can blind big data analyses to this mode of activity and engagement.", "query_type": "comparison_explanation"}
{"query_id": "l1_1403.7400_1403.7400_fig_3_29", "query": "Why would the subtweet exchange shown require intimate familiarity with broader conversation context to understand?", "answer": "The exchange would be unintelligible to anyone who did not already follow the broader conversation and was not intimately familiar with the context. Despite lacking explicit mentions, the exchange was clear enough to be reported in newspapers for those with contextual knowledge.", "doc_id": "1403.7400", "figure_id": "1403.7400_fig_3", "figure_number": 3, "image_path": "data/mineru_output/1403.7400/1403.7400/hybrid_auto/images/1403.7400_page0_fig2.jpg", "caption": "Figure 3: Two peeople “subtweetting” each other  without mentionning names. Thee exchange was cclear enough, hoowever, to be re-- ported in nnewspapers.", "figure_type": "example", "visual_anchor": "The Turkish text in both tweets that refers to each other without explicit names", "text_evidence": "In another exxample drawn  from my primmary research oon TTurkey, figure  3 shows a subbtweet exchangge between twwo pprominent indi viduals that wwould be uninteelligible to anyyoone who did nnot already follow the broadder conversatioon aand was not inntimately familiar with the context.", "query_type": "value_context"}
{"query_id": "l1_1403.7400_1403.7400_fig_4_30", "query": "Why does the columnist use three embedded screenshots instead of direct quotes to respond to critics?", "answer": "Using screen captures rather than quotes makes the engagement algorithmically invisible, as this practice prevents Twitter's algorithms from detecting mentions, links, or quotes that would normally signal engagement.", "doc_id": "1403.7400", "figure_id": "1403.7400_fig_4", "figure_number": 4, "image_path": "data/mineru_output/1403.7400/1403.7400/hybrid_auto/images/1403.7400_page0_fig3.jpg", "caption": "Figure 4. This ppractice is so wwidespread thaat a single hourr following the saame purposive  sample resultted in more thhan 300 instancees in which useers employed suuch “caps.”", "figure_type": "example", "visual_anchor": "Three embedded tweet screenshots in the middle section of the Twitter post", "text_evidence": "A \"caps\" is done when Twitter users reference each other's tweets through screen captures rather than links, mentions or quotes", "query_type": "comparison_explanation"}
{"query_id": "l1_1403.7400_1403.7400_fig_4_31", "query": "How widespread is the practice shown in these embedded screenshots based on one hour of sampling?", "answer": "The practice is extremely widespread, with a single hour of purposive sampling resulting in more than 300 instances where users employed such screen capture \"caps\" instead of direct engagement.", "doc_id": "1403.7400", "figure_id": "1403.7400_fig_4", "figure_number": 4, "image_path": "data/mineru_output/1403.7400/1403.7400/hybrid_auto/images/1403.7400_page0_fig3.jpg", "caption": "Figure 4. This ppractice is so wwidespread thaat a single hourr following the saame purposive  sample resultted in more thhan 300 instancees in which useers employed suuch “caps.”", "figure_type": "example", "visual_anchor": "The three screenshot captures embedded within the main tweet dated 27 Dec 2013", "text_evidence": "This practice is so widespread that a single hour following the same purposive sample resulted in more than 300 instances in which users employed such \"caps.\"", "query_type": "value_context"}
{"query_id": "l1_1403.7400_1403.7400_fig_4_32", "query": "What type of algorithmic detection does the screenshot-based response at 7:50 AM evade compared to traditional mentions?", "answer": "The screen capture method adds to the invisibility of engagement to algorithms, preventing detection of references that would normally be captured through links, mentions, or quotes in Twitter's engagement tracking systems.", "doc_id": "1403.7400", "figure_id": "1403.7400_fig_4", "figure_number": 4, "image_path": "data/mineru_output/1403.7400/1403.7400/hybrid_auto/images/1403.7400_page0_fig3.jpg", "caption": "Figure 4. This ppractice is so wwidespread thaat a single hourr following the saame purposive  sample resultted in more thhan 300 instancees in which useers employed suuch “caps.”", "figure_type": "example", "visual_anchor": "The main Twitter post timestamp of 7:50 AM - 27 Dec 2013 containing screenshot captures", "text_evidence": "Using screen captures rather than quotes is another practice that adds to the invisibility of engagement to algorithms", "query_type": "anomaly_cause"}
{"query_id": "l1_1403.7400_1403.7400_fig_5_33", "query": "Why does Blake Hounshell's January 2014 tweet demonstrate the assumption that information spreads like germs across platforms?", "answer": "The tweet exemplifies how researchers import methods from other fields on the implicit assumption that social media networks operate like disease vectors or airline networks. However, such assumptions are rarely explicitly addressed in research papers.", "doc_id": "1403.7400", "figure_id": "1403.7400_fig_5", "figure_number": 5, "image_path": "data/mineru_output/1403.7400/1403.7400/hybrid_auto/images/1403.7400_page0_fig4.jpg", "caption": "Figure 5: Cleaar meaning onlyy in context and ttime.", "figure_type": "example", "visual_anchor": "Tweet dated '11:18 AM - 1 Jan 2014' with text 'Getting crowded under that bus.'", "text_evidence": "Do social mediia networks opperate through similar mechaannisms as netwoorks of airliness? Does informmation work thhe wway germs doo? Such questiions are rarelyy explicitly addddressed even tthough many  papers importt methods fromm oother fields on  the implicit asssumption that  the answer is  a yyes.", "query_type": "value_context"}
{"query_id": "l1_1403.7400_1403.7400_fig_5_34", "query": "How does the 64 retweets metric shown relate to studies limited to single platforms with restricted explanatory power?", "answer": "The engagement metrics (64 retweets, 55 favorites) represent platform-specific measurements. Studies examining information spread like Romero et al. and Lerman et al. are often limited to single or few platforms, which constrains their explanatory power.", "doc_id": "1403.7400", "figure_id": "1403.7400_fig_5", "figure_number": 5, "image_path": "data/mineru_output/1403.7400/1403.7400/hybrid_auto/images/1403.7400_page0_fig4.jpg", "caption": "Figure 5: Cleaar meaning onlyy in context and ttime.", "figure_type": "example", "visual_anchor": "Engagement counter showing '64 RETWEETS' and '55 FAVORITES'", "text_evidence": "Studies thaat do look at tthis question, ssuch as Romerro eet al. (2011) annd Lerman et aal. (2010), are  often limited tto ssingle, or few , platforms, wwhich limit thheir explanatorry ppower", "query_type": "comparison_explanation"}
{"query_id": "l1_1403.7400_1403.7400_fig_5_35", "query": "Why does the verified checkmark and follower count exemplify how humans respond to the metrics researchers measure?", "answer": "The verified badge and visible engagement metrics demonstrate reflexivity: humans understand, evaluate, and respond to the same measurements that big data researchers track. This includes deliberate attempts by activists to manipulate metrics like trending hashtags.", "doc_id": "1403.7400", "figure_id": "1403.7400_fig_5", "figure_number": 5, "image_path": "data/mineru_output/1403.7400/1403.7400/hybrid_auto/images/1403.7400_page0_fig4.jpg", "caption": "Figure 5: Cleaar meaning onlyy in context and ttime.", "figure_type": "example", "visual_anchor": "Blue verified checkmark icon next to Blake Hounshell's name", "text_evidence": "Unlike disease  vectors or gasses in a chambber, humans unndderstand, evaluuate and responnd to the same  metrics that biig ddata researcherrs are measurinng. For exampple, political acctiivists, especiallly in countriees such as Bahhrain, where thhe uunrest and reprression have re ceived less maainstream globaal mmedia attentio n, often undeertake deliberaate attempts tto mmake a hashtagg \"trend.\"", "query_type": "visual_definition"}
{"query_id": "l1_1403.7400_1403.7400_fig_6_36", "query": "Why did the Ankara Mayor's tweet with 4,206 retweets exemplify deliberate attempts to make hashtags trend?", "answer": "Political activists in countries with less mainstream global media attention, like Bahrain, deliberately attempt to make hashtags trend to gain visibility. The Ankara Mayor's #stoplyingCNN campaign demonstrates this strategic use of social media metrics to amplify political messaging.", "doc_id": "1403.7400", "figure_id": "1403.7400_fig_6", "figure_number": 6, "image_path": "data/mineru_output/1403.7400/1403.7400/hybrid_auto/images/1403.7400_page0_fig5.jpg", "caption": "Figure 6: Ankkara Mayor leadsds a hashtag campmpaign that will eeventually trendd worldwide. [Tr anslation: Yes…… I’m announcingg ur hashtag. #sstoplyingCNN]", "figure_type": "photo", "visual_anchor": "4,206 retweets metric displayed below the tweet", "text_evidence": "political acctiivists, especiallly in countriees such as Bahhrain, where thhe uunrest and reprression have re ceived less maainstream globaal mmedia attentio n, often undeertake deliberaate attempts tto mmake a hashtagg \"trend.\"", "query_type": "value_context"}
{"query_id": "l1_1403.7400_1403.7400_fig_6_37", "query": "Does the #stoplyingCNN hashtag campaign illustrate how humans respond to the same big data metrics researchers measure?", "answer": "Yes, the campaign demonstrates reflexivity where humans understand and respond to metrics like trending hashtags. This exemplifies how unlike disease vectors or gases, humans evaluate and strategically manipulate the same measurements that big data researchers study.", "doc_id": "1403.7400", "figure_id": "1403.7400_fig_6", "figure_number": 6, "image_path": "data/mineru_output/1403.7400/1403.7400/hybrid_auto/images/1403.7400_page0_fig5.jpg", "caption": "Figure 6: Ankkara Mayor leadsds a hashtag campmpaign that will eeventually trendd worldwide. [Tr anslation: Yes…… I’m announcingg ur hashtag. #sstoplyingCNN]", "figure_type": "photo", "visual_anchor": "#stoplyingCNN hashtag visible in the tweet text", "text_evidence": "UUnlike disease vectors or gasses in a chambber, humans unndderstand, evaluuate and responnd to the same metrics that biig ddata researcherrs are measurinng.", "query_type": "comparison_explanation"}
{"query_id": "l1_1403.7400_1403.7400_fig_6_38", "query": "How does the June 25, 2013 timestamp relate to understanding context of human communications on social media?", "answer": "The specific timestamp exemplifies the temporal context crucial for interpreting social media data. Understanding when communications occur is essential because meaning, context, and socio-cultural interactions are multi-faceted and complex, requiring careful methodological grounding beyond simple data collection.", "doc_id": "1403.7400", "figure_id": "1403.7400_fig_6", "figure_number": 6, "image_path": "data/mineru_output/1403.7400/1403.7400/hybrid_auto/images/1403.7400_page0_fig5.jpg", "caption": "Figure 6: Ankkara Mayor leadsds a hashtag campmpaign that will eeventually trendd worldwide. [Tr anslation: Yes…… I’m announcingg ur hashtag. #sstoplyingCNN]", "figure_type": "photo", "visual_anchor": "Timestamp reading 8:41 AM - 25 Jun 13", "text_evidence": "Meaniing of social mmedia imprints , context of huuman communiccations, and naature of socio -cultural interaactions are multi--faceted and ccomplex.", "query_type": "value_context"}
{"query_id": "l1_1409.0575_1409.0575_fig_1_39", "query": "Why does the Lizard appear in the Number of Instances row when ILSVRC2012 replaced 90 synsets with dog breeds?", "answer": "The Lizard example illustrates variation in instance count across object categories, while the 90 dog breed synsets replaced in ILSVRC2012 demonstrate fine-grained classification within the 1000 object classes, representing a different dimension of dataset complexity.", "doc_id": "1409.0575", "figure_id": "1409.0575_fig_1", "figure_number": 2, "image_path": "data/mineru_output/1409.0575/1409.0575/hybrid_auto/images/1409.0575_page0_fig0.jpg", "caption": "Figure 2. The synsets have remained consistent since year 2012. Appendix A provides the complete list of object categories used in ILSVRC2012-2014.", "figure_type": "diagram", "visual_anchor": "Lizard image in the Number of Instances row, second position from left", "text_evidence": "In ILSVRC2012, 90 synsets were replaced with categories corresponding to dog breeds to allow for evaluation of more fine-grained object classification, as shown in Figure 2.", "query_type": "comparison_explanation"}
{"query_id": "l1_1409.0575_1409.0575_fig_1_40", "query": "How does the Object Scale row spanning from Candle to Spider Web relate to the 1000 object classes annotation goal?", "answer": "The Object Scale row demonstrates the scale variability challenge that algorithms must handle across the 1000 object classes, showing that the dataset requires systems to recognize objects ranging from small items like candles to large structures like spider webs.", "doc_id": "1409.0575", "figure_id": "1409.0575_fig_1", "figure_number": 2, "image_path": "data/mineru_output/1409.0575/1409.0575/hybrid_auto/images/1409.0575_page0_fig0.jpg", "caption": "Figure 2. The synsets have remained consistent since year 2012. Appendix A provides the complete list of object categories used in ILSVRC2012-2014.", "figure_type": "diagram", "visual_anchor": "Top row labeled 'Object Scale' with Candle on left and Spider Web on right", "text_evidence": "The image classification task tests the ability of an algorithm to name the objects present in the image, without necessarily localizing them.", "query_type": "value_context"}
{"query_id": "l1_1409.0575_1409.0575_fig_1_41", "query": "Does the seven-dimensional organization visible in the rows support using multiple search engines and expanded multilingual queries during collection?", "answer": "Yes, the seven dimensions (Object Scale, Number of Instances, Image Clutter, Deformability, Amount of Texture, Color Distinctiveness, Shape Distinctiveness, Real-world Size) demonstrate the dataset's complexity, requiring diverse collection strategies including multiple search engines and multilingual queries to capture sufficient variation across all these dimensions.", "doc_id": "1409.0575", "figure_id": "1409.0575_fig_1", "figure_number": 2, "image_path": "data/mineru_output/1409.0575/1409.0575/hybrid_auto/images/1409.0575_page0_fig0.jpg", "caption": "Figure 2. The synsets have remained consistent since year 2012. Appendix A provides the complete list of object categories used in ILSVRC2012-2014.", "figure_type": "diagram", "visual_anchor": "Seven distinct horizontal rows with bidirectional arrows showing dimensional ranges", "text_evidence": "collecting a diverse set of candidate images by using multiple search engines and an expanded set of queries in multiple languages (Section 3.1.2), and finally filtering the millions of collected i", "query_type": "comparison_explanation"}
{"query_id": "l1_1409.0575_1409.0575_fig_55_42", "query": "Do the metallic measuring tools shown represent easiest or hardest classes in ILSVRC2012-2014 object detection?", "answer": "The figure shows object detection results for ILSVRC2012-2014, displaying examples that represent either the easiest or hardest classes based on best performance entries. The specific classification depends on the detection accuracy achieved for these kitchen utensil categories.", "doc_id": "1409.0575", "figure_id": "1409.0575_fig_55", "figure_number": 12, "image_path": "data/mineru_output/1409.0575/1409.0575/hybrid_auto/images/1409.0575_page0_fig54.jpg", "caption": "Figure 12.", "figure_type": "photo", "visual_anchor": "metallic measuring scoops on wooden cutting board", "text_evidence": "For each object category, we take the best performance of any entry submitted to ILSVRC2012-2014 (including entries using additional training data). Given these 'optimistic' results we show the easiest and harder classes for each task.", "query_type": "value_context"}
{"query_id": "l1_1409.0575_1409.0575_fig_55_43", "query": "Do the wooden cutting board and kitchen items represent classes with best or worst detection performance?", "answer": "The items shown represent examples from the optimistic results showing either easiest or hardest classes for the object detection task. The figure demonstrates the range of detection difficulty across different object categories in ILSVRC2012-2014.", "doc_id": "1409.0575", "figure_id": "1409.0575_fig_55", "figure_number": 12, "image_path": "data/mineru_output/1409.0575/1409.0575/hybrid_auto/images/1409.0575_page0_fig54.jpg", "caption": "Figure 12.", "figure_type": "photo", "visual_anchor": "wooden cutting board with various kitchen implements", "text_evidence": "Fig. 12 For each object category, we take the best performance of any entry submitted to ILSVRC2012-2014 (including entries using additional training data). Given these 'optimistic' results we show the easiest and harder classes for the object detection task, i.e., classes with best and worst result", "query_type": "comparison_explanation"}
{"query_id": "l1_1409.0575_1409.0575_fig_75_44", "query": "Are these bundled pink-tipped objects classified among the easiest or hardest categories based on optimistic ILSVRC2012-2014 performance?", "answer": "The classification difficulty cannot be determined from the figure alone. The referenced analysis shows both easiest and hardest classes based on best submitted entry performance across multiple years.", "doc_id": "1409.0575", "figure_id": "1409.0575_fig_75", "figure_number": 11, "image_path": "data/mineru_output/1409.0575/1409.0575/hybrid_auto/images/1409.0575_page0_fig74.jpg", "caption": "Figure 11.", "figure_type": "photo", "visual_anchor": "entire bundle of pink-tipped cylindrical objects", "text_evidence": "For each object category, we take the best performance of any entry submitted to ILSVRC2012-2014 (including entries using additional training data). Given these 'optimistic' results we show the easiest and harder classes for each task.", "query_type": "comparison_explanation"}
{"query_id": "l1_1409.0575_1409.0575_fig_75_45", "query": "How does the average image area occupied by these pink-tipped objects relate to their detection performance on ILSVRC2014 validation?", "answer": "The performance would correspond to a specific dot position in Figure 13, where x-axis represents the average fraction of image area occupied by instances of this object class, and y-axis shows the best detection performance achieved.", "doc_id": "1409.0575", "figure_id": "1409.0575_fig_75", "figure_number": 11, "image_path": "data/mineru_output/1409.0575/1409.0575/hybrid_auto/images/1409.0575_page0_fig74.jpg", "caption": "Figure 11.", "figure_type": "photo", "visual_anchor": "pink-tipped cylindrical objects occupying the frame", "text_evidence": "Average scale (x-axis) is computed as the average fraction of the image area occupied by an instance of that object class on the ILSVRC2014 validation set. 'Optimistic' performance (y-axis) corresponds to the best performance on the test set of any entry submitted to ILSVRC2012-2014", "query_type": "comparison_explanation"}
{"query_id": "l1_1409.0575_1409.0575_fig_93_46", "query": "Why do the blue bars show increasing accuracy from 0.90 to 0.97 despite object scale being normalized within each bin?", "answer": "The scale normalization controls for size effects within each bin, but the increasing trend reflects the underlying difficulty differences between artificial and natural categories. The normalization ensures that the observed accuracy differences are not simply due to object size variations.", "doc_id": "1409.0575", "figure_id": "1409.0575_fig_93", "figure_number": 1, "image_path": "data/mineru_output/1409.0575/1409.0575/hybrid_auto/images/1409.0575_page0_fig92.jpg", "caption": "Figure 1. The y-axis is the average accuracy of the “optimistic” model. Note that the range of the y-axis is different for each task to make the trends more visible. The black circle is the average accuracy of the model on all object classes that fall into each bin. We control for the effects of object scale by normalizing the object scale within each bin (details in Section 6.3.4). The color bars show the model accuracy averaged across the remaining classes. Error bars show the $9 5 \\%$ confidence interval obtained with bootstrapping. Some bins are missing color bars because less than 5 object classes remained in the bin after scale normalization. For example, the bar for XL real-world object detection classes is missing because that bin has only 3 object classes (airplane, bus, train) and after normalizing by scale no classes remain.", "figure_type": "plot", "visual_anchor": "Blue bars increasing from approximately 0.90 (leftmost) to 0.97 (rightmost)", "text_evidence": "We control for the effects of object scale by normalizing the object scale within each bin (details in Section 6.3.4).", "query_type": "comparison_explanation"}
{"query_id": "l1_1409.0575_1409.0575_fig_93_47", "query": "Do the black circles positioned above the blue bars confirm that model accuracy varies across object classes within bins?", "answer": "Yes, the black circles represent the average accuracy of the model on all object classes falling into each bin, showing variation across different bins. This demonstrates that accuracy differs depending on which classes are grouped together.", "doc_id": "1409.0575", "figure_id": "1409.0575_fig_93", "figure_number": 1, "image_path": "data/mineru_output/1409.0575/1409.0575/hybrid_auto/images/1409.0575_page0_fig92.jpg", "caption": "Figure 1. The y-axis is the average accuracy of the “optimistic” model. Note that the range of the y-axis is different for each task to make the trends more visible. The black circle is the average accuracy of the model on all object classes that fall into each bin. We control for the effects of object scale by normalizing the object scale within each bin (details in Section 6.3.4). The color bars show the model accuracy averaged across the remaining classes. Error bars show the $9 5 \\%$ confidence interval obtained with bootstrapping. Some bins are missing color bars because less than 5 object classes remained in the bin after scale normalization. For example, the bar for XL real-world object detection classes is missing because that bin has only 3 object classes (airplane, bus, train) and after normalizing by scale no classes remain.", "figure_type": "plot", "visual_anchor": "Black circles positioned at approximately 0.90, 0.93, 0.96, and 0.97 above respective blue bars", "text_evidence": "The black circle is the average accuracy of the model on all object classes that fall into each bin.", "query_type": "value_context"}
{"query_id": "l1_1409.0575_1409.0575_fig_93_48", "query": "Why are some bins missing color bars despite having blue bars and black circles at those positions?", "answer": "Some bins have fewer than 5 object classes remaining after scale normalization, which is insufficient for displaying color bars. For example, the XL real-world object detection bin had only 3 classes (airplane, bus, train) and no classes remained after normalization.", "doc_id": "1409.0575", "figure_id": "1409.0575_fig_93", "figure_number": 1, "image_path": "data/mineru_output/1409.0575/1409.0575/hybrid_auto/images/1409.0575_page0_fig92.jpg", "caption": "Figure 1. The y-axis is the average accuracy of the “optimistic” model. Note that the range of the y-axis is different for each task to make the trends more visible. The black circle is the average accuracy of the model on all object classes that fall into each bin. We control for the effects of object scale by normalizing the object scale within each bin (details in Section 6.3.4). The color bars show the model accuracy averaged across the remaining classes. Error bars show the $9 5 \\%$ confidence interval obtained with bootstrapping. Some bins are missing color bars because less than 5 object classes remained in the bin after scale normalization. For example, the bar for XL real-world object detection classes is missing because that bin has only 3 object classes (airplane, bus, train) and after normalizing by scale no classes remain.", "figure_type": "plot", "visual_anchor": "Blue bars present at all four x-axis positions while color bars are selectively missing", "text_evidence": "Some bins are missing color bars because less than 5 object classes remained in the bin after scale normalization. For example, the bar for XL real-world object detection classes is missing because that bin has only 3 object classes (airplane, bus, train) and after normalizing by scale no classes remain.", "query_type": "anomaly_cause"}
{"query_id": "l1_1412.3756_1412.3756_fig_1_49", "query": "Why does the black curve peak at 475 when male scores average 400 and female scores average 550?", "answer": "The black curve represents the fully repaired distribution with μ=475, σ=75, which is the median distribution used to eliminate disparate impact between the original male (red, μ=400) and female (blue, μ=550) distributions.", "doc_id": "1412.3756", "figure_id": "1412.3756_fig_1", "figure_number": 1, "image_path": "data/mineru_output/1412.3756/1412.3756/hybrid_auto/images/1412.3756_page0_fig0.jpg", "caption": "Fig. 1: Consider the fake probability density functions shown here where the blue curve shows the distribution of SAT scores (Y) for $X =$ female, with $\\mu = 5 5 0 , \\sigma = 1 0 0 ,$ , while the red curve shows the distribution of SAT scores for $X \\ = \\ \\mathtt { m a l e }$ , with $\\mu = 4 0 0 , \\sigma = 5 0 .$ The resulting fully repaired data is the distribution in black, with $\\mu = 4 7 5 , \\sigma = 7 5$ . Male students who originally had scores in the 95th percentile, i.e., had scores of 500, are given scores of 625 in the 95th percentile of the new distribution in $\\bar { Y } _ { \\cdot }$ , while women with scores of 625 in $\\bar { Y }$ originally had scores of 750.", "figure_type": "plot", "visual_anchor": "black curve peak at SAT score 475", "text_evidence": "The resulting fully repaired data is the distribution in black, with μ=475, σ=75", "query_type": "value_context"}
{"query_id": "l1_1412.3756_1412.3756_fig_1_50", "query": "How does the red curve's narrower spread with σ=50 compared to the blue curve's σ=100 affect repair?", "answer": "The narrower red distribution (male, σ=50) means male students at the 95th percentile (score 500) are mapped to score 625 in the repaired distribution, a larger upward shift than for the wider female distribution, demonstrating how different variances impact quantile-based repair.", "doc_id": "1412.3756", "figure_id": "1412.3756_fig_1", "figure_number": 1, "image_path": "data/mineru_output/1412.3756/1412.3756/hybrid_auto/images/1412.3756_page0_fig0.jpg", "caption": "Fig. 1: Consider the fake probability density functions shown here where the blue curve shows the distribution of SAT scores (Y) for $X =$ female, with $\\mu = 5 5 0 , \\sigma = 1 0 0 ,$ , while the red curve shows the distribution of SAT scores for $X \\ = \\ \\mathtt { m a l e }$ , with $\\mu = 4 0 0 , \\sigma = 5 0 .$ The resulting fully repaired data is the distribution in black, with $\\mu = 4 7 5 , \\sigma = 7 5$ . Male students who originally had scores in the 95th percentile, i.e., had scores of 500, are given scores of 625 in the 95th percentile of the new distribution in $\\bar { Y } _ { \\cdot }$ , while women with scores of 625 in $\\bar { Y }$ originally had scores of 750.", "figure_type": "plot", "visual_anchor": "red curve narrower width compared to blue curve width", "text_evidence": "Male students who originally had scores in the 95th percentile, i.e., had scores of 500, are given scores of 625 in the 95th percentile of the new distribution", "query_type": "comparison_explanation"}
{"query_id": "l1_1412.3756_1412.3756_fig_1_51", "query": "Does the blue curve's higher peak score of 550 explain why women lose 125 points at their 95th percentile?", "answer": "Yes, because the repair process moves inverse quantile distributions toward the median (μ=475), women with original scores of 750 (95th percentile of blue distribution with μ=550) are mapped down to 625 in the repaired distribution, resulting in a 125-point reduction.", "doc_id": "1412.3756", "figure_id": "1412.3756_fig_1", "figure_number": 1, "image_path": "data/mineru_output/1412.3756/1412.3756/hybrid_auto/images/1412.3756_page0_fig0.jpg", "caption": "Fig. 1: Consider the fake probability density functions shown here where the blue curve shows the distribution of SAT scores (Y) for $X =$ female, with $\\mu = 5 5 0 , \\sigma = 1 0 0 ,$ , while the red curve shows the distribution of SAT scores for $X \\ = \\ \\mathtt { m a l e }$ , with $\\mu = 4 0 0 , \\sigma = 5 0 .$ The resulting fully repaired data is the distribution in black, with $\\mu = 4 7 5 , \\sigma = 7 5$ . Male students who originally had scores in the 95th percentile, i.e., had scores of 500, are given scores of 625 in the 95th percentile of the new distribution in $\\bar { Y } _ { \\cdot }$ , while women with scores of 625 in $\\bar { Y }$ originally had scores of 750.", "figure_type": "plot", "visual_anchor": "blue curve peak at SAT score 550", "text_evidence": "while women with scores of 625 in Ȳ originally had scores of 750", "query_type": "comparison_explanation"}
{"query_id": "l1_1412.3756_1412.3756_fig_2_52", "query": "Why do no points appear above the vertical BER threshold yet below the 0.8 horizontal line?", "answer": "The certification algorithm guarantees that points to the right of the BER threshold are also above τ = 0.8, the threshold for legal disparate impact, ensuring no violations occur in that region.", "doc_id": "1412.3756", "figure_id": "1412.3756_fig_2", "figure_number": 2, "image_path": "data/mineru_output/1412.3756/1412.3756/hybrid_auto/images/1412.3756_page0_fig1.jpg", "caption": "Fig. 2: Lack of predictability (BER) of the protected attributes on the German Credit Adult Income, and Ricci data sets as compared to the disparate impact found in the test set when the class is predicted from the non-protected attributes. The certification algorithm guarantees that points to the right of the BER threshold are also above $\\tau = 0 . 8$ , the threshold for legal disparate impact. For clarity, we only show results using the combinatorial repair, but the geometric repair results follow the same pattern.", "figure_type": "plot", "visual_anchor": "Vertical gray line at BER threshold (~0.47) and horizontal gray line at Fairness (DI) = 0.8", "text_evidence": "The certification algorithm guarantees that points to the right of the BER threshold are also above $\\tau = 0 . 8$ , the threshold for legal disparate impact.", "query_type": "anomaly_cause"}
{"query_id": "l1_1412.3756_1412.3756_fig_2_53", "query": "How does the BER value relate to disparate impact when predicting class from non-protected attributes?", "answer": "The BER from predicting the protected attribute is compared to the disparate impact (DI) measured when classifiers predict the class given non-protected attributes, establishing the relationship between predictability and fairness.", "doc_id": "1412.3756", "figure_id": "1412.3756_fig_2", "figure_number": 2, "image_path": "data/mineru_output/1412.3756/1412.3756/hybrid_auto/images/1412.3756_page0_fig1.jpg", "caption": "Fig. 2: Lack of predictability (BER) of the protected attributes on the German Credit Adult Income, and Ricci data sets as compared to the disparate impact found in the test set when the class is predicted from the non-protected attributes. The certification algorithm guarantees that points to the right of the BER threshold are also above $\\tau = 0 . 8$ , the threshold for legal disparate impact. For clarity, we only show results using the combinatorial repair, but the geometric repair results follow the same pattern.", "figure_type": "plot", "visual_anchor": "Scatter points showing BER (x-axis) versus Fairness DI (y-axis) relationship across all three panels", "text_evidence": "The resulting BER is compared to $\\mathsf { D } \\mathsf { I } ( g )$ where $g : Y \\to C ,$ i.e., the disparate impact value as measured when some classifier attempts to predict the class given the non-protected attributes.", "query_type": "value_context"}
{"query_id": "l1_1412.3756_1412.3756_fig_2_54", "query": "Does the clustering of points near the upper-right region in all panels support the confusion matrix analysis?", "answer": "Yes, the clustering near high BER and high fairness values aligns with the confusion matrix analysis of classifier g, showing that lower predictability of protected attributes correlates with better fairness outcomes.", "doc_id": "1412.3756", "figure_id": "1412.3756_fig_2", "figure_number": 2, "image_path": "data/mineru_output/1412.3756/1412.3756/hybrid_auto/images/1412.3756_page0_fig1.jpg", "caption": "Fig. 2: Lack of predictability (BER) of the protected attributes on the German Credit Adult Income, and Ricci data sets as compared to the disparate impact found in the test set when the class is predicted from the non-protected attributes. The certification algorithm guarantees that points to the right of the BER threshold are also above $\\tau = 0 . 8$ , the threshold for legal disparate impact. For clarity, we only show results using the combinatorial repair, but the geometric repair results follow the same pattern.", "figure_type": "plot", "visual_anchor": "Dense clustering of GNB (red circles), SVM (green triangles), and LR (blue squares) near BER=0.50 and Fairness=1.0", "text_evidence": "Consider the confusion matrix associated with $g .$ , depicted in Table 2.", "query_type": "comparison_explanation"}
{"query_id": "l1_1412.3756_1412.3756_fig_3_55", "query": "Does the utility decay pattern visible across all scatter points support the expected tradeoff between fairness and utility?", "answer": "Yes, the scatter points show declining utility (from ~1.0 to ~0.5-0.9) as disparate impact increases toward the green line at DI=1.0, demonstrating the expected decay over utility as fairness increases.", "doc_id": "1412.3756", "figure_id": "1412.3756_fig_3", "figure_number": 3, "image_path": "data/mineru_output/1412.3756/1412.3756/hybrid_auto/images/1412.3756_page0_fig2.jpg", "caption": "Fig. 3: Disparate impact (DI) vs. utility (1-BER) from our combinatorial and geometric partial repair processes using the SVM to classify on the Adult Income and German Credit data sets and the simple threshold classifier on the Ricci data set. Recall that only points with $\\mathsf { D } \\mathsf { I } \\ge \\tau = 0 . 8$ are legal. $\\mathsf { D } \\mathsf { I } = 1 . 0$ represents full fairness.", "figure_type": "plot", "visual_anchor": "Dashed trend lines connecting scatter points showing downward slope from left to right", "text_evidence": "The results, shown in Figure 3, demonstrate the expected decay over utility as fairness increases.", "query_type": "comparison_explanation"}
{"query_id": "l1_1412.3756_1412.3756_fig_3_56", "query": "Why do both repair methods show points in the pink region left of the red line at 0.8?", "answer": "The pink region represents illegal disparate impact values (DI < 0.8). Points in this region indicate partial repair states that have not yet achieved the legal threshold of DI ≥ 0.8.", "doc_id": "1412.3756", "figure_id": "1412.3756_fig_3", "figure_number": 3, "image_path": "data/mineru_output/1412.3756/1412.3756/hybrid_auto/images/1412.3756_page0_fig2.jpg", "caption": "Fig. 3: Disparate impact (DI) vs. utility (1-BER) from our combinatorial and geometric partial repair processes using the SVM to classify on the Adult Income and German Credit data sets and the simple threshold classifier on the Ricci data set. Recall that only points with $\\mathsf { D } \\mathsf { I } \\ge \\tau = 0 . 8$ are legal. $\\mathsf { D } \\mathsf { I } = 1 . 0$ represents full fairness.", "figure_type": "plot", "visual_anchor": "Pink shaded region left of red vertical line at DI=0.8", "text_evidence": "Recall that only points with $\\mathsf { D } \\mathsf { I } \\ge \\tau = 0 . 8$ are legal.", "query_type": "visual_definition"}
{"query_id": "l1_1412.3756_1412.3756_fig_3_57", "query": "Do the German Credit middle row results validate the SVM classification approach for partial repair processes?", "answer": "Yes, the German Credit results show clear utility-fairness tradeoffs with points transitioning from the pink illegal region through the red threshold line to the green legal region, validating the SVM's effectiveness in controlling the repair process.", "doc_id": "1412.3756", "figure_id": "1412.3756_fig_3", "figure_number": 3, "image_path": "data/mineru_output/1412.3756/1412.3756/hybrid_auto/images/1412.3756_page0_fig2.jpg", "caption": "Fig. 3: Disparate impact (DI) vs. utility (1-BER) from our combinatorial and geometric partial repair processes using the SVM to classify on the Adult Income and German Credit data sets and the simple threshold classifier on the Ricci data set. Recall that only points with $\\mathsf { D } \\mathsf { I } \\ge \\tau = 0 . 8$ are legal. $\\mathsf { D } \\mathsf { I } = 1 . 0$ represents full fairness.", "figure_type": "plot", "visual_anchor": "Middle row scatter points and trend lines for German Credit data", "text_evidence": "Disparate impact (DI) vs. utility (1-BER) from our combinatorial and geometric partial repair processes using the SVM to classify on the Adult Income and German Credit data sets", "query_type": "value_context"}
{"query_id": "l1_1412.3756_1412.3756_fig_4_58", "query": "Why does the blue Race and Gender curve reach utility 0.714 at disparate impact 1.05, near the maximum individual losses?", "answer": "The joint distribution utility loss is close to the maximum of the utility loss over each protected attribute considered individually, as shown by the blue curve ending near the lowest points of the red and green curves.", "doc_id": "1412.3756", "figure_id": "1412.3756_fig_4", "figure_number": 4, "image_path": "data/mineru_output/1412.3756/1412.3756/hybrid_auto/images/1412.3756_page0_fig3.jpg", "caption": "Fig. 4: Disparate impact (DI) vs. utility (1-BER) from our combinatorial and geometric partial repair processes using the SVM as the classifier. For clarity in the figure, only the combinatorial repairs are shown, though the geometric repairs follow the same pattern.", "figure_type": "plot", "visual_anchor": "Blue square line ending at approximately (1.05, 0.714), positioned near the convergence of red and green lines", "text_evidence": "The results, shown in Figure 4, show that the utility loss over the joint distribution is close to the maximum of the utility loss over each protected attribute considered on its own.", "query_type": "comparison_explanation"}
{"query_id": "l1_1412.3756_1412.3756_fig_4_59", "query": "Does the Race green triangle curve maintaining utility above 0.732 across all disparate impacts reflect differences in repair performance?", "answer": "Yes, the Race attribute maintains consistently higher utility than Gender or Race and Gender combined, demonstrating substantial differences in repair algorithm performance depending on the specific protected attributes chosen.", "doc_id": "1412.3756", "figure_id": "1412.3756_fig_4", "figure_number": 4, "image_path": "data/mineru_output/1412.3756/1412.3756/hybrid_auto/images/1412.3756_page0_fig3.jpg", "caption": "Fig. 4: Disparate impact (DI) vs. utility (1-BER) from our combinatorial and geometric partial repair processes using the SVM as the classifier. For clarity in the figure, only the combinatorial repairs are shown, though the geometric repairs follow the same pattern.", "figure_type": "plot", "visual_anchor": "Green triangle line staying between 0.732 and 0.738 utility across the entire disparate impact range", "text_evidence": "Our experiments show a substantial difference in the performance of our repair algorithm depending on the specific algorithms we chose. Given the myriad classification algorithms used in practice, there is a clear need for a future systematic study of the relationship between dataset features, algorithms, and repair performance.", "query_type": "value_context"}
{"query_id": "l1_1412.3756_1412.3756_fig_4_60", "query": "Do the red Gender and blue Race and Gender curves converging at utility 0.720 support using SVM as classifier?", "answer": "The convergence demonstrates SVM classifier behavior under repair processes. The comparison uses SVM as a baseline against other methods like logistic regression, fair naive Bayes, and learned fair representations.", "doc_id": "1412.3756", "figure_id": "1412.3756_fig_4", "figure_number": 4, "image_path": "data/mineru_output/1412.3756/1412.3756/hybrid_auto/images/1412.3756_page0_fig3.jpg", "caption": "Fig. 4: Disparate impact (DI) vs. utility (1-BER) from our combinatorial and geometric partial repair processes using the SVM as the classifier. For clarity in the figure, only the combinatorial repairs are shown, though the geometric repairs follow the same pattern.", "figure_type": "plot", "visual_anchor": "Red circle line and blue square line converging around disparate impact 1.0 at utility approximately 0.720", "text_evidence": "Logistic regression is used as a baseline comparison, fair naive Bayes is the solution from Kamiran and Calders [8], regularized logistic regression is the repair method from Kamishima et al. [10], and learned fair representations is Zemel et al.'s solution [26].", "query_type": "comparison_explanation"}
{"query_id": "l1_1412.3756_1412.3756_fig_5_61", "query": "Do the blue GNB squares clustering near 0.75-0.8 accuracy in Geometric and Combinatorial columns demonstrate flexibility with respect to classifier choice?", "answer": "Yes, the consistent high accuracy of GNB (blue squares) across both repair methods, along with the varied performance of SVM (red circles) and LR (green triangles), demonstrates that the method can be flexible with respect to the chosen classifier.", "doc_id": "1412.3756", "figure_id": "1412.3756_fig_5", "figure_number": 5, "image_path": "data/mineru_output/1412.3756/1412.3756/hybrid_auto/images/1412.3756_page0_fig4.jpg", "caption": "Fig. 5: Zemel fairness vs. accuracy from our combinatorial and geometric partial repairs 0.65 0.80 0.85 0.90 0.95 1.00 LFR0.30 0.85 0.90 0.95 1.00as compared to previous work. Legend: RLR, Regularized Logistic Regression [10]; 0.80 0.85 0.90 0.95 1.00 0.8 0.9 1.0 1.1LFR, Learned Fair Representations [26]; FNB, Fair Na¨ıve Bayes [8]; GNB, Gaussian 0.65Na¨ıve Bayes with balanced prior; LR, Logistic Regression; SVM, Support Vector Machine.", "figure_type": "plot", "visual_anchor": "Blue GNB squares at approximately 0.75-0.8 accuracy in both Geometric and Combinatorial columns", "text_evidence": "Figure 5 shows that our method can be flexible with respect to the chosen classifier.", "query_type": "comparison_explanation"}
{"query_id": "l1_1412.3756_1412.3756_fig_5_62", "query": "Why do the red SVM circles span from 0.49 to 0.73 accuracy in Geometric repairs, illustrating substantial performance differences?", "answer": "The wide accuracy range of SVM circles (0.49-0.73) reflects the substantial difference in repair algorithm performance depending on specific algorithms chosen, demonstrating the need for systematic study of the relationship between dataset features, algorithms, and repair performance.", "doc_id": "1412.3756", "figure_id": "1412.3756_fig_5", "figure_number": 5, "image_path": "data/mineru_output/1412.3756/1412.3756/hybrid_auto/images/1412.3756_page0_fig4.jpg", "caption": "Fig. 5: Zemel fairness vs. accuracy from our combinatorial and geometric partial repairs 0.65 0.80 0.85 0.90 0.95 1.00 LFR0.30 0.85 0.90 0.95 1.00as compared to previous work. Legend: RLR, Regularized Logistic Regression [10]; 0.80 0.85 0.90 0.95 1.00 0.8 0.9 1.0 1.1LFR, Learned Fair Representations [26]; FNB, Fair Na¨ıve Bayes [8]; GNB, Gaussian 0.65Na¨ıve Bayes with balanced prior; LR, Logistic Regression; SVM, Support Vector Machine.", "figure_type": "plot", "visual_anchor": "Red SVM circles ranging from approximately 0.49 to 0.73 accuracy in the Geometric column", "text_evidence": "Our experiments show a substantial difference in the performance of our repair algorithm depending on the specific algorithms we chose. Given the myriad classification algorithms used in practice, there is a clear need for a future systematic study of the relationship between dataset features, algorithms, and repair performance.", "query_type": "anomaly_cause"}
{"query_id": "l1_1412.3756_1412.3756_fig_5_63", "query": "How do the fairness-accuracy tradeoffs visible across lambda increments in both Geometric and Combinatorial columns align with the partial repair methodology?", "answer": "The scattered points across different fairness values (0.8-1.1) represent different lambda values from 0 to 1 at 0.1 increments, showing how partial repairs balance fairness and accuracy tradeoffs for all three classifiers (SVM, LR, GNB).", "doc_id": "1412.3756", "figure_id": "1412.3756_fig_5", "figure_number": 5, "image_path": "data/mineru_output/1412.3756/1412.3756/hybrid_auto/images/1412.3756_page0_fig4.jpg", "caption": "Fig. 5: Zemel fairness vs. accuracy from our combinatorial and geometric partial repairs 0.65 0.80 0.85 0.90 0.95 1.00 LFR0.30 0.85 0.90 0.95 1.00as compared to previous work. Legend: RLR, Regularized Logistic Regression [10]; 0.80 0.85 0.90 0.95 1.00 0.8 0.9 1.0 1.1LFR, Learned Fair Representations [26]; FNB, Fair Na¨ıve Bayes [8]; GNB, Gaussian 0.65Na¨ıve Bayes with balanced prior; LR, Logistic Regression; SVM, Support Vector Machine.", "figure_type": "plot", "visual_anchor": "Multiple points for each method (SVM, LR, GNB) distributed across fairness values 0.8-1.1 in Geometric and Combinatorial columns", "text_evidence": "Figure 5 shows our fairness and accuracy results for both combinatorial and geometric partial repairs for values of $\\lambda \\in [ 0 , 1 ]$ at increments of 0.1 using all three classifiers described above.", "query_type": "value_context"}
{"query_id": "l1_1511.00830_1511.00830_fig_1_64", "query": "Why does node S point directly to X rather than through Z in this unsupervised representation learning model?", "answer": "In unsupervised representation learning, S represents noise or nuisance variables that directly affect the observed data X, independent of the informative latent factors Z. This direct connection models uninformative dimensions that are typically detrimental for the task under consideration.", "doc_id": "1511.00830", "figure_id": "1511.00830_fig_1", "figure_number": 1, "image_path": "data/mineru_output/1511.00830/1511.00830/hybrid_auto/images/1511.00830_page0_fig0.jpg", "caption": "Figure 1: Unsupervised model", "figure_type": "diagram", "visual_anchor": "arrow from node S (top-left) directly to node X (bottom-center)", "text_evidence": "Uninformative dimensions are often called \"noise\" or \"nuisance variables\" while informative dimensions are usually called latent or hidden factors of variation.", "query_type": "comparison_explanation"}
{"query_id": "l1_1511.00830_1511.00830_fig_1_65", "query": "Does the arrow from Z to X indicate that hidden factors are informative for representation learning tasks?", "answer": "Yes, the arrow from Z to X shows that Z represents latent or hidden factors of variation that are informative for the task. The model aims to find representations that preserve these informative dimensions while removing uninformative ones.", "doc_id": "1511.00830", "figure_id": "1511.00830_fig_1", "figure_number": 1, "image_path": "data/mineru_output/1511.00830/1511.00830/hybrid_auto/images/1511.00830_page0_fig0.jpg", "caption": "Figure 1: Unsupervised model", "figure_type": "diagram", "visual_anchor": "arrow from node Z (top-right) to node X (bottom-center)", "text_evidence": "In \"Representation Learning\" one tries to find representations of the data that are informative for a particular task while removing the factors of variation that are uninformative", "query_type": "value_context"}
{"query_id": "l1_1511.00830_1511.00830_fig_1_66", "query": "What does the convergence of both S and Z arrows at node X represent for machine learning algorithms?", "answer": "The convergence at X represents the observed data that results from both informative latent factors (Z) and uninformative nuisance variables (S). Many machine learning algorithms work to separate these influences, keeping informative dimensions while removing detrimental noise.", "doc_id": "1511.00830", "figure_id": "1511.00830_fig_1", "figure_number": 1, "image_path": "data/mineru_output/1511.00830/1511.00830/hybrid_auto/images/1511.00830_page0_fig0.jpg", "caption": "Figure 1: Unsupervised model", "figure_type": "diagram", "visual_anchor": "node X (bottom-center) receiving arrows from both S and Z", "text_evidence": "Many machine learning algorithms can be understood in this way: pri", "query_type": "visual_definition"}
{"query_id": "l1_1511.00830_1511.00830_fig_2_67", "query": "Why does the shaded node x receive arrows from both z1 and s in this generative process?", "answer": "The generative process defines p_θ(x|z,s), meaning x is generated conditioned on both the latent variable z (modeled through z1) and the observed variable s representing variations to remove.", "doc_id": "1511.00830", "figure_id": "1511.00830_fig_2", "figure_number": 2, "image_path": "data/mineru_output/1511.00830/1511.00830/hybrid_auto/images/1511.00830_page0_fig1.jpg", "caption": "Figure 2: Semi-supervised model", "figure_type": "diagram", "visual_anchor": "shaded node x at bottom with incoming arrows from z1 and s", "text_evidence": "This generative process can be formally defined as: where $p _ { \\theta } ( \\mathbf { x } | \\mathbf { z } , \\mathbf { s } )$ is an appropriate probabi", "query_type": "visual_definition"}
{"query_id": "l1_1511.00830_1511.00830_fig_2_68", "query": "How does the independent node z2 at the top right relate to modeling remaining information after removing undesired variations?", "answer": "The continuous latent variable z (represented by z2 and z1) models all the remaining information after the observed variable s accounts for variations that need to be removed, as they are defined as independent sources.", "doc_id": "1511.00830", "figure_id": "1511.00830_fig_2", "figure_number": 2, "image_path": "data/mineru_output/1511.00830/1511.00830/hybrid_auto/images/1511.00830_page0_fig1.jpg", "caption": "Figure 2: Semi-supervised model", "figure_type": "diagram", "visual_anchor": "node z2 at top right, separate from s", "text_evidence": "admits two distinct (independent) \"sources\"; an observed variable s, which denotes the variations that we want to remove, and a continuous latent variable z which models all the remaining information", "query_type": "comparison_explanation"}
{"query_id": "l1_1511.00830_1511.00830_fig_2_69", "query": "Why does node y lack any outgoing arrows to x despite being part of the semi-supervised model?", "answer": "In the semi-supervised formulation, y influences the latent structure through z1 but does not directly generate x. The model factors out undesired variations s while y provides supervisory information at the latent level.", "doc_id": "1511.00830", "figure_id": "1511.00830_fig_2", "figure_number": 2, "image_path": "data/mineru_output/1511.00830/1511.00830/hybrid_auto/images/1511.00830_page0_fig1.jpg", "caption": "Figure 2: Semi-supervised model", "figure_type": "diagram", "visual_anchor": "node y at top left with arrow to z1 but no direct path to x", "text_evidence": "Factoring out undesired variations from the data can be easily formulated as a general probabilistic model", "query_type": "anomaly_cause"}
{"query_id": "l1_1511.00830_1511.00830_fig_9_70", "query": "Why does the red LR bar reach approximately 0.80 at X, given the objective requires low accuracy on S?", "answer": "The red LR bar at X shows high accuracy (~0.80) on sensitive attribute S, which violates the fairness objective. This indicates that X does not provide a fair encoding, as a linear classifier can easily predict S.", "doc_id": "1511.00830", "figure_id": "1511.00830_fig_9", "figure_number": 3, "image_path": "data/mineru_output/1511.00830/1511.00830/hybrid_auto/images/1511.00830_page0_fig8.jpg", "caption": "Figure 3: Fair classification results. Columns correspond to each evaluation scenario (in order): Random/RF/LR accuracy on s, Discrimination/Discrimination prob. against s and Random/Model accuracy on y. Note that the objective of a “fair” encoding is to have low accuracy on S (where LR is a linear classifier and RF is nonlinear), low discrimination against S and high accuracy on Y.", "figure_type": "plot", "visual_anchor": "Red bar at position X reaching approximately 0.80", "text_evidence": "Note that the objective of a \"fair\" encoding is to have low accuracy on S (where LR is a linear classifier and RF is nonlinear), low discrimination against S and high accuracy on Y.", "query_type": "anomaly_cause"}
{"query_id": "l1_1511.00830_1511.00830_fig_9_71", "query": "Does the blue RF bar's height of approximately 0.69 at LFR demonstrate lower nonlinear classification accuracy than linear?", "answer": "Yes, the blue RF bar at LFR (~0.69) is higher than the red LR bar (~0.57), indicating that the nonlinear Random Forest classifier achieves higher accuracy on S than the linear Logistic Regression classifier at this evaluation scenario.", "doc_id": "1511.00830", "figure_id": "1511.00830_fig_9", "figure_number": 3, "image_path": "data/mineru_output/1511.00830/1511.00830/hybrid_auto/images/1511.00830_page0_fig8.jpg", "caption": "Figure 3: Fair classification results. Columns correspond to each evaluation scenario (in order): Random/RF/LR accuracy on s, Discrimination/Discrimination prob. against s and Random/Model accuracy on y. Note that the objective of a “fair” encoding is to have low accuracy on S (where LR is a linear classifier and RF is nonlinear), low discrimination against S and high accuracy on Y.", "figure_type": "plot", "visual_anchor": "Blue RF bar at LFR position at approximately 0.69", "text_evidence": "Columns correspond to each evaluation scenario (in order): Random/RF/LR accuracy on s, Discrimination/Discrimination prob. against s and Random/Model accuracy on y.", "query_type": "comparison_explanation"}
{"query_id": "l1_1511.00830_1511.00830_fig_9_72", "query": "How do the VAE bars at approximately 0.66 compare to results shown for the other two datasets?", "answer": "The VAE bars (both blue RF and red LR at ~0.66) are higher than most other scenarios except X, indicating moderate accuracy on S. Comparing across all three datasets shown, VAE performs better than LFR but worse than X in terms of S prediction.", "doc_id": "1511.00830", "figure_id": "1511.00830_fig_9", "figure_number": 3, "image_path": "data/mineru_output/1511.00830/1511.00830/hybrid_auto/images/1511.00830_page0_fig8.jpg", "caption": "Figure 3: Fair classification results. Columns correspond to each evaluation scenario (in order): Random/RF/LR accuracy on s, Discrimination/Discrimination prob. against s and Random/Model accuracy on y. Note that the objective of a “fair” encoding is to have low accuracy on S (where LR is a linear classifier and RF is nonlinear), low discrimination against S and high accuracy on Y.", "figure_type": "plot", "visual_anchor": "Both blue and red bars at VAE position reaching approximately 0.66", "text_evidence": "The results for all three datasets can be seen in Figure 3.", "query_type": "comparison_explanation"}
{"query_id": "l1_1511.00830_1511.00830_fig_13_73", "query": "Why do the blue and red points heavily overlap in this visualization, given the model uses both s and MMD?", "answer": "The model successfully factors out domain (gender) information by incorporating both the sensitive attribute s and MMD regularization, resulting in representations where male and female samples are not separable and cluster together, demonstrating fairness through indistinguishability.", "doc_id": "1511.00830", "figure_id": "1511.00830_fig_13", "figure_number": 4, "image_path": "data/mineru_output/1511.00830/1511.00830/hybrid_auto/images/1511.00830_page0_fig12.jpg", "caption": "Figure 4: t-SNE (van der Maaten, 2013) visualizations from the Adult dataset on: (a): original x , (b): latent $\\mathbf { z } _ { 1 }$ without s and MMD, (c): latent $\\mathbf { z } _ { 1 }$ with s and without MMD, (d): latent $\\mathbf { z } _ { 1 }$ with s and MMD. Blue colour corresponds to males whereas red colour corresponds to females.", "figure_type": "plot", "visual_anchor": "overlapping blue (male) and red (female) scattered point clusters", "text_evidence": "Our model was successful in factoring out the domain information, since the accuracy, measured both linearly (LR) and non-linearly (RF), was towards random chance", "query_type": "comparison_explanation"}
{"query_id": "l1_1511.00830_1511.00830_fig_13_74", "query": "Does the mixed distribution of blue and red points confirm successful removal of sensitive attribute information?", "answer": "Yes, the visualization shows that male (blue) and female (red) samples are thoroughly intermixed without clear separation, which aligns with the model's success in making the learned representations indistinguishable with respect to gender.", "doc_id": "1511.00830", "figure_id": "1511.00830_fig_13", "figure_number": 4, "image_path": "data/mineru_output/1511.00830/1511.00830/hybrid_auto/images/1511.00830_page0_fig12.jpg", "caption": "Figure 4: t-SNE (van der Maaten, 2013) visualizations from the Adult dataset on: (a): original x , (b): latent $\\mathbf { z } _ { 1 }$ without s and MMD, (c): latent $\\mathbf { z } _ { 1 }$ with s and without MMD, (d): latent $\\mathbf { z } _ { 1 }$ with s and MMD. Blue colour corresponds to males whereas red colour corresponds to females.", "figure_type": "plot", "visual_anchor": "intermixed blue and red points without clear cluster separation", "text_evidence": "latent $\\mathbf { z } _ { 1 }$ with s and MMD. Blue colour corresponds to males whereas red colour corresponds to females", "query_type": "value_context"}
{"query_id": "l1_1511.00830_1511.00830_fig_13_75", "query": "What relationship between blue and red point distributions validates the two-dimensional Barnes-Hut SNE embedding approach?", "answer": "The Barnes-Hut SNE embedding reveals that the latent representations z₁ achieve gender-invariance by producing overlapping distributions for males and females, allowing visual assessment of whether the model successfully disentangles sensitive attributes from the learned features.", "doc_id": "1511.00830", "figure_id": "1511.00830_fig_13", "figure_number": 4, "image_path": "data/mineru_output/1511.00830/1511.00830/hybrid_auto/images/1511.00830_page0_fig12.jpg", "caption": "Figure 4: t-SNE (van der Maaten, 2013) visualizations from the Adult dataset on: (a): original x , (b): latent $\\mathbf { z } _ { 1 }$ without s and MMD, (c): latent $\\mathbf { z } _ { 1 }$ with s and without MMD, (d): latent $\\mathbf { z } _ { 1 }$ with s and MMD. Blue colour corresponds to males whereas red colour corresponds to females.", "figure_type": "plot", "visual_anchor": "two-dimensional scattered distribution showing blue and red point overlap", "text_evidence": "In order to further assess the nature of our new representations, we visualized two dimensional Barnes-Hut SNE (van der Maaten, 2013) embeddings of the $\\mathbf { z } _ { 1 }$ representations", "query_type": "visual_definition"}
{"query_id": "l1_1511.00830_1511.00830_fig_16_76", "query": "Why are images clustered by lighting conditions in visualization (a) despite the goal of learning invariant representations?", "answer": "Visualization (a) shows the original representation x where lighting conditions s are identifiable with almost perfect accuracy, demonstrating that invariance has not yet been achieved in the original space before the VFAE transformation.", "doc_id": "1511.00830", "figure_id": "1511.00830_fig_16", "figure_number": 5, "image_path": "data/mineru_output/1511.00830/1511.00830/hybrid_auto/images/1511.00830_page0_fig15.jpg", "caption": "Figure 5: t-SNE (van der Maaten, 2013) visualizations of the Extended Yale B training set. (a): original x , (b): latent $\\mathbf { z } _ { 1 }$ from VFAE. Each example is plotted with the person ID and the image. Zoom in to see details.", "figure_type": "plot", "visual_anchor": "Visualization (a) showing scattered clusters with person IDs 10, 17, 18, 19, 22, 26", "text_evidence": "This can also be seen in the two dimensional embeddings of the original space x in Figure 5a: the images are mostly clustered according to the lighting conditions.", "query_type": "visual_definition"}
{"query_id": "l1_1511.00830_1511.00830_fig_16_77", "query": "Does the spatial arrangement difference between visualizations (a) and (b) demonstrate latent variable z1's invariance to lighting conditions?", "answer": "Yes, the different clustering patterns between (a) and (b) reflect that VFAE's latent z1 learns representations that are explicitly invariant with respect to lighting conditions while the original x clusters by lighting.", "doc_id": "1511.00830", "figure_id": "1511.00830_fig_16", "figure_number": 5, "image_path": "data/mineru_output/1511.00830/1511.00830/hybrid_auto/images/1511.00830_page0_fig15.jpg", "caption": "Figure 5: t-SNE (van der Maaten, 2013) visualizations of the Extended Yale B training set. (a): original x , (b): latent $\\mathbf { z } _ { 1 }$ from VFAE. Each example is plotted with the person ID and the image. Zoom in to see details.", "figure_type": "plot", "visual_anchor": "Contrasting cluster patterns between visualization (a) original x and visualization (b) latent z1", "text_evidence": "We introduce the Variational Fair Autoencoder (VFAE), an extension of the semi-supervised variational autoencoder in order to learn representations that are explicitly invariant with respect to some known aspect of a dataset", "query_type": "comparison_explanation"}
{"query_id": "l1_1511.00830_1511.00830_fig_16_78", "query": "How does the person ID labeling in both visualizations support the claim about retaining remaining information beyond lighting?", "answer": "The person ID labels (10, 17, 18, 19, etc.) visible in both visualizations demonstrate that VFAE retains identity information in z1 while removing lighting variance, fulfilling the goal of preserving remaining information.", "doc_id": "1511.00830", "figure_id": "1511.00830_fig_16", "figure_number": 5, "image_path": "data/mineru_output/1511.00830/1511.00830/hybrid_auto/images/1511.00830_page0_fig15.jpg", "caption": "Figure 5: t-SNE (van der Maaten, 2013) visualizations of the Extended Yale B training set. (a): original x , (b): latent $\\mathbf { z } _ { 1 }$ from VFAE. Each example is plotted with the person ID and the image. Zoom in to see details.", "figure_type": "plot", "visual_anchor": "Person ID numbers 10, 17, 18, 19, 22, 26 labeling points in both (a) and (b)", "text_evidence": "learn representations that are explicitly invariant with respect to some known aspect of a dataset while retaining as much remaining information as possible", "query_type": "value_context"}
{"query_id": "l1_1511.00830_1511.00830_fig_18_79", "query": "Why do the clustered points near 0.5 on both axes indicate domain invariance in VFAE's z1 representation?", "answer": "The points clustered near 0.5 on both axes show that VFAE's z1 representations have low PAD scores, indicating the domains are less distinguishable compared to raw input, demonstrating successful domain adaptation.", "doc_id": "1511.00830", "figure_id": "1511.00830_fig_18", "figure_number": 6, "image_path": "data/mineru_output/1511.00830/1511.00830/hybrid_auto/images/1511.00830_page0_fig17.jpg", "caption": "Figure 6: Proxy A-distances (PAD) for the Amazon reviews dataset: left from our VFAE model, right from the DANN model (taken from Ganin et al. (2015))", "figure_type": "plot", "visual_anchor": "Clustered data points near coordinate (0.5, 0.5) in the left plot", "text_evidence": "we also calculated the Proxy A-distance (PAD) (Ben-David et al., 2007; 2010) scores for the raw data x and for the $\\mathbf { z } _ { 1 }$ representations of VFAE", "query_type": "value_context"}
{"query_id": "l1_1511.00830_1511.00830_fig_18_80", "query": "How does VFAE's clustering near 0.5 compare to DANN's approach using a linear Support Vector Machine classifier?", "answer": "VFAE achieves consistently low PAD scores near 0.5 for z1 representations across all raw input PAD values, while DANN's reference plot shows their domain adaptation performance using a linear SVM classifier.", "doc_id": "1511.00830", "figure_id": "1511.00830_fig_18", "figure_number": 6, "image_path": "data/mineru_output/1511.00830/1511.00830/hybrid_auto/images/1511.00830_page0_fig17.jpg", "caption": "Figure 6: Proxy A-distances (PAD) for the Amazon reviews dataset: left from our VFAE model, right from the DANN model (taken from Ganin et al. (2015))", "figure_type": "plot", "visual_anchor": "Left plot points clustered at 0.5 vertical position versus right DANN reference plot", "text_evidence": "we have also added the plot from DANN (Ganin et al., 2015), where they used a linear Support Vector Machine for the classifier, as a reference", "query_type": "comparison_explanation"}
{"query_id": "l1_1511.00830_1511.00830_fig_18_81", "query": "What does the dashed diagonal line from origin to (2,2) represent for measuring domain distinguishability approximation?", "answer": "The dashed diagonal line represents perfect correlation where PAD on z1 equals PAD on raw input. Points below this line indicate improved domain invariance, approximating the H-divergence measure of domain distinguishability.", "doc_id": "1511.00830", "figure_id": "1511.00830_fig_18", "figure_number": 6, "image_path": "data/mineru_output/1511.00830/1511.00830/hybrid_auto/images/1511.00830_page0_fig17.jpg", "caption": "Figure 6: Proxy A-distances (PAD) for the Amazon reviews dataset: left from our VFAE model, right from the DANN model (taken from Ganin et al. (2015))", "figure_type": "plot", "visual_anchor": "Dashed diagonal line extending from (0,0) to (2,2)", "text_evidence": "Proxy A-distance is an approximation to the $\\mathcal { H }$ -divergence measure of domain distinguishability proposed in Kifer et al. (2004) and Ben-David et al. (2007; 2010)", "query_type": "visual_definition"}
{"query_id": "l1_1602.05352_1602.05352_fig_1_82", "query": "Why does propensity matrix P show value 1 for Horror-lovers on Romance despite true rating Y being 1?", "answer": "The propensity matrix P captures selection bias, showing that Horror-lovers have low propensity to watch Romance even though they would rate it poorly. This illustrates the MNAR (missing not at random) nature of recommendation data where observation patterns don't reflect true preferences.", "doc_id": "1602.05352", "figure_id": "1602.05352_fig_1", "figure_number": 1, "image_path": "data/mineru_output/1602.05352/1602.05352/hybrid_auto/images/1602.05352_page0_fig0.jpg", "caption": "Figure 1. Movie-Lovers toy example. Top row: true rating matrix $Y$ , propensity matrix $P$ , observation indicator matrix $O$ . Bottom row: two rating prediction matrices $\\hat { Y _ { 1 } }$ and $\\hat { Y } _ { 2 }$ , and intervention indicator matrix $\\hat { Y } _ { 3 }$ .", "figure_type": "table", "visual_anchor": "Propensity matrix P top row showing Horror-lovers with value 1 for Romance column", "text_evidence": "Past work that explicitly dealt with the MNAR nature of recommendation data approached the problem as missing-data imputation based on the joint likelihood of the missing data model and the rating model", "query_type": "anomaly_cause"}
{"query_id": "l1_1602.05352_1602.05352_fig_1_83", "query": "How does observation indicator O value 3 for Horror-lovers on Drama relate to evaluating prediction accuracy?", "answer": "The observation indicator O shows which ratings are observed in the test set. The value 3 for Horror-lovers on Drama means this rating is available for evaluation, enabling calculation of error metrics like MAE or MSE on observed entries.", "doc_id": "1602.05352", "figure_id": "1602.05352_fig_1", "figure_number": 1, "image_path": "data/mineru_output/1602.05352/1602.05352/hybrid_auto/images/1602.05352_page0_fig0.jpg", "caption": "Figure 1. Movie-Lovers toy example. Top row: true rating matrix $Y$ , propensity matrix $P$ , observation indicator matrix $O$ . Bottom row: two rating prediction matrices $\\hat { Y _ { 1 } }$ and $\\hat { Y } _ { 2 }$ , and intervention indicator matrix $\\hat { Y } _ { 3 }$ .", "figure_type": "table", "visual_anchor": "Observation indicator matrix O top row showing value 3 in Drama column", "text_evidence": "Standard evaluation measures like Mean Absolute Error (MAE) or Mean Squared Error (MSE) can be written as: for an appropriately chosen δu,i(Y,Ŷ)", "query_type": "value_context"}
{"query_id": "l1_1602.05352_1602.05352_fig_1_84", "query": "Why do prediction matrices Ŷ₁ and Ŷ₂ differ for Romance-lovers on Drama despite identical observation patterns?", "answer": "Ŷ₁ predicts 5 while Ŷ₂ predicts 5 for Romance-lovers on Drama, demonstrating how different prediction models can produce varying estimates. This toy example illustrates the disastrous effect of selection bias on conventional evaluation using held-out ratings.", "doc_id": "1602.05352", "figure_id": "1602.05352_fig_1", "figure_number": 1, "image_path": "data/mineru_output/1602.05352/1602.05352/hybrid_auto/images/1602.05352_page0_fig0.jpg", "caption": "Figure 1. Movie-Lovers toy example. Top row: true rating matrix $Y$ , propensity matrix $P$ , observation indicator matrix $O$ . Bottom row: two rating prediction matrices $\\hat { Y _ { 1 } }$ and $\\hat { Y } _ { 2 }$ , and intervention indicator matrix $\\hat { Y } _ { 3 }$ .", "figure_type": "table", "visual_anchor": "Bottom row matrices showing Ŷ₁ with value 5 and Ŷ₂ with value 5 for Romance-lovers on Drama", "text_evidence": "Consider a toy example adapted from Steck (2010) to illustrate the disastrous effect that selection bias can have on conventional evaluation using a test set of held-out ratings", "query_type": "comparison_explanation"}
{"query_id": "l1_1602.05352_1602.05352_fig_4_85", "query": "Does the red Naive line's high RMSE at α=0.5 support why propensity-scored MF-IPS substantially outperforms conventional matrix factorization?", "answer": "Yes, the red Naive line maintains RMSE above 1.0 at α=0.5 while IPS/SNIPS stay near 0.01-0.02, demonstrating the substantial performance gap that Table 2 quantifies between propensity-scored and conventional approaches.", "doc_id": "1602.05352", "figure_id": "1602.05352_fig_4", "figure_number": 2, "image_path": "data/mineru_output/1602.05352/1602.05352/hybrid_auto/images/1602.05352_page0_fig3.jpg", "caption": "Figure 2. RMSE of the estimators in the experimental setting as the observed ratings exhibit varying degrees of selection bias.", "figure_type": "plot", "visual_anchor": "Red Naive line at RMSE > 1.0 versus blue IPS and green SNIPS lines at RMSE ≈ 0.01-0.02 when α=0.5 in MSE plot", "text_evidence": "Table 2 shows that our propensity-scored matrix factorization MF-IPS with learnt propensities substantially and significantly outperforms the conventional matrix factorization approach", "query_type": "comparison_explanation"}
{"query_id": "l1_1602.05352_1602.05352_fig_4_86", "query": "Why do the blue IPS and green SNIPS curves remain stable below 0.1 RMSE across varying α, given cross-validation requires propensity scaling?", "answer": "The propensity scaling by (k-1)/k for training folds and validation fold adjustments maintain consistent IPS/SNIPS estimation quality across selection bias degrees, keeping RMSE stable and low regardless of α.", "doc_id": "1602.05352", "figure_id": "1602.05352_fig_4", "figure_number": 2, "image_path": "data/mineru_output/1602.05352/1602.05352/hybrid_auto/images/1602.05352_page0_fig3.jpg", "caption": "Figure 2. RMSE of the estimators in the experimental setting as the observed ratings exhibit varying degrees of selection bias.", "figure_type": "plot", "visual_anchor": "Blue IPS and green SNIPS lines maintaining RMSE < 0.1 from α=0 to α=1.0 in MSE plot", "text_evidence": "We randomly split the observed MNAR ratings into k folds k = 4 in all experiments), training on k - 1 and evaluating on the remaining one using the IPS estimator. Reflecting this additional split requires scaling the propensities in the training folds by", "query_type": "comparison_explanation"}
{"query_id": "l1_1602.05352_1602.05352_fig_4_87", "query": "What explains the red Naive line's dramatic DCG drop from ~200 at α=0 to ~10 at α=0.7 in the experimental setting?", "answer": "As α increases, observed ratings exhibit stronger selection bias, causing the naive estimator's DCG to degrade dramatically because it fails to correct for the increasingly non-random missing data pattern.", "doc_id": "1602.05352", "figure_id": "1602.05352_fig_4", "figure_number": 2, "image_path": "data/mineru_output/1602.05352/1602.05352/hybrid_auto/images/1602.05352_page0_fig3.jpg", "caption": "Figure 2. RMSE of the estimators in the experimental setting as the observed ratings exhibit varying degrees of selection bias.", "figure_type": "plot", "visual_anchor": "Red Naive line decreasing from ~200 at α=0 to ~10 at α=0.7 in DCG plot", "text_evidence": "RMSE of the estimators in the experimental setting as the observed ratings exhibit varying degrees of selection bias", "query_type": "anomaly_cause"}
{"query_id": "l1_1602.05352_1602.05352_fig_5_88", "query": "Why does the blue MF-IPS curve achieve lower MSE than red MF-Naive across all alpha values?", "answer": "MF-IPS uses propensity weighting to correct for selection bias, while MF-Naive does not account for biased observations. This allows MF-IPS to achieve better prediction accuracy through improved risk estimation.", "doc_id": "1602.05352", "figure_id": "1602.05352_fig_5", "figure_number": 3, "image_path": "data/mineru_output/1602.05352/1602.05352/hybrid_auto/images/1602.05352_page0_fig4.jpg", "caption": "Figure 3. Prediction error (MSE) of matrix factorization methods as the observed ratings exhibit varying degrees of selection bias (left) and as propensity estimation quality degrades (right).", "figure_type": "plot", "visual_anchor": "blue MF-IPS curve consistently below red MF-Naive curve across alpha from 0 to 1", "text_evidence": "Using the same semi-synthetic ML100K dataset and observation model as above, we compare our matrix factorization MF-IPS with the traditional unweighted matrix factorization MF-Naive.", "query_type": "comparison_explanation"}
{"query_id": "l1_1602.05352_1602.05352_fig_5_89", "query": "Do the shaded confidence intervals around both curves support the claim of 95% confidence over 30 trials?", "answer": "Yes, the shaded regions around both the red MF-Naive and blue MF-IPS curves represent 95% confidence intervals computed over 30 experimental trials, as stated in the methodology.", "doc_id": "1602.05352", "figure_id": "1602.05352_fig_5", "figure_number": 3, "image_path": "data/mineru_output/1602.05352/1602.05352/hybrid_auto/images/1602.05352_page0_fig4.jpg", "caption": "Figure 3. Prediction error (MSE) of matrix factorization methods as the observed ratings exhibit varying degrees of selection bias (left) and as propensity estimation quality degrades (right).", "figure_type": "plot", "visual_anchor": "shaded regions surrounding red and blue curves throughout the alpha range", "text_evidence": "The results are plotted in Figure 3 (left), where shaded regions indicate $9 5 \\%$ confidence intervals over 30 trials.", "query_type": "visual_definition"}
{"query_id": "l1_1602.05352_1602.05352_fig_5_90", "query": "Does the MSE reduction from 1.75 to near 0.1 as alpha increases demonstrate propensity estimation quality impact?", "answer": "This left panel shows how selection bias severity affects learning as alpha varies. The right panel specifically examines propensity estimation quality degradation, which is a different experimental manipulation.", "doc_id": "1602.05352", "figure_id": "1602.05352_fig_5", "figure_number": 3, "image_path": "data/mineru_output/1602.05352/1602.05352/hybrid_auto/images/1602.05352_page0_fig4.jpg", "caption": "Figure 3. Prediction error (MSE) of matrix factorization methods as the observed ratings exhibit varying degrees of selection bias (left) and as propensity estimation quality degrades (right).", "figure_type": "plot", "visual_anchor": "MSE values decreasing from approximately 1.75 at alpha=0 to near 0.1 at alpha=1 for both curves", "text_evidence": "Figure 3 (right) shows how learning performance is affected by inaccurate propensities using the same setup as in Section 6.3.", "query_type": "value_context"}
{"query_id": "l1_1602.05352_1602.05352_fig_7_91", "query": "Why does the purple curve drop sharply at 10^2 MCAR ratings when propensity estimates degrade?", "answer": "The sharp drop at 10^2 MCAR ratings reflects the transition point where propensity estimates begin to degrade, yet IPS initially improves before deteriorating further as estimates worsen.", "doc_id": "1602.05352", "figure_id": "1602.05352_fig_7", "figure_number": 4, "image_path": "data/mineru_output/1602.05352/1602.05352/hybrid_auto/images/1602.05352_page0_fig6.jpg", "caption": "Figure 4. RMSE of IPS and SNIPS as propensity estimates degrade. IPS with true propensities and Naive are given as reference.", "figure_type": "plot", "visual_anchor": "Purple curve with sharp drop at 10^2 on x-axis", "text_evidence": "Figure 4 shows how the quality of the propensity estimates impacts evaluation using the same setup as in Section 6.2.", "query_type": "anomaly_cause"}
{"query_id": "l1_1602.05352_1602.05352_fig_7_92", "query": "Does the blue curve remaining below 10^-1 RMSE across all MCAR ratings support robustness claims?", "answer": "Yes, the blue curve stays consistently low (below 10^-1) across all MCAR rating counts, demonstrating robustness even with severely degraded propensity estimates.", "doc_id": "1602.05352", "figure_id": "1602.05352_fig_7", "figure_number": 4, "image_path": "data/mineru_output/1602.05352/1602.05352/hybrid_auto/images/1602.05352_page0_fig6.jpg", "caption": "Figure 4. RMSE of IPS and SNIPS as propensity estimates degrade. IPS with true propensities and Naive are given as reference.", "figure_type": "plot", "visual_anchor": "Blue curve consistently below 10^-1 RMSE line", "text_evidence": "severely degraded propensity estimates, demonstrating the robustness of the approach.", "query_type": "comparison_explanation"}
{"query_id": "l1_1602.05352_1602.05352_fig_7_93", "query": "How does the red reference line at 10^0 RMSE relate to unweighted matrix factorization performance?", "answer": "The red reference line at 10^0 RMSE represents the Naive baseline performance, corresponding to the traditional unweighted matrix factorization MF-Naive used for comparison.", "doc_id": "1602.05352", "figure_id": "1602.05352_fig_7", "figure_number": 4, "image_path": "data/mineru_output/1602.05352/1602.05352/hybrid_auto/images/1602.05352_page0_fig6.jpg", "caption": "Figure 4. RMSE of IPS and SNIPS as propensity estimates degrade. IPS with true propensities and Naive are given as reference.", "figure_type": "plot", "visual_anchor": "Red horizontal line at 10^0 RMSE", "text_evidence": "we compare our matrix factorization MF-IPS with the traditional unweighted matrix factorization MF-Naive. Both methods use the same factorization model", "query_type": "value_context"}
{"query_id": "l1_1603.07025_1603.07025_fig_1_94", "query": "Why does the nested indentation below SirPatStew's comment demonstrate the focus on sharing versus discussion?", "answer": "The indentation shows Patrick Stewart answering users' questions, illustrating that many subreddits are primarily about discussion rather than just sharing web content from other sites. The nested structure enables threaded conversations typical of discussion-focused communities.", "doc_id": "1603.07025", "figure_id": "1603.07025_fig_1", "figure_number": 1, "image_path": "data/mineru_output/1603.07025/1603.07025/hybrid_auto/images/1603.07025_page0_fig0.jpg", "caption": "Figure 1: Reddit interface when visualizing a submission. This is Patrick Stewart’s “AmA” (ask me anything) in “IAmA” (I am a), a submission where he answers users’ questions in the comments. We can see the most upvoted comment and Patrick’s answer right below.", "figure_type": "example", "visual_anchor": "Nested comment indentation below SirPatStew's blue username", "text_evidence": "Many subreddits are primarily about sharing web content from other sites", "query_type": "comparison_explanation"}
{"query_id": "l1_1603.07025_1603.07025_fig_1_95", "query": "How does the 8,789 readers statistic relate to Reddit's position among top 35 worldwide sites?", "answer": "The 8,789 readers shown for this single subreddit represents just a tiny fraction of Reddit's massive user base. This individual subreddit metric helps illustrate how Reddit's aggregation of 853,000 subreddits contributes to its ranking as a top 35 global website.", "doc_id": "1603.07025", "figure_id": "1603.07025_fig_1", "figure_number": 1, "image_path": "data/mineru_output/1603.07025/1603.07025/hybrid_auto/images/1603.07025_page0_fig0.jpg", "caption": "Figure 1: Reddit interface when visualizing a submission. This is Patrick Stewart’s “AmA” (ask me anything) in “IAmA” (I am a), a submission where he answers users’ questions in the comments. We can see the most upvoted comment and Patrick’s answer right below.", "figure_type": "example", "visual_anchor": "8,789 readers (8775 upvotes) displayed on right side", "text_evidence": "According to Alexa, as of late 2015 Reddit is in the top 15 sites in the U.S. and the top 35 in the world in terms of monthly unique visitors", "query_type": "value_context"}
{"query_id": "l1_1603.07025_1603.07025_fig_1_96", "query": "Does the Top 200 Comments section with show 500 option reflect the particular purpose focus of subreddits?", "answer": "Yes, the comment filtering demonstrates how IAmA subreddit serves its particular purpose of hosting Q&A sessions. The large volume requiring a 'show 500' option indicates this subreddit focuses specifically on facilitating celebrity question-and-answer interactions rather than general content sharing.", "doc_id": "1603.07025", "figure_id": "1603.07025_fig_1", "figure_number": 1, "image_path": "data/mineru_output/1603.07025/1603.07025/hybrid_auto/images/1603.07025_page0_fig0.jpg", "caption": "Figure 1: Reddit interface when visualizing a submission. This is Patrick Stewart’s “AmA” (ask me anything) in “IAmA” (I am a), a submission where he answers users’ questions in the comments. We can see the most upvoted comment and Patrick’s answer right below.", "figure_type": "example", "visual_anchor": "Top 200 Comments header with 'show 500' link", "text_evidence": "It consists of a large number of subreddits (853,000 as of June 21st, 2015), each of which focuses on a particular purpose", "query_type": "visual_definition"}
{"query_id": "l1_1603.07025_1603.07025_fig_2_97", "query": "Does the solid purple curve reaching 10^7 by 2014 align with the 1.65 billion comments collected through May 2015?", "answer": "The solid purple curve shows cumulative users reaching about 10 million by 2014, which is distinct from but consistent with the 1.65 billion comments dataset that extends through May 2015.", "doc_id": "1603.07025", "figure_id": "1603.07025_fig_2", "figure_number": 2, "image_path": "data/mineru_output/1603.07025/1603.07025/hybrid_auto/images/1603.07025_page0_fig1.jpg", "caption": "Figure 2: Figure (a) shows the cumulative growth of Reddit for users and subreddits. Figure (b) shows the number of active users and subreddits in Reddit over time. An active user or subreddit is one that had at least one post (comment or submission) in the time bin we used—here, discretized by month.", "figure_type": "plot", "visual_anchor": "solid dark purple line reaching approximately 10^7 on y-axis by 2014", "text_evidence": "Redditor Stuck In The Matrix used Reddit's API to compile a dataset of almost every publicly available comment [65] from October 2007 until May 2015. The dataset is composed of 1.65 billion comments", "query_type": "value_context"}
{"query_id": "l1_1603.07025_1603.07025_fig_2_98", "query": "Why does the dashed teal line remain two orders of magnitude below the purple line throughout 2008-2014?", "answer": "The dashed teal line represents cumulative subreddits while the purple line shows cumulative users. Reddit's structure allows many users to participate across relatively fewer subreddit communities.", "doc_id": "1603.07025", "figure_id": "1603.07025_fig_2", "figure_number": 2, "image_path": "data/mineru_output/1603.07025/1603.07025/hybrid_auto/images/1603.07025_page0_fig1.jpg", "caption": "Figure 2: Figure (a) shows the cumulative growth of Reddit for users and subreddits. Figure (b) shows the number of active users and subreddits in Reddit over time. An active user or subreddit is one that had at least one post (comment or submission) in the time bin we used—here, discretized by month.", "figure_type": "plot", "visual_anchor": "dashed teal line staying around 10^5 while solid purple line reaches 10^7", "text_evidence": "These datasets contain a total of 114 million submissions. An active user or subreddit is one that had at least one post (comment or submission) in the time bin we used—here, discretized by month", "query_type": "comparison_explanation"}
{"query_id": "l1_1603.07025_1603.07025_fig_2_99", "query": "Does the sharp exponential rise in both curves before 2009 reflect the October 2007 starting point of data collection?", "answer": "Yes, the steep initial rise in both curves starting around 2008 corresponds to the beginning of the dataset compilation from October 2007, capturing Reddit's early rapid growth phase.", "doc_id": "1603.07025", "figure_id": "1603.07025_fig_2", "figure_number": 2, "image_path": "data/mineru_output/1603.07025/1603.07025/hybrid_auto/images/1603.07025_page0_fig1.jpg", "caption": "Figure 2: Figure (a) shows the cumulative growth of Reddit for users and subreddits. Figure (b) shows the number of active users and subreddits in Reddit over time. An active user or subreddit is one that had at least one post (comment or submission) in the time bin we used—here, discretized by month.", "figure_type": "plot", "visual_anchor": "steep exponential rise in both curves from 10^1-10^2 range in 2008", "text_evidence": "He also compiled a submissions dataset for the period of October 2007 until December 2014 (made available for us upon request) containing a total of 114 million submissions", "query_type": "anomaly_cause"}
{"query_id": "l1_1603.07025_1603.07025_fig_4_100", "query": "Why does the purple curve rise from 13 to 17 posts between 2008 and 2011 despite the 350,000 unavailable comments?", "answer": "The 350,000 missing comments due to API call failures represent a negligible fraction of the 1.65 billion total comments, so they do not significantly impact the observed upward trend in average posts per active user during this period.", "doc_id": "1603.07025", "figure_id": "1603.07025_fig_4", "figure_number": 3, "image_path": "data/mineru_output/1603.07025/1603.07025/hybrid_auto/images/1603.07025_page0_fig3.jpg", "caption": "Figure 3: In Figure (a), monthly average posts per active user over clock time. In Figure (b), monthly average posts per active users in the user-time referential, i.e., message creation time is measured relative to the user’s first post. Each tick in the x-axis is one year. In both figures (and all later figures), we consider only active users during each month; users that are either temporarily or permanently away from Reddit are not included.", "figure_type": "plot", "visual_anchor": "purple curve rising from approximately 13 posts per user in 2008 to 17 posts by 2011", "text_evidence": "The dataset is composed of 1.65 billion comments, although due to API call failures, about 350,000 comments are unavailable.", "query_type": "anomaly_cause"}
{"query_id": "l1_1603.07025_1603.07025_fig_4_101", "query": "Does the plateau at 17 posts after 2011 reflect behavior only from users who remain active each month?", "answer": "Yes, the plateau reflects only active users during each month. Users temporarily or permanently away from Reddit are excluded from the monthly average calculation, which stabilizes the metric around 17-18 posts per user.", "doc_id": "1603.07025", "figure_id": "1603.07025_fig_4", "figure_number": 3, "image_path": "data/mineru_output/1603.07025/1603.07025/hybrid_auto/images/1603.07025_page0_fig3.jpg", "caption": "Figure 3: In Figure (a), monthly average posts per active user over clock time. In Figure (b), monthly average posts per active users in the user-time referential, i.e., message creation time is measured relative to the user’s first post. Each tick in the x-axis is one year. In both figures (and all later figures), we consider only active users during each month; users that are either temporarily or permanently away from Reddit are not included.", "figure_type": "plot", "visual_anchor": "plateau in the purple curve at approximately 17-18 posts per user from 2011 onwards", "text_evidence": "we consider only active users during each month; users that are either temporarily or permanently away from Reddit are not included.", "query_type": "value_context"}
{"query_id": "l1_1603.07025_1603.07025_fig_4_102", "query": "How does filtering bot-like usernames affect the consistency of the 17-post plateau visible from 2011 to 2014?", "answer": "Filtering out bot-like usernames removes automated posting activity that could artificially inflate or add noise to the average. This preprocessing helps ensure the plateau represents genuine human user behavior, making the metric more stable and meaningful.", "doc_id": "1603.07025", "figure_id": "1603.07025_fig_4", "figure_number": 3, "image_path": "data/mineru_output/1603.07025/1603.07025/hybrid_auto/images/1603.07025_page0_fig3.jpg", "caption": "Figure 3: In Figure (a), monthly average posts per active user over clock time. In Figure (b), monthly average posts per active users in the user-time referential, i.e., message creation time is measured relative to the user’s first post. Each tick in the x-axis is one year. In both figures (and all later figures), we consider only active users during each month; users that are either temporarily or permanently away from Reddit are not included.", "figure_type": "plot", "visual_anchor": "stable plateau around 17 posts per user maintained from 2011 through 2014", "text_evidence": "we did light preprocessing to filter out posts by deleted users, posts with no creation time, and posts by authors with bot-like names", "query_type": "comparison_explanation"}
{"query_id": "l1_1603.07025_1603.07025_fig_6_103", "query": "Why does the green 2008 line reach 30 posts per user while later cohorts plateau lower, given exponential growth?", "answer": "The exponential growth in user accounts from 2008-2009 shown in Figure 2a created a large influx of new users, which diluted the average posting rate for later cohorts despite the site's overall expansion.", "doc_id": "1603.07025", "figure_id": "1603.07025_fig_6", "figure_number": 4, "image_path": "data/mineru_output/1603.07025/1603.07025/hybrid_auto/images/1603.07025_page0_fig5.jpg", "caption": "Figure 4: Figure (a) shows the average number of posts per active user over clock time and Figure (b) per active user in the user-time referential, both segmented by users’ cohorts. The user cohort is defined by the year of the user’s creation time. For comparison, the black line in Figure (a) represents the overall average.", "figure_type": "plot", "visual_anchor": "green solid line (2008) reaching approximately 30 posts per user", "text_evidence": "After an initial extremely rapid expansion from 2008–2009, the number of users and subreddits have grown exponentially. As of the end of 2014, about 16.2 million distinct users and 327 thousand subreddits made/received at least one post based on our data.", "query_type": "comparison_explanation"}
{"query_id": "l1_1603.07025_1603.07025_fig_6_104", "query": "Does the black Overall line stabilizing around 18 posts per user support analyzing variation of posts over time?", "answer": "Yes, the Overall line's stabilization around 18 posts per user demonstrates a consistent pattern of user activity variation over time, which is appropriate for analyzing how posting behavior changes temporally.", "doc_id": "1603.07025", "figure_id": "1603.07025_fig_6", "figure_number": 4, "image_path": "data/mineru_output/1603.07025/1603.07025/hybrid_auto/images/1603.07025_page0_fig5.jpg", "caption": "Figure 4: Figure (a) shows the average number of posts per active user over clock time and Figure (b) per active user in the user-time referential, both segmented by users’ cohorts. The user cohort is defined by the year of the user’s creation time. For comparison, the black line in Figure (a) represents the overall average.", "figure_type": "plot", "visual_anchor": "black solid Overall line stabilizing around 18 posts per user", "text_evidence": "Approaches that consider the total number of posts per user in a particular dataset [23] and that analyze the variation of the number of posts per user over time [24] have been applied to online social networks.", "query_type": "value_context"}
{"query_id": "l1_1603.07025_1603.07025_fig_6_105", "query": "How does the 2013 purple dashed line reaching only 20 posts by late 2014 address measuring user activity quantity?", "answer": "The 2013 cohort's limited growth to 20 posts per user by late 2014 illustrates that measuring user activity quantity through posts over time reveals how newer users contribute less individually, addressing the research question about activity changes.", "doc_id": "1603.07025", "figure_id": "1603.07025_fig_6", "figure_number": 4, "image_path": "data/mineru_output/1603.07025/1603.07025/hybrid_auto/images/1603.07025_page0_fig5.jpg", "caption": "Figure 4: Figure (a) shows the average number of posts per active user over clock time and Figure (b) per active user in the user-time referential, both segmented by users’ cohorts. The user cohort is defined by the year of the user’s creation time. For comparison, the black line in Figure (a) represents the overall average.", "figure_type": "plot", "visual_anchor": "purple dashed line (2013) reaching approximately 20 posts per user by late 2014", "text_evidence": "In this section, we use this measure to address our first research question (RQ1): how does the amount of users' activity change ov", "query_type": "value_context"}
{"query_id": "l1_1603.07025_1603.07025_fig_8_106", "query": "Why does the black dotted line labeled 4 remain around 30 posts while all other groups decline?", "answer": "Users who survived longer (labeled 4) started at higher activity levels than shorter-tenured ones and maintained their engagement throughout their tenure. The figure shows longer-tenured users' sustained posting behavior contrasts with the declining activity of users who churned earlier.", "doc_id": "1603.07025", "figure_id": "1603.07025_fig_8", "figure_number": 5, "image_path": "data/mineru_output/1603.07025/1603.07025/hybrid_auto/images/1603.07025_page0_fig7.jpg", "caption": "Figure 5: Each Figure corresponds to one cohort, from 2010 to 2012, left to right. The users for each cohort are further divided in groups based on how long they survived: users that survived up to 1 year are labeled 0, from 1 to 2 years are labeled 1, and so on. For all cohorts, longer-tenured users started at higher activity levels than shorter-tenured ones.", "figure_type": "plot", "visual_anchor": "black dotted line labeled 4 staying at approximately 30 posts per user", "text_evidence": "For all cohorts, longer-tenured users started at higher activity levels than shorter-tenured ones.", "query_type": "comparison_explanation"}
{"query_id": "l1_1603.07025_1603.07025_fig_8_107", "query": "Does the green line labeled 0 dropping to near zero by time 1 support quantity-based activity measurement?", "answer": "Yes, the green line's rapid decline to zero by time 1 demonstrates that the number of posts per user over time effectively captures user disengagement. This validates quantity as a measure of user activity in online communities.", "doc_id": "1603.07025", "figure_id": "1603.07025_fig_8", "figure_number": 5, "image_path": "data/mineru_output/1603.07025/1603.07025/hybrid_auto/images/1603.07025_page0_fig7.jpg", "caption": "Figure 5: Each Figure corresponds to one cohort, from 2010 to 2012, left to right. The users for each cohort are further divided in groups based on how long they survived: users that survived up to 1 year are labeled 0, from 1 to 2 years are labeled 1, and so on. For all cohorts, longer-tenured users started at higher activity levels than shorter-tenured ones.", "figure_type": "plot", "visual_anchor": "green solid line labeled 0 declining from ~8 to approximately 0 by time 1", "text_evidence": "One common way to represent user activity in online communities is quantity: the number of posts people make over time. Approaches that consider the total number of posts per user in a particular dataset", "query_type": "value_context"}
{"query_id": "l1_1603.07025_1603.07025_fig_8_108", "query": "How does the blue dashed line labeled 2 declining from 17 to 5 relate to cohort posting averages?", "answer": "The blue dashed line shows users who survived 2-3 years gradually decreasing their posting activity, illustrating the cohort effect where users level off at lower posting averages. This pattern contributes to understanding whether new users follow older users' behavior trajectories.", "doc_id": "1603.07025", "figure_id": "1603.07025_fig_8", "figure_number": 5, "image_path": "data/mineru_output/1603.07025/1603.07025/hybrid_auto/images/1603.07025_page0_fig7.jpg", "caption": "Figure 5: Each Figure corresponds to one cohort, from 2010 to 2012, left to right. The users for each cohort are further divided in groups based on how long they survived: users that survived up to 1 year are labeled 0, from 1 to 2 years are labeled 1, and so on. For all cohorts, longer-tenured users started at higher activity levels than shorter-tenured ones.", "figure_type": "plot", "visual_anchor": "blue dashed line labeled 2 declining from approximately 17 to 5 posts", "text_evidence": "We can already observe a significant cohort effect: users from later cohorts appear to level off at significantly lower posting averages than users from ear", "query_type": "comparison_explanation"}
{"query_id": "l1_1603.07025_1603.07025_fig_13_109", "query": "Does the darker overall line's downward trend in Figure (b) support hypothesis H3 about decreasing user commitment?", "answer": "The downward trend might suggest decreasing commitment (H3), but this interpretation may not be best according to the analysis, which directs attention to user-referential patterns instead.", "doc_id": "1603.07025", "figure_id": "1603.07025_fig_13", "figure_number": 6, "image_path": "data/mineru_output/1603.07025/1603.07025/hybrid_auto/images/1603.07025_page0_fig12.jpg", "caption": "Figure 6: Figure (a) shows the average comment length over clock time and Figure (b) from the user-referential time. Both figures show the cohorted trends. The overall average length per comment decreases over time, although for any individual cohort, it increases after a sharp initial drop. Figures (c), (d) and (e), similar to Figure 5, show the monthly average comment length for active users in the cohorts of 2010, 2011 and 2012, segmented by the number of years that the user survived in the network. Opposite the analysis for average posts, which showed that low-activity users were the first to leave Reddit, here, people who start out as longer commenters are more likely to leave.", "figure_type": "plot", "visual_anchor": "darker overall line showing downward trend in Figure 6b", "text_evidence": "Based on the downwards tendency of the overall comment length in Figure 6a, one might hypothesize that users' commitment to the network is decreasing over time (H3)", "query_type": "value_context"}
{"query_id": "l1_1603.07025_1603.07025_fig_13_110", "query": "Why do individual cohort lines rise after time 1 despite the overall average length decreasing over time?", "answer": "For any individual cohort, comment length increases after a sharp initial drop, even though the overall network average decreases. This reveals a compositional effect where cohort-level behavior differs from aggregate trends.", "doc_id": "1603.07025", "figure_id": "1603.07025_fig_13", "figure_number": 6, "image_path": "data/mineru_output/1603.07025/1603.07025/hybrid_auto/images/1603.07025_page0_fig12.jpg", "caption": "Figure 6: Figure (a) shows the average comment length over clock time and Figure (b) from the user-referential time. Both figures show the cohorted trends. The overall average length per comment decreases over time, although for any individual cohort, it increases after a sharp initial drop. Figures (c), (d) and (e), similar to Figure 5, show the monthly average comment length for active users in the cohorts of 2010, 2011 and 2012, segmented by the number of years that the user survived in the network. Opposite the analysis for average posts, which showed that low-activity users were the first to leave Reddit, here, people who start out as longer commenters are more likely to leave.", "figure_type": "plot", "visual_anchor": "cohort lines (green, blue dashed, dark blue solid) rising after initial drop at time ~1", "text_evidence": "The overall average length per comment decreases over time, although for any individual cohort, it increases after a sharp initial drop.", "query_type": "comparison_explanation"}
{"query_id": "l1_1603.07025_1603.07025_fig_13_111", "query": "How does the green cohort 0 peak at approximately 210 relate to user survival patterns?", "answer": "Users who start as longer commenters (like those reaching ~210 length) are more likely to leave Reddit, opposite to the pattern observed for posting activity.", "doc_id": "1603.07025", "figure_id": "1603.07025_fig_13", "figure_number": 6, "image_path": "data/mineru_output/1603.07025/1603.07025/hybrid_auto/images/1603.07025_page0_fig12.jpg", "caption": "Figure 6: Figure (a) shows the average comment length over clock time and Figure (b) from the user-referential time. Both figures show the cohorted trends. The overall average length per comment decreases over time, although for any individual cohort, it increases after a sharp initial drop. Figures (c), (d) and (e), similar to Figure 5, show the monthly average comment length for active users in the cohorts of 2010, 2011 and 2012, segmented by the number of years that the user survived in the network. Opposite the analysis for average posts, which showed that low-activity users were the first to leave Reddit, here, people who start out as longer commenters are more likely to leave.", "figure_type": "plot", "visual_anchor": "green solid line (cohort 0) peaking at approximately 210 around time 1", "text_evidence": "Opposite the analysis for average posts, which showed that low-activity users were the first to leave Reddit, here, people who start out as longer commenters are more likely to leave.", "query_type": "anomaly_cause"}
{"query_id": "l1_1603.07025_1603.07025_fig_19_112", "query": "Why does cohort 5's dashed line reach nearly 20 while later cohorts stay below 10 throughout user-referential time?", "answer": "Users who survive longer in each cohort start with a relatively higher comments per submission ratio than users who abandon Reddit sooner, explaining cohort 5's elevated trajectory.", "doc_id": "1603.07025", "figure_id": "1603.07025_fig_19", "figure_number": 7, "image_path": "data/mineru_output/1603.07025/1603.07025/hybrid_auto/images/1603.07025_page0_fig18.jpg", "caption": "Figure 7: Figure (a) shows the average comment per submission ratio over clock time for the cohorts and the overall average. Figure (b) shows the average comment per submission from the user-referential time for the cohorts. Figures (c), (d), (e) and (f), similarly to Figure 5, shows the 2008, 2009, 2010, and 2011 cohorts, segmented by the number of years a user in the cohort survived. As with average posts per month, users who stay active longer appear to start their careers with a relatively higher comments per submission ratio than users who abandon Reddit sooner. Unlike that analysis, however, the early 2008 cohort ends up below the later cohorts in Figure (b).", "figure_type": "plot", "visual_anchor": "cohort 5 dashed line peaking near 20 versus other cohorts below 10", "text_evidence": "users who stay active longer appear to start their careers with a relatively higher comments per submission ratio than users who abandon Reddit sooner", "query_type": "comparison_explanation"}
{"query_id": "l1_1603.07025_1603.07025_fig_19_113", "query": "Does cohort 1's position below cohort 5 at time point 3 contradict the general pattern of early adopters?", "answer": "Yes, unlike the average posts per month analysis, the early 2008 cohort (cohort 1) ends up below the later cohorts, representing an anomaly.", "doc_id": "1603.07025", "figure_id": "1603.07025_fig_19", "figure_number": 7, "image_path": "data/mineru_output/1603.07025/1603.07025/hybrid_auto/images/1603.07025_page0_fig18.jpg", "caption": "Figure 7: Figure (a) shows the average comment per submission ratio over clock time for the cohorts and the overall average. Figure (b) shows the average comment per submission from the user-referential time for the cohorts. Figures (c), (d), (e) and (f), similarly to Figure 5, shows the 2008, 2009, 2010, and 2011 cohorts, segmented by the number of years a user in the cohort survived. As with average posts per month, users who stay active longer appear to start their careers with a relatively higher comments per submission ratio than users who abandon Reddit sooner. Unlike that analysis, however, the early 2008 cohort ends up below the later cohorts in Figure (b).", "figure_type": "plot", "visual_anchor": "cohort 1 solid line remaining below 6 while cohort 5 exceeds 15", "text_evidence": "Unlike that analysis, however, the early 2008 cohort ends up below the later cohorts in Figure (b)", "query_type": "anomaly_cause"}
{"query_id": "l1_1603.07025_1603.07025_fig_19_114", "query": "Do users in cohort 5 who reach 18 comments per submission at time 4 demonstrate hitting the ground running?", "answer": "Yes, the users who survive the longest in each cohort hit the ground running with a high comment-to-submission ratio relative to users who abandon Reddit more quickly.", "doc_id": "1603.07025", "figure_id": "1603.07025_fig_19", "figure_number": 7, "image_path": "data/mineru_output/1603.07025/1603.07025/hybrid_auto/images/1603.07025_page0_fig18.jpg", "caption": "Figure 7: Figure (a) shows the average comment per submission ratio over clock time for the cohorts and the overall average. Figure (b) shows the average comment per submission from the user-referential time for the cohorts. Figures (c), (d), (e) and (f), similarly to Figure 5, shows the 2008, 2009, 2010, and 2011 cohorts, segmented by the number of years a user in the cohort survived. As with average posts per month, users who stay active longer appear to start their careers with a relatively higher comments per submission ratio than users who abandon Reddit sooner. Unlike that analysis, however, the early 2008 cohort ends up below the later cohorts in Figure (b).", "figure_type": "plot", "visual_anchor": "cohort 5 reaching approximately 18 at time point 4", "text_evidence": "the users who survive the longest in each cohort are the ones who hit the ground running. They start out with a high comment-tosubmission ratio relative to users in their cohort who abandon Reddit more quickly", "query_type": "value_context"}
{"query_id": "l1_1607.06520_1607.06520_fig_1_115", "query": "Does the diagonal pattern from bottom-left to top-right validate the Spearman correlation of 0.81 for occupation gender bias?", "answer": "Yes, the strong diagonal pattern from approximately (-0.15, -0.15) to (0.35, 0.25) visually confirms the high positive correlation (ρ = 0.81), indicating that occupation gender biases are highly consistent across the two embedding methods.", "doc_id": "1607.06520", "figure_id": "1607.06520_fig_1", "figure_number": 4, "image_path": "data/mineru_output/1607.06520/1607.06520/hybrid_auto/images/1607.06520_page0_fig0.jpg", "caption": "Figure 4: Comparing the bias of two different embeddings–the w2vNEWS and the GloVe web-crawl embedding. In each embedding, the occupation words are projected onto the she-he direction. Each dot corresponds to one occupation word; the gender bias of occupations is highly consistent across embeddings (Spearman $\\rho = 0 . 8 1$ ).", "figure_type": "plot", "visual_anchor": "Diagonal trend of blue dots from bottom-left (-0.15, -0.15) to top-right (0.35, 0.25)", "text_evidence": "the gender bias of occupations is highly consistent across embeddings (Spearman $\\rho = 0 . 8 1$ )", "query_type": "comparison_explanation"}
{"query_id": "l1_1607.06520_1607.06520_fig_1_116", "query": "Why do most occupation dots cluster between -0.1 and 0.1 on both axes despite using 300-dimensional word2vec?", "answer": "The clustering reflects that most occupations have relatively neutral gender bias when projected onto the she-he direction, with only a few occupations showing extreme bias. The dimensionality of 300 is the embedding space size, not the projection axis range.", "doc_id": "1607.06520", "figure_id": "1607.06520_fig_1", "figure_number": 4, "image_path": "data/mineru_output/1607.06520/1607.06520/hybrid_auto/images/1607.06520_page0_fig0.jpg", "caption": "Figure 4: Comparing the bias of two different embeddings–the w2vNEWS and the GloVe web-crawl embedding. In each embedding, the occupation words are projected onto the she-he direction. Each dot corresponds to one occupation word; the gender bias of occupations is highly consistent across embeddings (Spearman $\\rho = 0 . 8 1$ ).", "figure_type": "plot", "visual_anchor": "Dense cluster of blue dots concentrated in the central region between -0.1 and 0.1 on both axes", "text_evidence": "the embedding we refer to in this paper is the aforementioned w2vNEWS embedding, a $d = 3 0 0$ -dimensional word2vec [24, 25] embedding", "query_type": "value_context"}
{"query_id": "l1_1607.06520_1607.06520_fig_1_117", "query": "How does the cosine similarity normalization relate to the occupation word positions scattered between x=-0.2 and x=0.4?", "answer": "The x-axis positions represent normalized cosine similarities (dot products) between occupation word vectors and the she-he direction in w2vNEWS embedding. Since words are normalized, the cosine similarity equals the dot product, producing values ranging from -0.2 to 0.4.", "doc_id": "1607.06520", "figure_id": "1607.06520_fig_1", "figure_number": 4, "image_path": "data/mineru_output/1607.06520/1607.06520/hybrid_auto/images/1607.06520_page0_fig0.jpg", "caption": "Figure 4: Comparing the bias of two different embeddings–the w2vNEWS and the GloVe web-crawl embedding. In each embedding, the occupation words are projected onto the she-he direction. Each dot corresponds to one occupation word; the gender bias of occupations is highly consistent across embeddings (Spearman $\\rho = 0 . 8 1$ ).", "figure_type": "plot", "visual_anchor": "X-axis range from -0.2 to 0.4 showing scattered occupation word positions", "text_evidence": "Since words are normalized $\\cos ( \\vec { w } _ { 1 } , \\vec { w } _ { 2 } ) = \\vec { w } _ { 1 } \\cdot \\vec { w } _ { 2 }$", "query_type": "visual_definition"}
{"query_id": "l1_1607.06520_1607.06520_fig_2_118", "query": "Why does the first bar at 0.6 explain more variance than others in gender pair vector differences?", "answer": "There is a single direction that explains the majority of variance in the normalized gender pair difference vectors, indicating that despite polysemy and sampling randomness, gender biases align along one dominant dimension.", "doc_id": "1607.06520", "figure_id": "1607.06520_fig_2", "figure_number": 6, "image_path": "data/mineru_output/1607.06520/1607.06520/hybrid_auto/images/1607.06520_page0_fig1.jpg", "caption": "Figure 6: Left: the percentage of variance explained in the PCA of these vector differences (each difference normalized to be a unit vector). The top component explains significantly more variance than any other. Right: for comparison, the corresponding percentages for random unit vectors (figure created by averaging over 1,000 draws of ten random unit vectors in 300 dimensions).", "figure_type": "plot", "visual_anchor": "first blue bar at height approximately 0.6", "text_evidence": "As Figure 6 shows, there is a single direction that explains the majority of variance in these vectors.", "query_type": "value_context"}
{"query_id": "l1_1607.06520_1607.06520_fig_2_119", "query": "Does the rapid drop from the first bar at 0.6 to subsequent bars below 0.15 contradict parallel gender differences?", "answer": "No, the steep drop confirms that gender pair differences are not parallel in practice due to different biases, polysemy (like grandfather as in to grandfather a regulation), and randomness in finite samples, but they share one dominant direction.", "doc_id": "1607.06520", "figure_id": "1607.06520_fig_2", "figure_number": 6, "image_path": "data/mineru_output/1607.06520/1607.06520/hybrid_auto/images/1607.06520_page0_fig1.jpg", "caption": "Figure 6: Left: the percentage of variance explained in the PCA of these vector differences (each difference normalized to be a unit vector). The top component explains significantly more variance than any other. Right: for comparison, the corresponding percentages for random unit vectors (figure created by averaging over 1,000 draws of ten random unit vectors in 300 dimensions).", "figure_type": "plot", "visual_anchor": "steep decline from first bar (~0.6) to second bar (~0.12)", "text_evidence": "However, gender pair differences are not parallel in practice, for multiple reasons. First, there are different biases associated with with different gender pairs. Second is polysemy, as mentioned, which in this case occurs due to the other use of grandfather as in to grandfather a regulation. Finally, randomness in the word counts in any finite sample will also lead to differences.", "query_type": "comparison_explanation"}
{"query_id": "l1_1607.06520_1607.06520_fig_2_120", "query": "How does the first component at 0.6 relate to measuring strict bias where c equals zero?", "answer": "The dominant component at 0.6 captures the primary gender bias direction g. When c=0, the bias measure is binary: it equals 0 only if the word vector has no overlap with this dominant gender direction, otherwise 1.", "doc_id": "1607.06520", "figure_id": "1607.06520_fig_2", "figure_number": 6, "image_path": "data/mineru_output/1607.06520/1607.06520/hybrid_auto/images/1607.06520_page0_fig1.jpg", "caption": "Figure 6: Left: the percentage of variance explained in the PCA of these vector differences (each difference normalized to be a unit vector). The top component explains significantly more variance than any other. Right: for comparison, the corresponding percentages for random unit vectors (figure created by averaging over 1,000 draws of ten random unit vectors in 300 dimensions).", "figure_type": "plot", "visual_anchor": "top component bar at approximately 0.6 height", "text_evidence": "where $c$ is a parameter that determines how strict do we want to in measuring bias. If $c$ is $0$ , then $| \\mathrm { c o s } ( \\vec { w } - g ) | ^ { c } = 0$ only if $\\vec { w }$ has no overlap with $g$ and otherwise it is 1.", "query_type": "value_context"}
{"query_id": "l1_1607.06520_1607.06520_fig_4_121", "query": "Why would words above the horizontal red dashed line collapse to the vertical line after applying the debiasing algorithm?", "answer": "The hard debiasing algorithm removes gender pair associations for gender neutral words, which are positioned above the horizontal line. Collapsing them to the vertical line eliminates their projection along the he-she axis while preserving gender neutrality.", "doc_id": "1607.06520", "figure_id": "1607.06520_fig_4", "figure_number": 7, "image_path": "data/mineru_output/1607.06520/1607.06520/hybrid_auto/images/1607.06520_page0_fig3.jpg", "caption": "Figure 7: Selected words projected along two axes: $x$ is a projection onto the difference between the embeddings of the words he and she, and $y$ is a direction learned in the embedding that captures gender neutrality, with gender neutral words above the line and gender specific words below the line. Our hard debiasing algorithm removes the gender pair associations for gender neutral words. In this figure, the words above the horizontal line would all be collapsed to the vertical line.", "figure_type": "plot", "visual_anchor": "horizontal red dashed line with words like 'homemaker', 'feminist', 'actresses' above it", "text_evidence": "Our hard debiasing algorithm removes the gender pair associations for gender neutral words. In this figure, the words above the horizontal line would all be collapsed to the vertical line.", "query_type": "visual_definition"}
{"query_id": "l1_1607.06520_1607.06520_fig_4_122", "query": "Does receptionist's closer proximity to softball than football demonstrate indirect bias even after removing gender pairs from the embedding?", "answer": "Yes, indirect gender association persists because gender-neutral words like receptionist remain closer to stereotypically female activities (softball) than male ones (football), even when explicit gender words are removed. This shows that simple word removal does not eliminate all gender bias in embeddings.", "doc_id": "1607.06520", "figure_id": "1607.06520_fig_4", "figure_number": 7, "image_path": "data/mineru_output/1607.06520/1607.06520/hybrid_auto/images/1607.06520_page0_fig3.jpg", "caption": "Figure 7: Selected words projected along two axes: $x$ is a projection onto the difference between the embeddings of the words he and she, and $y$ is a direction learned in the embedding that captures gender neutrality, with gender neutral words above the line and gender specific words below the line. Our hard debiasing algorithm removes the gender pair associations for gender neutral words. In this figure, the words above the horizontal line would all be collapsed to the vertical line.", "figure_type": "plot", "visual_anchor": "words positioned along x-axis showing distance relationships between gendered and neutral terms", "text_evidence": "There would still be indirect gender association in that a word that should be gender neutral, such as receptionist, is closer to softball than football", "query_type": "comparison_explanation"}
{"query_id": "l1_1607.06520_1607.06520_fig_4_123", "query": "How does the y-axis direction separate words like daughters below the line from words like priest near the center?", "answer": "The y-axis represents a learned direction capturing gender neutrality, with gender-specific words like 'daughters' positioned below the line and gender-neutral words like 'priest' positioned above or near the line. This learned direction enables the classifier to distinguish gender-specific from gender-neutral words.", "doc_id": "1607.06520", "figure_id": "1607.06520_fig_4", "figure_number": 7, "image_path": "data/mineru_output/1607.06520/1607.06520/hybrid_auto/images/1607.06520_page0_fig3.jpg", "caption": "Figure 7: Selected words projected along two axes: $x$ is a projection onto the difference between the embeddings of the words he and she, and $y$ is a direction learned in the embedding that captures gender neutrality, with gender neutral words above the line and gender specific words below the line. Our hard debiasing algorithm removes the gender pair associations for gender neutral words. In this figure, the words above the horizontal line would all be collapsed to the vertical line.", "figure_type": "plot", "visual_anchor": "y-axis with 'daughters' below horizontal line and 'priest' near center vertical line", "text_evidence": "y is a direction learned in the embedding that captures gender neutrality, with gender neutral words above the line and gender specific words below the line", "query_type": "visual_definition"}
{"query_id": "l1_1607.06520_1607.06520_fig_5_124", "query": "Why does the green hard-debiased curve remain below 10 stereotypic analogies across all 160 generated analogies?", "answer": "The hard-debiasing algorithm successfully reduces both direct and indirect gender biases while preserving desirable embedding properties, keeping stereotypical analogies minimal even as more analogies are generated.", "doc_id": "1607.06520", "figure_id": "1607.06520_fig_5", "figure_number": 8, "image_path": "data/mineru_output/1607.06520/1607.06520/hybrid_auto/images/1607.06520_page0_fig4.jpg", "caption": "Figure 8: Number of stereotypical (Left) and appropriate (Right) analogies generated by wordembeddings before and after debiasing.", "figure_type": "plot", "visual_anchor": "green line with x markers staying below y=10 across x-axis 0-160", "text_evidence": "We evaluated our debiasing algorithms to ensure that they preserve the desirable properties of the original embedding while reducing both direct and indirect gender biases.", "query_type": "comparison_explanation"}
{"query_id": "l1_1607.06520_1607.06520_fig_5_125", "query": "Does the soft-debiased red curve reaching 25 stereotypic analogies at 160 generated analogies align with crowd-worker ratings?", "answer": "Yes, the visual trend is consistent with the finding that 29 out of 150 analogies were rated as exhibiting gender stereotype by five or more crowd-workers, showing soft-debiasing still allows substantial stereotypical content.", "doc_id": "1607.06520", "figure_id": "1607.06520_fig_5", "figure_number": 8, "image_path": "data/mineru_output/1607.06520/1607.06520/hybrid_auto/images/1607.06520_page0_fig4.jpg", "caption": "Figure 8: Number of stereotypical (Left) and appropriate (Right) analogies generated by wordembeddings before and after debiasing.", "figure_type": "plot", "visual_anchor": "red triangles with dashed line reaching approximately y=25 at x=160", "text_evidence": "Overall, 72 out of 150 analogies were rated as gender-appropriate by five or more crowd-workers, and 29 analogies were rated as exhibiting gender stereotype by five or more crowd-workers (Figure 8).", "query_type": "value_context"}
{"query_id": "l1_1608.07187_1608.07187_fig_1_126", "query": "Why do red dots at x=80-100 show y values above 1.5, given that female terms are less associated with sciences?", "answer": "The red dots represent occupations with high female participation (80-100%), showing strong positive association with female gender. These likely represent arts/language fields, not sciences, where Nosek et al. found female terms less associated.", "doc_id": "1608.07187", "figure_id": "1608.07187_fig_1", "figure_number": null, "image_path": "data/mineru_output/1608.07187/1608.07187/hybrid_auto/images/1608.07187_page0_fig0.jpg", "caption": "", "figure_type": "plot", "visual_anchor": "Red dots clustered at top right corner (y=1.5-1.7, x=80-100)", "text_evidence": "In another laboratory study, Nosek et al. (2002b) found that female terms are less associated with the sciences, and male terms less associated with the arts.", "query_type": "comparison_explanation"}
{"query_id": "l1_1608.07187_1608.07187_fig_1_127", "query": "Does the Pearson correlation coefficient of 0.90 explain why blue dots at x=0-20 correspond to y values below negative one?", "answer": "Yes, the strong positive correlation (ρ=0.90, p<10^-18) indicates that as percentage of women workers decreases (x=0-20), the association with female gender becomes strongly negative (y<-1), showing male-dominated occupations have strong male associations.", "doc_id": "1608.07187", "figure_id": "1608.07187_fig_1", "figure_number": null, "image_path": "data/mineru_output/1608.07187/1608.07187/hybrid_auto/images/1608.07187_page0_fig0.jpg", "caption": "", "figure_type": "plot", "visual_anchor": "Blue dots clustered at bottom left (y=-1.2 to -1.5, x=0-20)", "text_evidence": "Pearson's correlation coefficient $\\rho = 0 . 9 0$ with $p$ -value $< 1 0 ^ { - 1 8 }$ .", "query_type": "value_context"}
{"query_id": "l1_1608.07187_1608.07187_fig_1_128", "query": "What causes the orange dots at x=50-80 to span y=0.2-0.9 despite the reported effect size of 1.47?", "answer": "The effect size of 1.47 from Nosek et al. measures the strength of math/science vs. arts/language associations in IAT tests. The orange dots represent real-world occupations with moderate female participation, showing weaker but positive female associations, which differs from the controlled IAT experiment measuring category-level stereotypes.", "doc_id": "1608.07187", "figure_id": "1608.07187_fig_1", "figure_number": null, "image_path": "data/mineru_output/1608.07187/1608.07187/hybrid_auto/images/1608.07187_page0_fig0.jpg", "caption": "", "figure_type": "plot", "visual_anchor": "Orange dots in upper middle region (y=0.2-0.9, x=50-80)", "text_evidence": "83 subjects took the IAT with a combination of math/science and art/language attributes, and the expected associations were observed with an effect size of 1.47 and a $p$ -value of $1 0 ^ { - 2 4 }$ , Nosek et al. (2002b, p. 51).", "query_type": "anomaly_cause"}
{"query_id": "l1_1608.07187_1608.07187_fig_3_129", "query": "Do the parallel dashed lines connecting brother-sister and nephew-niece demonstrate the algebraic relationships enabling the 0.84 correlation coefficient prediction?", "answer": "Yes, the parallel vector differences between gender pairs in the 300-dimensional space (shown in 2D projection) demonstrate the algebraic relationships that WEFAT exploits to predict gender percentages with ρ=0.84 correlation.", "doc_id": "1608.07187", "figure_id": "1608.07187_fig_3", "figure_number": 3, "image_path": "data/mineru_output/1608.07187/1608.07187/hybrid_auto/images/1608.07187_page0_fig2.jpg", "caption": "Figure 3. A 2D projection (first two principal components) of the 300-dimensional vector space of the GloVe word embedding (Pennington et al., 2014). The lines illustrate algebraic relationships between related words: pairs of words that differ only by gender map to pairs of vectors whose vector difference is roughly constant. Similar algebraic relationships have been shown for other semantic relationships, such as countries and their capital cities, companies and their CEOs, or simply different forms of the same word.", "figure_type": "plot", "visual_anchor": "Parallel dashed blue lines connecting brother-sister (left side, y≈-0.2 to 0.25) and nephew-niece (left side, y≈0.0 to 0.35)", "text_evidence": "By applying WEFAT, we are able to predict the percentage of people with a name who were women with Pearson's correlation coefficient of ρ = 0.84", "query_type": "comparison_explanation"}
{"query_id": "l1_1608.07187_1608.07187_fig_3_130", "query": "Why do the king-queen and duke-duchess vector pairs at the right side exhibit roughly constant differences despite different semantic contexts?", "answer": "The roughly constant vector differences reflect that pairs of words differing only by gender map to pairs of vectors with approximately constant vector difference, illustrating the algebraic relationships between related words independent of their specific royal or noble context.", "doc_id": "1608.07187", "figure_id": "1608.07187_fig_3", "figure_number": 3, "image_path": "data/mineru_output/1608.07187/1608.07187/hybrid_auto/images/1608.07187_page0_fig2.jpg", "caption": "Figure 3. A 2D projection (first two principal components) of the 300-dimensional vector space of the GloVe word embedding (Pennington et al., 2014). The lines illustrate algebraic relationships between related words: pairs of words that differ only by gender map to pairs of vectors whose vector difference is roughly constant. Similar algebraic relationships have been shown for other semantic relationships, such as countries and their capital cities, companies and their CEOs, or simply different forms of the same word.", "figure_type": "plot", "visual_anchor": "Parallel dashed lines connecting king-queen (right side, y≈-0.5 to -0.13) and duke-duchess (right side, y≈-0.16 to 0.29)", "text_evidence": "The lines illustrate algebraic relationships between related words: pairs of words that differ only by gender map to pairs of vectors whose vector difference is roughly constant.", "query_type": "comparison_explanation"}
{"query_id": "l1_1608.07187_1608.07187_fig_3_131", "query": "Does the heir-heiress vector relationship at horizontal position 0.1 exemplify the semantic relationships captured alongside countries-capitals and companies-CEOs?", "answer": "Yes, the heir-heiress pair demonstrates the same type of algebraic vector relationship shown for gender pairs, which is analogous to other semantic relationships like countries and capitals or companies and CEOs mentioned in the methodology.", "doc_id": "1608.07187", "figure_id": "1608.07187_fig_3", "figure_number": 3, "image_path": "data/mineru_output/1608.07187/1608.07187/hybrid_auto/images/1608.07187_page0_fig2.jpg", "caption": "Figure 3. A 2D projection (first two principal components) of the 300-dimensional vector space of the GloVe word embedding (Pennington et al., 2014). The lines illustrate algebraic relationships between related words: pairs of words that differ only by gender map to pairs of vectors whose vector difference is roughly constant. Similar algebraic relationships have been shown for other semantic relationships, such as countries and their capital cities, companies and their CEOs, or simply different forms of the same word.", "figure_type": "plot", "visual_anchor": "Dashed line connecting heir (y≈0.03) to heiress (y≈0.5) at horizontal position around 0.1", "text_evidence": "Similar algebraic relationships have been shown for other semantic relationships, such as countries and their capital cities, companies and their CEOs, or simply different forms of the same word.", "query_type": "value_context"}
{"query_id": "l1_1610.02413_1610.02413_fig_1_132", "query": "Why does the red star equal-odds optimum lie within the dark teal overlap region shared by both demographic groups?", "answer": "The optimization problem seeks a derived predictor with equalized odds, which requires satisfying constraints from both demographics simultaneously. The overlap region represents feasible solutions meeting both groups' constraints, where the optimal point must reside.", "doc_id": "1610.02413", "figure_id": "1610.02413_fig_1", "figure_number": 1, "image_path": "data/mineru_output/1610.02413/1610.02413/hybrid_auto/images/1610.02413_page0_fig0.jpg", "caption": "Figure 1: Finding the optimal equalized odds predictor (left), and equal opportunity predictor (right).", "figure_type": "diagram", "visual_anchor": "red star in the dark teal overlap region (left panel)", "text_evidence": "Combining Lemma 4.2 with Lemma 4.3, we see that the following optimization problem gives the optimal derived predictor with equalized odds", "query_type": "visual_definition"}
{"query_id": "l1_1610.02413_1610.02413_fig_1_133", "query": "How do the blue and green circles in the right panel differ from the red star solution shown in the left panel?", "answer": "The blue and green circles represent equal opportunity solutions for each demographic group separately (A=0 and A=1), while the red star represents the equalized odds optimum that jointly optimizes across both groups. Equal opportunity is a less restrictive constraint than equalized odds.", "doc_id": "1610.02413", "figure_id": "1610.02413_fig_1", "figure_number": 1, "image_path": "data/mineru_output/1610.02413/1610.02413/hybrid_auto/images/1610.02413_page0_fig0.jpg", "caption": "Figure 1: Finding the optimal equalized odds predictor (left), and equal opportunity predictor (right).", "figure_type": "diagram", "visual_anchor": "blue and green circles (right panel) versus red star (left panel)", "text_evidence": "Figure 1: Finding the optimal equalized odds predictor (left), and equal opportunity predictor (right).", "query_type": "comparison_explanation"}
{"query_id": "l1_1610.02413_1610.02413_fig_1_134", "query": "What do the first and second components of γₐ(Ŷ) represent geometrically in the achievable regions marked by blue and green?", "answer": "The first component represents the false positive rate of Ŷ within demographic A=a, and the second component represents the true positive rate. These two rates define the coordinates of points within each demographic's achievable region.", "doc_id": "1610.02413", "figure_id": "1610.02413_fig_1", "figure_number": 1, "image_path": "data/mineru_output/1610.02413/1610.02413/hybrid_auto/images/1610.02413_page0_fig0.jpg", "caption": "Figure 1: Finding the optimal equalized odds predictor (left), and equal opportunity predictor (right).", "figure_type": "diagram", "visual_anchor": "blue achievable region (A=0) and green achievable region (A=1)", "text_evidence": "The first component of $\\gamma _ { a } ( \\widehat { Y } )$ is the false positive rate of $\\widehat { Y }$ within the demographic satisfying $A = a$ . Similarly, the second component is the true", "query_type": "visual_definition"}
{"query_id": "l1_1610.02413_1610.02413_fig_4_135", "query": "Why is the black tangent line touching the blue ROC curve the profit-maximizing choice for each protected group?", "answer": "The tangent line represents the optimal threshold that maximizes profit within each group before applying fairness constraints. The horizontal gap between the ROC curve and this tangent line determines the cost for achieving a given true positive rate.", "doc_id": "1610.02413", "figure_id": "1610.02413_fig_4", "figure_number": 2, "image_path": "data/mineru_output/1610.02413/1610.02413/hybrid_auto/images/1610.02413_page0_fig3.jpg", "caption": "Figure 2: Finding the optimal equalized odds threshold predictor (middle), and equal opportunity threshold predictor (right). For the equal opportunity predictor, within each group the cost for a given true positive rate is proportional to the horizontal gap between the ROC curve and the profit-maximizing tangent line (i.e., the two curves on the left plot), so it is a convex function of the true positive rate (right). This lets us optimize it efficiently with ternary search.", "figure_type": "plot", "visual_anchor": "Black tangent line touching the blue ROC curve in the upper left region", "text_evidence": "within each group the cost for a given true positive rate is proportional to the horizontal gap between the ROC curve and the profit-maximizing tangent line", "query_type": "visual_definition"}
{"query_id": "l1_1610.02413_1610.02413_fig_4_136", "query": "How does the horizontal gap indicated by the lower-right arrow relate to optimizing the equal opportunity predictor?", "answer": "The horizontal gap between the ROC curve and the profit-maximizing tangent line is proportional to the cost for achieving a given true positive rate, which becomes a convex function that can be efficiently optimized using ternary search.", "doc_id": "1610.02413", "figure_id": "1610.02413_fig_4", "figure_number": 2, "image_path": "data/mineru_output/1610.02413/1610.02413/hybrid_auto/images/1610.02413_page0_fig3.jpg", "caption": "Figure 2: Finding the optimal equalized odds threshold predictor (middle), and equal opportunity threshold predictor (right). For the equal opportunity predictor, within each group the cost for a given true positive rate is proportional to the horizontal gap between the ROC curve and the profit-maximizing tangent line (i.e., the two curves on the left plot), so it is a convex function of the true positive rate (right). This lets us optimize it efficiently with ternary search.", "figure_type": "plot", "visual_anchor": "Black arrow pointing to horizontal gap between blue curve and tangent line in lower right", "text_evidence": "within each group the cost for a given true positive rate is proportional to the horizontal gap between the ROC curve and the profit-maximizing tangent line (i.e., the two curves on the left plot), so it is a convex function of the true positive rate", "query_type": "comparison_explanation"}
{"query_id": "l1_1610.02413_1610.02413_fig_4_137", "query": "Does the area above the diagonal line and below the blue ROC curve represent the feasible equalized odds predictor space?", "answer": "Yes, the feasible set of false/true positive rates for equalized odds predictors is the intersection of areas under the A-conditional ROC curves and above the main diagonal, as shown in the plot.", "doc_id": "1610.02413", "figure_id": "1610.02413_fig_4", "figure_number": 2, "image_path": "data/mineru_output/1610.02413/1610.02413/hybrid_auto/images/1610.02413_page0_fig3.jpg", "caption": "Figure 2: Finding the optimal equalized odds threshold predictor (middle), and equal opportunity threshold predictor (right). For the equal opportunity predictor, within each group the cost for a given true positive rate is proportional to the horizontal gap between the ROC curve and the profit-maximizing tangent line (i.e., the two curves on the left plot), so it is a convex function of the true positive rate (right). This lets us optimize it efficiently with ternary search.", "figure_type": "plot", "visual_anchor": "Area between the diagonal line from origin and the blue ROC curve", "text_evidence": "The feasible set of false/true positive rates of possible equalized odds predictors is thus the intersection of the areas under the $A$ -conditional ROC curves, and above the main diagonal", "query_type": "value_context"}
{"query_id": "l1_1610.02413_1610.02413_fig_7_138", "query": "Why does node R connect directly to both A and Y in deriving the optimal equal opportunity threshold predictor?", "answer": "The construction requires finding points on conditional ROC curves with the same true positive rates in both groups, necessitating R's connection to A and Y to capture group-conditional predictions.", "doc_id": "1610.02413", "figure_id": "1610.02413_fig_7", "figure_number": 3, "image_path": "data/mineru_output/1610.02413/1610.02413/hybrid_auto/images/1610.02413_page0_fig6.jpg", "caption": "Figure 3: Graphical model for the proof of Proposition 5.2.", "figure_type": "diagram", "visual_anchor": "Node R with directed edges to both node A and node Y", "text_evidence": "We only need to find points on the conditional ROC curves that have the same true positive rates in both groups.", "query_type": "comparison_explanation"}
{"query_id": "l1_1610.02413_1610.02413_fig_7_139", "query": "How does the central position of node A with connections to X, R, X', and Ỹ support the optimality characterization?", "answer": "The protected attribute A must be accessible from the Bayes optimal regressor R and connected to other variables to derive optimal equalized odds or equal opportunity predictors.", "doc_id": "1610.02413", "figure_id": "1610.02413_fig_7", "figure_number": 3, "image_path": "data/mineru_output/1610.02413/1610.02413/hybrid_auto/images/1610.02413_page0_fig6.jpg", "caption": "Figure 3: Graphical model for the proof of Proposition 5.2.", "figure_type": "diagram", "visual_anchor": "Central node A with edges to nodes X, R, X', and Ỹ", "text_evidence": "An optimal equalized odds predictor can be derived from the Bayes optimal regressor R and the protected attribute A. The same is true for an optimal equal opportunity predictor.", "query_type": "visual_definition"}
{"query_id": "l1_1610.02413_1610.02413_fig_7_140", "query": "Does the conditional Kolmogorov distance measure involve nodes R and A given their graphical relationship to Y?", "answer": "Yes, the conditional Kolmogorov distance is defined between random variables in the same probability space as A and Y, consistent with R's connections to both.", "doc_id": "1610.02413", "figure_id": "1610.02413_fig_7", "figure_number": 3, "image_path": "data/mineru_output/1610.02413/1610.02413/hybrid_auto/images/1610.02413_page0_fig6.jpg", "caption": "Figure 3: Graphical model for the proof of Proposition 5.2.", "figure_type": "diagram", "visual_anchor": "Nodes R and A both having paths to node Y", "text_evidence": "We define the conditional Kolmogorov distance between two random variables R , R ^ { \\prime } \\in [ 0 , 1 ] in the same probability space as A and Y", "query_type": "value_context"}
{"query_id": "l1_1610.02413_1610.02413_fig_8_141", "query": "Does node A's direct connection to Y in this graphical model relate to oblivious black box discrimination tests?", "answer": "Yes, the dependency structure in this graphical model represents Scenario I, which is analyzed to determine whether oblivious tests can identify discriminatory predictions in different dependency scenarios.", "doc_id": "1610.02413", "figure_id": "1610.02413_fig_8", "figure_number": 4, "image_path": "data/mineru_output/1610.02413/1610.02413/hybrid_auto/images/1610.02413_page0_fig7.jpg", "caption": "Figure 4: Graphical model for Scenario I.", "figure_type": "diagram", "visual_anchor": "Node A with direct arrow to node Y in upper left portion", "text_evidence": "Before turning to analyzing data, we pause to consider to what extent \"black box\" oblivious tests like ours can identify discriminatory predictions.", "query_type": "value_context"}
{"query_id": "l1_1610.02413_1610.02413_fig_8_142", "query": "Why does node R receive input from X₁ but node R̄ remains isolated in Scenario I's structure?", "answer": "The isolated R̄ node and connected R node represent different aspects of the dependency structure being analyzed. This distinction is critical for understanding how these two scenarios differ in their interpretations from the discrimination identification perspective.", "doc_id": "1610.02413", "figure_id": "1610.02413_fig_8", "figure_number": 4, "image_path": "data/mineru_output/1610.02413/1610.02413/hybrid_auto/images/1610.02413_page0_fig7.jpg", "caption": "Figure 4: Graphical model for Scenario I.", "figure_type": "diagram", "visual_anchor": "Node R̄ isolated on right side versus node R connected to X₁ and X₂", "text_evidence": "we introduce two possible scenarios for the dependency structure of the score, the target and the protected attribute. We will argue that while these two scenarios can have fundamentally different interpretations from the point of view o", "query_type": "comparison_explanation"}
{"query_id": "l1_1610.02413_1610.02413_fig_8_143", "query": "What causes both Y and R to point toward X₂ in the depicted dependency structure?", "answer": "X₂ depends on both Y and R in this graphical model, representing how the score, target, and protected attribute interact in Scenario I's dependency structure.", "doc_id": "1610.02413", "figure_id": "1610.02413_fig_8", "figure_number": 4, "image_path": "data/mineru_output/1610.02413/1610.02413/hybrid_auto/images/1610.02413_page0_fig7.jpg", "caption": "Figure 4: Graphical model for Scenario I.", "figure_type": "diagram", "visual_anchor": "Node X₂ receiving arrows from both node Y (top) and node R (bottom)", "text_evidence": "Scenario I Consider the dependency structure depicted in Figure 4.", "query_type": "anomaly_cause"}
{"query_id": "l1_1610.02413_1610.02413_fig_9_144", "query": "In Scenario II, does node X3 mediate between A and Y as shown by the arrows?", "answer": "Yes, in Scenario II depicted in the graphical model, X3 acts as a mediator between A and Y, with A pointing to X3 and X3 pointing to Y.", "doc_id": "1610.02413", "figure_id": "1610.02413_fig_9", "figure_number": 5, "image_path": "data/mineru_output/1610.02413/1610.02413/hybrid_auto/images/1610.02413_page0_fig8.jpg", "caption": "Figure 5: Graphical model for Scenario II.", "figure_type": "diagram", "visual_anchor": "Arrow from A to X3 and arrow from X3 to Y", "text_evidence": "Scenario II Now consider the dependency structure depicted in Figure 5.", "query_type": "visual_definition"}
{"query_id": "l1_1610.02413_1610.02413_fig_9_145", "query": "Why is the optimal score R* connected only to X3 rather than directly to A in this model?", "answer": "The optimal score R* is based only on a directly predictive feature (X3), not on A or its surrogate, distinguishing this scenario from one where R* depends directly on A.", "doc_id": "1610.02413", "figure_id": "1610.02413_fig_9", "figure_number": 5, "image_path": "data/mineru_output/1610.02413/1610.02413/hybrid_auto/images/1610.02413_page0_fig8.jpg", "caption": "Figure 5: Graphical model for Scenario II.", "figure_type": "diagram", "visual_anchor": "Arrow from X3 to R* with no arrow from A to R*", "text_evidence": "The optimal score $R ^ { * }$ is in one case based directly on $A$ or its surrogate, and in another only on a directly predictive feature", "query_type": "comparison_explanation"}
{"query_id": "l1_1610.02413_1610.02413_fig_9_146", "query": "How does the dependency structure with R connected to A relate to unidentifiability under equalized odds?", "answer": "The two scenarios with different dependency structures are indistinguishable using any oblivious test based only on target labels and protected attributes, suggesting a shortcoming of equalized odds.", "doc_id": "1610.02413", "figure_id": "1610.02413_fig_9", "figure_number": 5, "image_path": "data/mineru_output/1610.02413/1610.02413/hybrid_auto/images/1610.02413_page0_fig8.jpg", "caption": "Figure 5: Graphical model for Scenario II.", "figure_type": "diagram", "visual_anchor": "Node R with arrow from A", "text_evidence": "this is not apparent by considering the equalized odds criterion, suggesting a possible shortcoming of equalized odds. In fact, as we will now see, the two scenarios are indistinguishable using any oblivious test.", "query_type": "value_context"}
{"query_id": "l1_1610.02413_1610.02413_fig_10_147", "query": "Why does node A connect to both X3 and X1 when the optimal score is based directly on A or its surrogate?", "answer": "In scenario I, the optimal score R* depends directly on A or its surrogate, requiring A to have direct influence on both pathways (through X3 to Y and through X1-X2 to Y) shown in the dependency structure.", "doc_id": "1610.02413", "figure_id": "1610.02413_fig_10", "figure_number": 6, "image_path": "data/mineru_output/1610.02413/1610.02413/hybrid_auto/images/1610.02413_page0_fig9.jpg", "caption": "Figure 6: Two possible directed dependency structures for the variables in scenarios I and II. The undirected (infrastructure graph) versions of both graphs are also possible.", "figure_type": "diagram", "visual_anchor": "Node A with directed edges to both X3 (top path) and X1 (bottom path)", "text_evidence": "The optimal score $R ^ { * }$ is in one case based directly on $A$ or its surrogate, and in another only on a directly predictive feature", "query_type": "comparison_explanation"}
{"query_id": "l1_1610.02413_1610.02413_fig_10_148", "query": "How does the directed edge from X2 to Y support the scenario where optimal score depends only on a directly predictive feature?", "answer": "The X2 to Y edge represents scenario II where the optimal score R* is based only on a directly predictive feature (X2), independent of the protected attribute A's direct influence.", "doc_id": "1610.02413", "figure_id": "1610.02413_fig_10", "figure_number": 6, "image_path": "data/mineru_output/1610.02413/1610.02413/hybrid_auto/images/1610.02413_page0_fig9.jpg", "caption": "Figure 6: Two possible directed dependency structures for the variables in scenarios I and II. The undirected (infrastructure graph) versions of both graphs are also possible.", "figure_type": "diagram", "visual_anchor": "Directed edge from node X2 to node Y in the bottom-right portion", "text_evidence": "The optimal score $R ^ { * }$ is in one case based directly on $A$ or its surrogate, and in another only on a directly predictive feature, but this is not apparent by considering the equalized odds criterion", "query_type": "value_context"}
{"query_id": "l1_1610.02413_1610.02413_fig_10_149", "query": "Why are both directed structures shown when no test using only target labels and protected attribute can distinguish the scenarios?", "answer": "The two directed structures represent fundamentally different causal mechanisms that produce indistinguishable distributions under oblivious tests, demonstrating that equalized odds cannot reveal whether the score depends directly on A or only on predictive features.", "doc_id": "1610.02413", "figure_id": "1610.02413_fig_10", "figure_number": 6, "image_path": "data/mineru_output/1610.02413/1610.02413/hybrid_auto/images/1610.02413_page0_fig9.jpg", "caption": "Figure 6: Two possible directed dependency structures for the variables in scenarios I and II. The undirected (infrastructure graph) versions of both graphs are also possible.", "figure_type": "diagram", "visual_anchor": "Two distinct directed dependency structures with identical node sets but different causal interpretations", "text_evidence": "the two scenarios are indistinguishable using any oblivious test. That is, no test based only on the target labels, the protecte", "query_type": "anomaly_cause"}
{"query_id": "l1_1610.02413_1610.02413_fig_12_150", "query": "Does the steeper rise of the blue Asian curve between FICO 500-700 relate to optimal classification depending on summed variables?", "answer": "The steeper Asian curve reflects optimal classification based on combined predictors, as optimal classification only depends on the sum x₁ + x₂, making R* = X₁ + X₂ optimal.", "doc_id": "1610.02413", "figure_id": "1610.02413_fig_12", "figure_number": 7, "image_path": "data/mineru_output/1610.02413/1610.02413/hybrid_auto/images/1610.02413_page0_fig11.jpg", "caption": "Figure 7: These two marginals, and the number of people per group, constitute our input data.", "figure_type": "plot", "visual_anchor": "Solid blue line (Asian) showing steeper slope between FICO 500-700", "text_evidence": "optimal classification only depends on $x _ { 1 } + x _ { 2 }$ and so $R ^ { * } = X _ { 1 } + X _ { 2 }$ is optimal", "query_type": "comparison_explanation"}
{"query_id": "l1_1610.02413_1610.02413_fig_12_151", "query": "Why does the cyan Black curve lag below others around FICO 600, given monotone function optimality in scenario II?", "answer": "The lag reflects that in scenario II, any monotone function of P(Y|X₃) is optimal and remains optimal even with dependence on A, suggesting the Black curve follows a different conditional distribution pattern.", "doc_id": "1610.02413", "figure_id": "1610.02413_fig_12", "figure_number": 7, "image_path": "data/mineru_output/1610.02413/1610.02413/hybrid_auto/images/1610.02413_page0_fig11.jpg", "caption": "Figure 7: These two marginals, and the number of people per group, constitute our input data.", "figure_type": "plot", "visual_anchor": "Dotted cyan line (Black) positioned lower than other curves at FICO 600", "text_evidence": "since $P ( Y | X _ { 3 } )$ is monotone in $X _ { 3 }$ , any monotone function of it is optimal (unconstrained), and the dependency structure implies its optimal even if we allow dependence on A", "query_type": "anomaly_cause"}
{"query_id": "l1_1610.02413_1610.02413_fig_12_152", "query": "Do the converging curves near 100% at FICO 800 constitute the marginal distributions that form the input data?", "answer": "Yes, these marginal distributions by racial group, along with the number of people per group, constitute the input data as stated in the figure caption.", "doc_id": "1610.02413", "figure_id": "1610.02413_fig_12", "figure_number": 7, "image_path": "data/mineru_output/1610.02413/1610.02413/hybrid_auto/images/1610.02413_page0_fig11.jpg", "caption": "Figure 7: These two marginals, and the number of people per group, constitute our input data.", "figure_type": "plot", "visual_anchor": "All four curves (Asian, White, Hispanic, Black) converging near 100% at FICO score 800", "text_evidence": "These two marginals, and the number of people per group, constitute our input data", "query_type": "visual_definition"}
{"query_id": "l1_1610.02413_1610.02413_fig_14_153", "query": "Why does the gray shaded region start at FICO 620 if that threshold yields 82% non-default overall?", "answer": "The single threshold of 620 is chosen because it corresponds to an 82% non-default rate overall across all groups, representing the common lending standard.", "doc_id": "1610.02413", "figure_id": "1610.02413_fig_14", "figure_number": 8, "image_path": "data/mineru_output/1610.02413/1610.02413/hybrid_auto/images/1610.02413_page0_fig13.jpg", "caption": "Figure 8: The common FICO threshold of 620 corresponds to a non-default rate of $8 2 \\%$ Rescaling the $x$ axis to represent the within-group thresholds (right), $\\operatorname* { P r } [ \\widehat { Y } = 1 \\mid Y = 1 , A ]$ is the fraction of the area under the curve that is shaded. This means black non-defaulters are much less likely to qualify for loans than white or Asian ones, so a race blind score threshold violates our fairness definitions.", "figure_type": "plot", "visual_anchor": "gray shaded region starting at FICO score 620 with 82% non-default rate", "text_evidence": "Hence it will pick the single threshold at which 82% of people do not default overall, shown in Figure 8.", "query_type": "value_context"}
{"query_id": "l1_1610.02413_1610.02413_fig_14_154", "query": "Does the dotted cyan curve reaching only 70% at FICO 620 demonstrate why race-blind thresholds violate fairness?", "answer": "Yes, because black non-defaulters have much lower qualification rates at the 620 threshold compared to other groups, violating equal opportunity fairness definitions.", "doc_id": "1610.02413", "figure_id": "1610.02413_fig_14", "figure_number": 8, "image_path": "data/mineru_output/1610.02413/1610.02413/hybrid_auto/images/1610.02413_page0_fig13.jpg", "caption": "Figure 8: The common FICO threshold of 620 corresponds to a non-default rate of $8 2 \\%$ Rescaling the $x$ axis to represent the within-group thresholds (right), $\\operatorname* { P r } [ \\widehat { Y } = 1 \\mid Y = 1 , A ]$ is the fraction of the area under the curve that is shaded. This means black non-defaulters are much less likely to qualify for loans than white or Asian ones, so a race blind score threshold violates our fairness definitions.", "figure_type": "plot", "visual_anchor": "dotted cyan line (Black) at approximately 70% when other curves reach 82% at FICO 620", "text_evidence": "This means black non-defaulters are much less likely to qualify for loans than white or Asian ones, so a race blind score threshold violates our fairness definitions.", "query_type": "comparison_explanation"}
{"query_id": "l1_1610.02413_1610.02413_fig_14_155", "query": "How does the shaded area fraction relate to the probability that true non-defaulters qualify for loans?", "answer": "The shaded area fraction represents Pr[Ŷ=1|Y=1,A], the probability that actual non-defaulters from each group are predicted to qualify, showing disparities across racial groups.", "doc_id": "1610.02413", "figure_id": "1610.02413_fig_14", "figure_number": 8, "image_path": "data/mineru_output/1610.02413/1610.02413/hybrid_auto/images/1610.02413_page0_fig13.jpg", "caption": "Figure 8: The common FICO threshold of 620 corresponds to a non-default rate of $8 2 \\%$ Rescaling the $x$ axis to represent the within-group thresholds (right), $\\operatorname* { P r } [ \\widehat { Y } = 1 \\mid Y = 1 , A ]$ is the fraction of the area under the curve that is shaded. This means black non-defaulters are much less likely to qualify for loans than white or Asian ones, so a race blind score threshold violates our fairness definitions.", "figure_type": "plot", "visual_anchor": "gray shaded region representing area under the curve from FICO 620 onwards", "text_evidence": "Rescaling the x axis to represent the within-group thresholds (right), Pr[Ŷ=1|Y=1,A] is the fraction of the area under the curve that is shaded.", "query_type": "visual_definition"}
{"query_id": "l1_1610.02413_1610.02413_fig_16_156", "query": "Do the Asian blue diamonds at Max profit and Demography confirm that optimal thresholds can be computed efficiently?", "answer": "Yes, the figure displays computed thresholds for Asian applicants across all five fairness definitions (ranging from ~650 to ~720), demonstrating that Section 4's efficient computation method successfully produces results for multiple fairness criteria.", "doc_id": "1610.02413", "figure_id": "1610.02413_fig_16", "figure_number": 9, "image_path": "data/mineru_output/1610.02413/1610.02413/hybrid_auto/images/1610.02413_page0_fig15.jpg", "caption": "Figure 9: FICO thresholds for various definitions of fairness. The equal odds method does not give a single threshold, but instead $\\operatorname* { P r } [ \\widehat { Y } = 1 \\mid R , A ]$ increases over some not uniquely defined range; we pick the one containing the fewest people. Observe that, within each race, the equal opportunity threshold and average equal odds threshold lie between the max profit threshold and equal demography thresholds.", "figure_type": "plot", "visual_anchor": "blue diamonds for Asian group spanning from Max profit (~650) to Demography (~720)", "text_evidence": "As shown in Section 4, the optimal thresholds can be computed efficiently; the results are shown in Figure 9.", "query_type": "value_context"}
{"query_id": "l1_1610.02413_1610.02413_fig_16_157", "query": "Why does the Hispanic red square at Max profit exceed 650 while the equal odds red square falls below 600?", "answer": "The max profit threshold prioritizes lender revenue and lies between opportunity and demography thresholds as stated. Equal odds constrains to balance error rates across races, resulting in a lower threshold for Hispanic applicants to achieve fairness.", "doc_id": "1610.02413", "figure_id": "1610.02413_fig_16", "figure_number": 9, "image_path": "data/mineru_output/1610.02413/1610.02413/hybrid_auto/images/1610.02413_page0_fig15.jpg", "caption": "Figure 9: FICO thresholds for various definitions of fairness. The equal odds method does not give a single threshold, but instead $\\operatorname* { P r } [ \\widehat { Y } = 1 \\mid R , A ]$ increases over some not uniquely defined range; we pick the one containing the fewest people. Observe that, within each race, the equal opportunity threshold and average equal odds threshold lie between the max profit threshold and equal demography thresholds.", "figure_type": "plot", "visual_anchor": "Hispanic red squares at Max profit (~670) and Equal odds (~590)", "text_evidence": "Observe that, within each race, the equal opportunity threshold and average equal odds threshold lie between the max profit threshold and equal demography thresholds.", "query_type": "comparison_explanation"}
{"query_id": "l1_1610.02413_1610.02413_fig_16_158", "query": "Does the green circle range spanning from ~520 to ~680 address demographic parity's conceptual shortcomings?", "answer": "Yes, the figure displays thresholds under multiple fairness definitions for White applicants. The proposed equal opportunity and equal odds measures (between Max profit and Demography) remedy demographic parity's shortcomings by lying between profit-maximizing and purely demographic criteria.", "doc_id": "1610.02413", "figure_id": "1610.02413_fig_16", "figure_number": 9, "image_path": "data/mineru_output/1610.02413/1610.02413/hybrid_auto/images/1610.02413_page0_fig15.jpg", "caption": "Figure 9: FICO thresholds for various definitions of fairness. The equal odds method does not give a single threshold, but instead $\\operatorname* { P r } [ \\widehat { Y } = 1 \\mid R , A ]$ increases over some not uniquely defined range; we pick the one containing the fewest people. Observe that, within each race, the equal opportunity threshold and average equal odds threshold lie between the max profit threshold and equal demography thresholds.", "figure_type": "plot", "visual_anchor": "White green circles spanning from Equal odds (~520) to Demography (~680)", "text_evidence": "We proposed a fairness measure that accomplishes two important desiderata. First, it remedies the main conceptual shortcomings of demographic parity as a fairness notion.", "query_type": "value_context"}
{"query_id": "l1_1610.02413_1610.02413_fig_18_159", "query": "Why does equality of opportunity select points along the horizontal line at y≈0.4 despite different group ROC curves?", "answer": "Equality of opportunity enforces equal true positive rates across groups, constraining selections to the same horizontal line regardless of individual group ROC performance.", "doc_id": "1610.02413", "figure_id": "1610.02413_fig_18", "figure_number": 10, "image_path": "data/mineru_output/1610.02413/1610.02413/hybrid_auto/images/1610.02413_page0_fig17.jpg", "caption": "Figure 10: The ROC curve for using FICO score to identify non-defaulters. Within a group, we can achieve any convex combination of these outcomes. Equality of opportunity picks points along the same horizontal line. Equal odds picks a point below all lines.", "figure_type": "plot", "visual_anchor": "Horizontal line at approximately y=0.4 intersecting all four ROC curves", "text_evidence": "Equality of opportunity picks points along the same horizontal line.", "query_type": "comparison_explanation"}
{"query_id": "l1_1610.02413_1610.02413_fig_18_160", "query": "How do the ROC curves enable any convex combination of outcomes within each demographic group?", "answer": "Points along each ROC curve represent different classification thresholds. Any weighted combination of two threshold choices produces a point on the line segment connecting them, creating convex combinations.", "doc_id": "1610.02413", "figure_id": "1610.02413_fig_18", "figure_number": 10, "image_path": "data/mineru_output/1610.02413/1610.02413/hybrid_auto/images/1610.02413_page0_fig17.jpg", "caption": "Figure 10: The ROC curve for using FICO score to identify non-defaulters. Within a group, we can achieve any convex combination of these outcomes. Equality of opportunity picks points along the same horizontal line. Equal odds picks a point below all lines.", "figure_type": "plot", "visual_anchor": "Four distinct ROC curves (blue solid, green dashed, red dash-dot, cyan dotted) showing continuous paths from origin", "text_evidence": "Within a group, we can achieve any convex combination of these outcomes.", "query_type": "visual_definition"}
{"query_id": "l1_1610.02413_1610.02413_fig_18_161", "query": "Does the green dashed curve reaching y≈0.9 at x≈0.2 support remedying demographic parity shortcomings?", "answer": "Yes, the White group's superior ROC performance demonstrates that demographic parity would ignore predictive differences. The proposed fairness measure addresses this by considering true positive rates instead.", "doc_id": "1610.02413", "figure_id": "1610.02413_fig_18", "figure_number": 10, "image_path": "data/mineru_output/1610.02413/1610.02413/hybrid_auto/images/1610.02413_page0_fig17.jpg", "caption": "Figure 10: The ROC curve for using FICO score to identify non-defaulters. Within a group, we can achieve any convex combination of these outcomes. Equality of opportunity picks points along the same horizontal line. Equal odds picks a point below all lines.", "figure_type": "plot", "visual_anchor": "Green dashed line (White) reaching approximately y=0.9 at x=0.2, higher than other curves", "text_evidence": "First, it remedies the main conceptual shortcomings of demographic parity as a fairness notion.", "query_type": "value_context"}
{"query_id": "l1_1610.02413_1610.02413_fig_20_162", "query": "Why does the blue Max profit bar reach 0.85 for Asian borrowers if fairness classifiers are built with independency constraints?", "answer": "The Max profit approach optimizes loan approval without fairness constraints, achieving 0.85 approval rate for Asian non-defaulters. Building classifiers with independency constraints, as mentioned in the references, would lower this rate to match fairness notions like the yellow Demography bar at approximately 0.63.", "doc_id": "1610.02413", "figure_id": "1610.02413_fig_20", "figure_number": 11, "image_path": "data/mineru_output/1610.02413/1610.02413/hybrid_auto/images/1610.02413_page0_fig19.jpg", "caption": "Figure 11: On the left, we see the fraction of non-defaulters that would get loans. On the right, we see the profit achievable for each notion of fairness, as a function of the false positive/negative trade-off.", "figure_type": "plot", "visual_anchor": "Blue bar at approximately 0.85 for Asian demographic group", "text_evidence": "Building classifiers with independency constraints. In In Proc. IEEE International Conference on Data Mining Workshops, pages 13–18, 2009.", "query_type": "comparison_explanation"}
{"query_id": "l1_1610.02413_1610.02413_fig_20_163", "query": "Does the green Single threshold bar at 0.55 for Black borrowers align with claims about disparate impact in big data?", "answer": "Yes, the green Single threshold bar showing only 0.55 loan approval for Black non-defaulters, compared to 0.82 for Asian borrowers, demonstrates disparate impact. This significant disparity across demographics exemplifies how algorithmic decisions can produce unequal outcomes even when targeting non-defaulters.", "doc_id": "1610.02413", "figure_id": "1610.02413_fig_20", "figure_number": 11, "image_path": "data/mineru_output/1610.02413/1610.02413/hybrid_auto/images/1610.02413_page0_fig19.jpg", "caption": "Figure 11: On the left, we see the fraction of non-defaulters that would get loans. On the right, we see the profit achievable for each notion of fairness, as a function of the false positive/negative trade-off.", "figure_type": "plot", "visual_anchor": "Green bar at approximately 0.55 for Black demographic group", "text_evidence": "Solon Barocas and Andrew Selbst. Big data's disparate impact. California Law Review, 104, 2016.", "query_type": "value_context"}
{"query_id": "l1_1610.02413_1610.02413_fig_20_164", "query": "Why do the yellow Demography bars show 0.63 to 0.90 range across demographics when learning fair classifiers?", "answer": "The yellow Demography bars vary from approximately 0.63 for Asian to 0.90 for Black borrowers because demographic fairness constraints aim to equalize approval rates across groups. This approach, related to learning fair classifiers, adjusts thresholds per demographic to balance loan access, though perfect equalization is not achieved.", "doc_id": "1610.02413", "figure_id": "1610.02413_fig_20", "figure_number": 11, "image_path": "data/mineru_output/1610.02413/1610.02413/hybrid_auto/images/1610.02413_page0_fig19.jpg", "caption": "Figure 11: On the left, we see the fraction of non-defaulters that would get loans. On the right, we see the profit achievable for each notion of fairness, as a function of the false positive/negative trade-off.", "figure_type": "plot", "visual_anchor": "Yellow bars ranging from approximately 0.63 (Asian) to 0.90 (Black)", "text_evidence": "Muhammad Bilal Zafar, Isabel Valera, Manuel Gomez Rodriguez, and Krishna P Gummadi. Learning fair classifiers. CoRR, abs:1507.05259, 2015.", "query_type": "comparison_explanation"}
{"query_id": "l1_1610.07524_1610.07524_fig_1_165", "query": "Do the overlapping Black and White bars at each decile score verify the test fairness condition adherence?", "answer": "Yes, the close overlap of Black and White bars across all decile scores demonstrates that P(Y=1|S=s,R) is similar for both races at each score level, which Flores et al. verified using logistic regression as adherence to test fairness.", "doc_id": "1610.07524", "figure_id": "1610.07524_fig_1", "figure_number": 1, "image_path": "data/mineru_output/1610.07524/1610.07524/hybrid_auto/images/1610.07524_page0_fig0.jpg", "caption": "Figure 1: Plot shows $\\mathbb { P } ( Y = 1 \\mid S = s , R )$ for the COM-PAS decile score, with $R \\in \\{ \\mathrm { B l a c k } , \\mathrm { W h i t e } \\}$ . Error bars represent 95% confidence intervals.", "figure_type": "plot", "visual_anchor": "overlapping gray and yellow bars at decile scores 1-10", "text_evidence": "We can see that the COMPAS RPI appears to adhere well to the test fairness condition. In their response to the ProPublica investigation, Flores et al. [10] further verify this adherence using logistic regression.", "query_type": "comparison_explanation"}
{"query_id": "l1_1610.07524_1610.07524_fig_1_166", "query": "Why does the observed probability rise from approximately 0.20 at decile 1 to 0.85 at decile 10?", "answer": "The increasing probability across decile scores reflects that higher COMPAS scores indicate higher risk of recidivism, as the decile score is designed to stratify defendants by predicted recidivism risk.", "doc_id": "1610.07524", "figure_id": "1610.07524_fig_1", "figure_number": 1, "image_path": "data/mineru_output/1610.07524/1610.07524/hybrid_auto/images/1610.07524_page0_fig0.jpg", "caption": "Figure 1: Plot shows $\\mathbb { P } ( Y = 1 \\mid S = s , R )$ for the COM-PAS decile score, with $R \\in \\{ \\mathrm { B l a c k } , \\mathrm { W h i t e } \\}$ . Error bars represent 95% confidence intervals.", "figure_type": "plot", "visual_anchor": "Black bars rising from 0.20 at decile 1 to 0.85 at decile 10", "text_evidence": "Figure 1 shows a plot of the observed recidivism rates across all possible values of the COMPAS score.", "query_type": "value_context"}
{"query_id": "l1_1610.07524_1610.07524_fig_1_167", "query": "How do the error bars widening at decile 10 relate to creating the coarsened classifier mentioned for simplification?", "answer": "The wider confidence intervals at extreme decile scores suggest uncertainty in fine-grained predictions, motivating the introduction of a coarsened binary score (high-risk vs. low-risk) that simplifies the classifier into a confusion matrix for error rate discussion.", "doc_id": "1610.07524", "figure_id": "1610.07524_fig_1", "figure_number": 1, "image_path": "data/mineru_output/1610.07524/1610.07524/hybrid_auto/images/1610.07524_page0_fig0.jpg", "caption": "Figure 1: Plot shows $\\mathbb { P } ( Y = 1 \\mid S = s , R )$ for the COM-PAS decile score, with $R \\in \\{ \\mathrm { B l a c k } , \\mathrm { W h i t e } \\}$ . Error bars represent 95% confidence intervals.", "figure_type": "plot", "visual_anchor": "wide error bars at decile 10 compared to middle decile scores", "text_evidence": "To facilitate a simpler discussion of error rates, we introduce the coarsened score $S _ { c }$ , wh", "query_type": "anomaly_cause"}
{"query_id": "l1_1610.07524_1610.07524_fig_2_168", "query": "Why does the gray bar reach approximately 0.9 at seven to ten priors, supporting claims about higher prevalence groups?", "answer": "When using a test-fair RPI in populations where recidivism prevalence differs, the higher recidivism prevalence group will generally have a higher FPR. This explains why Black defendants (gray bars) show escalating false positive rates reaching 0.9, resulting in greater penalties for defendants in the higher prevalence group.", "doc_id": "1610.07524", "figure_id": "1610.07524_fig_2", "figure_number": 2, "image_path": "data/mineru_output/1610.07524/1610.07524/hybrid_auto/images/1610.07524_page0_fig1.jpg", "caption": "Figure 2: False positive rates across prior record count for defendants charged with a Misdemeanor offense. Plot is based on assessing a defendant as “high-risk” if their COMPAS decile score is $> 4$ . Error bars represent 95% confidence intervals.   ", "figure_type": "plot", "visual_anchor": "Gray bar at 7-10 priors reaching approximately 0.9 false positive rate", "text_evidence": "When using a test-fair RPI in populations where recidivism prevalence differs across groups, it will generally be the case that the higher recidivism prevalence group will have a higher FPR and lower FNR. From equations (3.2) and (3.3), we can see that this would result in greater penalties for defendants in the higher prevalence group", "query_type": "comparison_explanation"}
{"query_id": "l1_1610.07524_1610.07524_fig_2_169", "query": "Does the gray bar pattern from 0 to greater than ten priors illustrate disparate impact from predictive bias-free instruments?", "answer": "Yes, the escalating gray bars demonstrate how disparate impact can result even when using a recidivism prediction instrument free from predictive bias. The primary contribution was showing this phenomenon in the simple binary risk assessment setting.", "doc_id": "1610.07524", "figure_id": "1610.07524_fig_2", "figure_number": 2, "image_path": "data/mineru_output/1610.07524/1610.07524/hybrid_auto/images/1610.07524_page0_fig1.jpg", "caption": "Figure 2: False positive rates across prior record count for defendants charged with a Misdemeanor offense. Plot is based on assessing a defendant as “high-risk” if their COMPAS decile score is $> 4$ . Error bars represent 95% confidence intervals.   ", "figure_type": "plot", "visual_anchor": "Increasing trend in gray bars across all five prior count categories from approximately 0.2 to 0.9", "text_evidence": "The primary contribution of this paper was to show how disparate impact can result from the use of a recidivism prediction instrument that is known to be free from predictive bias. Our analysis focussed on the simple setting where a binary risk assessment was used to inform a binary penalty policy.", "query_type": "value_context"}
{"query_id": "l1_1610.07524_1610.07524_fig_2_170", "query": "Why do differences persist between gray and orange bars within each prior category despite equal overall FPR claims?", "answer": "Even if overall FPR is equal between Black and White defendants, differences in error rates can still appear within individual prior record score categories. This explains the persistent gap between gray and orange bars across all five groupings shown.", "doc_id": "1610.07524", "figure_id": "1610.07524_fig_2", "figure_number": 2, "image_path": "data/mineru_output/1610.07524/1610.07524/hybrid_auto/images/1610.07524_page0_fig1.jpg", "caption": "Figure 2: False positive rates across prior record count for defendants charged with a Misdemeanor offense. Plot is based on assessing a defendant as “high-risk” if their COMPAS decile score is $> 4$ . Error bars represent 95% confidence intervals.   ", "figure_type": "plot", "visual_anchor": "Consistent separation between gray and orange bars across all five x-axis categories (0, 1-3, 4-6, 7-10, >10)", "text_evidence": "That is, even if $\\mathrm { F P R } _ { b } = \\mathrm { F P R } _ { w }$ , we may still see differences in error rates within prior record score categories (see e.g., Figure 2).", "query_type": "anomaly_cause"}
{"query_id": "l1_1610.08452_1610.08452_fig_1_171", "query": "Why does C1's decision to stop both Female 1 and Male 1 demonstrate disparate impact despite identical non-sensitive attributes?", "answer": "C1 shows disparate impact because it stops individuals with the same bulge (1) and crime proximity (✓) values, but the decision disproportionately affects members based on their gender group, which is the sensitive attribute.", "doc_id": "1610.08452", "figure_id": "1610.08452_fig_1", "figure_number": 1, "image_path": "data/mineru_output/1610.08452/1610.08452/hybrid_auto/images/1610.08452_page0_fig0.jpg", "caption": "Figure 1: Decisions of three fictitious classifiers $\\mathbf { C _ { 1 } }$ , C2 and $\\mathbf { C _ { 3 } }$ ) on whether (1) or not (0) to stop a pedestrian on the suspicion of possessing an illegal weapon. Gender is a sensitive attribute, whereas the other two attributes (suspicious bulge in clothing and proximity to a crime scene) are non-sensitive. Ground truth on whether the person is actually in possession of an illegal weapon is also shown.", "figure_type": "table", "visual_anchor": "C1 column showing decision 1 for both Female 1 and Male 1 with identical Bulge=1 and Crime=✓", "text_evidence": "there is disparate impact when the decision outcomes disproportionately benefit or hurt members of certain sensitive attribute value groups", "query_type": "comparison_explanation"}
{"query_id": "l1_1610.08452_1610.08452_fig_1_172", "query": "Does C2's decision of 0 for Male 2 with Bulge=0 minimize misclassifications given the training objective described?", "answer": "No, C2's decision creates a false negative since Male 2's ground truth is 1 (actually possessing a weapon), which represents a misclassification error that training aims to minimize.", "doc_id": "1610.08452", "figure_id": "1610.08452_fig_1", "figure_number": 1, "image_path": "data/mineru_output/1610.08452/1610.08452/hybrid_auto/images/1610.08452_page0_fig0.jpg", "caption": "Figure 1: Decisions of three fictitious classifiers $\\mathbf { C _ { 1 } }$ , C2 and $\\mathbf { C _ { 3 } }$ ) on whether (1) or not (0) to stop a pedestrian on the suspicion of possessing an illegal weapon. Gender is a sensitive attribute, whereas the other two attributes (suspicious bulge in clothing and proximity to a crime scene) are non-sensitive. Ground truth on whether the person is actually in possession of an illegal weapon is also shown.", "figure_type": "table", "visual_anchor": "C2 column showing decision 0 for Male 2 with Ground Truth=1", "text_evidence": "In order to maximize the utility of these systems (or, classifiers), their training involves minimizing the errors (or, misclassifications) over the given historical data", "query_type": "anomaly_cause"}
{"query_id": "l1_1610.08452_1610.08452_fig_1_173", "query": "How does C3's checkmark pattern across all three individuals illustrate the distinction between sensitive and non-sensitive attributes?", "answer": "C3 stops all individuals regardless of Gender (sensitive attribute) and regardless of varying Bulge and Crime values (non-sensitive attributes), showing decisions that ignore the distinction between attribute types.", "doc_id": "1610.08452", "figure_id": "1610.08452_fig_1", "figure_number": 1, "image_path": "data/mineru_output/1610.08452/1610.08452/hybrid_auto/images/1610.08452_page0_fig0.jpg", "caption": "Figure 1: Decisions of three fictitious classifiers $\\mathbf { C _ { 1 } }$ , C2 and $\\mathbf { C _ { 3 } }$ ) on whether (1) or not (0) to stop a pedestrian on the suspicion of possessing an illegal weapon. Gender is a sensitive attribute, whereas the other two attributes (suspicious bulge in clothing and proximity to a crime scene) are non-sensitive. Ground truth on whether the person is actually in possession of an illegal weapon is also shown.", "figure_type": "table", "visual_anchor": "C3 column showing all checkmarks for Female 1, Male 1, and Male 2", "text_evidence": "Gender is a sensitive attribute, whereas the other two attributes (suspicious bulge in clothing and proximity to a crime scene) are non-sensitive", "query_type": "visual_definition"}
{"query_id": "l1_1610.08452_1610.08452_fig_2_174", "query": "Why do the blue asterisks decrease from 0.25 to 0.13 while magenta squares rise, achieving the scenario where disparate mistreatment exists only in false positive rate?", "answer": "The fairness constraints rotate the decision boundary to move z=0 examples into the negative class (decreasing their false positives) while moving z=1 examples into the positive class (increasing their false positives), addressing the scenario where only false positive rate shows disparate mistreatment while false negative rate remains fair.", "doc_id": "1610.08452", "figure_id": "1610.08452_fig_2", "figure_number": 2, "image_path": "data/mineru_output/1610.08452/1610.08452/hybrid_auto/images/1610.08452_page0_fig1.jpg", "caption": "Figure 2: [Synthetic data] Panel (a) shows that decreasing the covariance threshold causes the false positive rates for both groups to become similar. Panel (b) shows that an increasing degree of fairness corresponds to a steady decrease in accuracy. Panel (c) shows the original decision boundary (solid line) and fair decision boundary (dashed line), along with corresponding accuracy and false positive rates for groups $z = 0$ (crosses) and $z = 1$ (circles). Fairness constraints cause the original decision boundary to rotate such that previously misclassified examples with $z = 0$ are moved into the negative class (decreasing false positives), while well-classified examples with $z = 1$ are moved into the positive class (increasing false positives), leading to equal false positive rates for both groups.", "figure_type": "plot", "visual_anchor": "blue asterisk markers decreasing from ~0.25 to ~0.13 and magenta square markers increasing from ~0.05 to ~0.12", "text_evidence": "The first scenario considers a case where a classifier trained on the ground truth data leads to disparate mistreatment in terms of only the false positive rate (false negative rate), while being fair with respect to false negative rate (false positive rate), i.e., $D _ { F P R } \\neq 0$ and $D _ { F N R } = 0$", "query_type": "comparison_explanation"}
{"query_id": "l1_1610.08452_1610.08452_fig_2_175", "query": "Does the convergence of both marker types at m approaching zero demonstrate the relation between decision-boundary covariance and false positive rates?", "answer": "Yes, as the covariance multiplicative factor decreases toward zero, the false positive rates for both groups converge to approximately 0.12-0.13, demonstrating that decreasing the covariance threshold causes the false positive rates to become similar across groups.", "doc_id": "1610.08452", "figure_id": "1610.08452_fig_2", "figure_number": 2, "image_path": "data/mineru_output/1610.08452/1610.08452/hybrid_auto/images/1610.08452_page0_fig1.jpg", "caption": "Figure 2: [Synthetic data] Panel (a) shows that decreasing the covariance threshold causes the false positive rates for both groups to become similar. Panel (b) shows that an increasing degree of fairness corresponds to a steady decrease in accuracy. Panel (c) shows the original decision boundary (solid line) and fair decision boundary (dashed line), along with corresponding accuracy and false positive rates for groups $z = 0$ (crosses) and $z = 1$ (circles). Fairness constraints cause the original decision boundary to rotate such that previously misclassified examples with $z = 0$ are moved into the negative class (decreasing false positives), while well-classified examples with $z = 1$ are moved into the positive class (increasing false positives), leading to equal false positive rates for both groups.", "figure_type": "plot", "visual_anchor": "convergence point at m≈0 where blue asterisks and magenta squares meet at ~0.12-0.13", "text_evidence": "Figure 2 summarizes the results for this scenario by showing (a) the relation between decision-boundary covariance and the false positive rates for both sensitive attribute values", "query_type": "value_context"}
{"query_id": "l1_1610.08452_1610.08452_fig_2_176", "query": "How does the gap between blue asterisks and magenta squares narrowing from 0.20 to near-zero relate to the more complex disparate mistreatment scenario?", "answer": "The narrowing gap demonstrates fairness enforcement in false positive rates, which is a prerequisite before addressing the more complex scenario where both false positive and false negative rates show non-zero disparate mistreatment simultaneously.", "doc_id": "1610.08452", "figure_id": "1610.08452_fig_2", "figure_number": 2, "image_path": "data/mineru_output/1610.08452/1610.08452/hybrid_auto/images/1610.08452_page0_fig1.jpg", "caption": "Figure 2: [Synthetic data] Panel (a) shows that decreasing the covariance threshold causes the false positive rates for both groups to become similar. Panel (b) shows that an increasing degree of fairness corresponds to a steady decrease in accuracy. Panel (c) shows the original decision boundary (solid line) and fair decision boundary (dashed line), along with corresponding accuracy and false positive rates for groups $z = 0$ (crosses) and $z = 1$ (circles). Fairness constraints cause the original decision boundary to rotate such that previously misclassified examples with $z = 0$ are moved into the negative class (decreasing false positives), while well-classified examples with $z = 1$ are moved into the positive class (increasing false positives), leading to equal false positive rates for both groups.", "figure_type": "plot", "visual_anchor": "gap between blue asterisk and magenta square markers narrowing from ~0.20 at m=1 to ~0.01 at m=0", "text_evidence": "In this section, we consider a more complex scenario, where the outcomes of the classifier suffer from disparate mistreatment with respect to both false positive rate and false negative rate, i.e., both $D _ { F P R }$ and $D _ { F N R }$ are non-zero.", "query_type": "comparison_explanation"}
{"query_id": "l1_1610.08452_1610.08452_fig_6_177", "query": "Why does the solid cyan boundary achieve opposite-signed FPR values 0.14 and 0.30 when disparate mistreatment signs are opposite?", "answer": "The opposite signs of D_FPR and D_FNR in this scenario mean that removing disparate mistreatment on FPR can potentially help remove disparate mistreatment on FNR, resulting in the observed FPR disparity.", "doc_id": "1610.08452", "figure_id": "1610.08452_fig_6", "figure_number": 3, "image_path": "data/mineru_output/1610.08452/1610.08452/hybrid_auto/images/1610.08452_page0_fig5.jpg", "caption": "Figure 3: [Synthetic data] $D _ { F P R }$ and $D _ { F N R }$ have opposite signs. Removing disparate mistreatment on FPR can potentially help remove disparate mistreatment on FNR. Removing disparate mistreatment on both at the same time leads to very similar results.   ", "figure_type": "plot", "visual_anchor": "Solid cyan line with FPR=0.14:0.30 in legend", "text_evidence": "$D _ { F P R }$ and $D _ { F N R }$ have opposite signs. Removing disparate mistreatment on FPR can potentially help remove disparate mistreatment on FNR.", "query_type": "comparison_explanation"}
{"query_id": "l1_1610.08452_1610.08452_fig_6_178", "query": "Does the dashed purple boundary's nearly equal FNR of 0.24:0.24 confirm that constraining both metrics simultaneously yields similar results?", "answer": "Yes, the dashed line shows FNR=0.24:0.24, demonstrating that removing disparate mistreatment on both FPR and FNR at the same time leads to very similar fairness results across groups.", "doc_id": "1610.08452", "figure_id": "1610.08452_fig_6", "figure_number": 3, "image_path": "data/mineru_output/1610.08452/1610.08452/hybrid_auto/images/1610.08452_page0_fig5.jpg", "caption": "Figure 3: [Synthetic data] $D _ { F P R }$ and $D _ { F N R }$ have opposite signs. Removing disparate mistreatment on FPR can potentially help remove disparate mistreatment on FNR. Removing disparate mistreatment on both at the same time leads to very similar results.   ", "figure_type": "plot", "visual_anchor": "Dashed purple line with FNR=0.24:0.24 in legend", "text_evidence": "Removing disparate mistreatment on both at the same time leads to very similar results.", "query_type": "value_context"}
{"query_id": "l1_1610.08452_1610.08452_fig_6_179", "query": "How do the solid cyan and dashed purple boundaries illustrate the unconstrained versus constrained fair classifiers summarized for this scenario?", "answer": "The solid cyan line represents the constrained classifier optimizing FPR fairness, while the dashed purple line represents constraining both FPR and FNR simultaneously, demonstrating different fairness-accuracy trade-offs.", "doc_id": "1610.08452", "figure_id": "1610.08452_fig_6", "figure_number": 3, "image_path": "data/mineru_output/1610.08452/1610.08452/hybrid_auto/images/1610.08452_page0_fig5.jpg", "caption": "Figure 3: [Synthetic data] $D _ { F P R }$ and $D _ { F N R }$ have opposite signs. Removing disparate mistreatment on FPR can potentially help remove disparate mistreatment on FNR. Removing disparate mistreatment on both at the same time leads to very similar results.   ", "figure_type": "plot", "visual_anchor": "Solid cyan and dashed purple decision boundaries separating red and green data points", "text_evidence": "Figure 3 summarizes the results for this scenario by showing the decision boundaries for the unconstrained classifier (solid) and the constrained fair classifiers.", "query_type": "visual_definition"}
{"query_id": "l1_1610.08452_1610.08452_fig_9_180", "query": "Why does the dashed blue boundary achieve FPR=0.63:0.07 while the solid achieves 0.33:0.08?", "answer": "The dashed boundary implements fairness constraints to avoid disparate mistreatment by controlling false positive rates between groups, causing it to shift and equalize FPR across protected attributes despite lower overall accuracy.", "doc_id": "1610.08452", "figure_id": "1610.08452_fig_9", "figure_number": 4, "image_path": "data/mineru_output/1610.08452/1610.08452/hybrid_auto/images/1610.08452_page0_fig8.jpg", "caption": "Figure 4: [Synthetic data] $D _ { F P R }$ and $D _ { F N R }$ have the same sign. Removing disparate mistreatment on FPR can potentially increase disparate mistreatment on FNR. Removing disparate mistreatment on both at the same time causes a larger drop in accuracy.", "figure_type": "plot", "visual_anchor": "dashed blue line with FPR=0.63:0.07 versus solid cyan line with FPR=0.33:0.08 in legend", "text_evidence": "Our method: implements our scheme to avoid disparate treatment and disparate mistreatment simultaneously. Disparate mistreatment is avoided by using fairness constraints", "query_type": "comparison_explanation"}
{"query_id": "l1_1610.08452_1610.08452_fig_9_181", "query": "Does the accuracy drop from 0.80 to 0.77 shown in the legend support controlling both rates simultaneously?", "answer": "Yes, the accuracy decrease from the solid to dashed boundary demonstrates that removing disparate mistreatment on both FPR and FNR simultaneously causes a larger drop in accuracy, as stated in the methodology.", "doc_id": "1610.08452", "figure_id": "1610.08452_fig_9", "figure_number": 4, "image_path": "data/mineru_output/1610.08452/1610.08452/hybrid_auto/images/1610.08452_page0_fig8.jpg", "caption": "Figure 4: [Synthetic data] $D _ { F P R }$ and $D _ { F N R }$ have the same sign. Removing disparate mistreatment on FPR can potentially increase disparate mistreatment on FNR. Removing disparate mistreatment on both at the same time causes a larger drop in accuracy.", "figure_type": "plot", "visual_anchor": "legend showing Acc=0.80 for solid line versus Acc=0.77 for dashed line", "text_evidence": "Removing disparate mistreatment on both at the same time causes a larger drop in accuracy", "query_type": "value_context"}
{"query_id": "l1_1610.08452_1610.08452_fig_9_182", "query": "What causes the dashed boundary to separate more red circles from green ones compared to the solid boundary?", "answer": "The fair constrained classifier shifts the decision boundary to equalize false positive and negative rates across groups, which the research compares against unconstrained classifiers to demonstrate performance trade-offs when avoiding disparate mistreatment.", "doc_id": "1610.08452", "figure_id": "1610.08452_fig_9", "figure_number": 4, "image_path": "data/mineru_output/1610.08452/1610.08452/hybrid_auto/images/1610.08452_page0_fig8.jpg", "caption": "Figure 4: [Synthetic data] $D _ { F P R }$ and $D _ { F N R }$ have the same sign. Removing disparate mistreatment on FPR can potentially increase disparate mistreatment on FNR. Removing disparate mistreatment on both at the same time causes a larger drop in accuracy.", "figure_type": "plot", "visual_anchor": "dashed blue line positioned differently from solid cyan line relative to red and green markers", "text_evidence": "Figure 4 summarizes the results by showing the decision boundaries for both the unconstrained classifiers (solid) and the fair constrained classifier (dashed) when controlling for disparate mistreatment", "query_type": "comparison_explanation"}
{"query_id": "l1_1611.07438_1611.07438_fig_1_183", "query": "Does the direct arrow from gender to admission represent the discriminatory effect quantified by |ΔP|b < τ in each subpopulation?", "answer": "Yes, the direct arrow from gender to admission represents potential direct discrimination. The criterion |ΔP|b < τ ensures each subpopulation b contains no discriminatory effect.", "doc_id": "1611.07438", "figure_id": "1611.07438_fig_1", "figure_number": 1, "image_path": "data/mineru_output/1611.07438/1611.07438/hybrid_auto/images/1611.07438_page0_fig0.jpg", "caption": "Figure 1: Causal graph of an example university admission system.", "figure_type": "diagram", "visual_anchor": "direct arrow from gender node to admission node", "text_evidence": "if each block set B is examined and ensures that | ΔP | _ { \\bf b } | < τ holds for each subpopulation b, we can guarantee that the dataset contains no discriminatory effect and is not liable for any claim of direct discrimination", "query_type": "visual_definition"}
{"query_id": "l1_1611.07438_1611.07438_fig_1_184", "query": "Why does major point to both gender and admission when the dataset includes gender as a protected attribute?", "answer": "Major influences both admission decisions and correlates with gender distribution. The dataset includes gender as the protected attribute along with admission as the decision attribute and major as a non-protected attribute.", "doc_id": "1611.07438", "figure_id": "1611.07438_fig_1", "figure_number": 1, "image_path": "data/mineru_output/1611.07438/1611.07438/hybrid_auto/images/1611.07438_page0_fig0.jpg", "caption": "Figure 1: Causal graph of an example university admission system.", "figure_type": "diagram", "visual_anchor": "two outgoing arrows from major node, one to gender and one to admission", "text_evidence": "Each individual in $\\mathcal { D }$ is specified by a set of attributes V, which includes the protected attribute (e.g., gender), the decision attribute (e.g., admission), and the non-protected attributes (e.g., major)", "query_type": "comparison_explanation"}
{"query_id": "l1_1611.07438_1611.07438_fig_1_185", "query": "Which three nodes have direct causal paths to admission in this university system's causal graph structure?", "answer": "Major, gender, and test_score all have direct arrows pointing to the admission node, representing their direct causal influence on admission decisions in the university system.", "doc_id": "1611.07438", "figure_id": "1611.07438_fig_1", "figure_number": 1, "image_path": "data/mineru_output/1611.07438/1611.07438/hybrid_auto/images/1611.07438_page0_fig0.jpg", "caption": "Figure 1: Causal graph of an example university admission system.", "figure_type": "diagram", "visual_anchor": "three incoming arrows to the admission node from major, gender, and test_score", "text_evidence": "Consider a dataset $\\mathcal { D }$ which may contain discrimination against a certain protected group. Each individual in $\\mathcal { D }$ is specified by a set of attributes V", "query_type": "value_context"}
{"query_id": "l1_1611.07438_1611.07438_fig_2_186", "query": "Why do multiple green nodes point to the blue income node while the red sex node lacks direct connection?", "answer": "The green nodes represent set Q containing non-protected attributes that causally influence income. Sex is the protected attribute (red) whose effect is mediated through Q variables rather than directly impacting income.", "doc_id": "1611.07438", "figure_id": "1611.07438_fig_2", "figure_number": 2, "image_path": "data/mineru_output/1611.07438/1611.07438/hybrid_auto/images/1611.07438_page0_fig1.jpg", "caption": "Figure 2: Causal graph for Adult dataset: the red node represents the protected attribute, the blue node represents the decision, the green nodes represent set Q.", "figure_type": "diagram", "visual_anchor": "blue income node at bottom with incoming arrows from green nodes but no direct arrow from red sex node", "text_evidence": "the red node represents the protected attribute, the blue node represents the decision, the green nodes represent set Q", "query_type": "comparison_explanation"}
{"query_id": "l1_1611.07438_1611.07438_fig_2_187", "query": "Does examining major alone versus major with test score explain why green nodes form set Q in this graph?", "answer": "Yes. The example shows that examining individual attributes (like major) may miss discriminatory effects, but examining combinations reveals bias. Set Q contains attributes whose combinations expose conditional disparities, similar to how {major, test score} revealed discrimination.", "doc_id": "1611.07438", "figure_id": "1611.07438_fig_2", "figure_number": 2, "image_path": "data/mineru_output/1611.07438/1611.07438/hybrid_auto/images/1611.07438_page0_fig1.jpg", "caption": "Figure 2: Causal graph for Adult dataset: the red node represents the protected attribute, the blue node represents the decision, the green nodes represent set Q.", "figure_type": "diagram", "visual_anchor": "multiple green nodes (edu, work_class, occupation) forming set Q with interconnected edges", "text_evidence": "For the example shown in Table 2, although examining {major} shows no discriminatory effect, when examining {major,test score} we observe $| \\Delta P | _ { \\mathrm { \\{ m a t h , B \\} } } | = 0", "query_type": "visual_definition"}
{"query_id": "l1_1611.07438_1611.07438_fig_2_188", "query": "How does the variance formula relate to edges connecting green nodes to the blue income decision node?", "answer": "The variance formula σ²_B measures disparity across different combinations of attributes B. The edges from green nodes to income represent how different attribute combinations (b) produce varying discrimination levels ΔP|_b, which the variance formula aggregates.", "doc_id": "1611.07438", "figure_id": "1611.07438_fig_2", "figure_number": 2, "image_path": "data/mineru_output/1611.07438/1611.07438/hybrid_auto/images/1611.07438_page0_fig1.jpg", "caption": "Figure 2: Causal graph for Adult dataset: the red node represents the protected attribute, the blue node represents the decision, the green nodes represent set Q.", "figure_type": "diagram", "visual_anchor": "directed edges from green nodes (race, age, edu, native_country, hour, work_class, marital, occupation, relationship) converging to blue income node", "text_evidence": "where $\\begin{array} { r } { \\hat { \\mu } _ { \\mathbf { B } } = \\sum _ { \\mathbf { B } } \\operatorname* { P r } ( \\mathbf { b } ) \\cdot \\Delta P | _ { \\mathbf { b } } } \\end{array}$ and $\\begin{array} { r } { \\hat { \\sigma } _ { \\mathbf { B } } ^ { 2 } = \\sum _ { \\mathbf { B } } \\operatorname* { P r } ( \\mathbf { b } ) ( \\Delta P | _ { \\mathbf { b } } - \\hat { \\mu } _ { \\mathbf { B } } ) ^ { 2 } , } \\end{array}$", "query_type": "value_context"}
{"query_id": "l1_1611.07438_1611.07438_fig_3_189", "query": "Why is sex marked as the red node when the graph represents the Dutch Census causal structure?", "answer": "Sex is the protected attribute in this discrimination discovery analysis of the Dutch Census dataset, requiring special identification in the causal framework.", "doc_id": "1611.07438", "figure_id": "1611.07438_fig_3", "figure_number": 3, "image_path": "data/mineru_output/1611.07438/1611.07438/hybrid_auto/images/1611.07438_page0_fig2.jpg", "caption": "Figure 3: Causal graph for Dutch Census dataset: the red node represents the protected attribute, the blue node represents the decision, the green nodes represent set Q, and the black nodes represent the others.", "figure_type": "diagram", "visual_anchor": "red node labeled 'sex' in top right area", "text_evidence": "the red node represents the protected attribute, the blue node represents the decision, the green nodes represent set Q", "query_type": "visual_definition"}
{"query_id": "l1_1611.07438_1611.07438_fig_3_190", "query": "How does the green node age relate to the blue decision node edu in discovering conditional discrimination patterns?", "answer": "Age is part of set Q, which helps partition the population to detect conditional discrimination in educational outcomes, as the authors extended prior conditional discrimination approaches.", "doc_id": "1611.07438", "figure_id": "1611.07438_fig_3", "figure_number": 3, "image_path": "data/mineru_output/1611.07438/1611.07438/hybrid_auto/images/1611.07438_page0_fig2.jpg", "caption": "Figure 3: Causal graph for Dutch Census dataset: the red node represents the protected attribute, the blue node represents the decision, the green nodes represent set Q, and the black nodes represent the others.", "figure_type": "diagram", "visual_anchor": "green node 'age' with edge pointing to blue node 'edu'", "text_evidence": "Zliobaitˇ e et˙ al. [28] proposed conditional discrimination. However, their approaches cannot determine whether a partition is meaning", "query_type": "comparison_explanation"}
{"query_id": "l1_1611.07438_1611.07438_fig_3_191", "query": "What discrimination detection methods do the black nodes like citizenship and prev_residence support beyond classification rule-based approaches?", "answer": "These nodes represent additional variables beyond the protected attribute, decision, and set Q, enabling more comprehensive discrimination pattern discovery than simple elift or belift rule-based methods.", "doc_id": "1611.07438", "figure_id": "1611.07438_fig_3", "figure_number": 3, "image_path": "data/mineru_output/1611.07438/1611.07438/hybrid_auto/images/1611.07438_page0_fig2.jpg", "caption": "Figure 3: Causal graph for Dutch Census dataset: the red node represents the protected attribute, the blue node represents the decision, the green nodes represent set Q, and the black nodes represent the others.", "figure_type": "diagram", "visual_anchor": "black nodes 'citizenship' and 'prev_residence' in middle section", "text_evidence": "Classification rule-based methods such as elift [23] and belift [20] were proposed to represent certain discrimination patterns.", "query_type": "value_context"}
{"query_id": "l1_1611.07509_1611.07509_fig_1_192", "query": "Does the direct arrow from Race to Loan represent the path-specific effect modeling direct discrimination in historical data?", "answer": "Yes, the direct arrow from Race to Loan models direct discrimination as a path-specific effect, which the authors use to distinguish and discover discriminatory patterns before building classifiers.", "doc_id": "1611.07509", "figure_id": "1611.07509_fig_1", "figure_number": 1, "image_path": "data/mineru_output/1611.07509/1611.07509/hybrid_auto/images/1611.07509_page0_fig0.jpg", "caption": "Figure 1: The toy model.", "figure_type": "diagram", "visual_anchor": "direct arrow from Race node to Loan node", "text_evidence": "Then we model direct and indirect discrimination as the path-specific effects, which explicitly distinguish", "query_type": "visual_definition"}
{"query_id": "l1_1611.07509_1611.07509_fig_1_193", "query": "Why does Race connect to Loan through Zip_code when analyzing the expected change treating disadvantaged applicants as advantaged?", "answer": "The Race-to-Zip_code-to-Loan path captures indirect discrimination, representing the expected change in loan approval when the disadvantaged group had the same racial makeups shown in Zip code as the advantaged group.", "doc_id": "1611.07509", "figure_id": "1611.07509_fig_1", "figure_number": 1, "image_path": "data/mineru_output/1611.07509/1611.07509/hybrid_auto/images/1611.07509_page0_fig0.jpg", "caption": "Figure 1: The toy model.", "figure_type": "diagram", "visual_anchor": "path from Race through Zip_code to Loan", "text_evidence": "When applied to the example in Figure 1, it means the expected change in loan approval of the disadvantage group if they had the same racial makeups shown in the Zip code as the advantage group.", "query_type": "comparison_explanation"}
{"query_id": "l1_1611.07509_1611.07509_fig_1_194", "query": "Does the arrow from Race directly to Loan capture treating black applicants as white in loan approval expectations?", "answer": "Yes, the direct Race-to-Loan arrow models the expected change in loan approval when applications from the disadvantaged group (e.g., black) are instructed to be treated as from the advantaged group (e.g., white).", "doc_id": "1611.07509", "figure_id": "1611.07509_fig_1", "figure_number": 1, "image_path": "data/mineru_output/1611.07509/1611.07509/hybrid_auto/images/1611.07509_page0_fig0.jpg", "caption": "Figure 1: The toy model.", "figure_type": "diagram", "visual_anchor": "single arrow connecting Race directly to Loan", "text_evidence": "When applied to the example in Figure 1, it means the expected change in loan approval of applications actually from the disadvantage group (e.g., black), when the bank is instructed to treat the applicants as from the advantage group (e.g., white).", "query_type": "value_context"}
{"query_id": "l1_1611.07509_1611.07509_fig_2_195", "query": "Does the direct arrow from X to Z1 represent the intervention needed to compute the post-intervention distribution formula shown?", "answer": "Yes, the arrow from X to Z1 corresponds to the intervention on attribute X in the post-intervention distribution formula P(Y|do(X=x)), which marginalizes over all other attributes.", "doc_id": "1611.07509", "figure_id": "1611.07509_fig_2", "figure_number": 2, "image_path": "data/mineru_output/1611.07509/1611.07509/hybrid_auto/images/1611.07509_page0_fig1.jpg", "caption": "Figure 2: An example with the recanting witness criterion satisfied.", "figure_type": "diagram", "visual_anchor": "Direct arrow from node X to node Z1", "text_evidence": "Specifically, the post-intervention distribution of a single attribute Y given an intervention on a single attribute X is given by", "query_type": "value_context"}
{"query_id": "l1_1611.07509_1611.07509_fig_2_196", "query": "Why does the path X→Z1→Y allow total causal effect calculation from X to Y as defined by do-calculus?", "answer": "The path X→Z1→Y represents one causal pathway through which X affects Y. The total causal effect sums over all such pathways, including the indirect path through Z2, as computed using do-calculus.", "doc_id": "1611.07509", "figure_id": "1611.07509_fig_2", "figure_number": 2, "image_path": "data/mineru_output/1611.07509/1611.07509/hybrid_auto/images/1611.07509_page0_fig1.jpg", "caption": "Figure 2: An example with the recanting witness criterion satisfied.", "figure_type": "diagram", "visual_anchor": "Path from X through Z1 directly to Y", "text_evidence": "By using the do-calculus, the total causal effect of X on Y is defined as fo", "query_type": "comparison_explanation"}
{"query_id": "l1_1611.07509_1611.07509_fig_2_197", "query": "How does the mediator Z2 receiving input from Z1 relate to attributes that cannot be objectively justified in decisions?", "answer": "Z2 acts as a mediator between Z1 and Y, potentially representing non-protected attributes that should not be used in decision-making. The recanting witness criterion helps identify such unjustifiable pathways in discrimination analysis.", "doc_id": "1611.07509", "figure_id": "1611.07509_fig_2", "figure_number": 2, "image_path": "data/mineru_output/1611.07509/1611.07509/hybrid_auto/images/1611.07509_page0_fig1.jpg", "caption": "Figure 2: An example with the recanting witness criterion satisfied.", "figure_type": "diagram", "visual_anchor": "Arrow from Z1 to Z2 and arrow from Z2 to Y", "text_evidence": "Among the non-protected attributes, assume there is a set of attributes that cannot be objectively justified if used in the decision making process, whi", "query_type": "visual_definition"}
{"query_id": "l1_1611.07509_1611.07509_fig_4_198", "query": "Why is the green path from sex to income considered direct when the Adult dataset contains 11 attributes?", "answer": "The green path represents the direct causal effect from sex to income without mediating variables. The Adult dataset's 11 attributes were analyzed to construct this causal network, identifying sex as having both direct (green) and indirect (blue through marital_status) effects on income.", "doc_id": "1611.07509", "figure_id": "1611.07509_fig_4", "figure_number": 3, "image_path": "data/mineru_output/1611.07509/1611.07509/hybrid_auto/images/1611.07509_page0_fig3.jpg", "caption": "Figure 3: Causal network for Adult dataset: the green path represents the direct path, and the blue paths represent the indirect paths passing through marital status.", "figure_type": "diagram", "visual_anchor": "green path connecting sex node directly to income node", "text_evidence": "The Adult dataset consists of 65123 tuples with 11 attributes such as age, education, sex, occupation, income, marital status etc.", "query_type": "value_context"}
{"query_id": "l1_1611.07509_1611.07509_fig_4_199", "query": "How do the blue paths through marital_status differ from the green direct path given the binarization preprocessing?", "answer": "The blue paths represent indirect causal effects where sex influences income through the mediating variable marital_status, while the green path shows direct influence. Binarization reduced domain sizes to two classes for computational feasibility, enabling the PC algorithm to discover both path types.", "doc_id": "1611.07509", "figure_id": "1611.07509_fig_4", "figure_number": 3, "image_path": "data/mineru_output/1611.07509/1611.07509/hybrid_auto/images/1611.07509_page0_fig3.jpg", "caption": "Figure 3: Causal network for Adult dataset: the green path represents the direct path, and the blue paths represent the indirect paths passing through marital status.", "figure_type": "diagram", "visual_anchor": "blue paths from sex through marital_status node versus green direct path to income", "text_evidence": "we binarize each attribute's domain values into two classes to reduce the domain sizes", "query_type": "comparison_explanation"}
{"query_id": "l1_1611.07509_1611.07509_fig_4_200", "query": "Why does marital_status appear as a central node with multiple blue incoming edges despite temporal priority considerations?", "answer": "Marital_status receives blue edges from age and sex because these attributes have temporal priority in the three-tier ordering. Sex and age are in the first tier and naturally precede and causally influence marital status, which then mediates their effects on income through indirect blue paths.", "doc_id": "1611.07509", "figure_id": "1611.07509_fig_4", "figure_number": 3, "image_path": "data/mineru_output/1611.07509/1611.07509/hybrid_auto/images/1611.07509_page0_fig3.jpg", "caption": "Figure 3: Causal network for Adult dataset: the green path represents the direct path, and the blue paths represent the indirect paths passing through marital status.", "figure_type": "diagram", "visual_anchor": "marital_status node with multiple blue incoming edges from age and sex", "text_evidence": "We use three tiers in the partial order for temporal priority: sex, age, native", "query_type": "anomaly_cause"}
{"query_id": "l1_1611.07509_1611.07509_fig_5_201", "query": "Why is the green path from sex to occupation classified as direct rather than indirect in the network?", "answer": "The green path represents the direct path from sex to occupation, bypassing mediating variables like marital status, which defines indirect paths shown in blue.", "doc_id": "1611.07509", "figure_id": "1611.07509_fig_5", "figure_number": 4, "image_path": "data/mineru_output/1611.07509/1611.07509/hybrid_auto/images/1611.07509_page0_fig4.jpg", "caption": "Figure 4: Causal network for Dutch dataset: the green path represents the direct path, and the blue paths represent the indirect paths passing through marital status.", "figure_type": "diagram", "visual_anchor": "green path connecting sex node to occupation node", "text_evidence": "the green path represents the direct path, and the blue paths represent the indirect paths passing through marital status", "query_type": "visual_definition"}
{"query_id": "l1_1611.07509_1611.07509_fig_5_202", "query": "Do the blue paths through marital_status align with temporal priority placing sex in the first tier?", "answer": "Yes, sex is in the first tier and influences marital_status, which then affects other variables, consistent with the temporal ordering where sex, age, native country, and race are defined first.", "doc_id": "1611.07509", "figure_id": "1611.07509_fig_5", "figure_number": 4, "image_path": "data/mineru_output/1611.07509/1611.07509/hybrid_auto/images/1611.07509_page0_fig4.jpg", "caption": "Figure 4: Causal network for Dutch dataset: the green path represents the direct path, and the blue paths represent the indirect paths passing through marital status.", "figure_type": "diagram", "visual_anchor": "blue paths passing through marital_status node", "text_evidence": "We use three tiers in the partial order for temporal priority: sex, age, native country, race are defined in the firs", "query_type": "comparison_explanation"}
{"query_id": "l1_1611.07509_1611.07509_fig_5_203", "query": "How does the causal network structure support elift's discrimination pattern representation in the Dutch dataset?", "answer": "The network shows direct and indirect paths from protected attributes like sex and age to outcomes, enabling rule-based methods like elift to identify discrimination patterns through these causal relationships.", "doc_id": "1611.07509", "figure_id": "1611.07509_fig_5", "figure_number": 4, "image_path": "data/mineru_output/1611.07509/1611.07509/hybrid_auto/images/1611.07509_page0_fig4.jpg", "caption": "Figure 4: Causal network for Dutch dataset: the green path represents the direct path, and the blue paths represent the indirect paths passing through marital status.", "figure_type": "diagram", "visual_anchor": "directed edges from sex and age nodes to multiple downstream nodes", "text_evidence": "Classification rule-based methods such as elift (Pedreshi, Ruggieri, and Turini 2008) and belift (Mancuhan and Clifton 2014) were proposed to represent certain discrimination patterns", "query_type": "value_context"}
{"query_id": "l1_1701.08230_1701.08230_fig_2_204", "query": "Do the two distributions drawn from beta distributions with equal means in the right panel share the same peak location?", "answer": "No, the blue curve peaks before 25% (near the dashed line) while the red curve peaks slightly after 25%, despite both distributions having equal means as stated.", "doc_id": "1701.08230", "figure_id": "1701.08230_fig_2", "figure_number": 1, "image_path": "data/mineru_output/1701.08230/1701.08230/hybrid_auto/images/1701.08230_page0_fig1.jpg", "caption": "Figure 1: Top: distribution of risk scores for Broward County data (le), and simulated data drawn from two beta distributions with equal means (right). Bottom: using a single threshold which detains $3 0 \\%$ of defendants in Broward County violates statistical parity (as measured by detention rate), predictive equality (false positive rate), and conditional statistical parity (detention rate conditional on number of prior arrests). We omit the last measure for the simulated data since that would require making additional assumptions about the relationship of priors and risk in the hypothetical populations.", "figure_type": "plot", "visual_anchor": "Blue and red curves with vertical dashed line at 25% in top-right simulated panel", "text_evidence": "simulated data drawn from two beta distributions with equal means (right)", "query_type": "comparison_explanation"}
{"query_id": "l1_1701.08230_1701.08230_fig_2_205", "query": "Why does detaining 30% of defendants using a single threshold violate statistical parity despite the distributions shown?", "answer": "Because the different shapes of the red and blue distributions mean a single threshold at any point will capture different proportions of each group, creating disparate detention rates between groups.", "doc_id": "1701.08230", "figure_id": "1701.08230_fig_2", "figure_number": 1, "image_path": "data/mineru_output/1701.08230/1701.08230/hybrid_auto/images/1701.08230_page0_fig1.jpg", "caption": "Figure 1: Top: distribution of risk scores for Broward County data (le), and simulated data drawn from two beta distributions with equal means (right). Bottom: using a single threshold which detains $3 0 \\%$ of defendants in Broward County violates statistical parity (as measured by detention rate), predictive equality (false positive rate), and conditional statistical parity (detention rate conditional on number of prior arrests). We omit the last measure for the simulated data since that would require making additional assumptions about the relationship of priors and risk in the hypothetical populations.", "figure_type": "plot", "visual_anchor": "Different shapes of red and blue probability curves in simulated panel", "text_evidence": "using a single threshold which detains $3 0 \\%$ of defendants in Broward County violates statistical parity (as measured by detention rate), predictive equality (false positive rate), and conditional statistical parity", "query_type": "comparison_explanation"}
{"query_id": "l1_1701.08230_1701.08230_fig_2_206", "query": "Can the vertical dashed line at 25% serve as a threshold to evaluate average risk above thresholds for fairness?", "answer": "No, detention rates and false positive rates are poor proxies for thresholds because these infra-marginal statistics consider average risk above the thresholds, and can differ even with identical thresholds as the distributions show.", "doc_id": "1701.08230", "figure_id": "1701.08230_fig_2", "figure_number": 1, "image_path": "data/mineru_output/1701.08230/1701.08230/hybrid_auto/images/1701.08230_page0_fig1.jpg", "caption": "Figure 1: Top: distribution of risk scores for Broward County data (le), and simulated data drawn from two beta distributions with equal means (right). Bottom: using a single threshold which detains $3 0 \\%$ of defendants in Broward County violates statistical parity (as measured by detention rate), predictive equality (false positive rate), and conditional statistical parity (detention rate conditional on number of prior arrests). We omit the last measure for the simulated data since that would require making additional assumptions about the relationship of priors and risk in the hypothetical populations.", "figure_type": "plot", "visual_anchor": "Vertical dashed line at 25% probability in simulated plot", "text_evidence": "detention rates and false positive rates are poor proxies for the thresholds: these infra-marginal statistics consider average risk above the thresholds, and so can dier even if the thresholds are identical", "query_type": "value_context"}
{"query_id": "l1_1701.08230_1701.08230_fig_5_207", "query": "Why do the red and blue lines track closely together across all risk scores if COMPAS scores are calibrated?", "answer": "Calibrated scores mean that defendants of different races with the same risk score have equal recidivism rates. The closely tracking red (Black) and blue (White) lines demonstrate this calibration property across all risk levels.", "doc_id": "1701.08230", "figure_id": "1701.08230_fig_5", "figure_number": 2, "image_path": "data/mineru_output/1701.08230/1701.08230/hybrid_auto/images/1701.08230_page0_fig4.jpg", "caption": "Figure 2: Recidivism rate by COMPAS risk score and race. White and black defendants with the same risk score are roughly equally likely to reoend, indicating that the scores are calibrated. e $y$ -axis shows the proportion of defenydants re-arrested for any crime, including non-violent offenses; the gray bands show $9 5 \\%$ condence intervals.", "figure_type": "plot", "visual_anchor": "Red and blue lines tracking closely together from risk score 1 to 10", "text_evidence": "White and black defendants with the same risk score are roughly equally likely to reoend, indicating that the scores are calibrated.", "query_type": "comparison_explanation"}
{"query_id": "l1_1701.08230_1701.08230_fig_5_208", "query": "Does the convergence of both lines near 70-75% at high risk scores align with any-crime rearrest definitions?", "answer": "Yes, the high recidivism rates (70-75%) at elevated risk scores reflect the broad definition used, which includes re-arrests for any crime, not just violent offenses, capturing a wider range of recidivism events.", "doc_id": "1701.08230", "figure_id": "1701.08230_fig_5", "figure_number": 2, "image_path": "data/mineru_output/1701.08230/1701.08230/hybrid_auto/images/1701.08230_page0_fig4.jpg", "caption": "Figure 2: Recidivism rate by COMPAS risk score and race. White and black defendants with the same risk score are roughly equally likely to reoend, indicating that the scores are calibrated. e $y$ -axis shows the proportion of defenydants re-arrested for any crime, including non-violent offenses; the gray bands show $9 5 \\%$ condence intervals.", "figure_type": "plot", "visual_anchor": "Both red and blue lines converging at approximately 70-75% likelihood between risk scores 8-10", "text_evidence": "the $y$ -axis shows the proportion of defenydants re-arrested for any crime, including non-violent offenses", "query_type": "comparison_explanation"}
{"query_id": "l1_1701.08230_1701.08230_fig_6_209", "query": "Why does the red distribution peak left of the dashed detention threshold despite algorithms outputting scores claimed to indicate defendant risk?", "answer": "The red line shows strategically altered risk scores that are less informative than true risk (black line), ensuring no defendants fall above the detention threshold while maintaining calibration, demonstrating how scores can be discriminatory even when calibrated.", "doc_id": "1701.08230", "figure_id": "1701.08230_fig_6", "figure_number": 3, "image_path": "data/mineru_output/1701.08230/1701.08230/hybrid_auto/images/1701.08230_page0_fig5.jpg", "caption": "Figure 3: Calibration is insucient to assess discrimination. In the le plot, the black line shows the distribution of risk in a hypothetical population, and the red line shows strategically altered risk estimates in the same population. Both sets of risk scores are calibrated (right plot), but the altered risk scores are less informative and as a result guarantee that no defendants fall above the detention threshold (dashed vertical line).", "figure_type": "plot", "visual_anchor": "red distribution peak positioned left of the dashed vertical detention threshold line", "text_evidence": "In practice, however, algorithms like COMPAS typically output a score $s ( x )$ that is claimed to indicate a defendant's risk ${ p / X | X }$ ; decision makers then use these risk estimates to select Y Xan action (e.g., release or detain).", "query_type": "comparison_explanation"}
{"query_id": "l1_1701.08230_1701.08230_fig_6_210", "query": "How does the red line demonstrate the concern that undisclosed score procedures might be discriminatory?", "answer": "The red line represents strategically altered risk estimates that guarantee no defendants exceed the detention threshold, illustrating how opaque scoring procedures can produce discriminatory outcomes while appearing calibrated.", "doc_id": "1701.08230", "figure_id": "1701.08230_fig_6", "figure_number": 3, "image_path": "data/mineru_output/1701.08230/1701.08230/hybrid_auto/images/1701.08230_page0_fig5.jpg", "caption": "Figure 3: Calibration is insucient to assess discrimination. In the le plot, the black line shows the distribution of risk in a hypothetical population, and the red line shows strategically altered risk estimates in the same population. Both sets of risk scores are calibrated (right plot), but the altered risk scores are less informative and as a result guarantee that no defendants fall above the detention threshold (dashed vertical line).", "figure_type": "plot", "visual_anchor": "red line showing altered distribution with no mass beyond the dashed vertical threshold", "text_evidence": "In some cases, neither the procedure nor the data used to generate these scores is disclosed, prompting worry that the scores are themselves discriminatory.", "query_type": "visual_definition"}
{"query_id": "l1_1701.08230_1701.08230_fig_6_211", "query": "What relationship between the black and red distributions illustrates the general method for constructing discriminatory scores from true risk?", "answer": "The black line shows true risk distribution while the red line shows altered scores that shift the distribution leftward, demonstrating how true risk estimates can be manipulated to produce discriminatory yet calibrated scores.", "doc_id": "1701.08230", "figure_id": "1701.08230_fig_6", "figure_number": 3, "image_path": "data/mineru_output/1701.08230/1701.08230/hybrid_auto/images/1701.08230_page0_fig5.jpg", "caption": "Figure 3: Calibration is insucient to assess discrimination. In the le plot, the black line shows the distribution of risk in a hypothetical population, and the red line shows strategically altered risk estimates in the same population. Both sets of risk scores are calibrated (right plot), but the altered risk scores are less informative and as a result guarantee that no defendants fall above the detention threshold (dashed vertical line).", "figure_type": "plot", "visual_anchor": "comparison between black distribution centered around 25% and red distribution peaked left of detention threshold", "text_evidence": "Figure 3 illustrates a general method for constructing such discriminatory scores from true risk estimates.", "query_type": "comparison_explanation"}
{"query_id": "l1_1703.06856_1703.06856_fig_2_212", "query": "How does node A in the DAG relate to the protected attribute in the insurance pricing scenario?", "answer": "Node A represents the protected attribute (such as gender or race) in the causal model. In the insurance pricing scenario, the company must predict accident rate Y while considering fairness with respect to this protected attribute A.", "doc_id": "1703.06856", "figure_id": "1703.06856_fig_2", "figure_number": 1, "image_path": "data/mineru_output/1703.06856/1703.06856/hybrid_auto/images/1703.06856_page0_fig1.jpg", "caption": "Figure 1: (a), (b) Two causal models for different real-world fair prediction scenarios. See Section 3.1 for discussion. (c) The graph corresponding to a causal model with $A$ being the protected attribute and $Y$ some outcome of interest, with background variables assumed to be independent. (d) Expanding the model to include an intermediate variable indicating whether the individual is employed with two (latent) background variables Prejudiced (if the person offering the job is prejudiced) and Qualifications (a measure of the individual’s qualifications). (e) A twin network representation of this system [28] under two different counterfactual levels for $A$ . This is created by copying nodes descending from $A$ , which inherit unaffected parents from the factual world.", "figure_type": "diagram", "visual_anchor": "Node A (white circle, top-left position) with directed edge to node X", "text_evidence": "A car insurance company wishes to price insurance for car owners by predicting their accident rate $Y$.", "query_type": "value_context"}
{"query_id": "l1_1703.06856_1703.06856_fig_2_213", "query": "Why does node U connect to both X and Y while node A only connects to X?", "answer": "Node U represents independent background variables that can directly affect both intermediate variables and outcomes. Node A (protected attribute) influences Y only through X, reflecting the causal pathway where the protected attribute affects outcomes through intermediate factors rather than directly.", "doc_id": "1703.06856", "figure_id": "1703.06856_fig_2", "figure_number": 1, "image_path": "data/mineru_output/1703.06856/1703.06856/hybrid_auto/images/1703.06856_page0_fig1.jpg", "caption": "Figure 1: (a), (b) Two causal models for different real-world fair prediction scenarios. See Section 3.1 for discussion. (c) The graph corresponding to a causal model with $A$ being the protected attribute and $Y$ some outcome of interest, with background variables assumed to be independent. (d) Expanding the model to include an intermediate variable indicating whether the individual is employed with two (latent) background variables Prejudiced (if the person offering the job is prejudiced) and Qualifications (a measure of the individual’s qualifications). (e) A twin network representation of this system [28] under two different counterfactual levels for $A$ . This is created by copying nodes descending from $A$ , which inherit unaffected parents from the factual world.", "figure_type": "diagram", "visual_anchor": "Node U (white circle, top-right) with edges to both X and Y, versus node A with single edge to X", "text_evidence": "Consider the DAG $A  Y$ , shown in Figure 1(c) with the explicit inclusion of set $U$ of independent background variables.", "query_type": "comparison_explanation"}
{"query_id": "l1_1703.06856_1703.06856_fig_2_214", "query": "What real-world scenarios do the two different causal graph structures in parts a and b represent?", "answer": "The two causal graphs correspond to insurance pricing and crime prediction scenarios. These different structures reflect how protected attributes and background variables causally influence outcomes differently across real-world fair prediction problems.", "doc_id": "1703.06856", "figure_id": "1703.06856_fig_2", "figure_number": 1, "image_path": "data/mineru_output/1703.06856/1703.06856/hybrid_auto/images/1703.06856_page0_fig1.jpg", "caption": "Figure 1: (a), (b) Two causal models for different real-world fair prediction scenarios. See Section 3.1 for discussion. (c) The graph corresponding to a causal model with $A$ being the protected attribute and $Y$ some outcome of interest, with background variables assumed to be independent. (d) Expanding the model to include an intermediate variable indicating whether the individual is employed with two (latent) background variables Prejudiced (if the person offering the job is prejudiced) and Qualifications (a measure of the individual’s qualifications). (e) A twin network representation of this system [28] under two different counterfactual levels for $A$ . This is created by copying nodes descending from $A$ , which inherit unaffected parents from the factual world.", "figure_type": "diagram", "visual_anchor": "Two separate graph structures labeled (a) and (b) showing different node arrangements", "text_evidence": "To provide an intuition for counterfactual fairness, we will consider two real-world fair prediction scenarios: insurance pricing and crime prediction. Each of these correspond to one of the two causal graphs in Figure 1(a),(b).", "query_type": "visual_definition"}
{"query_id": "l1_1703.06856_1703.06856_fig_7_215", "query": "Why do the Full row distributions show greater cyan-pink separation than Unaware row for black↔white predictions?", "answer": "The Full model uses race and sex directly to predict FYA, creating larger counterfactual differences between groups. The Unaware model excludes these attributes, reducing the distributional gap between actual and counterfactual predictions.", "doc_id": "1703.06856", "figure_id": "1703.06856_fig_7", "figure_number": 2, "image_path": "data/mineru_output/1703.06856/1703.06856/hybrid_auto/images/1703.06856_page0_fig6.jpg", "caption": "Figure 2: Left: A causal model for the problem of predicting law school success fairly. Right: Density plots of predicted $\\mathrm { F Y A } _ { a }$ and $\\mathrm { F Y A } _ { a ^ { \\prime } }$ .", "figure_type": "plot", "visual_anchor": "Cyan and pink density curves in top-left 'Full' panel versus bottom-left 'Unaware' panel for black↔white comparison", "text_evidence": "We estimate the error terms εG, εL by first fitting two models that each use race and sex to individually predict GPA and LSAT.", "query_type": "comparison_explanation"}
{"query_id": "l1_1703.06856_1703.06856_fig_7_216", "query": "Do the nearly overlapping cyan and pink curves in the Unaware female↔male panel support Fair Add's use of residuals?", "answer": "Yes, the overlapping distributions demonstrate that using residual estimates εG and εL removes the direct influence of sex on predictions, achieving fairness by making actual and counterfactual predictions nearly identical.", "doc_id": "1703.06856", "figure_id": "1703.06856_fig_7", "figure_number": 2, "image_path": "data/mineru_output/1703.06856/1703.06856/hybrid_auto/images/1703.06856_page0_fig6.jpg", "caption": "Figure 2: Left: A causal model for the problem of predicting law school success fairly. Right: Density plots of predicted $\\mathrm { F Y A } _ { a }$ and $\\mathrm { F Y A } _ { a ^ { \\prime } }$ .", "figure_type": "plot", "visual_anchor": "Bottom-right panel showing cyan and pink curves with minimal separation in Unaware row for female↔male", "text_evidence": "We then compute the residuals of each model (e.g., εG = GPA − ŶG PA(R,S)). We use these residual estimates of εG, εL to predict FYA. We call this Fair Add.", "query_type": "value_context"}
{"query_id": "l1_1703.06856_1703.06856_fig_7_217", "query": "How does the causal path from R through GPA and LSAT explain the Full row's bimodal pattern in black↔white?", "answer": "The causal model shows race R influences both GPA and LSAT (via εG and εL), which then determine FYA. This creates distinct prediction distributions for actual versus counterfactual racial groups, manifesting as separated cyan and pink peaks.", "doc_id": "1703.06856", "figure_id": "1703.06856_fig_7", "figure_number": 2, "image_path": "data/mineru_output/1703.06856/1703.06856/hybrid_auto/images/1703.06856_page0_fig6.jpg", "caption": "Figure 2: Left: A causal model for the problem of predicting law school success fairly. Right: Density plots of predicted $\\mathrm { F Y A } _ { a }$ and $\\mathrm { F Y A } _ { a ^ { \\prime } }$ .", "figure_type": "plot", "visual_anchor": "Left causal diagram with R→εG→GPA→FYA and R→εL→LSAT→FYA paths, corresponding to separated peaks in top-left Full panel", "text_evidence": "The causal graph corresponding to this model is shown in Figure 2, (Level 2).", "query_type": "visual_definition"}
{"query_id": "l1_1703.06856_1703.06856_fig_8_218", "query": "Why does Race have directed edges to Arrest, Searched, Frisked, and Force in the 2014 NYPD dataset model?", "answer": "The directed edges from Race to multiple outcome nodes represent potential causal pathways through which race may influence police actions in the 38,609 records of males stopped during 2014.", "doc_id": "1703.06856", "figure_id": "1703.06856_fig_8", "figure_number": 3, "image_path": "data/mineru_output/1703.06856/1703.06856/hybrid_auto/images/1703.06856_page0_fig7.jpg", "caption": "Figure 3: A causal model for the stop and frisk dataset.", "figure_type": "diagram", "visual_anchor": "Race node with four outgoing directed edges to Arrest, Searched, Frisked, and Force", "text_evidence": "We consider the data collected on males stopped during 2014 which constitutes 38,609 records. We limit our analysis to looking", "query_type": "comparison_explanation"}
{"query_id": "l1_1703.06856_1703.06856_fig_8_219", "query": "Does the edge from Searched to Arrest represent a path in the controlled direct effects definition?", "answer": "Yes, this edge would be part of the paths in P_G_A that are considered 'direct' paths when intervening on some paths from A to Y but not others.", "doc_id": "1703.06856", "figure_id": "1703.06856_fig_8", "figure_number": 3, "image_path": "data/mineru_output/1703.06856/1703.06856/hybrid_auto/images/1703.06856_page0_fig7.jpg", "caption": "Figure 3: A causal model for the stop and frisk dataset.", "figure_type": "diagram", "visual_anchor": "Directed edge from Searched node to Arrest node", "text_evidence": "This notion is related to controlled direct effects [29], where we intervene on some paths from A to Y, but not others. Paths in P_G_A are considered here to be the 'direct' paths", "query_type": "value_context"}
{"query_id": "l1_1703.06856_1703.06856_fig_8_220", "query": "How does the Weapon node's single incoming edge from Criminality relate to counterfactual fairness conditioning?", "answer": "The single edge indicates Weapon is conditionally independent of Race given Criminality, which aligns with conditioning on X and A similarly to the probability of sufficiency definition.", "doc_id": "1703.06856", "figure_id": "1703.06856_fig_8", "figure_number": 3, "image_path": "data/mineru_output/1703.06856/1703.06856/hybrid_auto/images/1703.06856_page0_fig7.jpg", "caption": "Figure 3: A causal model for the stop and frisk dataset.", "figure_type": "diagram", "visual_anchor": "Weapon node with only one incoming directed edge from Criminality node", "text_evidence": "and we condition on X and A similarly to the definition of probability of sufficiency (3). This definition is the same as the original counterfactual fairness definition", "query_type": "visual_definition"}
{"query_id": "l1_1703.06856_1703.06856_fig_9_221", "query": "Do the 38,609 male stops from 2014 explain why White and Black Hispanic distributions appear in distinct geographic clusters?", "answer": "Yes, the analysis is limited to males stopped during 2014 constituting 38,609 records, which forms the basis for visualizing the distribution of White and Black Hispanic populations shown in the leftmost map.", "doc_id": "1703.06856", "figure_id": "1703.06856_fig_9", "figure_number": 4, "image_path": "data/mineru_output/1703.06856/1703.06856/hybrid_auto/images/1703.06856_page0_fig8.jpg", "caption": "Figure 4: How race affects arrest. The above maps show how altering one’s race affects whether or not they will be arrested, according to the model. The left-most plot shows the distribution of White and Black Hispanic populations in the stop-and-frisk dataset. The second plot shows the true arrests for all of the stops. Given our model we can compute whether or not every individual in the dataset would be arrest had they been white. We show this counterfactual in the third plot. Similarly, we can compute this counterfactual if everyone had been Black Hispanic, as shown in the fourth plot.", "figure_type": "plot", "visual_anchor": "Blue and orange dots in leftmost 'Race (data)' map showing population distribution", "text_evidence": "We consider the data collected on males stopped during 2014 which constitutes 38,609 records.", "query_type": "value_context"}
{"query_id": "l1_1703.06856_1703.06856_fig_9_222", "query": "Why does the third plot show predominantly white dots with red outlines compared to the second plot's arrest distribution?", "answer": "The third plot displays a counterfactual scenario where the model computed whether every individual would be arrested if they had been White, showing fewer predicted arrests than the actual data.", "doc_id": "1703.06856", "figure_id": "1703.06856_fig_9", "figure_number": 4, "image_path": "data/mineru_output/1703.06856/1703.06856/hybrid_auto/images/1703.06856_page0_fig8.jpg", "caption": "Figure 4: How race affects arrest. The above maps show how altering one’s race affects whether or not they will be arrested, according to the model. The left-most plot shows the distribution of White and Black Hispanic populations in the stop-and-frisk dataset. The second plot shows the true arrests for all of the stops. Given our model we can compute whether or not every individual in the dataset would be arrest had they been white. We show this counterfactual in the third plot. Similarly, we can compute this counterfactual if everyone had been Black Hispanic, as shown in the fourth plot.", "figure_type": "plot", "visual_anchor": "Third plot 'Arrest if White (counterfactual)' showing mostly white dots versus second plot 'Arrested (data)' showing mixed red and white dots", "text_evidence": "Given our model we can compute whether or not every individual in the dataset would be arrest had they been white. We show this counterfactual in the third plot.", "query_type": "comparison_explanation"}
{"query_id": "l1_1703.06856_1703.06856_fig_9_223", "query": "How does the fourth plot's increased red dot density in the yellow-circled region demonstrate racial effects on arrest outcomes?", "answer": "The fourth plot shows the counterfactual where everyone is treated as Black Hispanic, resulting in more arrests (red dots) compared to other scenarios, demonstrating that race significantly affects arrest likelihood in the model.", "doc_id": "1703.06856", "figure_id": "1703.06856_fig_9", "figure_number": 4, "image_path": "data/mineru_output/1703.06856/1703.06856/hybrid_auto/images/1703.06856_page0_fig8.jpg", "caption": "Figure 4: How race affects arrest. The above maps show how altering one’s race affects whether or not they will be arrested, according to the model. The left-most plot shows the distribution of White and Black Hispanic populations in the stop-and-frisk dataset. The second plot shows the true arrests for all of the stops. Given our model we can compute whether or not every individual in the dataset would be arrest had they been white. We show this counterfactual in the third plot. Similarly, we can compute this counterfactual if everyone had been Black Hispanic, as shown in the fourth plot.", "figure_type": "plot", "visual_anchor": "Fourth plot 'Arrest if Black Hispanic (counterfactual)' showing higher concentration of red dots in yellow-circled northern region", "text_evidence": "Similarly, we can compute this counterfactual if everyone had been Black Hispanic, as shown in the fourth plot.", "query_type": "comparison_explanation"}
{"query_id": "l1_1705.10378_1705.10378_fig_1_224", "query": "How many mediators exist between A and Y in this graph with vertices representing observed random variables?", "answer": "There is a single mediator M between A and Y, as shown by the paths A→M→Y in the causal graph where vertices represent observed random variables.", "doc_id": "1705.10378", "figure_id": "1705.10378_fig_1", "figure_number": 1, "image_path": "data/mineru_output/1705.10378/1705.10378/hybrid_auto/images/1705.10378_page0_fig0.jpg", "caption": "Figure 1: (a) A causal graph with a single mediator. (b) A causal graph with two mediators, one confounded with the outcome via an unobserved common cause. (c) A causal graph with a single mediator where the natural direct effect is not identified.", "figure_type": "diagram", "visual_anchor": "Node M at bottom-right with incoming arrow from A and outgoing arrow to Y", "text_evidence": "In such graphs, vertices represent observed random variables, and absence of directed edges represents absence of direct causal relationships.", "query_type": "value_context"}
{"query_id": "l1_1705.10378_1705.10378_fig_1_225", "query": "Does the path A→Y represent a direct causal pathway distinct from the A→M→Y mediated pathway?", "answer": "Yes, the direct arrow A→Y represents one causal pathway, while A→M→Y represents a mediated pathway through M, allowing decomposition of effects along particular causal pathways.", "doc_id": "1705.10378", "figure_id": "1705.10378_fig_1", "figure_number": 1, "image_path": "data/mineru_output/1705.10378/1705.10378/hybrid_auto/images/1705.10378_page0_fig0.jpg", "caption": "Figure 1: (a) A causal graph with a single mediator. (b) A causal graph with two mediators, one confounded with the outcome via an unobserved common cause. (c) A causal graph with a single mediator where the natural direct effect is not identified.", "figure_type": "diagram", "visual_anchor": "Blue arrow directly connecting node A to node Y, bypassing node M", "text_evidence": "we may wish to decompose the effect of $A$ on $Y$ into the contribution of the path $A  W  Y$ , and the path bundle $A  Y$ and $A \\to M \\to W \\to Y .$", "query_type": "comparison_explanation"}
{"query_id": "l1_1705.10378_1705.10378_fig_1_226", "query": "Why does C point to both A and Y if causal models generalize Bayesian networks?", "answer": "C pointing to both A and Y indicates C is a common cause (confounder) that creates conditional dependencies, which causal models encode beyond standard Bayesian network independence structures.", "doc_id": "1705.10378", "figure_id": "1705.10378_fig_1", "figure_number": 1, "image_path": "data/mineru_output/1705.10378/1705.10378/hybrid_auto/images/1705.10378_page0_fig0.jpg", "caption": "Figure 1: (a) A causal graph with a single mediator. (b) A causal graph with two mediators, one confounded with the outcome via an unobserved common cause. (c) A causal graph with a single mediator where the natural direct effect is not identified.", "figure_type": "diagram", "visual_anchor": "Node C at top-left with blue arrows pointing to both A (bottom-left) and Y (top-right)", "text_evidence": "Such models generalize independence models on directed acyclic graphs, also known as Bayesian networks (Pearl 1988), to also encode conditional independence statements on counterfactual random variables", "query_type": "visual_definition"}
{"query_id": "l1_1705.10378_1705.10378_fig_4_227", "query": "Why does the blue arrow from A to M represent prior convictions as the mediator in COMPAS?", "answer": "In the COMPAS dataset, A denotes race and M represents prior convictions as the mediator between race and recidivism outcome Y, showing how race indirectly affects recidivism through prior convictions.", "doc_id": "1705.10378", "figure_id": "1705.10378_fig_4", "figure_number": 2, "image_path": "data/mineru_output/1705.10378/1705.10378/hybrid_auto/images/1705.10378_page0_fig3.jpg", "caption": "Figure 2: Causal graphs for (a) the COMPAS dataset, and (b) the Adult dataset.", "figure_type": "diagram", "visual_anchor": "Blue arrow from node A to node M in diagram (a)", "text_evidence": "The simplified causal graph model for this task is given in Figure 2 (a), where $A$ denotes race, prior convictions is the mediator $M$ , demographic information such as age and gender are collected in C, and $Y$ is recidivism.", "query_type": "visual_definition"}
{"query_id": "l1_1705.10378_1705.10378_fig_4_228", "query": "Does the green arrow from M to Y in diagram b represent a different causal pathway than COMPAS?", "answer": "Yes, the green arrow indicates a distinct mediating pathway in the Adult dataset where M represents different mediating variables (like education and job-related factors) affecting income outcome Y, unlike COMPAS where M is prior convictions.", "doc_id": "1705.10378", "figure_id": "1705.10378_fig_4", "figure_number": 2, "image_path": "data/mineru_output/1705.10378/1705.10378/hybrid_auto/images/1705.10378_page0_fig3.jpg", "caption": "Figure 2: Causal graphs for (a) the COMPAS dataset, and (b) the Adult dataset.", "figure_type": "diagram", "visual_anchor": "Green arrow from node M to node Y in diagram (b)", "text_evidence": "The \"adult\" dataset from the UCI repository has records on 14 attributes such as demographic information, level of education, and job related variables such as occupation and work class on 48842 instances along with their income", "query_type": "comparison_explanation"}
{"query_id": "l1_1705.10378_1705.10378_fig_4_229", "query": "How does estimating NDE using the causal paths shown enable robust fair inference despite Y model misspecification?", "answer": "The NDE (Natural Direct Effect) quantifying discrimination can be estimated through the causal paths without relying on correct specification of the Y model, allowing fair inferences to remain valid even when the outcome model is misspecified.", "doc_id": "1705.10378", "figure_id": "1705.10378_fig_4", "figure_number": 2, "image_path": "data/mineru_output/1705.10378/1705.10378/hybrid_auto/images/1705.10378_page0_fig3.jpg", "caption": "Figure 2: Causal graphs for (a) the COMPAS dataset, and (b) the Adult dataset.", "figure_type": "diagram", "visual_anchor": "Direct and indirect paths from A through M to Y in both diagrams", "text_evidence": "We also illustrate how a part of the model involving the outcome $Y$ may be regularized without compromising fair inferences if the NDE quantifying discrimination is estimated using methods that are robust to misspecification of the $Y$ model.", "query_type": "value_context"}
{"query_id": "l1_1706.02409_1706.02409_fig_4_230", "query": "Why do the cyan and green Individual curves decline gradually across fairness loss 0 to 0.05?", "answer": "The curves are obtained by varying the weight λ on the fairness regularizer, showing the trade-off between accuracy (MSE) and fairness as λ increases across different values.", "doc_id": "1706.02409", "figure_id": "1706.02409_fig_4", "figure_number": 1, "image_path": "data/mineru_output/1706.02409/1706.02409/hybrid_auto/images/1706.02409_page0_fig3.jpg", "caption": "Figure 1: Efficient frontiers of accuracy vs. fairness for each dataset. For datasets with binary-valued targets (logistic regression), we consider three fairness notions (group, individual and hybrid), and for each examine building a single model or separate models for each group, yielding a total of six curves. For real-valued targets (linear regression), we consider two fairness notions (group and individual), and again single or separate models, yielding a total of four curves.", "figure_type": "plot", "visual_anchor": "cyan (Individual, single) and green (Individual, separate) curves showing gradual decline", "text_evidence": "These curves are shown in Figure 1, and are obtained by varying the weight $\\lambda$ on the fairness regularizer, and for each value of $\\lambda$ finding the model which minimizes the associated regularized loss function.", "query_type": "comparison_explanation"}
{"query_id": "l1_1706.02409_1706.02409_fig_4_231", "query": "Does the purple Hybrid separate curve dropping sharply near zero fairness loss support qualitative cross-dataset comparison?", "answer": "Yes, the sharp drop demonstrates that in some datasets the fairness penalty can be substantially decreased with little cost to accuracy, enabling qualitative comparisons across datasets.", "doc_id": "1706.02409", "figure_id": "1706.02409_fig_4", "figure_number": 1, "image_path": "data/mineru_output/1706.02409/1706.02409/hybrid_auto/images/1706.02409_page0_fig3.jpg", "caption": "Figure 1: Efficient frontiers of accuracy vs. fairness for each dataset. For datasets with binary-valued targets (logistic regression), we consider three fairness notions (group, individual and hybrid), and for each examine building a single model or separate models for each group, yielding a total of six curves. For real-valued targets (linear regression), we consider two fairness notions (group and individual), and again single or separate models, yielding a total of four curves.", "figure_type": "plot", "visual_anchor": "purple curve (Hybrid, separate) showing sharp drop from MSE ~0.17 to ~0.15 near fairness loss = 0", "text_evidence": "The efficient fairness/accuracy frontiers pictured in Figure 1 can be compared across data sets in a qualitative sense — e.g. to see that in some datasets, the fairness penalty can be substantially decreased with little cost to accuracy.", "query_type": "value_context"}
{"query_id": "l1_1706.02409_1706.02409_fig_4_232", "query": "Why are only six curves shown when race or gender protected features define the subgroups?", "answer": "For binary-valued targets, three fairness notions (group, individual, hybrid) are examined with either single or separate models per group, yielding six curves total (3 notions × 2 model types).", "doc_id": "1706.02409", "figure_id": "1706.02409_fig_4", "figure_number": 1, "image_path": "data/mineru_output/1706.02409/1706.02409/hybrid_auto/images/1706.02409_page0_fig3.jpg", "caption": "Figure 1: Efficient frontiers of accuracy vs. fairness for each dataset. For datasets with binary-valued targets (logistic regression), we consider three fairness notions (group, individual and hybrid), and for each examine building a single model or separate models for each group, yielding a total of six curves. For real-valued targets (linear regression), we consider two fairness notions (group and individual), and again single or separate models, yielding a total of four curves.", "figure_type": "plot", "visual_anchor": "six distinct curves in the legend: three fairness types (Group, Hybrid, Individual) each with single and separate variants", "text_evidence": "For datasets with binary-valued targets (logistic regression), we consider three fairness notions (group, individual and hybrid), and for each examine building a single model or separate models for each group, yielding a total of six curves.", "query_type": "visual_definition"}
{"query_id": "l1_1706.02409_1706.02409_fig_10_233", "query": "Do the cyan and green bars representing individual fairness regularizers reach the highest Price of Fairness at alpha 0.01?", "answer": "Yes, the cyan (individual, separate) and green (individual, single) bars at alpha 0.01 reach approximately 1.6, the highest values shown for individual fairness regularizers across all alpha values in the plot.", "doc_id": "1706.02409", "figure_id": "1706.02409_fig_10", "figure_number": 2, "image_path": "data/mineru_output/1706.02409/1706.02409/hybrid_auto/images/1706.02409_page0_fig9.jpg", "caption": "Figure 2: The “Price of Fairness” across data sets, for each type of fairness regularizer, in both the single and separate model case.", "figure_type": "plot", "visual_anchor": "cyan and green bars at rightmost position (alpha=0.01)", "text_evidence": "Figure 2 displays the PoF on each of the 6 datasets we study, for each fairness regularizer (individual, hybrid, and group), and for the single and separate model case.", "query_type": "value_context"}
{"query_id": "l1_1706.02409_1706.02409_fig_10_234", "query": "Why do the blue and red bars for group fairness remain near 1.0 across all alpha values compared to other regularizers?", "answer": "The group fairness regularizers (blue and red bars) consistently show the lowest Price of Fairness near 1.0, suggesting that group-based fairness constraints impose minimal performance cost compared to individual or hybrid approaches across different regularization strengths.", "doc_id": "1706.02409", "figure_id": "1706.02409_fig_10", "figure_number": 2, "image_path": "data/mineru_output/1706.02409/1706.02409/hybrid_auto/images/1706.02409_page0_fig9.jpg", "caption": "Figure 2: The “Price of Fairness” across data sets, for each type of fairness regularizer, in both the single and separate model case.", "figure_type": "plot", "visual_anchor": "blue (Group, separate) and red (Group, single) bars consistently at approximately 1.0 height", "text_evidence": "The \"Price of Fairness\" across data sets, for each type of fairness regularizer, in both the single and separate model case.", "query_type": "comparison_explanation"}
{"query_id": "l1_1706.02409_1706.02409_fig_10_235", "query": "What explains the progressive height increase in magenta and yellow hybrid bars as alpha decreases from 0.5 to 0.01?", "answer": "As alpha decreases (stronger regularization), the hybrid fairness regularizers show increasing Price of Fairness, indicating that stricter hybrid fairness constraints progressively sacrifice more model performance to achieve fairness goals.", "doc_id": "1706.02409", "figure_id": "1706.02409_fig_10", "figure_number": 2, "image_path": "data/mineru_output/1706.02409/1706.02409/hybrid_auto/images/1706.02409_page0_fig9.jpg", "caption": "Figure 2: The “Price of Fairness” across data sets, for each type of fairness regularizer, in both the single and separate model case.", "figure_type": "plot", "visual_anchor": "magenta (Hybrid, separate) and yellow (Hybrid, single) bars increasing from left to right", "text_evidence": "In Proceedings of the 16th International Conference on Data Mining, pages 1–10, 2016. [2] Julia Angwin, Jeff Larson, Surya Mattu, and Lauren Kirchner. Machine bias. Propublica, 2016.", "query_type": "anomaly_cause"}
{"query_id": "l1_1706.02744_1706.02744_fig_1_236", "query": "Why does the arrow from A to X matter when evaluating the direct effect of gender on admission R?", "answer": "The direct effect of gender A on admission R cannot be ascribed to department choice X when X is on the causal path from A to R. The arrow from A to X shows that gender influences department choice, which then affects admission.", "doc_id": "1706.02744", "figure_id": "1706.02744_fig_1", "figure_number": 1, "image_path": "data/mineru_output/1706.02744/1706.02744/hybrid_auto/images/1706.02744_page0_fig0.jpg", "caption": "Figure 1: The admission decision $R$ does not only directly depend on gender $A$ , but also on department choice $X$ , which in turn is also affected by gender $A$ .", "figure_type": "diagram", "visual_anchor": "Arrow from node A to node X", "text_evidence": "From the causal point of view, what matters is the direct effect of the protected attribute (here, gender $A$ ) on the decision (here, college admission $R$ ) that cannot be ascribed to a resolving variable such as department choice $X$", "query_type": "comparison_explanation"}
{"query_id": "l1_1706.02744_1706.02744_fig_1_237", "query": "Does the arrow from X to R support the claim that admission depends on department choice?", "answer": "Yes, the arrow from X to R directly represents that the admission decision R depends on department choice X, as stated in the context that R depends on X.", "doc_id": "1706.02744", "figure_id": "1706.02744_fig_1", "figure_number": 1, "image_path": "data/mineru_output/1706.02744/1706.02744/hybrid_auto/images/1706.02744_page0_fig0.jpg", "caption": "Figure 1: The admission decision $R$ does not only directly depend on gender $A$ , but also on department choice $X$ , which in turn is also affected by gender $A$ .", "figure_type": "diagram", "visual_anchor": "Arrow from node X to node R", "text_evidence": "The admission decision $R$ does not only directly depend on gender $A$ , but also on department choice $X$ , which in turn is also affected by gender $A$", "query_type": "value_context"}
{"query_id": "l1_1706.02744_1706.02744_fig_1_238", "query": "What role do nodes A, X, and R play in organizing assumptions about the data generating process?", "answer": "Node A represents the protected attribute (gender), node X represents features (department choice), and node R represents the predictor (admission decision). These nodes organize causal assumptions about how gender affects admission both directly and through department choice.", "doc_id": "1706.02744", "figure_id": "1706.02744_fig_1", "figure_number": 1, "image_path": "data/mineru_output/1706.02744/1706.02744/hybrid_auto/images/1706.02744_page0_fig0.jpg", "caption": "Figure 1: The admission decision $R$ does not only directly depend on gender $A$ , but also on department choice $X$ , which in turn is also affected by gender $A$ .", "figure_type": "diagram", "visual_anchor": "Three nodes A, X, and R with connecting arrows", "text_evidence": "Causal graphs are a convenient way of organizing assumptions about the data generating process. We will generally consider causal graphs involving a protected attribute $A$ , a set of proxy variables $P _ { - }$ , features $X$ , a predictor $R$", "query_type": "visual_definition"}
{"query_id": "l1_1706.02744_1706.02744_fig_2_239", "query": "Does the green node X₁ exhibit unresolved discrimination in the right graph along the red paths?", "answer": "Yes, if X₁ is a resolving variable, R* exhibits unresolved discrimination in the right graph along the red paths, but not in the left one.", "doc_id": "1706.02744", "figure_id": "1706.02744_fig_2", "figure_number": 2, "image_path": "data/mineru_output/1706.02744/1706.02744/hybrid_auto/images/1706.02744_page0_fig1.jpg", "caption": "Figure 2: Two graphs that may generate the same joint distribution for the Bayes optimal unconstrained predictor $R ^ { * }$ . If $X _ { 1 }$ is a resolving variable, $R ^ { * }$ exhibits unresolved discrimination in the right graph (along the red paths), but not in the left one.", "figure_type": "diagram", "visual_anchor": "green node X₁ with red paths to Y and X₂ in right graph", "text_evidence": "If $X _ { 1 }$ is a resolving variable, $R ^ { * }$ exhibits unresolved discrimination in the right graph (along the red paths), but not in the left one.", "query_type": "value_context"}
{"query_id": "l1_1706.02744_1706.02744_fig_2_240", "query": "Why can both graphs generate identical joint distributions for R* despite different edge structures between X₁ and other nodes?", "answer": "Both graphs represent different causal structures that may produce the same joint distribution for the Bayes optimal unconstrained predictor R*, illustrating observational equivalence despite different causal mechanisms.", "doc_id": "1706.02744", "figure_id": "1706.02744_fig_2", "figure_number": 2, "image_path": "data/mineru_output/1706.02744/1706.02744/hybrid_auto/images/1706.02744_page0_fig1.jpg", "caption": "Figure 2: Two graphs that may generate the same joint distribution for the Bayes optimal unconstrained predictor $R ^ { * }$ . If $X _ { 1 }$ is a resolving variable, $R ^ { * }$ exhibits unresolved discrimination in the right graph (along the red paths), but not in the left one.", "figure_type": "diagram", "visual_anchor": "two distinct graph structures with node R* at top", "text_evidence": "Two graphs that may generate the same joint distribution for the Bayes optimal unconstrained predictor $R ^ { * }$", "query_type": "comparison_explanation"}
{"query_id": "l1_1706.02744_1706.02744_fig_2_241", "query": "What makes interventions on variables like those represented by node A difficult to perform in randomized trials?", "answer": "Interventions on deeply rooted individual properties such as gender or race are notoriously difficult to conceptualize at an individual level and impossible to perform in randomized trials.", "doc_id": "1706.02744", "figure_id": "1706.02744_fig_2", "figure_number": 2, "image_path": "data/mineru_output/1706.02744/1706.02744/hybrid_auto/images/1706.02744_page0_fig1.jpg", "caption": "Figure 2: Two graphs that may generate the same joint distribution for the Bayes optimal unconstrained predictor $R ^ { * }$ . If $X _ { 1 }$ is a resolving variable, $R ^ { * }$ exhibits unresolved discrimination in the right graph (along the red paths), but not in the left one.", "figure_type": "diagram", "visual_anchor": "node A with outgoing arrows to X₁ and X₂", "text_evidence": "Interventions on deeply rooted individual properties such as gender or race are notoriously difficult to conceptualize—especially at an individual level, and impossible to perform in a randomized trial.", "query_type": "value_context"}
{"query_id": "l1_1706.02744_1706.02744_fig_4_242", "query": "Why does the intervened graph remove the edge from green A to red P when proxy discrimination is being avoided?", "answer": "The benevolent viewpoint aims to guarantee that proxy P has no overall influence on prediction by adjusting P→R to cancel influence along P→X→R, requiring intervention on the A→P edge.", "doc_id": "1706.02744", "figure_id": "1706.02744_fig_4", "figure_number": 3, "image_path": "data/mineru_output/1706.02744/1706.02744/hybrid_auto/images/1706.02744_page0_fig3.jpg", "caption": "Figure 3: A template graph $\\tilde { \\mathcal { G } }$ for proxy discrimination (left) with its intervened version $\\mathcal { G }$ (right). While from the benevolent viewpoint we do not generically prohibit any influence from $A$ on $R$ , we want to guarantee that the proxy $P$ has no overall influence on the prediction, by adjusting $P  R$ to cancel the influence along $P $ $X  R$ in the intervened graph.", "figure_type": "diagram", "visual_anchor": "removed edge from green node A to red node P in the right graph", "text_evidence": "we want to guarantee that the proxy $P$ has no overall influence on the prediction, by adjusting $P  R$ to cancel the influence along $P $ $X  R$ in the intervened graph", "query_type": "comparison_explanation"}
{"query_id": "l1_1706.02744_1706.02744_fig_4_243", "query": "Does the red node P receiving input from green A and black NP align with proxies having additional unobserved causes?", "answer": "Yes, the diagram shows P receives inputs from both protected attribute A and unobserved cause NP, where NP, NX, and A are pairwise independent as stated.", "doc_id": "1706.02744", "figure_id": "1706.02744_fig_4", "figure_number": 3, "image_path": "data/mineru_output/1706.02744/1706.02744/hybrid_auto/images/1706.02744_page0_fig3.jpg", "caption": "Figure 3: A template graph $\\tilde { \\mathcal { G } }$ for proxy discrimination (left) with its intervened version $\\mathcal { G }$ (right). While from the benevolent viewpoint we do not generically prohibit any influence from $A$ on $R$ , we want to guarantee that the proxy $P$ has no overall influence on the prediction, by adjusting $P  R$ to cancel the influence along $P $ $X  R$ in the intervened graph.", "figure_type": "diagram", "visual_anchor": "red node P with incoming edges from green node A and black node NP", "text_evidence": "A protected attribute $A$ affects a proxy $P$ as well as a feature $X$ . Both $P$ and $X$ have additional unobserved causes $N _ { P }$ and $N _ { X }$ , where $N _ { P } , N _ { X } , A$ are pairwise independent", "query_type": "value_context"}
{"query_id": "l1_1706.02744_1706.02744_fig_4_244", "query": "What structural change between left and right graphs illustrates the procedure for the example in Figure 3?", "answer": "The right intervened graph removes the A→P edge while preserving other relationships, demonstrating the intervention step of the general procedure being illustrated through this example.", "doc_id": "1706.02744", "figure_id": "1706.02744_fig_4", "figure_number": 3, "image_path": "data/mineru_output/1706.02744/1706.02744/hybrid_auto/images/1706.02744_page0_fig3.jpg", "caption": "Figure 3: A template graph $\\tilde { \\mathcal { G } }$ for proxy discrimination (left) with its intervened version $\\mathcal { G }$ (right). While from the benevolent viewpoint we do not generically prohibit any influence from $A$ on $R$ , we want to guarantee that the proxy $P$ has no overall influence on the prediction, by adjusting $P  R$ to cancel the influence along $P $ $X  R$ in the intervened graph.", "figure_type": "diagram", "visual_anchor": "comparison between left template graph and right intervened graph structure", "text_evidence": "While presenting the general procedure, we illustrate each step in the example shown in Figure 3", "query_type": "comparison_explanation"}
{"query_id": "l1_1706.02744_1706.02744_fig_5_245", "query": "Why does the left graph show green E with arrows to both X and R while requiring A's influence on R to be canceled?", "answer": "The skeptical viewpoint generically does not want A to influence R, but first intervenes on E to interrupt all paths through E, then cancels only the remaining influence from A to R.", "doc_id": "1706.02744", "figure_id": "1706.02744_fig_5", "figure_number": 4, "image_path": "data/mineru_output/1706.02744/1706.02744/hybrid_auto/images/1706.02744_page0_fig4.jpg", "caption": "Figure 4: A template graph $\\tilde { \\mathcal { G } }$ for unresolved discrimination (left) with its intervened version $\\mathcal { G }$ (right). While from the skeptical viewpoint we generically do not want $A$ to influence $R$ , we first intervene on $E$ interrupting all paths through $E$ and only cancel the remaining influence on $A$ to $R$ .", "figure_type": "diagram", "visual_anchor": "Green node E with outgoing arrows to X and R in the left template graph \\tilde{\\mathcal{G}}", "text_evidence": "While from the skeptical viewpoint we generically do not want $A$ to influence $R$ , we first intervene on $E$ interrupting all paths through $E$ and only cancel the remaining influence on $A$ to $R$", "query_type": "comparison_explanation"}
{"query_id": "l1_1706.02744_1706.02744_fig_5_246", "query": "How does the right graph's structure enable writing R_θ(P,X) using only root nodes from the intervened version?", "answer": "By intervening on E (eliminating its outgoing arrows in the right graph), variables can be iteratively substituted until R_θ is expressed as R_θ(P, g(t\\bar{a}^{\\mathcal{G}}(X))) with only root nodes {A, P, N_X} remaining.", "doc_id": "1706.02744", "figure_id": "1706.02744_fig_5", "figure_number": 4, "image_path": "data/mineru_output/1706.02744/1706.02744/hybrid_auto/images/1706.02744_page0_fig4.jpg", "caption": "Figure 4: A template graph $\\tilde { \\mathcal { G } }$ for unresolved discrimination (left) with its intervened version $\\mathcal { G }$ (right). While from the skeptical viewpoint we generically do not want $A$ to influence $R$ , we first intervene on $E$ interrupting all paths through $E$ and only cancel the remaining influence on $A$ to $R$ .", "figure_type": "diagram", "visual_anchor": "Right intervened graph \\mathcal{G} with green node E having no outgoing arrows", "text_evidence": "Iteratively substitute variables in the equation for $R _ { \\theta }$ from their structural equations until only root nodes of the intervened graph are left, i.e. write $R _ { \\theta } ( P , X )$ as $R _ { \\theta } ( P , g ( t \\bar { a } ^ { \\mathcal { G } } ( X ) ) )$ for some function $g$ . In the example, $t \\bar { a ( X ) } = \\{ A , P , N _ { X } \\}$", "query_type": "value_context"}
{"query_id": "l1_1706.02744_1706.02744_fig_5_247", "query": "What procedure modification does the right graph implement by fixing green E to a random variable η?", "answer": "The procedure adjusts to avoid unresolved discrimination by intervening on E, fixing it to a random variable η with distribution matching E's marginal distribution in the template graph.", "doc_id": "1706.02744", "figure_id": "1706.02744_fig_5", "figure_number": 4, "image_path": "data/mineru_output/1706.02744/1706.02744/hybrid_auto/images/1706.02744_page0_fig4.jpg", "caption": "Figure 4: A template graph $\\tilde { \\mathcal { G } }$ for unresolved discrimination (left) with its intervened version $\\mathcal { G }$ (right). While from the skeptical viewpoint we generically do not want $A$ to influence $R$ , we first intervene on $E$ interrupting all paths through $E$ and only cancel the remaining influence on $A$ to $R$ .", "figure_type": "diagram", "visual_anchor": "Green node E in the right intervened graph \\mathcal{G}", "text_evidence": "Let us now try to adjust the previous procedure to the context of avoiding unresolved discrimination. 1. Intervene on $E$ by fixing it to a random variable $\\eta$ with $\\mathbb { P } ( \\eta ) = \\mathbb { P } ( E )$ , the marginal distribution of $E$ in $\\tilde { \\mathcal { G } }$", "query_type": "visual_definition"}
{"query_id": "l1_1706.02744_1706.02744_fig_6_248", "query": "Why must the solid arrow from A to P remain when requiring distribution of R_θ to be independent of p?", "answer": "The solid arrow from A to P represents a direct causal path that must be considered when ensuring R_θ's distribution independence from p, as this structural relationship affects how the predictor separates P from other roots in the hypothesis class specification.", "doc_id": "1706.02744", "figure_id": "1706.02744_fig_6", "figure_number": 5, "image_path": "data/mineru_output/1706.02744/1706.02744/hybrid_auto/images/1706.02744_page0_fig5.jpg", "caption": "Figure 5: Left: A generic graph $\\tilde { \\mathcal { G } }$ to describe proxy discrimination. Right: The graph corresponding to an intervention on $P$ . The circle labeled “DAG” represents any sub-DAG of $\\tilde { \\mathcal { G } }$ and $\\mathcal { G }$ containing an arbitrary number of variables that is compatible with the shown arrows. Dashed arrows can, but do not have to be present in a given scenario.", "figure_type": "diagram", "visual_anchor": "solid arrow from node A to node P", "text_evidence": "We seek to write the predictor as a function of P and all the other roots of G separately. If our hypothesis class is such that there exists θ̃ such that R_θ(P,g(ta(X)))=R_θ̃(P,g̃(ta(X)∖{P})), we call the structural equation model and hypothesis class spec", "query_type": "comparison_explanation"}
{"query_id": "l1_1706.02744_1706.02744_fig_6_249", "query": "How does the dashed arrow from P to DAG enable blocking all directed paths from ancestors of P to X?", "answer": "The dashed arrow from P to DAG indicates optional paths that can be blocked by P. When P blocks all directed paths from its ancestors to X, predictors using adjusted features X̃:=X-E[X|P] exhibit no proxy discrimination.", "doc_id": "1706.02744", "figure_id": "1706.02744_fig_6", "figure_number": 5, "image_path": "data/mineru_output/1706.02744/1706.02744/hybrid_auto/images/1706.02744_page0_fig5.jpg", "caption": "Figure 5: Left: A generic graph $\\tilde { \\mathcal { G } }$ to describe proxy discrimination. Right: The graph corresponding to an intervention on $P$ . The circle labeled “DAG” represents any sub-DAG of $\\tilde { \\mathcal { G } }$ and $\\mathcal { G }$ containing an arbitrary number of variables that is compatible with the shown arrows. Dashed arrows can, but do not have to be present in a given scenario.", "figure_type": "diagram", "visual_anchor": "dashed arrow from node P to DAG circle", "text_evidence": "Under the assumptions of Theorem 2, if all directed paths from any ancestor of P to X in the graph G are blocked by P, then any predictor based on the adjusted features X̃:=X-E[X|P] exhibits no proxy discrimination", "query_type": "comparison_explanation"}
{"query_id": "l1_1706.02744_1706.02744_fig_6_250", "query": "Why does the dashed arrow from X to DAG matter for individual proxy discrimination comparing examples with same features X?", "answer": "The dashed arrow from X to DAG represents potential paths affecting unobserved non-feature variables. Individual proxy discrimination compares examples with identical X but different P values, which can involve different unobserved variables connected through this DAG structure.", "doc_id": "1706.02744", "figure_id": "1706.02744_fig_6", "figure_number": 5, "image_path": "data/mineru_output/1706.02744/1706.02744/hybrid_auto/images/1706.02744_page0_fig5.jpg", "caption": "Figure 5: Left: A generic graph $\\tilde { \\mathcal { G } }$ to describe proxy discrimination. Right: The graph corresponding to an intervention on $P$ . The circle labeled “DAG” represents any sub-DAG of $\\tilde { \\mathcal { G } }$ and $\\mathcal { G }$ containing an arbitrary number of variables that is compatible with the shown arrows. Dashed arrows can, but do not have to be present in a given scenario.", "figure_type": "diagram", "visual_anchor": "dashed arrow from node X to DAG circle", "text_evidence": "Individual proxy discrimination aims at comparing examples with the same features X, for different values of P. Note that this can be individuals with different values for the unobserved non-feature variables.", "query_type": "value_context"}
{"query_id": "l1_1707.00574_1707.00574_fig_2_251", "query": "Why does the dark purple region shift leftward toward lower β values as α increases from 1.0 to 3.0?", "answer": "The maximum average quality q occurs at approximately β=0.4 when α=1, but for α=3 the maximum shifts to a lower β value, indicating that higher popularity bias requires reduced β to maintain optimal quality.", "doc_id": "1707.00574", "figure_id": "1707.00574_fig_2", "figure_number": 1, "image_path": "data/mineru_output/1707.00574/1707.00574/hybrid_auto/images/1707.00574_page0_fig1.jpg", "caption": "Figure 1: Effects of popularity bias on average quality and faithfulness.. (a) Heatmap of average quality $q$ as a function of $\\alpha$ and $\\beta$ , showing that $q$ reaches a maximum for $\\alpha = 1$ and $\\beta \\approx 0 . 4$ , while for $\\alpha = 3$ the maximum is attained for a lower $\\beta$ . ( $b$ ) The location of the maximum $q$ as a function of $\\beta$ depends on $\\alpha$ , here shown for $\\alpha = 0 , 0 . 5 , 1 . 0$ . (c) Faithfulness $\\tau$ of the algorithm as a function of $\\alpha$ and $\\beta$ . ( $d$ ) $\\tau$ as a function of $\\beta$ for the same three values of $\\alpha$ . Standard errors are shown in panels $( b , d )$ and are smaller than the markers.", "figure_type": "plot", "visual_anchor": "Dark purple region in heatmap showing maximum q location shifting from center-right (α=1.0) to left (α=3.0)", "text_evidence": "Heatmap of average quality $q$ as a function of $\\alpha$ and $\\beta$ , showing that $q$ reaches a maximum for $\\alpha = 1$ and $\\beta \\approx 0 . 4$ , while for $\\alpha = 3$ the maximum is attained for a lower $\\beta$", "query_type": "comparison_explanation"}
{"query_id": "l1_1707.00574_1707.00574_fig_2_252", "query": "Does the predominantly yellow coloring across most β values at low α support algorithm faithfulness optimization?", "answer": "The yellow coloring indicates lower quality values. Panel (c) shows faithfulness τ, which is a separate metric from quality q shown in panel (a), so the yellow region reflects quality rather than faithfulness performance.", "doc_id": "1707.00574", "figure_id": "1707.00574_fig_2", "figure_number": 1, "image_path": "data/mineru_output/1707.00574/1707.00574/hybrid_auto/images/1707.00574_page0_fig1.jpg", "caption": "Figure 1: Effects of popularity bias on average quality and faithfulness.. (a) Heatmap of average quality $q$ as a function of $\\alpha$ and $\\beta$ , showing that $q$ reaches a maximum for $\\alpha = 1$ and $\\beta \\approx 0 . 4$ , while for $\\alpha = 3$ the maximum is attained for a lower $\\beta$ . ( $b$ ) The location of the maximum $q$ as a function of $\\beta$ depends on $\\alpha$ , here shown for $\\alpha = 0 , 0 . 5 , 1 . 0$ . (c) Faithfulness $\\tau$ of the algorithm as a function of $\\alpha$ and $\\beta$ . ( $d$ ) $\\tau$ as a function of $\\beta$ for the same three values of $\\alpha$ . Standard errors are shown in panels $( b , d )$ and are smaller than the markers.", "figure_type": "plot", "visual_anchor": "Yellow-colored region dominating the heatmap at α values between 0.0 and 1.5 across most β values", "text_evidence": "Faithfulness $\\tau$ of the algorithm as a function of $\\alpha$ and $\\beta$ . ( $d$ ) $\\tau$ as a function of $\\beta$ for the same three values of $\\alpha$", "query_type": "value_context"}
{"query_id": "l1_1707.00574_1707.00574_fig_2_253", "query": "What explains the sharp transition from yellow to dark purple at the upper-right corner near α=3.0 and β=1.0?", "answer": "This represents a dramatic increase in average quality q at extreme parameter combinations. The location of maximum q depends on α, with the relationship between these parameters determining optimal quality as popularity bias intensifies.", "doc_id": "1707.00574", "figure_id": "1707.00574_fig_2", "figure_number": 1, "image_path": "data/mineru_output/1707.00574/1707.00574/hybrid_auto/images/1707.00574_page0_fig1.jpg", "caption": "Figure 1: Effects of popularity bias on average quality and faithfulness.. (a) Heatmap of average quality $q$ as a function of $\\alpha$ and $\\beta$ , showing that $q$ reaches a maximum for $\\alpha = 1$ and $\\beta \\approx 0 . 4$ , while for $\\alpha = 3$ the maximum is attained for a lower $\\beta$ . ( $b$ ) The location of the maximum $q$ as a function of $\\beta$ depends on $\\alpha$ , here shown for $\\alpha = 0 , 0 . 5 , 1 . 0$ . (c) Faithfulness $\\tau$ of the algorithm as a function of $\\alpha$ and $\\beta$ . ( $d$ ) $\\tau$ as a function of $\\beta$ for the same three values of $\\alpha$ . Standard errors are shown in panels $( b , d )$ and are smaller than the markers.", "figure_type": "plot", "visual_anchor": "Abrupt color change from yellow to dark purple/black in top-right corner at α≈3.0, β≈1.0", "text_evidence": "The location of the maximum $q$ as a function of $\\beta$ depends on $\\alpha$ , here shown for $\\alpha = 0 , 0 . 5 , 1 . 0$", "query_type": "anomaly_cause"}
{"query_id": "l1_1707.00574_1707.00574_fig_5_254", "query": "Why does the black curve for lower exploration converge earlier than the red one, supporting sub-optimal outcomes?", "answer": "With α=2 (lower exploration), the system converges early to sub-optimal quality around 0.67, while α=1 (higher exploration) allows continued quality improvement, demonstrating that reduced exploration traps the system in local optima.", "doc_id": "1707.00574", "figure_id": "1707.00574_fig_5", "figure_number": 2, "image_path": "data/mineru_output/1707.00574/1707.00574/hybrid_auto/images/1707.00574_page0_fig4.jpg", "caption": "Figure 2: Temporal evolution of average quality. Average quality $q$ is traced over time for different values of popularity bias $\\beta$ , in two cases of higher and lower exploration ( $\\alpha = 1$ and $\\alpha = 2$ , respectively). Error bars represent standard errors across runs. With less exploration the system converges early to sub-optimal quality.", "figure_type": "plot", "visual_anchor": "Black curve (α=2) plateaus before 10⁴ while red curve (α=1) continues rising", "text_evidence": "With less exploration the system converges early to sub-optimal quality.", "query_type": "comparison_explanation"}
{"query_id": "l1_1707.00574_1707.00574_fig_5_255", "query": "Does the red curve reaching 0.67 at 10⁵ reflect difficulty in establishing inherent quality in cultural markets?", "answer": "Yes, the gradual convergence of the α=1 curve demonstrates that systems must explore extensively over time to approach optimal quality when inherent product quality is difficult to establish objectively.", "doc_id": "1707.00574", "figure_id": "1707.00574_fig_5", "figure_number": 2, "image_path": "data/mineru_output/1707.00574/1707.00574/hybrid_auto/images/1707.00574_page0_fig4.jpg", "caption": "Figure 2: Temporal evolution of average quality. Average quality $q$ is traced over time for different values of popularity bias $\\beta$ , in two cases of higher and lower exploration ( $\\alpha = 1$ and $\\alpha = 2$ , respectively). Error bars represent standard errors across runs. With less exploration the system converges early to sub-optimal quality.", "figure_type": "plot", "visual_anchor": "Red curve (α=1) gradually approaching 0.67 at time 10⁵", "text_evidence": "The inherent quality of cultural products is often difficult to establish, therefore relying on measurable quantitative features like the popularity of an item is hugely advantageous", "query_type": "value_context"}
{"query_id": "l1_1707.00574_1707.00574_fig_5_256", "query": "How does the popularity bias β=0.1 relate to the structural features causing both curves to plateau?", "answer": "The β=0.1 popularity bias creates competition for limited attention, which constrains both exploration levels from achieving higher quality beyond 0.67, reflecting structural limitations in cultural markets where popularity competes with quality discovery.", "doc_id": "1707.00574", "figure_id": "1707.00574_fig_5", "figure_number": 2, "image_path": "data/mineru_output/1707.00574/1707.00574/hybrid_auto/images/1707.00574_page0_fig4.jpg", "caption": "Figure 2: Temporal evolution of average quality. Average quality $q$ is traced over time for different values of popularity bias $\\beta$ , in two cases of higher and lower exploration ( $\\alpha = 1$ and $\\alpha = 2$ , respectively). Error bars represent standard errors across runs. With less exploration the system converges early to sub-optimal quality.", "figure_type": "plot", "visual_anchor": "Both curves plateau near 0.67 with β=0.1 label shown", "text_evidence": "Success in these markets may strongly depend on structural or subjective features, like competition for limited attention", "query_type": "comparison_explanation"}
{"query_id": "l1_1707.09457_1707.09457_fig_1_257", "query": "Why does the fifth image show MAN as agent when training data contains 33% men in cooking roles?", "answer": "The fifth image represents the minority case in the imSitu dataset where men appear as cooking agents. This 33% training distribution becomes amplified to only 16% after CRF training, demonstrating bias amplification.", "doc_id": "1707.09457", "figure_id": "1707.09457_fig_1", "figure_number": 1, "image_path": "data/mineru_output/1707.09457/1707.09457/hybrid_auto/images/1707.09457_page0_fig0.jpg", "caption": "Figure 1: Five example images from the imSitu visual semantic role labeling (vSRL) dataset. Each image is paired with a table describing a situation: the verb, cooking, its semantic roles, i.e agent, and noun values filling that role, i.e. woman. In the imSitu training set, $33 \\%$ of cooking images have man in the agent role while the rest have woman. After training a Conditional Random Field (CRF), bias is amplified: man fills $16 \\%$ of agent roles in cooking images. To reduce this bias amplification our calibration method adjusts weights of CRF potentials associated with biased predictions. After applying our methods, man appears in the agent role of $20 \\%$ of cooking images, reducing the bias amplification by $2 5 \\%$ , while keeping the CRF vSRL performance unchanged.", "figure_type": "example", "visual_anchor": "Fifth image table showing AGENT-MAN pair", "text_evidence": "In the imSitu training set, $33 \\%$ of cooking images have man in the agent role while the rest have woman. After training a Conditional Random Field (CRF), bias is amplified: man fills $16 \\%$ of agent roles in cooking images.", "query_type": "comparison_explanation"}
{"query_id": "l1_1707.09457_1707.09457_fig_1_258", "query": "How does the orange-boxed AGENT-WOMAN pair in the fourth image relate to bias amplification from 67% to 84%?", "answer": "The highlighted AGENT-WOMAN pair represents the majority class that becomes over-represented after CRF training. The original 67% (100%-33%) of women agents amplifies to 84% (100%-16%), showing the model's tendency to reinforce existing biases.", "doc_id": "1707.09457", "figure_id": "1707.09457_fig_1", "figure_number": 1, "image_path": "data/mineru_output/1707.09457/1707.09457/hybrid_auto/images/1707.09457_page0_fig0.jpg", "caption": "Figure 1: Five example images from the imSitu visual semantic role labeling (vSRL) dataset. Each image is paired with a table describing a situation: the verb, cooking, its semantic roles, i.e agent, and noun values filling that role, i.e. woman. In the imSitu training set, $33 \\%$ of cooking images have man in the agent role while the rest have woman. After training a Conditional Random Field (CRF), bias is amplified: man fills $16 \\%$ of agent roles in cooking images. To reduce this bias amplification our calibration method adjusts weights of CRF potentials associated with biased predictions. After applying our methods, man appears in the agent role of $20 \\%$ of cooking images, reducing the bias amplification by $2 5 \\%$ , while keeping the CRF vSRL performance unchanged.", "figure_type": "example", "visual_anchor": "Orange box highlighting AGENT-WOMAN in fourth image table", "text_evidence": "After training a Conditional Random Field (CRF), bias is amplified: man fills $16 \\%$ of agent roles in cooking images. To reduce this bias amplification our calibration method adjusts weights of CRF potentials associated with biased predictions.", "query_type": "visual_definition"}
{"query_id": "l1_1707.09457_1707.09457_fig_1_259", "query": "Does the SPATULA tool value appearing in multiple image tables support predictions based on cooking activity correlations?", "answer": "Yes, spatula appears as the TOOL value in multiple cooking images, demonstrating that structured prediction models can leverage correlations between activities and common tools. This illustrates how co-occurring labels help recognition but may also encode biases.", "doc_id": "1707.09457", "figure_id": "1707.09457_fig_1", "figure_number": 1, "image_path": "data/mineru_output/1707.09457/1707.09457/hybrid_auto/images/1707.09457_page0_fig0.jpg", "caption": "Figure 1: Five example images from the imSitu visual semantic role labeling (vSRL) dataset. Each image is paired with a table describing a situation: the verb, cooking, its semantic roles, i.e agent, and noun values filling that role, i.e. woman. In the imSitu training set, $33 \\%$ of cooking images have man in the agent role while the rest have woman. After training a Conditional Random Field (CRF), bias is amplified: man fills $16 \\%$ of agent roles in cooking images. To reduce this bias amplification our calibration method adjusts weights of CRF potentials associated with biased predictions. After applying our methods, man appears in the agent role of $20 \\%$ of cooking images, reducing the bias amplification by $2 5 \\%$ , while keeping the CRF vSRL performance unchanged.", "figure_type": "example", "visual_anchor": "TOOL-SPATULA pairs visible in first and fifth image tables", "text_evidence": "Structured prediction models are used in these tasks to take advantage of correlations between co-occurring labels and visual input but risk inadvertently encoding social biases found in web corpora.", "query_type": "value_context"}
{"query_id": "l1_1707.09457_1707.09457_fig_2_260", "query": "Why does shopping appear near 0.2 on both axes when the dataset shows 64.6% of verbs favor males?", "answer": "Shopping is one of the few female-biased verbs in the dataset. While 64.6% of verbs overall favor males with average bias of 0.707, shopping is positioned at approximately 0.2 training bias, indicating it strongly favors females (values near zero indicate bias toward women).", "doc_id": "1707.09457", "figure_id": "1707.09457_fig_2", "figure_number": 2, "image_path": "data/mineru_output/1707.09457/1707.09457/hybrid_auto/images/1707.09457_page0_fig1.jpg", "caption": "Figure 2: Gender bias analysis of imSitu vSRL and MS-COCO MLC. (a) gender bias of verbs toward man in the training set versus bias on a predicted development set. (b) gender bias of nouns toward man in the training set versus bias on the predicted development set. Values near zero indicate bias toward woman while values near 0.5 indicate unbiased variables. Across both dataset, there is significant bias toward males, and significant bias amplification after training on biased training data.", "figure_type": "plot", "visual_anchor": "shopping label positioned at approximately (0.2, 0.2) in lower left quadrant", "text_evidence": "Overall, the dataset is heavily biased toward male agents, with $6 4 . 6 \\%$ of verbs favoring a male agent by an average bias of 0.707 (roughly 3:1 male).", "query_type": "anomaly_cause"}
{"query_id": "l1_1707.09457_1707.09457_fig_2_261", "query": "Do verbs like tuning and coaching above the diagonal line demonstrate that models amplify existing gender bias?", "answer": "Yes, verbs positioned above the diagonal line (like tuning and coaching at approximately 0.95 predicted ratio versus 0.85-0.90 training ratio) show the model predicting higher male bias than present in training data, confirming bias amplification. This supports the hypothesis that models trained on biased data amplify the existing gender bias.", "doc_id": "1707.09457", "figure_id": "1707.09457_fig_2", "figure_number": 2, "image_path": "data/mineru_output/1707.09457/1707.09457/hybrid_auto/images/1707.09457_page0_fig1.jpg", "caption": "Figure 2: Gender bias analysis of imSitu vSRL and MS-COCO MLC. (a) gender bias of verbs toward man in the training set versus bias on a predicted development set. (b) gender bias of nouns toward man in the training set versus bias on the predicted development set. Values near zero indicate bias toward woman while values near 0.5 indicate unbiased variables. Across both dataset, there is significant bias toward males, and significant bias amplification after training on biased training data.", "figure_type": "plot", "visual_anchor": "tuning and coaching labels positioned above the blue diagonal trend line in upper right", "text_evidence": "We confirmed our hypothesis that (a) both the im-Situ and MS-COCO datasets, gathered from the web, are heavily gender biased and that (b) models trained to perform prediction on these datasets amplify the existing gender bias when evaluated on development data.", "query_type": "comparison_explanation"}
{"query_id": "l1_1707.09457_1707.09457_fig_2_262", "query": "Does the clustering of cooking, serving, microwaving, and washing below 0.4 support the claim about extremely biased verbs?", "answer": "Yes, these verbs clustered below 0.4 on the training axis represent extremely female-biased categories (values near zero favor women). This supports the finding that 46.95% of verbs favor a gender with a bias of at least 0.7, showing polarized gender associations in both male and female directions.", "doc_id": "1707.09457", "figure_id": "1707.09457_fig_2", "figure_number": 2, "image_path": "data/mineru_output/1707.09457/1707.09457/hybrid_auto/images/1707.09457_page0_fig1.jpg", "caption": "Figure 2: Gender bias analysis of imSitu vSRL and MS-COCO MLC. (a) gender bias of verbs toward man in the training set versus bias on a predicted development set. (b) gender bias of nouns toward man in the training set versus bias on the predicted development set. Values near zero indicate bias toward woman while values near 0.5 indicate unbiased variables. Across both dataset, there is significant bias toward males, and significant bias amplification after training on biased training data.", "figure_type": "plot", "visual_anchor": "cooking, serving, microwaving, washing labels clustered between 0.25-0.35 on x-axis in lower portion", "text_evidence": "Nearly half of verbs are extremely biased in the male or female direction: $4 6 . 9 5 \\%$ of verbs favor a gender with a bias of at least 0.7.6", "query_type": "value_context"}
{"query_id": "l1_1707.09457_1707.09457_fig_7_263", "query": "How does the 0.05 margin shown by dotted blue lines relate to the 40% reduction in violations?", "answer": "The dotted blue lines define the acceptable bias range. RBA reduced violations (red dots exceeding this margin) by 40%, demonstrating successful constraint enforcement.", "doc_id": "1707.09457", "figure_id": "1707.09457_fig_7", "figure_number": 3, "image_path": "data/mineru_output/1707.09457/1707.09457/hybrid_auto/images/1707.09457_page0_fig6.jpg", "caption": "Figure 3: Results of reducing bias amplification using RBA on imSitu vSRL and MS-COCO MLC. Figures 3(a)-(d) show initial training set bias along the x-axis and development set bias along the yaxis. Dotted blue lines indicate the 0.05 margin used in RBA, with points violating the margin shown in red while points meeting the margin are shown in green. Across both settings adding RBA significantly reduces the number of violations, and reduces the bias amplification significantly. Figures 3(e)-(f) demonstrate bias amplification as a function of training bias, with and without RBA. Across all initial training biases, RBA is able to reduce the bias amplification.", "figure_type": "plot", "visual_anchor": "dotted blue lines indicating 0.05 margin, with red dots as violations", "text_evidence": "Similarly to vSRL, we are able to reduce the number of objects whose bias exceeds the original training bias by $5 \\%$ , by $40 \\%$ (Viol.).", "query_type": "comparison_explanation"}
{"query_id": "l1_1707.09457_1707.09457_fig_7_264", "query": "Why do green dots dominate the upper-right region while red dots cluster at lower training ratios?", "answer": "RBA successfully reduces bias amplification across all initial training biases. Lower training ratios initially showed more violations, but after RBA application, most points (green) meet the margin constraint.", "doc_id": "1707.09457", "figure_id": "1707.09457_fig_7", "figure_number": 3, "image_path": "data/mineru_output/1707.09457/1707.09457/hybrid_auto/images/1707.09457_page0_fig6.jpg", "caption": "Figure 3: Results of reducing bias amplification using RBA on imSitu vSRL and MS-COCO MLC. Figures 3(a)-(d) show initial training set bias along the x-axis and development set bias along the yaxis. Dotted blue lines indicate the 0.05 margin used in RBA, with points violating the margin shown in red while points meeting the margin are shown in green. Across both settings adding RBA significantly reduces the number of violations, and reduces the bias amplification significantly. Figures 3(e)-(f) demonstrate bias amplification as a function of training bias, with and without RBA. Across all initial training biases, RBA is able to reduce the bias amplification.", "figure_type": "plot", "visual_anchor": "green dots in upper-right region versus red dots at lower x-axis values", "text_evidence": "Across all initial training biases, RBA is able to reduce the bias amplification.", "query_type": "comparison_explanation"}
{"query_id": "l1_1707.09457_1707.09457_fig_7_265", "query": "Does the shift from red to green dots support the 39% decrease in training set distribution distance?", "answer": "Yes, the transition from red violations to green compliant points visually demonstrates reduced bias amplification. This corresponds to the overall 39% decrease in distance to the training distribution after applying RBA.", "doc_id": "1707.09457", "figure_id": "1707.09457_fig_7", "figure_number": 3, "image_path": "data/mineru_output/1707.09457/1707.09457/hybrid_auto/images/1707.09457_page0_fig6.jpg", "caption": "Figure 3: Results of reducing bias amplification using RBA on imSitu vSRL and MS-COCO MLC. Figures 3(a)-(d) show initial training set bias along the x-axis and development set bias along the yaxis. Dotted blue lines indicate the 0.05 margin used in RBA, with points violating the margin shown in red while points meeting the margin are shown in green. Across both settings adding RBA significantly reduces the number of violations, and reduces the bias amplification significantly. Figures 3(e)-(f) demonstrate bias amplification as a function of training bias, with and without RBA. Across all initial training biases, RBA is able to reduce the bias amplification.", "figure_type": "plot", "visual_anchor": "red dots violating margin versus green dots meeting margin", "text_evidence": "In Figure 3(c) we can see that the overall distance to the training set distribution after applying RBA decreased significantly, over $39 \\%$ .", "query_type": "value_context"}
{"query_id": "l1_1709.02012_1709.02012_fig_3_266", "query": "Why does the blue line h₁ start at the origin while representing a trivial classifier μ₁?", "answer": "The blue line represents the trivial classifier h^μ₁, which starts at the origin (0,0) in the false-positive/false-negative plane, indicating zero error rates when the classifier always predicts according to the base rate of group 1.", "doc_id": "1709.02012", "figure_id": "1709.02012_fig_3", "figure_number": 1, "image_path": "data/mineru_output/1709.02012/1709.02012/hybrid_auto/images/1709.02012_page0_fig2.jpg", "caption": "Figure 1: Calibration, trivial classifiers, and equal-cost constraints – plotted in the false-pos./false-neg. plane. $\\mathcal { H } _ { 1 } ^ { \\ast } , \\mathcal { H } _ { 2 } ^ { \\ast }$ are the set of cal. classifiers for the two groups, and $h ^ { \\mu _ { 1 } } , h ^ { \\mu _ { 2 } }$ are trivial classifiers.   ", "figure_type": "plot", "visual_anchor": "Blue line h₁ originating at point O (origin)", "text_evidence": "Calibration, trivial classifiers, and equal-cost constraints – plotted in the false-pos./false-neg. plane. h^μ₁, h^μ₂ are trivial classifiers.", "query_type": "visual_definition"}
{"query_id": "l1_1709.02012_1709.02012_fig_3_267", "query": "Do the calibrated classifier sets H₁* and H₂* both lie on the black diagonal boundary?", "answer": "Yes, the sets H₁* and H₂* of calibrated classifiers for the two groups both lie on the black diagonal boundary, representing the Pareto frontier of achievable false-positive and false-negative trade-offs for each group.", "doc_id": "1709.02012", "figure_id": "1709.02012_fig_3", "figure_number": 1, "image_path": "data/mineru_output/1709.02012/1709.02012/hybrid_auto/images/1709.02012_page0_fig2.jpg", "caption": "Figure 1: Calibration, trivial classifiers, and equal-cost constraints – plotted in the false-pos./false-neg. plane. $\\mathcal { H } _ { 1 } ^ { \\ast } , \\mathcal { H } _ { 2 } ^ { \\ast }$ are the set of cal. classifiers for the two groups, and $h ^ { \\mu _ { 1 } } , h ^ { \\mu _ { 2 } }$ are trivial classifiers.   ", "figure_type": "plot", "visual_anchor": "Black diagonal line from top-left to bottom-right", "text_evidence": "H₁*, H₂* are the set of cal. classifiers for the two groups", "query_type": "value_context"}
{"query_id": "l1_1709.02012_1709.02012_fig_3_268", "query": "Does the horizontal dashed line crossing h₁ and h₂ represent an equal-cost constraint across both groups?", "answer": "Yes, the horizontal dashed line represents an equal-cost constraint, intersecting both the blue line h₁ and red line h₂ at the same generalized false-negative rate, enforcing fairness by equalizing costs across the two groups.", "doc_id": "1709.02012", "figure_id": "1709.02012_fig_3", "figure_number": 1, "image_path": "data/mineru_output/1709.02012/1709.02012/hybrid_auto/images/1709.02012_page0_fig2.jpg", "caption": "Figure 1: Calibration, trivial classifiers, and equal-cost constraints – plotted in the false-pos./false-neg. plane. $\\mathcal { H } _ { 1 } ^ { \\ast } , \\mathcal { H } _ { 2 } ^ { \\ast }$ are the set of cal. classifiers for the two groups, and $h ^ { \\mu _ { 1 } } , h ^ { \\mu _ { 2 } }$ are trivial classifiers.   ", "figure_type": "plot", "visual_anchor": "Horizontal dashed line intersecting blue h₁ and red h₂", "text_evidence": "Calibration, trivial classifiers, and equal-cost constraints – plotted in the false-pos./false-neg. plane.", "query_type": "comparison_explanation"}
{"query_id": "l1_1709.02012_1709.02012_fig_6_269", "query": "Why does the blue point h1 lie above the dashed line connecting origin to the black point h'^μ2?", "answer": "The diagram illustrates interpolation for Calibration-Preserving Parity, where h1 represents a different group's classifier position while maintaining calibration through the interpolation scheme between points.", "doc_id": "1709.02012", "figure_id": "1709.02012_fig_6", "figure_number": 2, "image_path": "data/mineru_output/1709.02012/1709.02012/hybrid_auto/images/1709.02012_page0_fig5.jpg", "caption": "Figure 2: Calibration-Preserving Parity through interpolation.", "figure_type": "diagram", "visual_anchor": "Blue point h1 positioned above the dashed diagonal line", "text_evidence": "We introduce a general relaxation that seeks to satisfy a single equal-cost constraint while maintaining calibration for each group G_t", "query_type": "visual_definition"}
{"query_id": "l1_1709.02012_1709.02012_fig_6_270", "query": "Does the red point h̃2 position relative to h1 demonstrate the equalization of false-positive and false-negative rates?", "answer": "Yes, the red point h̃2 and blue point h1 are positioned to show different trade-offs between generalized FP and FN rates, illustrating the constraint equalization that Equalized Odds seeks to achieve.", "doc_id": "1709.02012", "figure_id": "1709.02012_fig_6", "figure_number": 2, "image_path": "data/mineru_output/1709.02012/1709.02012/hybrid_auto/images/1709.02012_page0_fig5.jpg", "caption": "Figure 2: Calibration-Preserving Parity through interpolation.", "figure_type": "diagram", "visual_anchor": "Red point h̃2 positioned lower and right of blue point h1", "text_evidence": "Equalized Odds sets constraints to equalize false-positives c_fp(h_t) and false-negatives c_fn(h_t)", "query_type": "comparison_explanation"}
{"query_id": "l1_1709.02012_1709.02012_fig_6_271", "query": "How do the solid lines from origin to h1 and h̃2 represent calibration preservation for each group?", "answer": "The solid lines represent the calibration-preserving paths for different groups, showing how interpolation maintains calibration while achieving parity constraints through the relaxation approach.", "doc_id": "1709.02012", "figure_id": "1709.02012_fig_6", "figure_number": 2, "image_path": "data/mineru_output/1709.02012/1709.02012/hybrid_auto/images/1709.02012_page0_fig5.jpg", "caption": "Figure 2: Calibration-Preserving Parity through interpolation.", "figure_type": "diagram", "visual_anchor": "Solid blue and red lines radiating from origin (0,0)", "text_evidence": "a substantially simplified notion of Equalized Odds is compatible with calibration", "query_type": "value_context"}
{"query_id": "l1_1709.02012_1709.02012_fig_9_272", "query": "Why do the diamond points h̃₁ and h̃₂ lie on their respective blue and red curves in both panels?", "answer": "The diamond points represent post-processed classifiers that interpolate between the original classifiers and optimal ones. This interpolation ensures they lie on the cost curves connecting h₁ to h^μ₁ and h₂ to h^μ₂.", "doc_id": "1709.02012", "figure_id": "1709.02012_fig_9", "figure_number": 3, "image_path": "data/mineru_output/1709.02012/1709.02012/hybrid_auto/images/1709.02012_page0_fig8.jpg", "caption": "Figure 3: Generalized F.P. and F.N. rates for two groups under Equalized Odds and the calibrated relaxation. Diamonds represent post-processed classifiers. Points on the Equalized Odds (trained) graph represent classifiers achieved by modifying constraint hyperparameters.", "figure_type": "plot", "visual_anchor": "Diamond points h̃₁ and h̃₂ positioned on the blue and red curves", "text_evidence": "g₂(h̃₂) = (1 - α)g₂(h₂) + αg₂(h^μ₂), and thus setting α = [J₁(h₁) - g₂(h₂)] / [g₂(h^μ₂) - g₂(h₂)]", "query_type": "visual_definition"}
{"query_id": "l1_1709.02012_1709.02012_fig_9_273", "query": "What performance impact do the experiments aim to reveal when moving from filled circles to diamonds in these plots?", "answer": "The experiments aim to show that imposing calibration and equal-cost constraints (represented by the diamond points) often results in performance degradation while simultaneously increasing other notions of disparity.", "doc_id": "1709.02012", "figure_id": "1709.02012_fig_9", "figure_number": 3, "image_path": "data/mineru_output/1709.02012/1709.02012/hybrid_auto/images/1709.02012_page0_fig8.jpg", "caption": "Figure 3: Generalized F.P. and F.N. rates for two groups under Equalized Odds and the calibrated relaxation. Diamonds represent post-processed classifiers. Points on the Equalized Odds (trained) graph represent classifiers achieved by modifying constraint hyperparameters.", "figure_type": "plot", "visual_anchor": "Transition from filled circle points (h₁, h₂) to diamond points (h̃₁, h̃₂)", "text_evidence": "our goal is to understand the impact of imposing calibration and an equalcost constraint on real-world datasets. We will empirically show that, in many cases, this will result in performance degradation, while simultaneously increasing other notions of disparity.", "query_type": "comparison_explanation"}
{"query_id": "l1_1709.02012_1709.02012_fig_9_274", "query": "How does the left panel's blue curve trajectory differ from the right panel's regarding constraint modification?", "answer": "Points on the Equal Odds (left panel) curves represent classifiers achieved by modifying constraint hyperparameters during training, while the right panel shows calibrated relaxation with post-processing represented by diamonds.", "doc_id": "1709.02012", "figure_id": "1709.02012_fig_9", "figure_number": 3, "image_path": "data/mineru_output/1709.02012/1709.02012/hybrid_auto/images/1709.02012_page0_fig8.jpg", "caption": "Figure 3: Generalized F.P. and F.N. rates for two groups under Equalized Odds and the calibrated relaxation. Diamonds represent post-processed classifiers. Points on the Equalized Odds (trained) graph represent classifiers achieved by modifying constraint hyperparameters.", "figure_type": "plot", "visual_anchor": "Blue curve in left panel (Equal Odds Derived) versus right panel (Calib. + Equal F.N.)", "text_evidence": "Points on the Equalized Odds (trained) graph represent classifiers achieved by modifying constraint hyperparameters.", "query_type": "comparison_explanation"}
{"query_id": "l1_1710.08615_1710.08615_fig_1_275", "query": "Does the plateau at 0.18-0.20 probability after 50 friends illustrate heterogeneity in observational data about human behavior?", "answer": "Yes, the plateau suggests different subgroups within the population vary in behavior and size. Users with fewer friends show rapid probability increases, while those with more friends stabilize, demonstrating heterogeneous behavioral patterns across population subgroups.", "doc_id": "1710.08615", "figure_id": "1710.08615_fig_1", "figure_number": null, "image_path": "data/mineru_output/1710.08615/1710.08615/hybrid_auto/images/1710.08615_page0_fig0.jpg", "caption": "", "figure_type": "plot", "visual_anchor": "Blue curve plateau at probability 0.18-0.20 occurring after 50 tweeting friends", "text_evidence": "Observational data about human behavior is often heterogeneous, i.e., generated by subgroups within the population under study that vary in size and behavior.", "query_type": "comparison_explanation"}
{"query_id": "l1_1710.08615_1710.08615_fig_1_276", "query": "Why does probability rise steeply to 0.15 before 50 friends if responsive users with fewer friends quickly drop out?", "answer": "The steep initial rise reflects the behavior of responsive users who are still active in the analysis at lower friend counts. These users show high retweet probability before they drop out of the dataset due to fewer exposures.", "doc_id": "1710.08615", "figure_id": "1710.08615_fig_1", "figure_number": null, "image_path": "data/mineru_output/1710.08615/1710.08615/hybrid_auto/images/1710.08615_page0_fig0.jpg", "caption": "", "figure_type": "plot", "visual_anchor": "Steep curve rise from 0 to approximately 0.15 probability in the 0-50 tweeting friends range", "text_evidence": "responsive users (with fewer friends) quickly drop out of analysis (since they are generally exposed fewer times), leaving only the highly connecte", "query_type": "anomaly_cause"}
{"query_id": "l1_1710.08615_1710.08615_fig_1_277", "query": "Could aggregating the blue curve from 0 to 150 friends mask opposing trends predicted by Simpson's paradox?", "answer": "Yes, Simpson's paradox warns that aggregated trends can differ substantially from subgroup trends. The overall increasing curve may hide that different friend-count subgroups exhibit reversed or different retweet probability patterns when analyzed separately.", "doc_id": "1710.08615", "figure_id": "1710.08615_fig_1", "figure_number": null, "image_path": "data/mineru_output/1710.08615/1710.08615/hybrid_auto/images/1710.08615_page0_fig0.jpg", "caption": "", "figure_type": "plot", "visual_anchor": "Overall blue curve trend spanning entire x-axis range from 0 to 150 tweeting friends", "text_evidence": "Heterogeneity predisposes analysis to Simpson's paradox, whereby the trends observed in data that has been aggregated over the entire population may be substantially different from those of the underlying subgroups.", "query_type": "comparison_explanation"}
{"query_id": "l1_1710.11214_1710.11214_fig_1_278", "query": "How does the curved arrow labeled recommendation with the star icon relate to systems being retrained regularly?", "answer": "The recommendation arrow shows how live systems are updated using new data influenced by the system itself, forming the feedback loop where recommendations affect future training data.", "doc_id": "1710.11214", "figure_id": "1710.11214_fig_1", "figure_number": 1, "image_path": "data/mineru_output/1710.11214/1710.11214/hybrid_auto/images/1710.11214_page0_fig0.jpg", "caption": "Figure 1: The feedback loop between user behavior and algorithmic recommendation systems. Confounding occurs when a platform attempts to model user behavior without accounting for recommendations. User preferences act as confounding factors, influencing both recommendations (through past interactions) and current interactions.", "figure_type": "diagram", "visual_anchor": "curved arrow labeled 'recommendation' from PLATFORM to USER with yellow star icon below it", "text_evidence": "Live systems are updated or retrained regularly to incorporate new data that was influenced by the recommendation system itself, forming a feedback loop (figure 1).", "query_type": "value_context"}
{"query_id": "l1_1710.11214_1710.11214_fig_1_279", "query": "Why does the pink thought bubble labeled preference connect to USER rather than directly to the PLATFORM box?", "answer": "User preferences act as confounding factors that influence both current interactions and recommendations through past interactions, rather than being directly observable by the platform model.", "doc_id": "1710.11214", "figure_id": "1710.11214_fig_1", "figure_number": 1, "image_path": "data/mineru_output/1710.11214/1710.11214/hybrid_auto/images/1710.11214_page0_fig0.jpg", "caption": "Figure 1: The feedback loop between user behavior and algorithmic recommendation systems. Confounding occurs when a platform attempts to model user behavior without accounting for recommendations. User preferences act as confounding factors, influencing both recommendations (through past interactions) and current interactions.", "figure_type": "diagram", "visual_anchor": "pink thought bubble labeled 'preference' connected to USER icon on the right side", "text_evidence": "User preferences act as confounding factors, influencing both recommendations (through past interactions) and current interactions.", "query_type": "comparison_explanation"}
{"query_id": "l1_1710.11214_1710.11214_fig_1_280", "query": "What problem arises when the green gear model attempts to predict the interaction arrow without considering recommendations?", "answer": "Confounding occurs because the model attempts to predict user behavior without accounting for algorithmic recommendations from previously deployed systems that influenced the training data.", "doc_id": "1710.11214", "figure_id": "1710.11214_fig_1", "figure_number": 1, "image_path": "data/mineru_output/1710.11214/1710.11214/hybrid_auto/images/1710.11214_page0_fig0.jpg", "caption": "Figure 1: The feedback loop between user behavior and algorithmic recommendation systems. Confounding occurs when a platform attempts to model user behavior without accounting for recommendations. User preferences act as confounding factors, influencing both recommendations (through past interactions) and current interactions.", "figure_type": "diagram", "visual_anchor": "green gears icon labeled 'model' connected to PLATFORM box on left side", "text_evidence": "the underlying recommendation model is trained using data that are confounded by algorithmic recommendations from a previously deployed system", "query_type": "anomaly_cause"}
{"query_id": "l1_1710.11214_1710.11214_fig_2_281", "query": "Why do the darker blocks appear sparse across the matrix despite user preferences being disproportionate in the real world?", "answer": "The sparse darker blocks reflect that user preferences are disproportionately distributed, meaning most users have strong preferences for only a few items. This sparsity matches commonly accepted intuitions about user behavior where users enjoy limited subsets of available content.", "doc_id": "1710.11214", "figure_id": "1710.11214_fig_2", "figure_number": 2, "image_path": "data/mineru_output/1710.11214/1710.11214/hybrid_auto/images/1710.11214_page0_fig1.jpg", "caption": "Figure 2: Example true utility matrix  for simulated data; Vdarker is higher utility. The distribution of user preferences is disproportionate, like the real world, and the structure is easily captured with matrix factorization.", "figure_type": "plot", "visual_anchor": "sparse darker blocks scattered across predominantly light matrix background", "text_evidence": "With these settings for user preferences and item attributes, the resulting matrix of true utility is sparse (e.g., figure 2), which matches commonly accepted intuitions about user behavior.", "query_type": "visual_definition"}
{"query_id": "l1_1710.11214_1710.11214_fig_2_282", "query": "Does the visible structure in darker regions support the claim that matrix factorization can easily capture this data?", "answer": "Yes, the darker regions show clustered patterns rather than random noise, indicating underlying structure in user-item relationships. This structured sparsity makes the utility matrix amenable to low-rank approximation through matrix factorization techniques.", "doc_id": "1710.11214", "figure_id": "1710.11214_fig_2", "figure_number": 2, "image_path": "data/mineru_output/1710.11214/1710.11214/hybrid_auto/images/1710.11214_page0_fig1.jpg", "caption": "Figure 2: Example true utility matrix  for simulated data; Vdarker is higher utility. The distribution of user preferences is disproportionate, like the real world, and the structure is easily captured with matrix factorization.", "figure_type": "plot", "visual_anchor": "clustered darker blocks showing structured patterns in utility values", "text_evidence": "The distribution of user preferences is disproportionate, like the real world, and the structure is easily captured with matrix factorization.", "query_type": "value_context"}
{"query_id": "l1_1710.11214_1710.11214_fig_2_283", "query": "How do the darker vertical bands between items 6000-8000 relate to the underlying score approach for recommendations?", "answer": "The vertical bands indicate items with higher utility across multiple users, which would produce higher underlying scores in the recommendation framework. These high-utility items would be prioritized by algorithms when computing scores of how much users will enjoy items at any given time.", "doc_id": "1710.11214", "figure_id": "1710.11214_fig_2", "figure_number": 2, "image_path": "data/mineru_output/1710.11214/1710.11214/hybrid_auto/images/1710.11214_page0_fig1.jpg", "caption": "Figure 2: Example true utility matrix  for simulated data; Vdarker is higher utility. The distribution of user preferences is disproportionate, like the real world, and the structure is easily captured with matrix factorization.", "figure_type": "plot", "visual_anchor": "darker vertical bands concentrated in the region between items 6000-8000", "text_evidence": "The core idea of this framework is that each recommendation system provides some underlying score $s _ { u i } ( t )$ of how much a user $u$ uiwill enjoy item  at time .", "query_type": "comparison_explanation"}
{"query_id": "l1_1710.11214_1710.11214_fig_3_284", "query": "Why does the ideal black line stay at zero in both cases while homogenization effects occur?", "answer": "The ideal line represents the baseline behavior without recommendation influence. Since the Jaccard index measures change relative to ideal behavior, the ideal algorithm itself shows zero change by definition.", "doc_id": "1710.11214", "figure_id": "1710.11214_fig_3", "figure_number": 3, "image_path": "data/mineru_output/1710.11214/1710.11214/hybrid_auto/images/1710.11214_page0_fig2.jpg", "caption": "Figure 3: Change in Jaccard index of user behavior relative to ideal behavior; users paired by cosine similarity of . On the left, mild homogenization of behavior occurs soon after a single training, but then diminishes. On the right, recommendation systems that include repeated training homogenize user behavior more than is needed for ideal utility.", "figure_type": "plot", "visual_anchor": "Black solid line labeled 'ideal' at y=0 across all iterations in both panels", "text_evidence": "Change in Jaccard index of user behavior relative to ideal behavior; users paired by cosine similarity", "query_type": "visual_definition"}
{"query_id": "l1_1710.11214_1710.11214_fig_3_285", "query": "Does the popularity algorithm's lower peak around 0.16 in Case 2 contradict the claim about homogenization?", "answer": "No, popularity-based systems still homogenize behavior but less than MF and social algorithms. All non-ideal algorithms show substantial increases compared to Case 1, confirming that repeated training increases homogenization.", "doc_id": "1710.11214", "figure_id": "1710.11214_fig_3", "figure_number": 3, "image_path": "data/mineru_output/1710.11214/1710.11214/hybrid_auto/images/1710.11214_page0_fig2.jpg", "caption": "Figure 3: Change in Jaccard index of user behavior relative to ideal behavior; users paired by cosine similarity of . On the left, mild homogenization of behavior occurs soon after a single training, but then diminishes. On the right, recommendation systems that include repeated training homogenize user behavior more than is needed for ideal utility.", "figure_type": "plot", "visual_anchor": "Blue dotted popularity line reaching approximately 0.16 while red MF and cyan social lines reach 0.3 at iteration 100 in right panel", "text_evidence": "Recommendation systems that include repeated training homogenize user behavior more than is needed for ideal utility", "query_type": "comparison_explanation"}
{"query_id": "l1_1710.11214_1710.11214_fig_3_286", "query": "Why do all recommendation lines decline after iteration 50 in Case 1 despite initial mild homogenization?", "answer": "After a single training event, the homogenization effect is temporary and diminishes over time as users return to more diverse behavior patterns without further algorithmic reinforcement.", "doc_id": "1710.11214", "figure_id": "1710.11214_fig_3", "figure_number": 3, "image_path": "data/mineru_output/1710.11214/1710.11214/hybrid_auto/images/1710.11214_page0_fig2.jpg", "caption": "Figure 3: Change in Jaccard index of user behavior relative to ideal behavior; users paired by cosine similarity of . On the left, mild homogenization of behavior occurs soon after a single training, but then diminishes. On the right, recommendation systems that include repeated training homogenize user behavior more than is needed for ideal utility.", "figure_type": "plot", "visual_anchor": "All colored lines (content, MF, popularity, social) showing slight increase to ~0.03 near iteration 50, then declining toward zero by iteration 100 in left panel", "text_evidence": "On the left, mild homogenization of behavior occurs soon after a single training, but then diminishes", "query_type": "anomaly_cause"}
{"query_id": "l1_1710.11214_1710.11214_fig_4_287", "query": "Why does the blue popularity curve reach 0.17 while social reaches only 0.08 by iteration 100?", "answer": "Popularity increases homogenization the most globally among all non-random recommendation algorithms, causing the highest change in Jaccard index relative to ideal behavior.", "doc_id": "1710.11214", "figure_id": "1710.11214_fig_4", "figure_number": 4, "image_path": "data/mineru_output/1710.11214/1710.11214/hybrid_auto/images/1710.11214_page0_fig3.jpg", "caption": "Figure 4: For the repeated training case, change in Jaccard index of user behavior relative to ideal behavior; users paired randomly. Popularity increases homogenization the most globally, but all non-random recommendation algorithms also homogenize users globally.", "figure_type": "plot", "visual_anchor": "blue dotted line at ~0.17 versus teal dashed line at ~0.08 at iteration 100", "text_evidence": "Popularity increases homogenization the most globally, but all non-random recommendation algorithms also homogenize users globally.", "query_type": "comparison_explanation"}
{"query_id": "l1_1710.11214_1710.11214_fig_4_288", "query": "Do minority users experience the same utility changes as the orange content curve suggests at 0.025?", "answer": "No, minority users whose preferences are not well-captured by low dimensional representations may see lesser improvements or even decreases in utility when homogenization occurs.", "doc_id": "1710.11214", "figure_id": "1710.11214_fig_4", "figure_number": 4, "image_path": "data/mineru_output/1710.11214/1710.11214/hybrid_auto/images/1710.11214_page0_fig3.jpg", "caption": "Figure 4: For the repeated training case, change in Jaccard index of user behavior relative to ideal behavior; users paired randomly. Popularity increases homogenization the most globally, but all non-random recommendation algorithms also homogenize users globally.", "figure_type": "plot", "visual_anchor": "orange dotted line labeled 'content' at ~0.025", "text_evidence": "These minority users may see lesser improvements or even decreases in utility when homogenization occurs.", "query_type": "value_context"}
{"query_id": "l1_1710.11214_1710.11214_fig_4_289", "query": "How does the utility breakdown by user relate to the teal social line reaching 0.08 at iteration 100?", "answer": "The relationship between homogenization and utility varies by user, with Figure 5 breaking down how different users are affected by the homogenization shown in the social algorithm's curve.", "doc_id": "1710.11214", "figure_id": "1710.11214_fig_4", "figure_number": 4, "image_path": "data/mineru_output/1710.11214/1710.11214/hybrid_auto/images/1710.11214_page0_fig3.jpg", "caption": "Figure 4: For the repeated training case, change in Jaccard index of user behavior relative to ideal behavior; users paired randomly. Popularity increases homogenization the most globally, but all non-random recommendation algorithms also homogenize users globally.", "figure_type": "plot", "visual_anchor": "teal dashed line labeled 'social' at ~0.08 at iteration 100", "text_evidence": "Figure 5 breaks down the relationship between homogenization and utility by user; for all recommendation algorith", "query_type": "value_context"}
{"query_id": "l1_1710.11214_1710.11214_fig_5_290", "query": "Why does the random panel show a nearly horizontal trend line despite users paired by cosine similarity of theta?", "answer": "Random recommendations do not leverage user preferences or similarity, so utility losses are unrelated to behavioral homogenization. The pairing by cosine similarity of theta does not create the homogenization pattern seen in other recommendation algorithms.", "doc_id": "1710.11214", "figure_id": "1710.11214_fig_5", "figure_number": 5, "image_path": "data/mineru_output/1710.11214/1710.11214/hybrid_auto/images/1710.11214_page0_fig4.jpg", "caption": "Figure 5: For the repeated training case, change in Jaccard index of user behavior, relative to ideal behavior, and shown as a function of utility relative to the ideal platform; users paired by cosine similarity of . Each user is shown as a point, with a θlinear fit to highlight the general trend that users who experience losses in utility have higher homogenization.", "figure_type": "plot", "visual_anchor": "Nearly horizontal black trend line in random panel at y≈0", "text_evidence": "users paired by cosine similarity of . Each user is shown as a point, with a θlinear fit to highlight the general trend that users who experience losses in utility have higher homogenization", "query_type": "anomaly_cause"}
{"query_id": "l1_1710.11214_1710.11214_fig_5_291", "query": "Do the negative slopes in content, MF, popularity, and social panels support that lower utility users have higher homogenization?", "answer": "Yes, the negative slopes indicate that as utility relative to ideal decreases (moves left on x-axis), the change in Jaccard index increases, confirming higher homogenization for users experiencing utility losses.", "doc_id": "1710.11214", "figure_id": "1710.11214_fig_5", "figure_number": 5, "image_path": "data/mineru_output/1710.11214/1710.11214/hybrid_auto/images/1710.11214_page0_fig4.jpg", "caption": "Figure 5: For the repeated training case, change in Jaccard index of user behavior, relative to ideal behavior, and shown as a function of utility relative to the ideal platform; users paired by cosine similarity of . Each user is shown as a point, with a θlinear fit to highlight the general trend that users who experience losses in utility have higher homogenization.", "figure_type": "plot", "visual_anchor": "Black diagonal trend lines with negative slopes in content, MF, popularity, and social panels", "text_evidence": "for all recommendation algorithms, we find that users who experience lower utility generally have higher homogenization with their nearest neighbor", "query_type": "comparison_explanation"}
{"query_id": "l1_1710.11214_1710.11214_fig_5_292", "query": "How does the inequality measured by Gini coefficient relate to the green point distributions across the five recommendation systems?", "answer": "The Gini coefficient measures inequality in item consumption rates, which drives the different utility-homogenization patterns. Systems like random show minimal inequality impact, while content, MF, popularity, and social create stronger inequality leading to the negative trend correlations visible in their point distributions.", "doc_id": "1710.11214", "figure_id": "1710.11214_fig_5", "figure_number": 5, "image_path": "data/mineru_output/1710.11214/1710.11214/hybrid_auto/images/1710.11214_page0_fig4.jpg", "caption": "Figure 5: For the repeated training case, change in Jaccard index of user behavior, relative to ideal behavior, and shown as a function of utility relative to the ideal platform; users paired by cosine similarity of . Each user is shown as a point, with a θlinear fit to highlight the general trend that users who experience losses in utility have higher homogenization.", "figure_type": "plot", "visual_anchor": "Green point cloud distributions varying across five panels with different spread patterns", "text_evidence": "To explore the impacts of these systems on item consumption, we computed the Gini coefficient on the distribution of consumed items, which measures the inequality of the distribution", "query_type": "value_context"}
{"query_id": "l1_1710.11214_1710.11214_fig_6_293", "query": "Why do pink MF squares span Gini 0.65-0.8 while causing similar homogenization as content algorithms?", "answer": "Similar homogenization (Jaccard index change) can result in different item consumption distributions (Gini coefficients), as MF and content both increase homogeneity around 0.25-0.35 but produce distinct inequality patterns.", "doc_id": "1710.11214", "figure_id": "1710.11214_fig_6", "figure_number": 6, "image_path": "data/mineru_output/1710.11214/1710.11214/hybrid_auto/images/1710.11214_page0_fig5.jpg", "caption": "Figure 6: For the repeated training case, change in Jaccard index of user behavior (higher is more homogeneous), relative to the Gini coefficient of the item consumption distribution (higher is more unequal consumption of items). Each point is a single simulation. Similar homogenization can result in different item consumption distributions.", "figure_type": "plot", "visual_anchor": "Pink MF squares at higher Gini values (0.65-0.8) compared to yellow content diamonds (0.5-0.65)", "text_evidence": "Similar homogenization can result in different item consumption distributions.", "query_type": "comparison_explanation"}
{"query_id": "l1_1710.11214_1710.11214_fig_6_294", "query": "How do blue popularity triangles at Gini 0.7-0.8 relate to bias toward confounding algorithms in held-out data?", "answer": "Popularity algorithms create lower homogenization (Jaccard change 0.1-0.22) but high inequality (Gini 0.7-0.8). Evaluation using confounded held-out data biases results toward systems similar to the confounding algorithm, potentially favoring popularity-based approaches.", "doc_id": "1710.11214", "figure_id": "1710.11214_fig_6", "figure_number": 6, "image_path": "data/mineru_output/1710.11214/1710.11214/hybrid_auto/images/1710.11214_page0_fig5.jpg", "caption": "Figure 6: For the repeated training case, change in Jaccard index of user behavior (higher is more homogeneous), relative to the Gini coefficient of the item consumption distribution (higher is more unequal consumption of items). Each point is a single simulation. Similar homogenization can result in different item consumption distributions.", "figure_type": "plot", "visual_anchor": "Blue popularity triangles clustered at low Jaccard change (0.1-0.22) and high Gini (0.7-0.8)", "text_evidence": "When a recommendation system is evaluated using confounded held-out data, results are biased toward recommendation systems similar to the confounding algorithm.", "query_type": "value_context"}
{"query_id": "l1_1710.11214_1710.11214_fig_6_295", "query": "Do black ideal stars at zero Jaccard change support claims about researchers selecting data sets to highlight specific results?", "answer": "Yes, ideal algorithms show no homogenization (Jaccard change ≈0.0), contrasting sharply with all other methods. This demonstrates how data set choice can intentionally or unintentionally highlight particular algorithm characteristics.", "doc_id": "1710.11214", "figure_id": "1710.11214_fig_6", "figure_number": 6, "image_path": "data/mineru_output/1710.11214/1710.11214/hybrid_auto/images/1710.11214_page0_fig5.jpg", "caption": "Figure 6: For the repeated training case, change in Jaccard index of user behavior (higher is more homogeneous), relative to the Gini coefficient of the item consumption distribution (higher is more unequal consumption of items). Each point is a single simulation. Similar homogenization can result in different item consumption distributions.", "figure_type": "plot", "visual_anchor": "Black stars labeled 'ideal' positioned at Jaccard change near 0.0", "text_evidence": "Researchers can, intentionally or unintentionally, select data sets that highlight", "query_type": "value_context"}
{"query_id": "l1_1711.05144_1711.05144_fig_1_296", "query": "Why do curves stabilize at different error levels between 0.12 and 0.22 when varying γ parameter?", "answer": "Each curve corresponds to a different choice of the fairness parameter γ, and Learner chooses a linear threshold function over all 122 features at each round, producing different error-fairness tradeoffs.", "doc_id": "1711.05144", "figure_id": "1711.05144_fig_1", "figure_number": 1, "image_path": "data/mineru_output/1711.05144/1711.05144/hybrid_auto/images/1711.05144_page0_fig0.jpg", "caption": "Figure 1: Evolution of the error and unfairness of Learner’s classifier across iterations, for varying choices of γ. (a) Error $\\varepsilon _ { t }$ of Learner’s model vs iteration t. (b) Unfairness $\\gamma _ { t }$ of subgroup found by Auditor vs. iteration $t$ , as measured by Definition 2.3. See text for details.", "figure_type": "plot", "visual_anchor": "Multiple colored curves converging to distinct steady-state values between 0.12 and 0.22 on the y-axis", "text_evidence": "at each round Learner chooses a linear threshold function over all 122 features", "query_type": "comparison_explanation"}
{"query_id": "l1_1711.05144_1711.05144_fig_1_297", "query": "Do the curves reaching lowest error around 0.12 correspond to models with better cost sensitive classification implementation?", "answer": "No, all curves use the same cost sensitive classification oracle implemented via a two stage regression procedure; the different error levels reflect different γ parameter choices, not implementation differences.", "doc_id": "1711.05144", "figure_id": "1711.05144_fig_1", "figure_number": 1, "image_path": "data/mineru_output/1711.05144/1711.05144/hybrid_auto/images/1711.05144_page0_fig0.jpg", "caption": "Figure 1: Evolution of the error and unfairness of Learner’s classifier across iterations, for varying choices of γ. (a) Error $\\varepsilon _ { t }$ of Learner’s model vs iteration t. (b) Unfairness $\\gamma _ { t }$ of subgroup found by Auditor vs. iteration $t$ , as measured by Definition 2.3. See text for details.", "figure_type": "plot", "visual_anchor": "Lowest curves stabilizing near 0.12 error level after 2000 iterations", "text_evidence": "We implement the cost sensitive classification oracle via a two stage regression procedure", "query_type": "value_context"}
{"query_id": "l1_1711.05144_1711.05144_fig_1_298", "query": "How does the paired unfairness γt relate to curves stabilizing at 0.16 error after 4000 iterations?", "answer": "Each iteration t yields a pair of values ⟨εt, γt⟩, where εt is the error shown in the plot. Curves at 0.16 error represent specific Learner models with corresponding worst-case unfairness values from the auditing process.", "doc_id": "1711.05144", "figure_id": "1711.05144_fig_1", "figure_number": 1, "image_path": "data/mineru_output/1711.05144/1711.05144/hybrid_auto/images/1711.05144_page0_fig0.jpg", "caption": "Figure 1: Evolution of the error and unfairness of Learner’s classifier across iterations, for varying choices of γ. (a) Error $\\varepsilon _ { t }$ of Learner’s model vs iteration t. (b) Unfairness $\\gamma _ { t }$ of subgroup found by Auditor vs. iteration $t$ , as measured by Definition 2.3. See text for details.", "figure_type": "plot", "visual_anchor": "Curves stabilizing at approximately 0.16 error level after 4000 iterations", "text_evidence": "each iteration t, the two panels of Figure 1 yield a pair of realized values ⟨εt, γt⟩ from the experiment, corresponding to a Learner model whose error is εt", "query_type": "value_context"}
{"query_id": "l1_1711.05144_1711.05144_fig_3_299", "query": "Why do the color-coded trajectories form distinct curves rather than a single frontier, given the reductions approach?", "answer": "Each colored trajectory represents Pareto-optimal solutions for a specific gamma parameter value. The reductions approach by Agarwal et al. generates different error-unfairness trade-offs as gamma varies.", "doc_id": "1711.05144", "figure_id": "1711.05144_fig_3", "figure_number": 2, "image_path": "data/mineru_output/1711.05144/1711.05144/hybrid_auto/images/1711.05144_page0_fig2.jpg", "caption": "Figure 2: (a) Pareto-optimal error-unfairness values, color coded by varying values of the input parameter γ. (b) Aggregate Pareto frontier across all values of γ. Here the γ values cover the same range but are sampled more densely to get a smoother frontier. See text for details.", "figure_type": "plot", "visual_anchor": "Multiple distinct colored curves (blue, red, green, purple, orange) descending from upper-left", "text_evidence": "Pareto-optimal error-unfairness values, color coded by varying values of the input parameter γ", "query_type": "visual_definition"}
{"query_id": "l1_1711.05144_1711.05144_fig_3_300", "query": "Does the dense clustering of points between x-values 0.18-0.22 reflect the smoother sampling mentioned for aggregate analysis?", "answer": "Yes, the dense clustering reflects the denser gamma sampling used to achieve a smoother Pareto frontier. This sampling strategy creates more closely-spaced optimal points in that region.", "doc_id": "1711.05144", "figure_id": "1711.05144_fig_3", "figure_number": 2, "image_path": "data/mineru_output/1711.05144/1711.05144/hybrid_auto/images/1711.05144_page0_fig2.jpg", "caption": "Figure 2: (a) Pareto-optimal error-unfairness values, color coded by varying values of the input parameter γ. (b) Aggregate Pareto frontier across all values of γ. Here the γ values cover the same range but are sampled more densely to get a smoother frontier. See text for details.", "figure_type": "plot", "visual_anchor": "Dense cluster of colored points concentrated in the 0.18-0.22 x-axis range", "text_evidence": "Here the γ values cover the same range but are sampled more densely to get a smoother frontier", "query_type": "comparison_explanation"}
{"query_id": "l1_1711.05144_1711.05144_fig_3_301", "query": "How does the descending pattern from gamma 0.026 to lower values relate to fairness protection rules?", "answer": "The descending pattern shows the error-unfairness trade-off as gamma varies, which is relevant to understanding how algorithmic fairness rules balance accuracy against disparate impact, similar to concerns about differential protection.", "doc_id": "1711.05144", "figure_id": "1711.05144_fig_3", "figure_number": 2, "image_path": "data/mineru_output/1711.05144/1711.05144/hybrid_auto/images/1711.05144_page0_fig2.jpg", "caption": "Figure 2: (a) Pareto-optimal error-unfairness values, color coded by varying values of the input parameter γ. (b) Aggregate Pareto frontier across all values of γ. Here the γ values cover the same range but are sampled more densely to get a smoother frontier. See text for details.", "figure_type": "plot", "visual_anchor": "Descending curves from y-value 0.026 at upper-left to lower values at right", "text_evidence": "Facebooks secret censorship rules protect white men from hate speech but not black children", "query_type": "value_context"}
{"query_id": "l1_1711.07076_1711.07076_fig_1_302", "query": "How does the dashed diagonal line achieve near-parity compared to the vertical cyan line in hiring rates?", "answer": "The dashed diagonal DLP achieves near-parity by differentiating based on hair length (an irrelevant attribute) rather than only work experience, which the vertical unconstrained classifier uses. This allows it to balance hiring rates between men and women through treatment disparity.", "doc_id": "1711.07076", "figure_id": "1711.07076_fig_1", "figure_number": 1, "image_path": "data/mineru_output/1711.07076/1711.07076/hybrid_auto/images/1711.07076_page0_fig0.jpg", "caption": "Figure 1: Demonstration of a DLP’s undesirable side effects on a simple example of hiring data (see §4.1). An unconstrained classifier (vertical line) hires candidates based on work experience, yielding higher hiring rates for men than for women. A DLP (dashed diagonal) achieves near-parity by differentiating based on an irrelevant attribute (hair length). The DLP hurts some short-haired women, flipping their decisions to reject, and helps some long-haired men.", "figure_type": "plot", "visual_anchor": "dashed diagonal blue line versus vertical cyan line", "text_evidence": "An unconstrained classifier (vertical line) hires candidates based on work experience, yielding higher hiring rates for men than for women. A DLP (dashed diagonal) achieves near-parity by differentiating based on an irrelevant attribute (hair length).", "query_type": "comparison_explanation"}
{"query_id": "l1_1711.07076_1711.07076_fig_1_303", "query": "Why are orange triangles concentrated above 20 cm hair length while blue markers span the full range?", "answer": "The orange triangles represent women who generally have longer hair (20-30+ cm), while blue markers represent men with varying hair lengths. This clustering demonstrates how the DLP uses an irrelevant attribute (hair length) correlated with gender to achieve impact parity.", "doc_id": "1711.07076", "figure_id": "1711.07076_fig_1", "figure_number": 1, "image_path": "data/mineru_output/1711.07076/1711.07076/hybrid_auto/images/1711.07076_page0_fig0.jpg", "caption": "Figure 1: Demonstration of a DLP’s undesirable side effects on a simple example of hiring data (see §4.1). An unconstrained classifier (vertical line) hires candidates based on work experience, yielding higher hiring rates for men than for women. A DLP (dashed diagonal) achieves near-parity by differentiating based on an irrelevant attribute (hair length). The DLP hurts some short-haired women, flipping their decisions to reject, and helps some long-haired men.", "figure_type": "plot", "visual_anchor": "orange triangles clustered above 20 cm versus blue markers spanning 0-30 cm", "text_evidence": "The DLP hurts some short-haired women, flipping their decisions to reject, and helps some long-haired men.", "query_type": "comparison_explanation"}
{"query_id": "l1_1711.07076_1711.07076_fig_3_304", "query": "Why are downward triangles clustered near the horizontal dashed line at y=0.5 admission probability?", "answer": "Students whose decisions are flipped after applying the fairness constraint tend to be those close to the decision boundary, which is represented by the y=0.5 threshold on the admission probability axis.", "doc_id": "1711.07076", "figure_id": "1711.07076_fig_3", "figure_number": 2, "image_path": "data/mineru_output/1711.07076/1711.07076/hybrid_auto/images/1711.07076_page0_fig2.jpg", "caption": "Figure 2: (left) probability of the sensitive variable versus (unconstrained) admission probability, on unseen test data. Downward triangles indicate individuals rejected only after applying the DLP (“treatment”), while upward triangles indicate individuals accepted only by the DLP. The remaining ${ \\sim } 4 { , } 0 0 0$ blue/yellow dots indicate people whose decisions are not altered. Many students benefiting from the DLP are males who ‘look like’ females based on other features, whereas females who ‘look like’ males are hurt by the DLP. Detail view (center) and summary statistics (right) of the same plot.", "figure_type": "plot", "visual_anchor": "Downward triangles near horizontal dashed line at y=0.5", "text_evidence": "Considering the probability of admission for the unconstrained classifier (y-axis), students whose decisions are 'flipped' (after applying the fairness constraint) tend to be those close to the decision boundary.", "query_type": "comparison_explanation"}
{"query_id": "l1_1711.07076_1711.07076_fig_3_305", "query": "Do the upward blue triangles support the claim that males looking like females benefit from the DLP?", "answer": "Yes, the upward triangles indicate individuals accepted only by the DLP. Many of these are males with high p(female) values on the x-axis, confirming that males who 'look like' females based on other features benefit from the DLP.", "doc_id": "1711.07076", "figure_id": "1711.07076_fig_3", "figure_number": 2, "image_path": "data/mineru_output/1711.07076/1711.07076/hybrid_auto/images/1711.07076_page0_fig2.jpg", "caption": "Figure 2: (left) probability of the sensitive variable versus (unconstrained) admission probability, on unseen test data. Downward triangles indicate individuals rejected only after applying the DLP (“treatment”), while upward triangles indicate individuals accepted only by the DLP. The remaining ${ \\sim } 4 { , } 0 0 0$ blue/yellow dots indicate people whose decisions are not altered. Many students benefiting from the DLP are males who ‘look like’ females based on other features, whereas females who ‘look like’ males are hurt by the DLP. Detail view (center) and summary statistics (right) of the same plot.", "figure_type": "plot", "visual_anchor": "Upward blue triangles at high x-axis p(female) values", "text_evidence": "Many students benefiting from the DLP are males who 'look like' females based on other features, whereas females who 'look like' males are hurt by the DLP.", "query_type": "value_context"}
{"query_id": "l1_1711.07076_1711.07076_fig_3_306", "query": "What percentage of the total 4000 blue and yellow dots were used for training versus testing?", "answer": "70% of the observations (approximately 2800 dots) were used for training, while the remaining 30% (approximately 1200 dots) were reserved for model testing on the unseen test data shown in the plot.", "doc_id": "1711.07076", "figure_id": "1711.07076_fig_3", "figure_number": 2, "image_path": "data/mineru_output/1711.07076/1711.07076/hybrid_auto/images/1711.07076_page0_fig2.jpg", "caption": "Figure 2: (left) probability of the sensitive variable versus (unconstrained) admission probability, on unseen test data. Downward triangles indicate individuals rejected only after applying the DLP (“treatment”), while upward triangles indicate individuals accepted only by the DLP. The remaining ${ \\sim } 4 { , } 0 0 0$ blue/yellow dots indicate people whose decisions are not altered. Many students benefiting from the DLP are males who ‘look like’ females based on other features, whereas females who ‘look like’ males are hurt by the DLP. Detail view (center) and summary statistics (right) of the same plot.", "figure_type": "plot", "visual_anchor": "Blue and yellow dots representing approximately 4000 unchanged decisions", "text_evidence": "To construct the data, we sample $n _ { \\mathrm { a l l } } = 2 0 0 0$ total observations from the data-generating process described below. $70 \\%$ of the observations are used for training, and the remaining $30 \\%$ are reserved for model testing.", "query_type": "value_context"}
{"query_id": "l1_1711.08536_1711.08536_fig_1_307", "query": "Does the 32.1% US dominance in the yellow segment support concerns about limited geo-diversity affecting developing world applications?", "answer": "Yes, the US dominance at 32.1% indicates limited geo-diversity, which is problematic since practitioners use these datasets for developing world applications where resources are limited but local representation may be insufficient.", "doc_id": "1711.08536", "figure_id": "1711.08536_fig_1", "figure_number": 1, "image_path": "data/mineru_output/1711.08536/1711.08536/hybrid_auto/images/1711.08536_page0_fig0.jpg", "caption": "Figure 1: Fraction of Open Images and ImageNet images from each country. In both data sets, top represented locations include the US and Great Britain. Countries are represented by their two-letter ISO country codes.   ", "figure_type": "plot", "visual_anchor": "US: 32.1% (large yellow-green segment)", "text_evidence": "This is particularly desirable when using ML for the developing world, where resources for creating new data sets may be limited. However, if these data sets are not representative of the locations of interest, predictive performance of models may suffer.", "query_type": "value_context"}
{"query_id": "l1_1711.08536_1711.08536_fig_1_308", "query": "Why do the top two segments totaling 45% contrast with the claim about difficulty identifying geo-location for every image?", "answer": "Despite difficulty in identifying geo-location for every image, proxy information like textual metadata and URL data allowed recovery of reasonably reliable country-level location information for a large number of images, enabling this distribution analysis.", "doc_id": "1711.08536", "figure_id": "1711.08536_fig_1", "figure_number": 1, "image_path": "data/mineru_output/1711.08536/1711.08536/hybrid_auto/images/1711.08536_page0_fig0.jpg", "caption": "Figure 1: Fraction of Open Images and ImageNet images from each country. In both data sets, top represented locations include the US and Great Britain. Countries are represented by their two-letter ISO country codes.   ", "figure_type": "plot", "visual_anchor": "US (32.1%) and GB (12.9%) segments combining for approximately 45%", "text_evidence": "It is naturally difficult to identify the geo-location of every image in the open source data sets. However, proxy information such as textual / contextual information and URL metadata provided by a service allowed us to recover reasonably reliable location information at the country level for a large number of images in each data set.", "query_type": "comparison_explanation"}
{"query_id": "l1_1711.08536_1711.08536_fig_1_309", "query": "Does the concentration in few large segments explain why ImageNet shows a similar distribution pattern at country level?", "answer": "Yes, the concentration in a small number of countries (US, GB, FR dominating) mirrors ImageNet's pattern. Both datasets are similarly dominated by the same few countries despite ImageNet having lower coverage.", "doc_id": "1711.08536", "figure_id": "1711.08536_fig_1", "figure_number": 1, "image_path": "data/mineru_output/1711.08536/1711.08536/hybrid_auto/images/1711.08536_page0_fig0.jpg", "caption": "Figure 1: Fraction of Open Images and ImageNet images from each country. In both data sets, top represented locations include the US and Great Britain. Countries are represented by their two-letter ISO country codes.   ", "figure_type": "plot", "visual_anchor": "Large segments for US (32.1%), GB (12.9%), FR (4.3%) versus many small segments", "text_evidence": "We had lower coverage for ImageNet, but the distribution was similarly dominated by a small number of countries, as shown in Figure 1.", "query_type": "comparison_explanation"}
{"query_id": "l1_1711.08536_1711.08536_fig_4_310", "query": "Why does the green rater curve peak at log-likelihood -6 while the blue standard test curve peaks near 0?", "answer": "Hyderabad-located crowdsourced groom/bridegroom images are dramatically less likely to be recognized correctly by ImageNet-trained models compared to standard test set images, showing a 6-unit log-likelihood gap.", "doc_id": "1711.08536", "figure_id": "1711.08536_fig_4", "figure_number": 3, "image_path": "data/mineru_output/1711.08536/1711.08536/hybrid_auto/images/1711.08536_page0_fig3.jpg", "caption": "Figure 3: Density plots of log-likelihood attributed for groom, bridegroom images crowdsourced by raters in Hyderabad, India, as scored by a model trained on ImageNet (left) and Open Images (center), as compared to images in the standard test sets. In both cases, the images provided by Hyderabad-located crowdsourcing are dramatically less likely to be recognized correctly by these models. The plot on right shows a similar trend for the woman class in OpenImages which has no corresponding class in ImageNet.", "figure_type": "plot", "visual_anchor": "Green curve peak at -6 versus blue curve peak at 0", "text_evidence": "the images provided by Hyderabad-located crowdsourcing are dramatically less likely to be recognized correctly by these models", "query_type": "comparison_explanation"}
{"query_id": "l1_1711.08536_1711.08536_fig_4_311", "query": "Does the blue density peak reaching 0.47 at log-likelihood 0 indicate better model confidence than the green peak at 0.32?", "answer": "Yes, the higher blue peak at 0.47 indicates ImageNet models assign much higher confidence (log-likelihood near 0) to standard test images compared to crowdsourced Hyderabad images which peak lower at 0.32 around log-likelihood -6.", "doc_id": "1711.08536", "figure_id": "1711.08536_fig_4", "figure_number": 3, "image_path": "data/mineru_output/1711.08536/1711.08536/hybrid_auto/images/1711.08536_page0_fig3.jpg", "caption": "Figure 3: Density plots of log-likelihood attributed for groom, bridegroom images crowdsourced by raters in Hyderabad, India, as scored by a model trained on ImageNet (left) and Open Images (center), as compared to images in the standard test sets. In both cases, the images provided by Hyderabad-located crowdsourcing are dramatically less likely to be recognized correctly by these models. The plot on right shows a similar trend for the woman class in OpenImages which has no corresponding class in ImageNet.", "figure_type": "plot", "visual_anchor": "Blue peak density of 0.47 versus green peak density of 0.32", "text_evidence": "ask crowdsourced raters located in Hyderabad to find and return URLs of images on the internet that matched particular labels", "query_type": "comparison_explanation"}
{"query_id": "l1_1711.08536_1711.08536_fig_4_312", "query": "What explains the minimal overlap between the green and blue distributions across the -12 to 2 log-likelihood range?", "answer": "The lack of geo-diversity in ImageNet training data causes poor classification performance on geographically diverse images. Hyderabad-sourced images receive systematically lower log-likelihoods, creating separated distributions with minimal overlap.", "doc_id": "1711.08536", "figure_id": "1711.08536_fig_4", "figure_number": 3, "image_path": "data/mineru_output/1711.08536/1711.08536/hybrid_auto/images/1711.08536_page0_fig3.jpg", "caption": "Figure 3: Density plots of log-likelihood attributed for groom, bridegroom images crowdsourced by raters in Hyderabad, India, as scored by a model trained on ImageNet (left) and Open Images (center), as compared to images in the standard test sets. In both cases, the images provided by Hyderabad-located crowdsourcing are dramatically less likely to be recognized correctly by these models. The plot on right shows a similar trend for the woman class in OpenImages which has no corresponding class in ImageNet.", "figure_type": "plot", "visual_anchor": "Separation between green and blue curves with minimal overlap from -12 to 2", "text_evidence": "We examined how lack of geo-diversity in training data impacts classification performance on images drawn from a broader set of locations", "query_type": "anomaly_cause"}
{"query_id": "l1_1711.08536_1711.08536_fig_7_313", "query": "Why do non-US bridegroom density peaks fall between -2 and 0 while the US peak reaches 0.5 density?", "answer": "Groom images with non-US location tags tend to have lower likelihoods than groom images from the US, reflecting lack of geo-diversity in the Open Images training data that impacts classification performance on images from broader locations.", "doc_id": "1711.08536", "figure_id": "1711.08536_fig_7", "figure_number": 4, "image_path": "data/mineru_output/1711.08536/1711.08536/hybrid_auto/images/1711.08536_page0_fig6.jpg", "caption": "Figure 4: Density plots of log-likelihood attributed by the models trained on Open Images for images drawn from the groom, bridegroom, butcher, greengrocer, and police officer categories. Groom images with non-US location tags tend to have lower likelihoods than the groom images from the US.", "figure_type": "plot", "visual_anchor": "US curve peak at approximately 0.5 density versus non-US curves peaking at lower log-likelihood values between -2 and 0", "text_evidence": "Groom images with non-US location tags tend to have lower likelihoods than the groom images from the US.", "query_type": "comparison_explanation"}
{"query_id": "l1_1711.08536_1711.08536_fig_7_314", "query": "Does the leftward shift of Ethiopia and Nigeria curves below log-likelihood -2 support concerns about geo-diversity?", "answer": "Yes, the leftward shift indicates lower model likelihoods for these regions, directly supporting the finding that standard open source data sets like Open Images may not have sufficient geo-diversity for broad representation across the developing world.", "doc_id": "1711.08536", "figure_id": "1711.08536_fig_7", "figure_number": 4, "image_path": "data/mineru_output/1711.08536/1711.08536/hybrid_auto/images/1711.08536_page0_fig6.jpg", "caption": "Figure 4: Density plots of log-likelihood attributed by the models trained on Open Images for images drawn from the groom, bridegroom, butcher, greengrocer, and police officer categories. Groom images with non-US location tags tend to have lower likelihoods than the groom images from the US.", "figure_type": "plot", "visual_anchor": "Ethiopia and Nigeria density curves concentrated in the -4 to -2 log-likelihood range", "text_evidence": "It is clear that standard open source data sets such as ImageNet or Open Images may not have sufficient geo-diversity for broad representation across the developing world.", "query_type": "value_context"}
{"query_id": "l1_1711.08536_1711.08536_fig_7_315", "query": "How does the clustering of all eight country curves between -4 and 2 relate to crowdsourced testing?", "answer": "The clustering demonstrates geographic variation in model performance that was examined through crowdsourced data collection. Raters in Hyderabad were asked to find and return URLs of images matching particular labels to create stress-test data for assessing geo-diversity impact.", "doc_id": "1711.08536", "figure_id": "1711.08536_fig_7", "figure_number": 4, "image_path": "data/mineru_output/1711.08536/1711.08536/hybrid_auto/images/1711.08536_page0_fig6.jpg", "caption": "Figure 4: Density plots of log-likelihood attributed by the models trained on Open Images for images drawn from the groom, bridegroom, butcher, greengrocer, and police officer categories. Groom images with non-US location tags tend to have lower likelihoods than the groom images from the US.", "figure_type": "plot", "visual_anchor": "All eight country density curves clustered between log-likelihood values -4 and 2", "text_evidence": "Our first method of collecting stress-test data was to ask crowdsourced raters located in Hyderabad to find and return URLs of images on the internet that matched particular labels", "query_type": "visual_definition"}
{"query_id": "l1_1711.08536_1711.08536_fig_10_316", "query": "Why are Ethiopia and Pakistan photos more dispersed across the likelihood scale than the dense right-end clustering in United States?", "answer": "Images from Ethiopia and Pakistan are not classified as consistently as images from the United States and Australia, resulting in lower and more variable log-likelihood scores rather than concentrated high-confidence predictions.", "doc_id": "1711.08536", "figure_id": "1711.08536_fig_10", "figure_number": 5, "image_path": "data/mineru_output/1711.08536/1711.08536/hybrid_auto/images/1711.08536_page0_fig9.jpg", "caption": "Figure 5: Photos of bridegrooms from different countries aligned by the log-likelihood that the classifier trained on Open Images assigns to the bridegroom class. Images from Ethiopia and Pakistan are not classified as consistently as images from the United States and Australia.", "figure_type": "plot", "visual_anchor": "dispersed photo distribution from 1e-2 to 3e-1 for Ethiopia and Pakistan versus dense clustering at 2e-1 to 4e-1 for United States", "text_evidence": "Images from Ethiopia and Pakistan are not classified as consistently as images from the United States and Australia.", "query_type": "comparison_explanation"}
{"query_id": "l1_1711.08536_1711.08536_fig_10_317", "query": "Does the sparse left-side positioning of Ethiopia bridegroom photos at 4e-3 to 1e-2 illustrate insufficient geo-diversity in training data?", "answer": "Yes, the low log-likelihood assignments for Ethiopian images demonstrate that standard open source data sets like Open Images lack sufficient geo-diversity for broad representation across the developing world.", "doc_id": "1711.08536", "figure_id": "1711.08536_fig_10", "figure_number": 5, "image_path": "data/mineru_output/1711.08536/1711.08536/hybrid_auto/images/1711.08536_page0_fig9.jpg", "caption": "Figure 5: Photos of bridegrooms from different countries aligned by the log-likelihood that the classifier trained on Open Images assigns to the bridegroom class. Images from Ethiopia and Pakistan are not classified as consistently as images from the United States and Australia.", "figure_type": "plot", "visual_anchor": "left-positioned photos at 3e-3 to 4e-3 and 4e-3 on Ethiopia timeline", "text_evidence": "It is clear that standard open source data sets such as ImageNet or Open Images may not have sufficient geo-diversity for broad representation across the developing world.", "query_type": "value_context"}
{"query_id": "l1_1711.08536_1711.08536_fig_10_318", "query": "What causes Pakistan photos to scatter across 3e-3 to 4e-1 despite the classifier being trained on Open Images bridegroom class?", "answer": "The classifier was trained on data sets designed for specific purposes, and later adoption for other geographic contexts introduces problems when the training data lacks representation from regions like Pakistan.", "doc_id": "1711.08536", "figure_id": "1711.08536_fig_10", "figure_number": 5, "image_path": "data/mineru_output/1711.08536/1711.08536/hybrid_auto/images/1711.08536_page0_fig9.jpg", "caption": "Figure 5: Photos of bridegrooms from different countries aligned by the log-likelihood that the classifier trained on Open Images assigns to the bridegroom class. Images from Ethiopia and Pakistan are not classified as consistently as images from the United States and Australia.", "figure_type": "plot", "visual_anchor": "Pakistan timeline showing photos distributed from 3e-3 through 1e-1 to 4e-1 with no clear concentration", "text_evidence": "these data sets were designed for specific purposes, and it is only the practice of later adoption for other purposes that may introduce problems", "query_type": "anomaly_cause"}
{"query_id": "l1_1801.04385_1801.04385_fig_1_319", "query": "Does the upward dashed trend line from position 1 to 8 support the claim that later answers are more likely accepted?", "answer": "The upward trend in aggregated data suggests later answers are more likely accepted, but this is Simpson's paradox. When disaggregated by session length, the trend reverses.", "doc_id": "1801.04385", "figure_id": "1801.04385_fig_1", "figure_number": 1, "image_path": "data/mineru_output/1801.04385/1801.04385/hybrid_auto/images/1801.04385_page0_fig0.jpg", "caption": "Figure 1: Simpson’s paradox in Stack Exchange data. Both plots show the probability an answer is accepted as the best answer to a question as a function of its position within user’s activity session. (a) Acceptance probability calculated over aggregated data has an upward trend, suggesting that answers written later in a session are more likely to be accepted as best answers. However, when data is disaggregated by session length (b), the trend reverses. Among answers produced during sessions of the same length (dierent colors represent dierent-length sessions), later answers are less likely to be accepted as best answers.   ", "figure_type": "plot", "visual_anchor": "dashed black trend line showing upward slope from acceptance probability ~0.34 at position 1 to ~0.35 at position 8", "text_evidence": "Acceptance probability calculated over aggregated data has an upward trend, suggesting that answers written later in a session are more likely to be accepted as best answers", "query_type": "comparison_explanation"}
{"query_id": "l1_1801.04385_1801.04385_fig_1_320", "query": "Why does acceptance probability at position 7 reach approximately 0.352 despite the previously reported finding about answer position?", "answer": "The previously reported finding that acceptance probability decreases with answer position is itself an instance of Simpson's paradox revealed by the method, contradicting the aggregated upward trend shown.", "doc_id": "1801.04385", "figure_id": "1801.04385_fig_1", "figure_number": 1, "image_path": "data/mineru_output/1801.04385/1801.04385/hybrid_auto/images/1801.04385_page0_fig0.jpg", "caption": "Figure 1: Simpson’s paradox in Stack Exchange data. Both plots show the probability an answer is accepted as the best answer to a question as a function of its position within user’s activity session. (a) Acceptance probability calculated over aggregated data has an upward trend, suggesting that answers written later in a session are more likely to be accepted as best answers. However, when data is disaggregated by session length (b), the trend reverses. Among answers produced during sessions of the same length (dierent colors represent dierent-length sessions), later answers are less likely to be accepted as best answers.   ", "figure_type": "plot", "visual_anchor": "black dot at position 7 showing acceptance probability approximately 0.352", "text_evidence": "Our approach reveals that the previously reported nding that acceptance probability decreases with answer position [9] is an instance of Sim", "query_type": "anomaly_cause"}
{"query_id": "l1_1801.04385_1801.04385_fig_1_321", "query": "How many Simpson's pairs were identified from the 110 possible ones to explain the acceptance probability pattern from 0.34 to 0.36?", "answer": "Seven Simpson's pairs were identified among the 110 possible pairs from eleven variables in Stack Exchange data, which explain the paradoxical acceptance probability pattern.", "doc_id": "1801.04385", "figure_id": "1801.04385_fig_1", "figure_number": 1, "image_path": "data/mineru_output/1801.04385/1801.04385/hybrid_auto/images/1801.04385_page0_fig0.jpg", "caption": "Figure 1: Simpson’s paradox in Stack Exchange data. Both plots show the probability an answer is accepted as the best answer to a question as a function of its position within user’s activity session. (a) Acceptance probability calculated over aggregated data has an upward trend, suggesting that answers written later in a session are more likely to be accepted as best answers. However, when data is disaggregated by session length (b), the trend reverses. Among answers produced during sessions of the same length (dierent colors represent dierent-length sessions), later answers are less likely to be accepted as best answers.   ", "figure_type": "plot", "visual_anchor": "acceptance probability range spanning from approximately 0.34 to 0.36 across positions 1 to 8", "text_evidence": "e eleven variables in Stack Exchange data, result in 110 possible Simpson's pairs. Among these, our method identies seven as instance of paradox", "query_type": "value_context"}
{"query_id": "l1_1801.04385_1801.04385_fig_4_322", "query": "Why do all colored trend lines slope downward when users with more lifetime answers should have higher acceptance probability?", "answer": "This demonstrates Simpson's paradox: aggregated data shows upward trend, but when disaggregated by reputation bins (different colors), the trend reverses within each group.", "doc_id": "1801.04385", "figure_id": "1801.04385_fig_4", "figure_number": 2, "image_path": "data/mineru_output/1801.04385/1801.04385/hybrid_auto/images/1801.04385_page0_fig3.jpg", "caption": "Figure 2: Novel Simpson’s paradox discovered in Stack Exchange data. Plots show the probability an answer is accepted as best answer as a function of the number of lifetime answers written by user over his or her tenure. (a) Acceptance probability calculated over aggregated data has an upward trend, with answers written by more experienced users (who have already posted more answers) more likely to be accepted as best answers. However, when data is disaggregated by reputation (b), the trend reverses. Among answers written by users with the same reputation (dierent colors represent reputation bins), those posted by users who had already written more answers are less likely to be accepted as best answers.", "figure_type": "plot", "visual_anchor": "Multiple downward-sloping colored trend lines across all reputation bins", "text_evidence": "Acceptance probability calculated over aggregated data has an upward trend, with answers written by more experienced users (who have already posted more answers) more likely to be accepted as best answers. However, when data is disaggregated by reputation (b), the trend reverses.", "query_type": "anomaly_cause"}
{"query_id": "l1_1801.04385_1801.04385_fig_4_323", "query": "Do the green circles dropping from 0.24 at 100 answers to 0.10 at 150 answers contradict experience improving acceptance rates?", "answer": "No, because among users with the same reputation level (green bin), those who posted more answers are less likely to have answers accepted as best.", "doc_id": "1801.04385", "figure_id": "1801.04385_fig_4", "figure_number": 2, "image_path": "data/mineru_output/1801.04385/1801.04385/hybrid_auto/images/1801.04385_page0_fig3.jpg", "caption": "Figure 2: Novel Simpson’s paradox discovered in Stack Exchange data. Plots show the probability an answer is accepted as best answer as a function of the number of lifetime answers written by user over his or her tenure. (a) Acceptance probability calculated over aggregated data has an upward trend, with answers written by more experienced users (who have already posted more answers) more likely to be accepted as best answers. However, when data is disaggregated by reputation (b), the trend reverses. Among answers written by users with the same reputation (dierent colors represent reputation bins), those posted by users who had already written more answers are less likely to be accepted as best answers.", "figure_type": "plot", "visual_anchor": "Green circles showing steep decline from approximately 0.24 to 0.10 between 100-150 answers", "text_evidence": "Among answers written by users with the same reputation (dierent colors represent reputation bins), those posted by users who had already written more answers are less likely to be accepted as best answers.", "query_type": "comparison_explanation"}
{"query_id": "l1_1801.04385_1801.04385_fig_4_324", "query": "Are answers at 1000 lifetime posts with 0.06 acceptance probability from longer or shorter user sessions than expected?", "answer": "The plot shows lifetime answer count, not session length. However, answers from longer sessions are more likely to be accepted than this data point suggests.", "doc_id": "1801.04385", "figure_id": "1801.04385_fig_4", "figure_number": 2, "image_path": "data/mineru_output/1801.04385/1801.04385/hybrid_auto/images/1801.04385_page0_fig3.jpg", "caption": "Figure 2: Novel Simpson’s paradox discovered in Stack Exchange data. Plots show the probability an answer is accepted as best answer as a function of the number of lifetime answers written by user over his or her tenure. (a) Acceptance probability calculated over aggregated data has an upward trend, with answers written by more experienced users (who have already posted more answers) more likely to be accepted as best answers. However, when data is disaggregated by reputation (b), the trend reverses. Among answers written by users with the same reputation (dierent colors represent reputation bins), those posted by users who had already written more answers are less likely to be accepted as best answers.", "figure_type": "plot", "visual_anchor": "Cyan 'x' marker at approximately 1000 answers with acceptance probability near 0.06", "text_evidence": "answers produced during longer sessions are more likely to be accepted than answers", "query_type": "value_context"}
{"query_id": "l1_1801.04385_1801.04385_fig_5_325", "query": "Why does the blue region at reputation 10^1 show near-zero acceptance probability despite constant probability mass along same-length sessions?", "answer": "The probability mass at Xp = a+1 is normalized to account for sessions of length a, causing the rate of change of probability masses with respect to Xp to affect acceptance probability differently at lower reputation values.", "doc_id": "1801.04385", "figure_id": "1801.04385_fig_5", "figure_number": 3, "image_path": "data/mineru_output/1801.04385/1801.04385/hybrid_auto/images/1801.04385_page0_fig4.jpg", "caption": "Figure 3: Analysis of the Simpson’s paradox Reputation – Number of Answers variable pair. (a) Average acceptance probability as a function of two variables. (b) e distribution of the number of data points contributing to the value of the outcome variable for each pair of variable values.", "figure_type": "plot", "visual_anchor": "Blue region (acceptance probability near 0.0) at reputation values around 10^1", "text_evidence": "the probability mass at Xp = a + 1 c is the same as that at Xp = a (as the num-Xp a Xp aber of data points is constant along sessions of same length) but normalized to account for the sessions of length a", "query_type": "anomaly_cause"}
{"query_id": "l1_1801.04385_1801.04385_fig_5_326", "query": "How does acceptance probability change from the blue diagonal band to the red upper region as both reputation and answer count increase?", "answer": "Acceptance probability increases from near 0.0 (blue) to 0.7 (red) as reputation grows from 10^2 to 10^6 and answer count increases from 10^1 to 10^4, following the diagonal pattern.", "doc_id": "1801.04385", "figure_id": "1801.04385_fig_5", "figure_number": 3, "image_path": "data/mineru_output/1801.04385/1801.04385/hybrid_auto/images/1801.04385_page0_fig4.jpg", "caption": "Figure 3: Analysis of the Simpson’s paradox Reputation – Number of Answers variable pair. (a) Average acceptance probability as a function of two variables. (b) e distribution of the number of data points contributing to the value of the outcome variable for each pair of variable values.", "figure_type": "plot", "visual_anchor": "Diagonal gradient transitioning from blue (lower-left) to red (upper-right)", "text_evidence": "e rate of change of these probability masses with respect to Xp is", "query_type": "comparison_explanation"}
{"query_id": "l1_1801.04385_1801.04385_fig_5_327", "query": "Does the red concentration at reputation above 10^5 align with the probability mass function decreasing for higher Xc values?", "answer": "Yes, the red region (high acceptance probability 0.6-0.7) at reputation above 10^5 corresponds to the stated behavior where the probability mass function Pr(Xc = xc | Xp = a) decreases for Xc.", "doc_id": "1801.04385", "figure_id": "1801.04385_fig_5", "figure_number": 3, "image_path": "data/mineru_output/1801.04385/1801.04385/hybrid_auto/images/1801.04385_page0_fig4.jpg", "caption": "Figure 3: Analysis of the Simpson’s paradox Reputation – Number of Answers variable pair. (a) Average acceptance probability as a function of two variables. (b) e distribution of the number of data points contributing to the value of the outcome variable for each pair of variable values.", "figure_type": "plot", "visual_anchor": "Red region at reputation values 10^5 to 10^6 with acceptance probability 0.6-0.7", "text_evidence": "e probability mass function Pr ( Xc = xc | Xp = a ) decreases for Xc", "query_type": "value_context"}
{"query_id": "l1_1801.04385_1801.04385_fig_7_328", "query": "Why do red diamonds reach 0.57 acceptance probability while blue circles plateau at 0.20, despite both groups showing upward trends?", "answer": "The probability mass function increases at a greater rate for higher reputation subgroups (X_c > a) than the rate at which acceptance probability decreases for the smallest value subgroup, resulting in divergent upward trends when aggregated.", "doc_id": "1801.04385", "figure_id": "1801.04385_fig_7", "figure_number": 4, "image_path": "data/mineru_output/1801.04385/1801.04385/hybrid_auto/images/1801.04385_page0_fig6.jpg", "caption": "Figure 4: Relationship between acceptance probability and Reputation Rate, a new measure of user performance de-ned as reputation per number of answers users wrote over their entire tenure. Each line represents a subgroup with a dierent reputation score. e much smaller variance compared to Fig. 2b suggests that the new feature is a good proxy of answerer performance.", "figure_type": "plot", "visual_anchor": "red diamonds (Rep ≥ 10^5) reaching 0.57 versus blue circles (Rep ≤ 10) at 0.20", "text_evidence": "the rate of increase of this probability mass is greater than the rate at which the acceptance probability decreases, resulting in an upward trend when the data is aggregated", "query_type": "comparison_explanation"}
{"query_id": "l1_1801.04385_1801.04385_fig_7_329", "query": "Does the much smaller variance among the six colored subgroup lines validate Reputation Rate as a performance proxy?", "answer": "Yes, the caption explicitly states that the much smaller variance compared to Fig. 2b suggests that Reputation Rate (reputation per number of answers) is a good proxy of answerer performance.", "doc_id": "1801.04385", "figure_id": "1801.04385_fig_7", "figure_number": 4, "image_path": "data/mineru_output/1801.04385/1801.04385/hybrid_auto/images/1801.04385_page0_fig6.jpg", "caption": "Figure 4: Relationship between acceptance probability and Reputation Rate, a new measure of user performance de-ned as reputation per number of answers users wrote over their entire tenure. Each line represents a subgroup with a dierent reputation score. e much smaller variance compared to Fig. 2b suggests that the new feature is a good proxy of answerer performance.", "figure_type": "plot", "visual_anchor": "small variance among six colored lines (blue, purple, magenta, red) between 0.2 and 0.6", "text_evidence": "e much smaller variance compared to Fig. 2b suggests that the new feature is a good proxy of answerer performance", "query_type": "value_context"}
{"query_id": "l1_1801.04385_1801.04385_fig_7_330", "query": "How does disaggregating by reputation ranges reveal the paradox that the method aims to uncover systematically?", "answer": "The method identifies variable pairs where a trend in outcome disappears or reverses when data is disaggregated by conditioning on a second variable, creating subgroups. The plot shows such disaggregation with different reputation ranges revealing distinct acceptance probability patterns.", "doc_id": "1801.04385", "figure_id": "1801.04385_fig_7", "figure_number": 4, "image_path": "data/mineru_output/1801.04385/1801.04385/hybrid_auto/images/1801.04385_page0_fig6.jpg", "caption": "Figure 4: Relationship between acceptance probability and Reputation Rate, a new measure of user performance de-ned as reputation per number of answers users wrote over their entire tenure. Each line represents a subgroup with a dierent reputation score. e much smaller variance compared to Fig. 2b suggests that the new feature is a good proxy of answerer performance.", "figure_type": "plot", "visual_anchor": "six distinct colored lines showing different trends when disaggregated by reputation ranges", "text_evidence": "e method identies pairs of variables, such that a trend in an outcome as a function of one variable disappears or reverses itself when the same data is disaggregated by conditioning it on the second variable. e disaggregated data corresponds to subgroups within the population generating the data", "query_type": "visual_definition"}
{"query_id": "l1_1801.04385_1801.04385_fig_8_331", "query": "Why does the blue concentration at Answer Position below 10^1 illustrate a pair that multivariate logistic regression cannot find?", "answer": "The method identifies pairs of variables where a trend in an outcome as a function of one variable disappears or reverses when conditioned on the second variable. Multivariate logistic regression fails to detect such Simpson's paradox instances in this variable pair.", "doc_id": "1801.04385", "figure_id": "1801.04385_fig_8", "figure_number": 5, "image_path": "data/mineru_output/1801.04385/1801.04385/hybrid_auto/images/1801.04385_page0_fig7.jpg", "caption": "Figure 5: A pair which multivariate logistic regression cannot nd in the data. (a) Average acceptance probability as a function of Answer Position and Time Since Previous Answer. (b) e distribution of the number of data points contributing to the value of the outcome variable for each pair of variable values.", "figure_type": "plot", "visual_anchor": "blue concentration at Answer Position below 10^1", "text_evidence": "We presented a method for systematically uncovering instances of Simpson's paradox in data. e method identies pairs of variables, such that a trend in an outcome as a function of one variable disappears or reverses itself when the same data is disaggregated by conditioning it on the second variable.", "query_type": "visual_definition"}
{"query_id": "l1_1801.04385_1801.04385_fig_8_332", "query": "How does the red region at Answer Position 10^1 to 10^2 relate to subgroups within the population generating the data?", "answer": "The disaggregated data shown in the red region corresponds to subgroups within the population, revealing different acceptance probability patterns when conditioned on Time Since Previous Answer.", "doc_id": "1801.04385", "figure_id": "1801.04385_fig_8", "figure_number": 5, "image_path": "data/mineru_output/1801.04385/1801.04385/hybrid_auto/images/1801.04385_page0_fig7.jpg", "caption": "Figure 5: A pair which multivariate logistic regression cannot nd in the data. (a) Average acceptance probability as a function of Answer Position and Time Since Previous Answer. (b) e distribution of the number of data points contributing to the value of the outcome variable for each pair of variable values.", "figure_type": "plot", "visual_anchor": "red region at Answer Position 10^1 to 10^2", "text_evidence": "e disaggregated data corresponds to subgroups within the population generating the data.", "query_type": "value_context"}
{"query_id": "l1_1801.04385_1801.04385_fig_8_333", "query": "Does the acceptance probability gradient from blue to red across Answer Position values support funding for paradox detection research?", "answer": "Yes, the systematic uncovering of Simpson's paradox instances demonstrated by the color gradient pattern received funding support from the James S. McDonnell Foundation, Air Force Office of Scientific Research, and Army Research Office.", "doc_id": "1801.04385", "figure_id": "1801.04385_fig_8", "figure_number": 5, "image_path": "data/mineru_output/1801.04385/1801.04385/hybrid_auto/images/1801.04385_page0_fig7.jpg", "caption": "Figure 5: A pair which multivariate logistic regression cannot nd in the data. (a) Average acceptance probability as a function of Answer Position and Time Since Previous Answer. (b) e distribution of the number of data points contributing to the value of the outcome variable for each pair of variable values.", "figure_type": "plot", "visual_anchor": "acceptance probability gradient from blue (0.05-0.2) to red (0.4-0.6)", "text_evidence": "We acknowledge funding support from the James S. McDonnell Foundation, Air Force Oce of Scientic Research (FA9550-17-1- 0327), and by the Army Research Oce (W911NF-15-1-0142).", "query_type": "comparison_explanation"}
{"query_id": "l1_1801.07593_1801.07593_fig_1_334", "query": "How does the Predictor block with weights W minimize loss L_P(ŷ,y) during training?", "answer": "The predictor modifies weights W to minimize loss L_P(ŷ,y) using a gradient-based method such as stochastic gradient descent.", "doc_id": "1801.07593", "figure_id": "1801.07593_fig_1", "figure_number": 1, "image_path": "data/mineru_output/1801.07593/1801.07593/hybrid_auto/images/1801.07593_page0_fig0.jpg", "caption": "Figure 1: The architecture of the adversarial network.", "figure_type": "architecture", "visual_anchor": "Predictor Weights: W box with upward arrow to L_P(ŷ,y)", "text_evidence": "we assume that the model is trained by attempting to modify weights W to minimize some loss L_P(ŷ,y), using a gradient-based method such as stochastic gradient descent", "query_type": "value_context"}
{"query_id": "l1_1801.07593_1801.07593_fig_1_335", "query": "Why does the output ŷ from Predictor Weights W flow as input to the Adversary Weights U block?", "answer": "The output layer of the predictor is used as input to the adversary network, which then attempts to predict Z from the predictor's output.", "doc_id": "1801.07593", "figure_id": "1801.07593_fig_1", "figure_number": 1, "image_path": "data/mineru_output/1801.07593/1801.07593/hybrid_auto/images/1801.07593_page0_fig0.jpg", "caption": "Figure 1: The architecture of the adversarial network.", "figure_type": "architecture", "visual_anchor": "Arrow connecting Predictor output ŷ to Adversary Weights: U input", "text_evidence": "The output layer of the predictor is then used as an input to another network called the adversary which attempts to predict Z", "query_type": "comparison_explanation"}
{"query_id": "l1_1801.07593_1801.07593_fig_1_336", "query": "What problem motivates the two-network architecture with separate losses L_P and L_A for predictor and adversary?", "answer": "Training data frequently contains undesirable biases, and machine learning models faithful to such data can perpetuate these biases in decision making systems.", "doc_id": "1801.07593", "figure_id": "1801.07593_fig_1", "figure_number": 1, "image_path": "data/mineru_output/1801.07593/1801.07593/hybrid_auto/images/1801.07593_page0_fig0.jpg", "caption": "Figure 1: The architecture of the adversarial network.", "figure_type": "architecture", "visual_anchor": "Two separate loss functions L_P(ŷ,y) and L_A(ẑ,z) above their respective blocks", "text_evidence": "the available training data frequently contains biases with respect to things that we would rather not use for decision making. Machine learning builds models faithful to training data and can lead to perpetuating these undesirable biases", "query_type": "anomaly_cause"}
{"query_id": "l1_1801.07593_1801.07593_fig_2_337", "query": "Why does the g+h direction help the adversary, when h equals negative alpha times the gradient of adversary loss?", "answer": "Since h = -α∇_W L_A moves opposite to the adversary's gradient, adding it to g creates a direction that reduces adversary loss, effectively helping the adversary predict the protected attribute Z.", "doc_id": "1801.07593", "figure_id": "1801.07593_fig_2", "figure_number": 2, "image_path": "data/mineru_output/1801.07593/1801.07593/hybrid_auto/images/1801.07593_page0_fig1.jpg", "caption": "Figure 2: Diagram illustrating the gradients in Eqn. 1 and the relevance of the projection term $\\mathrm { p r o j } _ { h } g$ . Without the projection term, in the pictured scenario, the predictor would move in the direction labelled $g + h$ in the diagram, which actually helps the adversary. With the projection term, the predictor will never move in a direction that helps the adversary.", "figure_type": "diagram", "visual_anchor": "upper right arrow labeled 'g + h' and left arrow labeled 'h = -α ∇_W L_A'", "text_evidence": "The output layer of the predictor is then used as an input to another network called the adversary which attempts to predict Z.", "query_type": "comparison_explanation"}
{"query_id": "l1_1801.07593_1801.07593_fig_2_338", "query": "Does the upward vertical arrow labeled g minus proj_h g prevent movement that minimizes the predictor loss L_P?", "answer": "No, it removes only the component of g that aligns with h, preserving the orthogonal part that still reduces L_P without helping the adversary minimize its loss.", "doc_id": "1801.07593", "figure_id": "1801.07593_fig_2", "figure_number": 2, "image_path": "data/mineru_output/1801.07593/1801.07593/hybrid_auto/images/1801.07593_page0_fig1.jpg", "caption": "Figure 2: Diagram illustrating the gradients in Eqn. 1 and the relevance of the projection term $\\mathrm { p r o j } _ { h } g$ . Without the projection term, in the pictured scenario, the predictor would move in the direction labelled $g + h$ in the diagram, which actually helps the adversary. With the projection term, the predictor will never move in a direction that helps the adversary.", "figure_type": "diagram", "visual_anchor": "upward vertical arrow labeled 'g - proj_h g'", "text_evidence": "We begin with a model, which we call the predictor, trained to accomplish the task of predicting Y given X. As in Figure 1, we assume that the model is trained by attempting to modify weights W to minimize some loss L_P(ŷ, y), using a gradient-based method such as stochastic gradient descent.", "query_type": "comparison_explanation"}
{"query_id": "l1_1801.07593_1801.07593_fig_2_339", "query": "How does the projection term proj_h g shown as the bottom right arrow ensure the desired equality definitions are satisfied?", "answer": "By removing the component that helps the adversary predict from Ŷ, the projection ensures the adversary gains no advantage from information about Ŷ, which is the exact condition needed to guarantee desired equality definitions.", "doc_id": "1801.07593", "figure_id": "1801.07593_fig_2", "figure_number": 2, "image_path": "data/mineru_output/1801.07593/1801.07593/hybrid_auto/images/1801.07593_page0_fig1.jpg", "caption": "Figure 2: Diagram illustrating the gradients in Eqn. 1 and the relevance of the projection term $\\mathrm { p r o j } _ { h } g$ . Without the projection term, in the pictured scenario, the predictor would move in the direction labelled $g + h$ in the diagram, which actually helps the adversary. With the projection term, the predictor will never move in a direction that helps the adversary.", "figure_type": "diagram", "visual_anchor": "bottom right arrow labeled 'proj_h g'", "text_evidence": "We will show in the next propositions that the adversary gaining no advantage from information about Ŷ is exactly the condition needed to guarantee that desired definitions of equality are satisfied.", "query_type": "value_context"}
{"query_id": "l1_1802.08139_1802.08139_fig_1_340", "query": "Why does node C have edges pointing to both A and Y in this graphical structure?", "answer": "Node C represents a confounder that affects both the sensitive attribute A and the decision Y, creating statistical dependence through both fair and unfair pathways that must be distinguished in fair decision systems.", "doc_id": "1802.08139", "figure_id": "1802.08139_fig_1", "figure_number": 1, "image_path": "data/mineru_output/1802.08139/1802.08139/hybrid_auto/images/1802.08139_page0_fig0.jpg", "caption": "Figure 1. (a): GCM with a confounder $C$ for the causal effect of $A$ on $Y$ . (b): GCM with one direct and one indirect causal path from $A$ to $Y$ . (c): GCM with a confounder $C$ for the effect of $M$ on $Y$ .", "figure_type": "diagram", "visual_anchor": "node C with two outgoing directed edges to nodes A and Y", "text_evidence": "We consider the problem of learning fair decision systems in complex scenarios in which a sensitive attribute might affect the decision along both fair and unfair pathways.", "query_type": "visual_definition"}
{"query_id": "l1_1802.08139_1802.08139_fig_1_341", "query": "How does the direct edge from A to Y relate to correcting observations adversely affected by sensitive attributes?", "answer": "The direct edge from A to Y represents a causal pathway that must be evaluated when correcting observations, as the method aims to disregard effects along unfair pathways while preserving fair information conveyed through legitimate causal relationships.", "doc_id": "1802.08139", "figure_id": "1802.08139_fig_1", "figure_number": 1, "image_path": "data/mineru_output/1802.08139/1802.08139/hybrid_auto/images/1802.08139_page0_fig0.jpg", "caption": "Figure 1. (a): GCM with a confounder $C$ for the causal effect of $A$ on $Y$ . (b): GCM with one direct and one indirect causal path from $A$ to $Y$ . (c): GCM with a confounder $C$ for the effect of $M$ on $Y$ .", "figure_type": "diagram", "visual_anchor": "horizontal directed edge connecting node A to node Y at the bottom", "text_evidence": "Our method corrects observations adversely affected by the sensitive attribute, and uses these to form a decision. This avoids disregarding fair information", "query_type": "comparison_explanation"}
{"query_id": "l1_1802.08139_1802.08139_fig_1_342", "query": "What causal modeling framework allows nodes A, C, and Y to represent variables with directed links between them?", "answer": "Graphical Causal Models (GCMs) comprise nodes representing variables and links describing statistical and causal relationships, restricted here to directed acyclic graphs where no node can be an ancestor of itself.", "doc_id": "1802.08139", "figure_id": "1802.08139_fig_1", "figure_number": 1, "image_path": "data/mineru_output/1802.08139/1802.08139/hybrid_auto/images/1802.08139_page0_fig0.jpg", "caption": "Figure 1. (a): GCM with a confounder $C$ for the causal effect of $A$ on $Y$ . (b): GCM with one direct and one indirect causal path from $A$ to $Y$ . (c): GCM with a confounder $C$ for the effect of $M$ on $Y$ .", "figure_type": "diagram", "visual_anchor": "three circular nodes labeled C, A, Y connected by directed edges forming a triangle", "text_evidence": "Causal relationships among random variables can be visually expressed using Graphical Causal Models (GCMs), which are a special case of Graphical Models. GCMs comprise nodes representing the variables, and links describing statistical and causal relationships among them.", "query_type": "value_context"}
{"query_id": "l1_1802.08139_1802.08139_fig_4_343", "query": "Why does the green path from A to Y exclude node M when studying path-specific fairness?", "answer": "The green path represents the specific causal route of interest for path-specific counterfactual fairness analysis, isolating direct effects from A to Y while excluding indirect effects through mediator M.", "doc_id": "1802.08139", "figure_id": "1802.08139_fig_4", "figure_number": 2, "image_path": "data/mineru_output/1802.08139/1802.08139/hybrid_auto/images/1802.08139_page0_fig3.jpg", "caption": "Figure 2. (a)-(b): GCMs in which we are interested in the effects along the green paths. (c): GCM corresponding to Eq. (3).", "figure_type": "diagram", "visual_anchor": "green path connecting node A to node Y", "text_evidence": "To gain insights into the problem of path-specific fairness, consider the following linear model", "query_type": "comparison_explanation"}
{"query_id": "l1_1802.08139_1802.08139_fig_4_344", "query": "Does node C require latent variable correction given it is a descendant of the sensitive attribute?", "answer": "Yes, descendants of the sensitive attribute that need correction require latent variables to address general data-generation process mismatch, as explicitly stated for the extended GCM framework.", "doc_id": "1802.08139", "figure_id": "1802.08139_fig_4", "figure_number": 2, "image_path": "data/mineru_output/1802.08139/1802.08139/hybrid_auto/images/1802.08139_page0_fig3.jpg", "caption": "Figure 2. (a)-(b): GCMs in which we are interested in the effects along the green paths. (c): GCM corresponding to Eq. (3).", "figure_type": "diagram", "visual_anchor": "node C with incoming edge from sensitive attribute", "text_evidence": "we need to explicitly incorporate a latent variable for each descendant of the sensitive attribute that needs to be corrected. General equations for the GCM of Fig. 2(c) with extra latent variables $H _ { m", "query_type": "visual_definition"}
{"query_id": "l1_1802.08139_1802.08139_fig_7_345", "query": "Why does the black A=0 distribution peak near zero while the latent variable H_m follows a Gaussian?", "answer": "The black curve represents the empirical distribution of ε_m^n from Eq. (3) with a non-linear term f(A,C), while H_m is an explicit Gaussian latent variable added to the modified GCM for generating M.", "doc_id": "1802.08139", "figure_id": "1802.08139_fig_7", "figure_number": 3, "image_path": "data/mineru_output/1802.08139/1802.08139/hybrid_auto/images/1802.08139_page0_fig6.jpg", "caption": "Figure 3. (a): Empirical distribution of $\\epsilon _ { m } ^ { n }$ for the case in which $m ^ { n }$ is generated by Eq. (3) with an extra non-linear term $f ( A , C )$ (continuous lines). Histograms of $\\tilde { p } ( H _ { m } | A )$ (crossed lines), see (b). (b): Modification of the GCM corresponding to Eq. (3) to include an explicit latent variable $H _ { m }$ for the generation of $M$ .", "figure_type": "plot", "visual_anchor": "Black curve with x markers showing narrow peak centered near 0", "text_evidence": "Empirical distribution of $\\epsilon _ { m } ^ { n }$ for the case in which $m ^ { n }$ is generated by Eq. (3) with an extra non-linear term $f ( A , C )$ (continuous lines). Histograms of $\\tilde { p } ( H _ { m } | A )$ (crossed lines)", "query_type": "comparison_explanation"}
{"query_id": "l1_1802.08139_1802.08139_fig_7_346", "query": "Does the red A=1 histogram at position -3 demonstrate the small dependence on A encouraged by MMD criterion?", "answer": "Yes, the red histogram represents $\\tilde{p}(H_m|A=1)$, and the MMD criterion is used during training to encourage small dependence of $\\tilde{p}(H_m|A=a)$ on A, though the visual separation suggests some dependence remains.", "doc_id": "1802.08139", "figure_id": "1802.08139_fig_7", "figure_number": 3, "image_path": "data/mineru_output/1802.08139/1802.08139/hybrid_auto/images/1802.08139_page0_fig6.jpg", "caption": "Figure 3. (a): Empirical distribution of $\\epsilon _ { m } ^ { n }$ for the case in which $m ^ { n }$ is generated by Eq. (3) with an extra non-linear term $f ( A , C )$ (continuous lines). Histograms of $\\tilde { p } ( H _ { m } | A )$ (crossed lines), see (b). (b): Modification of the GCM corresponding to Eq. (3) to include an explicit latent variable $H _ { m }$ for the generation of $M$ .", "figure_type": "plot", "visual_anchor": "Red curve with x markers showing peak centered near -3", "text_evidence": "We encourage $\\tilde { p } ( H _ { m } | A = a )$ to have small dependence on $A$ during training through the maximum mean discrepancy (MMD) criterion (Gretton et al., 2012).", "query_type": "value_context"}
{"query_id": "l1_1802.08139_1802.08139_fig_7_347", "query": "How does the crossed-line histogram pattern relate to the joint Gaussian distribution over Z including H_m?", "answer": "The crossed-line histograms show $\\tilde{p}(H_m|A)$, which are marginals of the joint Gaussian distribution $p(Z=\\{Y,L,M,C,H_m\\}|A)$ where the latent variable H_m follows $\\mathcal{N}(\\theta^h, \\sigma_h^2)$ in the modified GCM.", "doc_id": "1802.08139", "figure_id": "1802.08139_fig_7", "figure_number": 3, "image_path": "data/mineru_output/1802.08139/1802.08139/hybrid_auto/images/1802.08139_page0_fig6.jpg", "caption": "Figure 3. (a): Empirical distribution of $\\epsilon _ { m } ^ { n }$ for the case in which $m ^ { n }$ is generated by Eq. (3) with an extra non-linear term $f ( A , C )$ (continuous lines). Histograms of $\\tilde { p } ( H _ { m } | A )$ (crossed lines), see (b). (b): Modification of the GCM corresponding to Eq. (3) to include an explicit latent variable $H _ { m }$ for the generation of $M$ .", "figure_type": "plot", "visual_anchor": "Crossed lines overlaying both black and red distribution curves", "text_evidence": "Consider the GCM in Fig. 3(a), corresponding to Eq. (3) with the addition of a Gaussian latent variable $H _ { m } \\sim \\mathcal { N } ( \\theta ^ { h } , \\sigma _ { h } ^ { 2 } )$ in the equation for $M$ . The joint distribution $p ( Z = \\{ Y , L , M , C , H _ { m } \\} | A )$ is Gaussian", "query_type": "visual_definition"}
{"query_id": "l1_1802.08139_1802.08139_fig_9_348", "query": "Why does node M receive three incoming solid green edges when its distribution depends on A, C, and H_m?", "answer": "The three incoming edges to M from A, C, and H_m directly represent the conditional probability distribution p_θ(M|A,C,H_m), which can be modeled by any function f_θ(A,C,H_m) such as a neural network.", "doc_id": "1802.08139", "figure_id": "1802.08139_fig_9", "figure_number": 4, "image_path": "data/mineru_output/1802.08139/1802.08139/hybrid_auto/images/1802.08139_page0_fig8.jpg", "caption": "Figure 4. (a): GCM for the UCI Adult dataset. (b): GCM for the UCI German Credit dataset.", "figure_type": "diagram", "visual_anchor": "Three solid green edges pointing to node M from nodes A, C, and H_m", "text_evidence": "If M is categorical, p_{θ}(M|A,C,H_{m}) = f_{θ}(A,C,H_{m}), where f_{θ}(A,C,H_{m}) can be any function, e.g. a neural network.", "query_type": "visual_definition"}
{"query_id": "l1_1802.08139_1802.08139_fig_9_349", "query": "Do the dashed green edges between M, L, and R indicate dependencies captured in the model likelihood expression?", "answer": "Yes, the dashed edges represent probabilistic dependencies among M, L, and R that are encoded in the joint model likelihood p_θ(A,C,M,L,Y), which captures the complete factorization of the GCM.", "doc_id": "1802.08139", "figure_id": "1802.08139_fig_9", "figure_number": 4, "image_path": "data/mineru_output/1802.08139/1802.08139/hybrid_auto/images/1802.08139_page0_fig8.jpg", "caption": "Figure 4. (a): GCM for the UCI Adult dataset. (b): GCM for the UCI German Credit dataset.", "figure_type": "diagram", "visual_anchor": "Dashed green edges connecting nodes M, L, and R in the bottom row", "text_evidence": "The model likelihood p_{θ}(A,C,M,L,Y), and posterior distributions p_{θ}(H_{m}|A,C,M,L) and p_{θ}(H_{l}|", "query_type": "comparison_explanation"}
{"query_id": "l1_1802.08139_1802.08139_fig_9_350", "query": "Does node L's position between M and R reflect the posterior distribution requiring inputs from both neighbors?", "answer": "Yes, node L's central position with connections to M reflects that the posterior distribution p_θ(H_l|...) requires information from neighboring nodes M and L, as indicated by the incomplete expression mentioning H_l conditioning.", "doc_id": "1802.08139", "figure_id": "1802.08139_fig_9", "figure_number": 4, "image_path": "data/mineru_output/1802.08139/1802.08139/hybrid_auto/images/1802.08139_page0_fig8.jpg", "caption": "Figure 4. (a): GCM for the UCI Adult dataset. (b): GCM for the UCI German Credit dataset.", "figure_type": "diagram", "visual_anchor": "Node L positioned centrally in bottom row between nodes M and R with bidirectional dashed edges", "text_evidence": "The model likelihood p_{θ}(A,C,M,L,Y), and posterior distributions p_{θ}(H_{m}|A,C,M,L) and p_{θ}(H_{l}|", "query_type": "value_context"}
{"query_id": "l1_1802.08139_1802.08139_fig_12_351", "query": "Why do the blue and red histograms show two distinct peaks around -5 and 15 for the housing variable dimension?", "answer": "The bimodal distribution represents the learned conditional distribution q̃(Hs|A) after training, where A likely encodes gender-specific housing patterns that create separate modes for male and female populations.", "doc_id": "1802.08139", "figure_id": "1802.08139_fig_12", "figure_number": 6, "image_path": "data/mineru_output/1802.08139/1802.08139/hybrid_auto/images/1802.08139_page0_fig11.jpg", "caption": "Figure 6. Histograms of $\\widetilde { q } ( H _ { s } | A )$ after 2,000 and 8,000 training steps for one dimension of the variable housing.", "figure_type": "plot", "visual_anchor": "two distinct peaks in both blue (male) and red (female) histograms at approximately -5 and 15", "text_evidence": "Histograms of $\\widetilde { q } ( H _ { s } | A )$ after 2,000 and 8,000 training steps for one dimension of the variable housing.", "query_type": "comparison_explanation"}
{"query_id": "l1_1802.08139_1802.08139_fig_12_352", "query": "Does the taller blue peak near -5 compared to the red peak indicate male dominance in that housing dimension?", "answer": "Yes, the taller blue histogram bars around -5 suggest a higher density of male samples in that region of the housing variable dimension compared to females.", "doc_id": "1802.08139", "figure_id": "1802.08139_fig_12", "figure_number": 6, "image_path": "data/mineru_output/1802.08139/1802.08139/hybrid_auto/images/1802.08139_page0_fig11.jpg", "caption": "Figure 6. Histograms of $\\widetilde { q } ( H _ { s } | A )$ after 2,000 and 8,000 training steps for one dimension of the variable housing.", "figure_type": "plot", "visual_anchor": "blue histogram peak at -5 being taller than the red histogram peak at the same location", "text_evidence": "Histograms of $\\widetilde { q } ( H _ { s } | A )$ after 2,000 and 8,000 training steps for one dimension of the variable housing.", "query_type": "comparison_explanation"}
{"query_id": "l1_1802.08139_1802.08139_fig_12_353", "query": "How does the red histogram's higher peak at 15 relate to the conditional distribution of housing given gender A?", "answer": "The red (female) histogram's higher peak at 15 indicates that q̃(Hs|A) assigns greater probability density to females in that housing dimension value, showing gender-conditional structure learned during training.", "doc_id": "1802.08139", "figure_id": "1802.08139_fig_12", "figure_number": 6, "image_path": "data/mineru_output/1802.08139/1802.08139/hybrid_auto/images/1802.08139_page0_fig11.jpg", "caption": "Figure 6. Histograms of $\\widetilde { q } ( H _ { s } | A )$ after 2,000 and 8,000 training steps for one dimension of the variable housing.", "figure_type": "plot", "visual_anchor": "red histogram peak at x=15 being taller than the blue peak at the same position", "text_evidence": "Histograms of $\\widetilde { q } ( H _ { s } | A )$ after 2,000 and 8,000 training steps for one dimension of the variable housing.", "query_type": "value_context"}
{"query_id": "l1_1802.08139_1802.08139_fig_15_354", "query": "Why do the blue and red histograms show two distinct peaks near -5 and 20 for the German Credit protected attribute?", "answer": "The bimodal distribution reflects the variational approximation q̃(Hm|A) learning to separate the latent representation by the protected attribute sex (A) during training on the UCI German Credit dataset with 1,000 loan applicants.", "doc_id": "1802.08139", "figure_id": "1802.08139_fig_15", "figure_number": 5, "image_path": "data/mineru_output/1802.08139/1802.08139/hybrid_auto/images/1802.08139_page0_fig14.jpg", "caption": "Figure 5. Histograms of (one dimension of) $\\tilde { q } ( H _ { m } | A )$ after 5,000, 8,000, 15,000 and 20,000 training steps.", "figure_type": "plot", "visual_anchor": "Two distinct peaks in both blue (male) and red (female) histograms at approximately x=-5 and x=20", "text_evidence": "The German Credit dataset from the UCI repository contains 20 attributes of 1,000 individuals applying for loans... We assume the GCM in Fig. 4(b), where A corresponds to the protected attribute sex", "query_type": "comparison_explanation"}
{"query_id": "l1_1802.08139_1802.08139_fig_15_355", "query": "How does the taller left peak near -5 relate to the checking account, savings, and housing attributes represented by S?", "answer": "The dominant left peak likely captures the triple S attributes (checking account, savings, housing) which may be more concentrated or determinative in the latent space than the financial request attributes represented in the right peak.", "doc_id": "1802.08139", "figure_id": "1802.08139_fig_15", "figure_number": 5, "image_path": "data/mineru_output/1802.08139/1802.08139/hybrid_auto/images/1802.08139_page0_fig14.jpg", "caption": "Figure 5. Histograms of (one dimension of) $\\tilde { q } ( H _ { m } | A )$ after 5,000, 8,000, 15,000 and 20,000 training steps.", "figure_type": "plot", "visual_anchor": "Left peak near x=-5 with noticeably greater height than the right peak at x=20", "text_evidence": "S to the triple status of checking account, savings, and housing", "query_type": "value_context"}
{"query_id": "l1_1802.08139_1802.08139_fig_17_356", "query": "Why does the gray node C in subfigure (a) become a red bidirected link in the ADMG representation?", "answer": "The gray node C represents an unobserved confounder. In ADMG notation, unobserved common causes are indicated by red bidirected links between the affected variables, which is why C's influence is shown as a red bidirected edge in subfigure (b).", "doc_id": "1802.08139", "figure_id": "1802.08139_fig_17", "figure_number": 7, "image_path": "data/mineru_output/1802.08139/1802.08139/hybrid_auto/images/1802.08139_page0_fig16.jpg", "caption": "Figure 7. (a): GCM with an unobserved confounder $C$ indicated with a gray node. (b): ADMG corresponding to (a). The causal effect along the green path $A  Y$ cannot be identified by only using observed variables.", "figure_type": "diagram", "visual_anchor": "Gray node C in subfigure (a) and red bidirected edge in subfigure (b)", "text_evidence": "An ADMG is a causal graph containing two kinds of links, directed links (either green or black depending on whether we are interested in the corresponding causal path), and red bidirected links, indicating the presence of an unobserved common cause.", "query_type": "visual_definition"}
{"query_id": "l1_1802.08139_1802.08139_fig_17_357", "query": "How does the green path from A to Y relate to the claim about achieving fair decisions?", "answer": "The green path from A to Y represents a causal pathway of interest. Fair decisions are achieved by correcting variables that are descendants of the protected attribute A along unfair pathways, which allows retaining fair information while addressing problematic causal influences.", "doc_id": "1802.08139", "figure_id": "1802.08139_fig_17", "figure_number": 7, "image_path": "data/mineru_output/1802.08139/1802.08139/hybrid_auto/images/1802.08139_page0_fig16.jpg", "caption": "Figure 7. (a): GCM with an unobserved confounder $C$ indicated with a gray node. (b): ADMG corresponding to (a). The causal effect along the green path $A  Y$ cannot be identified by only using observed variables.", "figure_type": "diagram", "visual_anchor": "Green path from node A to node Y", "text_evidence": "A fair decision is achieved by correcting the variables that are descendants of the protected attribute along unfair pathways, rather than by imposing constraints on the model parameters.", "query_type": "comparison_explanation"}
{"query_id": "l1_1802.08139_1802.08139_fig_17_358", "query": "What prevents the causal effect along the green A to Y path from being identified using only M, W, A, Y?", "answer": "The presence of the unobserved confounder C (shown as gray node) creates hidden common causes that cannot be controlled for using only the observed variables M, W, A, and Y, making the causal effect non-identifiable from observed data alone.", "doc_id": "1802.08139", "figure_id": "1802.08139_fig_17", "figure_number": 7, "image_path": "data/mineru_output/1802.08139/1802.08139/hybrid_auto/images/1802.08139_page0_fig16.jpg", "caption": "Figure 7. (a): GCM with an unobserved confounder $C$ indicated with a gray node. (b): ADMG corresponding to (a). The causal effect along the green path $A  Y$ cannot be identified by only using observed variables.", "figure_type": "diagram", "visual_anchor": "Green path between A and Y with gray node C present", "text_evidence": "The causal effect along the green path $A  Y$ cannot be identified by only using observed variables.", "query_type": "anomaly_cause"}
{"query_id": "l1_1802.08139_1802.08139_fig_36_359", "query": "Does the bell-shaped histogram centered near zero match the two-dimensional Gaussian with diagonal covariance used for the variational posterior?", "answer": "Yes, the symmetric bell-shaped distribution is consistent with a two-dimensional Gaussian with diagonal covariance, as specified for the variational posterior distribution q.", "doc_id": "1802.08139", "figure_id": "1802.08139_fig_36", "figure_number": 8, "image_path": "data/mineru_output/1802.08139/1802.08139/hybrid_auto/images/1802.08139_page0_fig35.jpg", "caption": "Figure 8. (a): Histograms of $\\tilde { q } ( H _ { m } | A )$ (two-dimensional), $\\tilde { q } ( H _ { l } | A )$ (two-dimensional), and $\\tilde { q } ( H _ { r } | A )$ (six-dimensional) after 5,000 training steps. (b): Prior distributions $p ( H _ { m } )$ , $p ( H _ { l } )$ , and $p ( H _ { r } )$ corresponding to mixtures of ten two-dimensional Gaussians.", "figure_type": "plot", "visual_anchor": "bell-shaped histogram curve centered near x=0", "text_evidence": "As the variational posterior distribution $q$ we used a two-dimensional Gaussian with diagonal covariance, with means and log variances obtained as the outputs of a neural network", "query_type": "visual_definition"}
{"query_id": "l1_1802.08139_1802.08139_fig_36_360", "query": "Why does the histogram span from approximately -8 to 6 when the prior uses ten two-dimensional Gaussian mixture components?", "answer": "The mixture of ten two-dimensional Gaussians creates a broader support range than a single Gaussian, allowing the histogram to span the wider range from -8 to 6.", "doc_id": "1802.08139", "figure_id": "1802.08139_fig_36", "figure_number": 8, "image_path": "data/mineru_output/1802.08139/1802.08139/hybrid_auto/images/1802.08139_page0_fig35.jpg", "caption": "Figure 8. (a): Histograms of $\\tilde { q } ( H _ { m } | A )$ (two-dimensional), $\\tilde { q } ( H _ { l } | A )$ (two-dimensional), and $\\tilde { q } ( H _ { r } | A )$ (six-dimensional) after 5,000 training steps. (b): Prior distributions $p ( H _ { m } )$ , $p ( H _ { l } )$ , and $p ( H _ { r } )$ corresponding to mixtures of ten two-dimensional Gaussians.", "figure_type": "plot", "visual_anchor": "x-axis range from -8 to 6", "text_evidence": "as the prior distribution $p$ for each latent variable we used a mixture of two-dimensional Gaussians with ten mixture components and diagonal covariances", "query_type": "comparison_explanation"}
{"query_id": "l1_1802.08139_1802.08139_fig_36_361", "query": "Is the smooth unimodal shape at 5000 training steps consistent with histograms shown for posterior distributions in latent space?", "answer": "Yes, the smooth unimodal histogram is consistent with the posterior distributions visualized after 5,000 training steps in the latent space analysis.", "doc_id": "1802.08139", "figure_id": "1802.08139_fig_36", "figure_number": 8, "image_path": "data/mineru_output/1802.08139/1802.08139/hybrid_auto/images/1802.08139_page0_fig35.jpg", "caption": "Figure 8. (a): Histograms of $\\tilde { q } ( H _ { m } | A )$ (two-dimensional), $\\tilde { q } ( H _ { l } | A )$ (two-dimensional), and $\\tilde { q } ( H _ { r } | A )$ (six-dimensional) after 5,000 training steps. (b): Prior distributions $p ( H _ { m } )$ , $p ( H _ { l } )$ , and $p ( H _ { r } )$ corresponding to mixtures of ten two-dimensional Gaussians.", "figure_type": "plot", "visual_anchor": "smooth bell-shaped curve with single peak", "text_evidence": "Histograms of $\\tilde { q } ( H _ { m } | A )$ (two-dimensional), $\\tilde { q } ( H _ { l } | A )$ (two-dimensional), and $\\tilde { q } ( H _ { r } | A )$ (six-dimensional) after 5,000 training steps", "query_type": "value_context"}
{"query_id": "l1_1802.08139_1802.08139_fig_46_362", "query": "How many dimensions does the savings attribute span in the histograms shown for 2,000 training steps?", "answer": "The savings attribute spans two dimensions in the histograms, as indicated by the bimodal distribution pattern visible in the plot.", "doc_id": "1802.08139", "figure_id": "1802.08139_fig_46", "figure_number": 9, "image_path": "data/mineru_output/1802.08139/1802.08139/hybrid_auto/images/1802.08139_page0_fig45.jpg", "caption": "Figure 9. Histograms of $\\tilde { q } ( H _ { s } | A )$ after 2,000 (first row) and 8,000 (second row) training steps. From left to right: status of checking account (two dimensions), savings (two dimensions), and housing (one dimension).", "figure_type": "plot", "visual_anchor": "middle histogram with overlapping red and blue bars showing bimodal pattern", "text_evidence": "From left to right: status of checking account (two dimensions), savings (two dimensions), and housing (one dimension).", "query_type": "value_context"}
{"query_id": "l1_1802.08139_1802.08139_fig_46_363", "query": "Why does the rightmost histogram show a single distribution peak while the middle one displays two distinct modes?", "answer": "The rightmost histogram represents housing with one dimension, producing a single peak, while the middle histogram shows savings with two dimensions, creating the bimodal pattern.", "doc_id": "1802.08139", "figure_id": "1802.08139_fig_46", "figure_number": 9, "image_path": "data/mineru_output/1802.08139/1802.08139/hybrid_auto/images/1802.08139_page0_fig45.jpg", "caption": "Figure 9. Histograms of $\\tilde { q } ( H _ { s } | A )$ after 2,000 (first row) and 8,000 (second row) training steps. From left to right: status of checking account (two dimensions), savings (two dimensions), and housing (one dimension).", "figure_type": "plot", "visual_anchor": "rightmost histogram with single peak versus middle histogram with two distinct modes", "text_evidence": "status of checking account (two dimensions), savings (two dimensions), and housing (one dimension)", "query_type": "comparison_explanation"}
{"query_id": "l1_1802.08139_1802.08139_fig_46_364", "query": "Do the overlapping red and blue bars represent the posterior distributions in latent space after training?", "answer": "Yes, the overlapping red and blue bars represent posterior distributions q̃(Hs|A) in the latent space, visualized as histograms after training steps.", "doc_id": "1802.08139", "figure_id": "1802.08139_fig_46", "figure_number": 9, "image_path": "data/mineru_output/1802.08139/1802.08139/hybrid_auto/images/1802.08139_page0_fig45.jpg", "caption": "Figure 9. Histograms of $\\tilde { q } ( H _ { s } | A )$ after 2,000 (first row) and 8,000 (second row) training steps. From left to right: status of checking account (two dimensions), savings (two dimensions), and housing (one dimension).", "figure_type": "plot", "visual_anchor": "overlapping red and blue histogram bars across x-axis range -10 to 15", "text_evidence": "In Fig. 9 we show histograms for posterior distributions in the latent space.", "query_type": "visual_definition"}
{"query_id": "l1_1803.04383_1803.04383_fig_2_365", "query": "Does the cyan rectangular region spanning most of selection rates represent the no harm regime in the outcome curve?", "answer": "Yes, the cyan region represents the no harm regime. The caption explicitly states that colors indicate regions of active harm, relative harm, and no harm, with the large cyan area depicting the no harm outcome regime.", "doc_id": "1803.04383", "figure_id": "1803.04383_fig_2", "figure_number": 1, "image_path": "data/mineru_output/1803.04383/1803.04383/hybrid_auto/images/1803.04383_page0_fig1.jpg", "caption": "Figure 1: The above figure shows the outcome curve. The horizontal axis represents the selection rate for the population; the vertical axis represents the mean change in score. (a) depicts the full spectrum of outcome regimes, and colors indicate regions of active harm, relative harm, and no harm. In (b): a group that has much potential for gain, in (c): a group that has no potential for gain.", "figure_type": "diagram", "visual_anchor": "Cyan rectangular region occupying the majority of the plot area from selection rate 0 to 1", "text_evidence": "The above figure shows the outcome curve. The horizontal axis represents the selection rate for the population; the vertical axis represents the mean change in score. (a) depicts the full spectrum of outcome regimes, and colors indicate regions of active harm, relative harm, and no harm.", "query_type": "visual_definition"}
{"query_id": "l1_1803.04383_1803.04383_fig_2_366", "query": "Why does the yellow dotted region curve from zero selection rate upward before transitioning to cyan at higher rates?", "answer": "The yellow region represents a group with much potential for gain, as stated in the caption for panel (b). The curved boundary shows how mean score change varies with selection rate for populations that can benefit significantly from the policy.", "doc_id": "1803.04383", "figure_id": "1803.04383_fig_2", "figure_number": 1, "image_path": "data/mineru_output/1803.04383/1803.04383/hybrid_auto/images/1803.04383_page0_fig1.jpg", "caption": "Figure 1: The above figure shows the outcome curve. The horizontal axis represents the selection rate for the population; the vertical axis represents the mean change in score. (a) depicts the full spectrum of outcome regimes, and colors indicate regions of active harm, relative harm, and no harm. In (b): a group that has much potential for gain, in (c): a group that has no potential for gain.", "figure_type": "diagram", "visual_anchor": "Yellow dotted curved region in the lower left, bounded by a curved line transitioning to cyan", "text_evidence": "In (b): a group that has much potential for gain, in (c): a group that has no potential for gain.", "query_type": "comparison_explanation"}
{"query_id": "l1_1803.04383_1803.04383_fig_2_367", "query": "How does the larger cyan region relate to the estimated MaxUtil policy underloan condition mentioned for relative improvement?", "answer": "The larger cyan region corresponds to more regimes under which fairness criteria can yield favorable outcomes. When MaxUtil underloans, the region for relative improvement in the outcome curve expands, creating more opportunities for fairness-constrained policies to improve outcomes.", "doc_id": "1803.04383", "figure_id": "1803.04383_fig_2", "figure_number": 1, "image_path": "data/mineru_output/1803.04383/1803.04383/hybrid_auto/images/1803.04383_page0_fig1.jpg", "caption": "Figure 1: The above figure shows the outcome curve. The horizontal axis represents the selection rate for the population; the vertical axis represents the mean change in score. (a) depicts the full spectrum of outcome regimes, and colors indicate regions of active harm, relative harm, and no harm. In (b): a group that has much potential for gain, in (c): a group that has no potential for gain.", "figure_type": "diagram", "visual_anchor": "Large cyan region spanning from approximately selection rate 0.2 to 1.0", "text_evidence": "Furthermore, since the estimated MaxUtil policy underloans, the region for relative improvement in the outcome curve (Figure 1) is larger, corresponding to more regimes under which fairness criteria can yield favorable outcomes.", "query_type": "value_context"}
{"query_id": "l1_1803.04383_1803.04383_fig_4_368", "query": "Why does the orange MU curve peak at β^MaxUtil, the leftmost position among all three utility curves?", "answer": "The institution's individual utility function is more stringent than expected score changes (u(x)>0 implies Δ(x)>0), causing MaxUtil to optimize at a lower, more selective threshold than fairness-constrained rules.", "doc_id": "1803.04383", "figure_id": "1803.04383_fig_4", "figure_number": 2, "image_path": "data/mineru_output/1803.04383/1803.04383/hybrid_auto/images/1803.04383_page0_fig3.jpg", "caption": "Figure 2: Both outcomes $\\Delta \\pmb { \\mu }$ and institution utilities $\\boldsymbol { u }$ can be plotted as a function of selection rate for one group. The maxima of the utility curves determine the selection rates resulting from various decision rules.", "figure_type": "plot", "visual_anchor": "Orange solid MU curve peaking at β^MaxUtil marker, leftmost vertical marker", "text_evidence": "The institution's individual utility function is more stringent than the expected score changes, u(x)>0 ⟹ Δ(x)>0", "query_type": "comparison_explanation"}
{"query_id": "l1_1803.04383_1803.04383_fig_4_369", "query": "Do the three distinct peak positions of the utility curves in the lower panel support difficulty comparing DemParity and EqOpt?", "answer": "Yes, the different peak locations reflect that comparing DemParity and EqOpt requires knowing full distributions πA, πB to compute the transfer function, and settings exist where each causes different outcomes.", "doc_id": "1803.04383", "figure_id": "1803.04383_fig_4", "figure_number": 2, "image_path": "data/mineru_output/1803.04383/1803.04383/hybrid_auto/images/1803.04383_page0_fig3.jpg", "caption": "Figure 2: Both outcomes $\\Delta \\pmb { \\mu }$ and institution utilities $\\boldsymbol { u }$ can be plotted as a function of selection rate for one group. The maxima of the utility curves determine the selection rates resulting from various decision rules.", "figure_type": "plot", "visual_anchor": "Three utility curve maxima at different horizontal positions (β^MaxUtil, β^EqOpt, β^DemParity)", "text_evidence": "it is difficult to compare DemParity and EqOpt without knowing the full distributions πA, πB, which is necessary to compute the transfer function G^(A→B). In fact, we have found that settings exist both in which DemParity causes harm while EqOpt causes improvement and in which DemP", "query_type": "value_context"}
{"query_id": "l1_1803.04383_1803.04383_fig_4_370", "query": "How do derivatives with respect to horizontal axis selection rate βA recover the concave utility curves shown in the lower panel?", "answer": "Projecting the EO and DP constraint curves from the top panel to the horizontal axis, then taking derivatives with respect to βA, transforms the constraints into the concave utility curves displayed below.", "doc_id": "1803.04383", "figure_id": "1803.04383_fig_4", "figure_number": 2, "image_path": "data/mineru_output/1803.04383/1803.04383/hybrid_auto/images/1803.04383_page0_fig3.jpg", "caption": "Figure 2: Both outcomes $\\Delta \\pmb { \\mu }$ and institution utilities $\\boldsymbol { u }$ can be plotted as a function of selection rate for one group. The maxima of the utility curves determine the selection rates resulting from various decision rules.", "figure_type": "plot", "visual_anchor": "Lower panel showing three concave utility curves (MU, DP, EQ) plotted against Selection Rate horizontal axis", "text_evidence": "The derivatives considered throughout Section 6 are taken with respect to the selection rate βA (horizontal axis); projecting the EO and DP constraint curves to the horizontal axis recovers concave utility curves such as those shown in the lower panel of Figure 2", "query_type": "visual_definition"}
{"query_id": "l1_1803.04383_1803.04383_fig_6_371", "query": "Why does the teal DP line have slope 1 while the magenta EO curve is nonlinear?", "answer": "The DemParity constraint requires equal selection rates across groups (slope 1 straight line), while the EqOpt constraint follows the graph of G^(A→B), which is a curved function of selection rates.", "doc_id": "1803.04383", "figure_id": "1803.04383_fig_6", "figure_number": 3, "image_path": "data/mineru_output/1803.04383/1803.04383/hybrid_auto/images/1803.04383_page0_fig5.jpg", "caption": "Figure 3: Considering the utility as a function of selection rates, fairness constraints correspond to restricting the optimization to one-dimensional curves. The DemParity (DP) constraint is a straight line with slope 1, while the EqOpt (EO) constraint is a curve given by the graph of $G ^ { ( \\mathsf { A } \\to \\mathsf { B } ) }$ . The derivatives considered throughout Section 6 are taken with respect to the selection rate $\\beta _ { \\mathsf { A } }$ (horizontal axis); projecting the EO and DP constraint curves to the horizontal axis recovers concave utility curves such as those shown in the lower panel of Figure 2 (where MaxUtil in is represented by a horizontal line through the MU optimal solution).", "figure_type": "plot", "visual_anchor": "Teal straight line labeled DP with slope 1 versus magenta curved line labeled EO", "text_evidence": "The DemParity (DP) constraint is a straight line with slope 1, while the EqOpt (EO) constraint is a curve given by the graph of $G ^ { ( \\mathsf { A } \\to \\mathsf { B } ) }$", "query_type": "comparison_explanation"}
{"query_id": "l1_1803.04383_1803.04383_fig_6_372", "query": "How do derivatives along the magenta EO curve relate to the horizontal axis representing selection rate β_A?", "answer": "Derivatives are taken with respect to the selection rate β_A on the horizontal axis. Projecting the EO constraint curve to this axis recovers concave utility curves shown in other figures.", "doc_id": "1803.04383", "figure_id": "1803.04383_fig_6", "figure_number": 3, "image_path": "data/mineru_output/1803.04383/1803.04383/hybrid_auto/images/1803.04383_page0_fig5.jpg", "caption": "Figure 3: Considering the utility as a function of selection rates, fairness constraints correspond to restricting the optimization to one-dimensional curves. The DemParity (DP) constraint is a straight line with slope 1, while the EqOpt (EO) constraint is a curve given by the graph of $G ^ { ( \\mathsf { A } \\to \\mathsf { B } ) }$ . The derivatives considered throughout Section 6 are taken with respect to the selection rate $\\beta _ { \\mathsf { A } }$ (horizontal axis); projecting the EO and DP constraint curves to the horizontal axis recovers concave utility curves such as those shown in the lower panel of Figure 2 (where MaxUtil in is represented by a horizontal line through the MU optimal solution).", "figure_type": "plot", "visual_anchor": "Horizontal axis labeled 'group A selection rate' and magenta EO curve", "text_evidence": "The derivatives considered throughout Section 6 are taken with respect to the selection rate $\\beta _ { \\mathsf { A } }$ (horizontal axis); projecting the EO and DP constraint curves to the horizontal axis recovers concave utility curves", "query_type": "value_context"}
{"query_id": "l1_1803.04383_1803.04383_fig_6_373", "query": "Does the orange MU point lying off the magenta EO curve illustrate that MaxUtil is unconstrained by EqOpt?", "answer": "Yes. The MU point represents the MaxUtil optimal solution without fairness constraints, while the magenta EO curve restricts optimization to solutions satisfying the EqOpt fairness constraint, which MU does not satisfy.", "doc_id": "1803.04383", "figure_id": "1803.04383_fig_6", "figure_number": 3, "image_path": "data/mineru_output/1803.04383/1803.04383/hybrid_auto/images/1803.04383_page0_fig5.jpg", "caption": "Figure 3: Considering the utility as a function of selection rates, fairness constraints correspond to restricting the optimization to one-dimensional curves. The DemParity (DP) constraint is a straight line with slope 1, while the EqOpt (EO) constraint is a curve given by the graph of $G ^ { ( \\mathsf { A } \\to \\mathsf { B } ) }$ . The derivatives considered throughout Section 6 are taken with respect to the selection rate $\\beta _ { \\mathsf { A } }$ (horizontal axis); projecting the EO and DP constraint curves to the horizontal axis recovers concave utility curves such as those shown in the lower panel of Figure 2 (where MaxUtil in is represented by a horizontal line through the MU optimal solution).", "figure_type": "plot", "visual_anchor": "Orange point labeled MU at approximately (0.18, 0.73) positioned away from magenta EO curve", "text_evidence": "fairness constraints correspond to restricting the optimization to one-dimensional curves. The DemParity (DP) constraint is a straight line with slope 1, while the EqOpt (EO) constraint is a curve given by the graph of $G ^ { ( \\mathsf { A } \\to \\mathsf { B } ) }$", "query_type": "visual_definition"}
{"query_id": "l1_1803.04383_1803.04383_fig_10_374", "query": "Why does the dashed gray curve plateau near 1.0 before CDF reaches 0.8 while the solid black curve continues rising?", "answer": "The dashed gray curve represents a group with higher repay probability at lower credit scores, showing that this group achieves near-maximum payback rates earlier in the credit score distribution compared to the solid black curve group.", "doc_id": "1803.04383", "figure_id": "1803.04383_fig_10", "figure_number": 4, "image_path": "data/mineru_output/1803.04383/1803.04383/hybrid_auto/images/1803.04383_page0_fig9.jpg", "caption": "Figure 4: The empirical payback rates as a function of credit score and CDF for both groups from the TransUnion TransRisk dataset.", "figure_type": "plot", "visual_anchor": "dashed gray curve plateauing at ~1.0 around CDF 0.8", "text_evidence": "we use this data to estimate the success probability given score, $\\rho _ { \\mathrm { j } } ( x )$ , which we allow to vary by group to match the empiric", "query_type": "comparison_explanation"}
{"query_id": "l1_1803.04383_1803.04383_fig_10_375", "query": "Does the solid black curve remaining below 0.2 until CDF 0.5 reflect the 90-day default prediction window used?", "answer": "Yes, the solid black curve's low payback rates (below 0.2) until CDF 0.5 reflect that individuals with lower TransRisk scores are more likely to default within the 90-day period used for labeling credit risk.", "doc_id": "1803.04383", "figure_id": "1803.04383_fig_10", "figure_number": 4, "image_path": "data/mineru_output/1803.04383/1803.04383/hybrid_auto/images/1803.04383_page0_fig9.jpg", "caption": "Figure 4: The empirical payback rates as a function of credit score and CDF for both groups from the TransUnion TransRisk dataset.", "figure_type": "plot", "visual_anchor": "solid black curve staying below 0.2 until CDF 0.5", "text_evidence": "Individuals were labeled as defaulted if they failed to pay a debt for at least 90 days on at least one account in the ensuing 18-24 month period", "query_type": "value_context"}
{"query_id": "l1_1803.04383_1803.04383_fig_10_376", "query": "How do the two curves at CDF 0.4 demonstrate group-specific differences in TransRisk score predictive power?", "answer": "At CDF 0.4, the dashed gray curve shows approximately 0.95 payback rate while the solid black curve shows only about 0.15, demonstrating that identical CDF positions correspond to vastly different repay probabilities between the two race groups in the TransUnion dataset.", "doc_id": "1803.04383", "figure_id": "1803.04383_fig_10", "figure_number": 4, "image_path": "data/mineru_output/1803.04383/1803.04383/hybrid_auto/images/1803.04383_page0_fig9.jpg", "caption": "Figure 4: The empirical payback rates as a function of credit score and CDF for both groups from the TransUnion TransRisk dataset.", "figure_type": "plot", "visual_anchor": "separation between curves at CDF 0.4 (gray ~0.95, black ~0.15)", "text_evidence": "These scores, corresponding to $x$ in our model, range from 300 to 850 and are meant to predict credit risk", "query_type": "comparison_explanation"}
{"query_id": "l1_1803.04383_1803.04383_fig_12_377", "query": "Why does the teal DemParity line at 0.27 intersect the black curve above the red harm line at 0.42?", "answer": "When the profit/loss ratio is 1/10, DemParity causes active harm to the Black group by setting a threshold that exceeds the harm criterion, as shown by the teal line crossing above the red dotted harm threshold.", "doc_id": "1803.04383", "figure_id": "1803.04383_fig_12", "figure_number": 5, "image_path": "data/mineru_output/1803.04383/1803.04383/hybrid_auto/images/1803.04383_page0_fig11.jpg", "caption": "Figure 5: The empirical CDFs of both groups are plotted along with the decision thresholds resulting from MaxUtil, DemParity, and EqOpt for a model with bank utilities set to (a) $\\frac { u _ { - } } { u _ { + } } = - 4$ and (b) $\\frac { u _ { - } } { u _ { + } } = - 1 0$ . The threshold for active harm is displayed; in (a) DemParity causes active harm while in (b) it does not. EqOpt and MaxUtil never cause active harm.", "figure_type": "plot", "visual_anchor": "teal dashed horizontal line at 0.27 positioned above red dotted line at 0.42", "text_evidence": "The threshold for active harm is displayed; in (a) DemParity causes active harm while in (b) it does not. EqOpt and MaxUtil never cause active harm.", "query_type": "anomaly_cause"}
{"query_id": "l1_1803.04383_1803.04383_fig_12_378", "query": "What explains the steeper slope of the black CDF curve between scores 400-600 compared to the white curve?", "answer": "The empirical CDFs show different distributions between Black and White groups. The steeper Black curve indicates a higher concentration of scores in the 400-600 range, affecting how different loaning strategies impact each group differently.", "doc_id": "1803.04383", "figure_id": "1803.04383_fig_12", "figure_number": 5, "image_path": "data/mineru_output/1803.04383/1803.04383/hybrid_auto/images/1803.04383_page0_fig11.jpg", "caption": "Figure 5: The empirical CDFs of both groups are plotted along with the decision thresholds resulting from MaxUtil, DemParity, and EqOpt for a model with bank utilities set to (a) $\\frac { u _ { - } } { u _ { + } } = - 4$ and (b) $\\frac { u _ { - } } { u _ { + } } = - 1 0$ . The threshold for active harm is displayed; in (a) DemParity causes active harm while in (b) it does not. EqOpt and MaxUtil never cause active harm.", "figure_type": "plot", "visual_anchor": "black solid curve with steep descent from 1.0 to 0.2 between scores 400-600, versus gradual white dashed curve", "text_evidence": "The empirical CDFs of both groups are plotted along with the decision thresholds resulting from MaxUtil, DemParity, and EqOpt for a model with bank utilities", "query_type": "comparison_explanation"}
{"query_id": "l1_1803.04383_1803.04383_fig_16_379", "query": "How does the purple dot-dashed utility curve's maximum at approximately 0.35 relate to determining decision rule thresholds?", "answer": "The relative positions of the utility maxima determine the position of the decision rule thresholds, as stated with the fixed ratio u−/u+ = −4. The purple curve's peak indicates the optimal selection rate for that group.", "doc_id": "1803.04383", "figure_id": "1803.04383_fig_16", "figure_number": 6, "image_path": "data/mineru_output/1803.04383/1803.04383/hybrid_auto/images/1803.04383_page0_fig15.jpg", "caption": "Figure 6: The outcome and utility curves are plotted for both groups against the group selection rates. The relative positions of the utility maxima determine the position of the decision rule thresholds. We hold $\\frac { u _ { - } } { u _ { + } } = - 4$ as fixed.", "figure_type": "plot", "visual_anchor": "Purple dot-dashed curve (EC) with maximum around y≈0.35", "text_evidence": "The relative positions of the utility maxima determine the position of the decision rule thresholds. We hold $\\frac { u _ { - } } { u _ { + } } = - 4$ as fixed.", "query_type": "value_context"}
{"query_id": "l1_1803.04383_1803.04383_fig_16_380", "query": "Why do the teal dashed and purple curves both drop below zero after x=0.8, given the examination of normalized outcome curves?", "answer": "These curves represent utility curves for white and black groups respectively. The negative values after x=0.8 indicate that high selection rates produce negative utility for both groups, reflecting the outcome-utility relationship examined in the figure.", "doc_id": "1803.04383", "figure_id": "1803.04383_fig_16", "figure_number": 6, "image_path": "data/mineru_output/1803.04383/1803.04383/hybrid_auto/images/1803.04383_page0_fig15.jpg", "caption": "Figure 6: The outcome and utility curves are plotted for both groups against the group selection rates. The relative positions of the utility maxima determine the position of the decision rule thresholds. We hold $\\frac { u _ { - } } { u _ { + } } = - 4$ as fixed.", "figure_type": "plot", "visual_anchor": "Teal dashed and purple dot-dashed curves crossing y=0 between x=0.8 and x=1.0", "text_evidence": "These results are further examined in Figure 6, which displays the normalized outcome curves and the utility curves for both the white and the black group.", "query_type": "comparison_explanation"}
{"query_id": "l1_1803.04383_1803.04383_fig_16_381", "query": "Does the teal dashed curve's lower maximum compared to purple explain different loan rates between groups?", "answer": "Yes, the different heights of utility maxima directly determine different loan (selection) rates for each group. The teal curve's lower peak at approximately 0.25 versus purple's 0.35 results in different optimal selection rates.", "doc_id": "1803.04383", "figure_id": "1803.04383_fig_16", "figure_number": 6, "image_path": "data/mineru_output/1803.04383/1803.04383/hybrid_auto/images/1803.04383_page0_fig15.jpg", "caption": "Figure 6: The outcome and utility curves are plotted for both groups against the group selection rates. The relative positions of the utility maxima determine the position of the decision rule thresholds. We hold $\\frac { u _ { - } } { u _ { + } } = - 4$ as fixed.", "figure_type": "plot", "visual_anchor": "Teal dashed curve maximum at y≈0.25 versus purple dot-dashed curve maximum at y≈0.35", "text_evidence": "Figure 6 highlights that the position of the utility optima in the lower panel determines the loan (selection) rates.", "query_type": "comparison_explanation"}
{"query_id": "l1_1804.06876_1804.06876_fig_3_382", "query": "Why must systems correctly resolve both the solid and dashed purple coreference lines to pass the test?", "answer": "Systems must perform equally well in pro-stereotypical scenarios (solid purple) and anti-stereotypical scenarios (dashed purple) to demonstrate they are not biased by gender stereotypes based on occupations.", "doc_id": "1804.06876", "figure_id": "1804.06876_fig_3", "figure_number": 1, "image_path": "data/mineru_output/1804.06876/1804.06876/hybrid_auto/images/1804.06876_page0_fig2.jpg", "caption": "Figure 1: Pairs of gender balanced co-reference tests in the WinoBias dataset. Male and female entities are marked in solid blue and dashed orange, respectively. For each example, the gender of the pronominal reference is irrelevant for the co-reference decision. Systems must be able to make correct linking predictions in pro-stereotypical scenarios (solid purple lines) and anti-stereotypical scenarios (dashed purple lines) equally well to pass the test. Importantly, stereotypical occupations are considered based on US Department of Labor statistics.", "figure_type": "example", "visual_anchor": "solid purple line and dashed purple line showing pro-stereotypical and anti-stereotypical coreferences", "text_evidence": "Systems must be able to make correct linking predictions in pro-stereotypical scenarios (solid purple lines) and anti-stereotypical scenarios (dashed purple lines) equally well to pass the test.", "query_type": "comparison_explanation"}
{"query_id": "l1_1804.06876_1804.06876_fig_3_383", "query": "How do the dashed orange markings on 'the secretary' and 'her' relate to US labor statistics?", "answer": "The dashed orange markings indicate female entities in occupations that are stereotypically female according to US Department of Labor statistics, such as secretary where the majority of workers are women.", "doc_id": "1804.06876", "figure_id": "1804.06876_fig_3", "figure_number": 1, "image_path": "data/mineru_output/1804.06876/1804.06876/hybrid_auto/images/1804.06876_page0_fig2.jpg", "caption": "Figure 1: Pairs of gender balanced co-reference tests in the WinoBias dataset. Male and female entities are marked in solid blue and dashed orange, respectively. For each example, the gender of the pronominal reference is irrelevant for the co-reference decision. Systems must be able to make correct linking predictions in pro-stereotypical scenarios (solid purple lines) and anti-stereotypical scenarios (dashed purple lines) equally well to pass the test. Importantly, stereotypical occupations are considered based on US Department of Labor statistics.", "figure_type": "example", "visual_anchor": "dashed orange boxes marking 'the secretary' and 'her'", "text_evidence": "Importantly, stereotypical occupations are considered based on US Department of Labor statistics.", "query_type": "value_context"}
{"query_id": "l1_1804.06876_1804.06876_fig_3_384", "query": "What makes the gender swap between solid blue 'him' and dashed orange 'her' irrelevant for coreference decisions?", "answer": "The gender of the pronominal reference is explicitly designed to be irrelevant because the coreference decision should be based on contextual clues rather than stereotypical gender-occupation associations.", "doc_id": "1804.06876", "figure_id": "1804.06876_fig_3", "figure_number": 1, "image_path": "data/mineru_output/1804.06876/1804.06876/hybrid_auto/images/1804.06876_page0_fig2.jpg", "caption": "Figure 1: Pairs of gender balanced co-reference tests in the WinoBias dataset. Male and female entities are marked in solid blue and dashed orange, respectively. For each example, the gender of the pronominal reference is irrelevant for the co-reference decision. Systems must be able to make correct linking predictions in pro-stereotypical scenarios (solid purple lines) and anti-stereotypical scenarios (dashed purple lines) equally well to pass the test. Importantly, stereotypical occupations are considered based on US Department of Labor statistics.", "figure_type": "example", "visual_anchor": "solid blue 'him' in top sentence versus dashed orange 'her' in bottom sentence", "text_evidence": "For each example, the gender of the pronominal reference is irrelevant for the co-reference decision.", "query_type": "visual_definition"}
{"query_id": "l1_1804.09301_1804.09301_fig_1_385", "query": "Why do the coref arrows appear for male and neutral pronouns but disappear for the female pronoun in the third example?", "answer": "The Stanford CoreNLP rule-based system exhibits systematic gender bias, resolving male and neutral pronouns as coreferent with \"The surgeon\" but failing to do so for the corresponding female pronoun.", "doc_id": "1804.09301", "figure_id": "1804.09301_fig_1", "figure_number": 1, "image_path": "data/mineru_output/1804.09301/1804.09301/hybrid_auto/images/1804.09301_page0_fig0.jpg", "caption": "Figure 1: Stanford CoreNLP rule-based coreference system resolves a male and neutral pronoun as coreferent with “The surgeon,” but does not for the corresponding female pronoun.", "figure_type": "example", "visual_anchor": "Absence of dashed coref arrows in the third example with 'her' pronouns, contrasted with present coref arrows in first and second examples", "text_evidence": "We present an empirical study of gender bias in coreference resolution systems. We first introduce a novel, Winograd schema-style set of minimal pair sentences that differ only by pronoun gender.", "query_type": "anomaly_cause"}
{"query_id": "l1_1804.09301_1804.09301_fig_1_386", "query": "Does the presence of coref arrows only for male pronouns in the first example confirm the systematic gender bias claim?", "answer": "Yes, the coref arrows connecting 'his' to 'The surgeon' in the first example, while absent for 'her' in the third example, directly demonstrates the systematic gender bias confirmed by evaluating three publicly-available coreference resolution systems.", "doc_id": "1804.09301", "figure_id": "1804.09301_fig_1", "figure_number": 1, "image_path": "data/mineru_output/1804.09301/1804.09301/hybrid_auto/images/1804.09301_page0_fig0.jpg", "caption": "Figure 1: Stanford CoreNLP rule-based coreference system resolves a male and neutral pronoun as coreferent with “The surgeon,” but does not for the corresponding female pronoun.", "figure_type": "example", "visual_anchor": "Dashed coref arrows connecting 'his' mentions to 'The surgeon' in the first example row", "text_evidence": "With these Winogender schemas, we evaluate and confirm systematic gender bias in three publicly-available coreference resolution systems, and correlate this bias with real-world and textual gender statistics.", "query_type": "value_context"}
{"query_id": "l1_1804.09301_1804.09301_fig_1_387", "query": "How does the differential coreference resolution across the three pronoun examples illustrate the riddle about occupation stereotypes?", "answer": "The figure shows that coreference systems are more likely to link male pronouns to 'surgeon' than female pronouns, mirroring how the classic riddle exploits gender stereotypes about occupations. This reveals how coreference resolution in English is tightly bound with questions of gender.", "doc_id": "1804.09301", "figure_id": "1804.09301_fig_1", "figure_number": 1, "image_path": "data/mineru_output/1804.09301/1804.09301/hybrid_auto/images/1804.09301_page0_fig0.jpg", "caption": "Figure 1: Stanford CoreNLP rule-based coreference system resolves a male and neutral pronoun as coreferent with “The surgeon,” but does not for the corresponding female pronoun.", "figure_type": "example", "visual_anchor": "Comparison of coref arrow patterns across all three examples showing 'his', 'their', and 'her' pronouns", "text_evidence": "There is a classic riddle: A man and his son get into a terrible ca... As the riddle reveals, the task of coreference resolution in English is tightly bound with questions of gender, for humans and automated systems alike", "query_type": "comparison_explanation"}
{"query_id": "l1_1804.09301_1804.09301_fig_2_388", "query": "Why do sentences 1a and 1b both use 'the passenger' and 'someone' when rule-based systems like Stanford multi-pass sieve lack large-scale training data?", "answer": "The schema tests whether systems can resolve pronouns correctly using different participant types. Early rule-based systems relied on expert knowledge rather than data-driven patterns to handle such coreference variations.", "doc_id": "1804.09301", "figure_id": "1804.09301_fig_2", "figure_number": 2, "image_path": "data/mineru_output/1804.09301/1804.09301/hybrid_auto/images/1804.09301_page0_fig1.jpg", "caption": "Figure 2: A “Winogender” schema for the occupation paramedic. Correct answers in bold. In general, OC-CUPATION and PARTICIPANT may appear in either order in the sentence.", "figure_type": "example", "visual_anchor": "sentences 1a and 1b showing 'the passenger' (blue) versus 'someone' (blue) as different participant types", "text_evidence": "In the absence of large-scale data for training coreference models, early systems relied heavily on expert knowledge. A frequently used example of this is the Stanford multi-pass sieve system", "query_type": "comparison_explanation"}
{"query_id": "l1_1804.09301_1804.09301_fig_2_389", "query": "How do the bold correct answers in sentences 2a and 2b relate to the claim that all systems perform worse on gotcha sentences?", "answer": "Sentences 2a and 2b represent cases where pronoun resolution is tested against majority gender expectations. The bold answers show the ground truth that systems struggle with when pronoun gender mismatches the occupation's typical gender distribution.", "doc_id": "1804.09301", "figure_id": "1804.09301_fig_2", "figure_number": 2, "image_path": "data/mineru_output/1804.09301/1804.09301/hybrid_auto/images/1804.09301_page0_fig1.jpg", "caption": "Figure 2: A “Winogender” schema for the occupation paramedic. Correct answers in bold. In general, OC-CUPATION and PARTICIPANT may appear in either order in the sentence.", "figure_type": "example", "visual_anchor": "bold text indicating correct answers in sentences 2a ('was/were already dead') and 2b ('was/were already dead')", "text_evidence": "We also identify so-called \"gotcha\" sentences in which pronoun gender does not match the occupation's majority gender (BLS) if OCCUPATION is the correct answer; all systems perform worse on these \"gotchas.\"", "query_type": "value_context"}
{"query_id": "l1_1804.09301_1804.09301_fig_2_390", "query": "Does the green highlighting of 'The paramedic' across all four sentences reflect the occupation data source from Bureau of Labor Statistics?", "answer": "Yes, the consistent green highlighting marks the occupation term that is paired with gender statistics. Each occupation's schema is constructed using Bureau of Labor Statistics data to determine majority gender for evaluation purposes.", "doc_id": "1804.09301", "figure_id": "1804.09301_fig_2", "figure_number": 2, "image_path": "data/mineru_output/1804.09301/1804.09301/hybrid_auto/images/1804.09301_page0_fig1.jpg", "caption": "Figure 2: A “Winogender” schema for the occupation paramedic. Correct answers in bold. In general, OC-CUPATION and PARTICIPANT may appear in either order in the sentence.", "figure_type": "example", "visual_anchor": "green highlighting on 'The paramedic' appearing identically in sentences 1a, 2a, 1b, and 2b", "text_evidence": "Bureau of Labor Statistics. For each occupation, we wrote two similar sentence templates: one in which PRONOUN is coreferent with OCCUPATION, and one in which it is coreferent with PARTICIPANT", "query_type": "visual_definition"}
{"query_id": "l1_1804.09301_1804.09301_fig_3_391", "query": "Why do most blue data points lie below the dotted diagonal when Pearson correlation equals 0.67?", "answer": "The Bergsma and Lin (2006) gender statistics are systematically male-skewed compared to Bureau of Labor Statistics, causing most points to fall below the 45-degree line despite positive correlation.", "doc_id": "1804.09301", "figure_id": "1804.09301_fig_3", "figure_number": 3, "image_path": "data/mineru_output/1804.09301/1804.09301/hybrid_auto/images/1804.09301_page0_fig2.jpg", "caption": "Figure 3: Gender statistics from Bergsma and Lin (2006) correlate with Bureau of Labor Statistics 2015. However, the former has systematically lower female percentages; most points lie well below the 45-degree line (dotted). Regression line and $9 5 \\%$ confidence interval in blue. Pearson $\\Gamma = 0 . 6 7$ .", "figure_type": "plot", "visual_anchor": "blue data points positioned below the dotted red 45-degree diagonal line", "text_evidence": "although the gender statistics from B&L correlate with BLS employment statistics, they are systematically male-skewed (Figure 3)", "query_type": "anomaly_cause"}
{"query_id": "l1_1804.09301_1804.09301_fig_3_392", "query": "Does the blue regression line's slope below the diagonal support claims about sequential amplification in NLP pipelines?", "answer": "Yes, the systematically lower slope indicates male-skewed data from Bergsma and Lin, which demonstrates how biased statistics can propagate and amplify through data-driven NLP systems.", "doc_id": "1804.09301", "figure_id": "1804.09301_fig_3", "figure_number": 3, "image_path": "data/mineru_output/1804.09301/1804.09301/hybrid_auto/images/1804.09301_page0_fig2.jpg", "caption": "Figure 3: Gender statistics from Bergsma and Lin (2006) correlate with Bureau of Labor Statistics 2015. However, the former has systematically lower female percentages; most points lie well below the 45-degree line (dotted). Regression line and $9 5 \\%$ confidence interval in blue. Pearson $\\Gamma = 0 . 6 7$ .", "figure_type": "plot", "visual_anchor": "solid blue regression line with slope less than 1, positioned below diagonal", "text_evidence": "datadriven NLP pipelines are susceptible to sequential amplification of bias throughout a pipeline", "query_type": "comparison_explanation"}
{"query_id": "l1_1804.09301_1804.09301_fig_3_393", "query": "How does the 95% confidence interval around the blue regression line relate to evaluating coreference system architectures?", "answer": "The confidence interval quantifies uncertainty in the biased gender statistics used by systems like Lee et al. (2011) RULE, Durrett and Klein (2013) STAT, and Clark and Manning (2016a) NEURAL that are being evaluated for gender bias.", "doc_id": "1804.09301", "figure_id": "1804.09301_fig_3", "figure_number": 3, "image_path": "data/mineru_output/1804.09301/1804.09301/hybrid_auto/images/1804.09301_page0_fig2.jpg", "caption": "Figure 3: Gender statistics from Bergsma and Lin (2006) correlate with Bureau of Labor Statistics 2015. However, the former has systematically lower female percentages; most points lie well below the 45-degree line (dotted). Regression line and $9 5 \\%$ confidence interval in blue. Pearson $\\Gamma = 0 . 6 7$ .", "figure_type": "plot", "visual_anchor": "shaded blue 95% confidence interval region around the regression line", "text_evidence": "We evaluate examples of each of the three coreference system architectures described in 2: the Lee et al. (2011) sieve system from the rulebased paradigm (referred to as RULE), Durrett and Klein (2013) from the statistical paradigm (STAT), and the Clark and Manning (2016a) deep reinforcement system from the neural paradigm (NEURAL)", "query_type": "value_context"}
{"query_id": "l1_1804.09301_1804.09301_fig_4_394", "query": "Why does the green RULE regression line cross y=0 near 50% female, given the evaluation set is gender-balanced?", "answer": "The horizontal dotted black line at y=0 represents a hypothetical system with 100% accuracy on the gender-balanced Winogender set. The green RULE line crossing y=0 at 50% female indicates that RULE only achieves unbiased resolution when occupations have equal gender representation in labor statistics.", "doc_id": "1804.09301", "figure_id": "1804.09301_fig_4", "figure_number": 4, "image_path": "data/mineru_output/1804.09301/1804.09301/hybrid_auto/images/1804.09301_page0_fig3.jpg", "caption": "Figure 4: These two plots show how gender bias in coreference systems corresponds with occupational gender statistics from the U.S Bureau of Labor Statistics (left) and from text as computed by Bergsma and Lin (2006) (right); each point represents one occupation. The y-axes measure the extent to which a coref system prefers to match female pronouns with a given occupation over male pronouns, as tested by our Winogender schemas. A value of 100 (maximum female bias) means the system always resolved female pronouns to the given occupation and never male pronouns $( 1 0 0 \\% - 0 \\% )$ ; a score of -100 (maximum male bias) is the reverse; and a value of 0 indicates no gender differential. Recall the Winogender evaluation set is gender-balanced for each occupation; thus the horizontal dotted black line $\\scriptstyle ( \\mathrm { y = 0 } )$ in both plots represents a hypothetical system with $100 \\%$ accuracy. Regression lines with $9 5 \\%$ confidence intervals are shown.", "figure_type": "plot", "visual_anchor": "green regression line crossing horizontal black dashed line at y=0 around x=50", "text_evidence": "Recall the Winogender evaluation set is gender-balanced for each occupation; thus the horizontal dotted black line (y=0) in both plots represents a hypothetical system with 100% accuracy.", "query_type": "comparison_explanation"}
{"query_id": "l1_1804.09301_1804.09301_fig_4_395", "query": "Do the three regression lines with 95% confidence intervals reveal varying degrees of gender bias across system architectures?", "answer": "Yes, the three regression lines (blue STAT, green RULE, red NEURAL) show different slopes and intercepts, with RULE showing the steepest positive slope and NEURAL showing the flattest. This visual pattern directly confirms that all three systems exhibit varying degrees of gender bias as measured by the Winogender schemas.", "doc_id": "1804.09301", "figure_id": "1804.09301_fig_4", "figure_number": 4, "image_path": "data/mineru_output/1804.09301/1804.09301/hybrid_auto/images/1804.09301_page0_fig3.jpg", "caption": "Figure 4: These two plots show how gender bias in coreference systems corresponds with occupational gender statistics from the U.S Bureau of Labor Statistics (left) and from text as computed by Bergsma and Lin (2006) (right); each point represents one occupation. The y-axes measure the extent to which a coref system prefers to match female pronouns with a given occupation over male pronouns, as tested by our Winogender schemas. A value of 100 (maximum female bias) means the system always resolved female pronouns to the given occupation and never male pronouns $( 1 0 0 \\% - 0 \\% )$ ; a score of -100 (maximum male bias) is the reverse; and a value of 0 indicates no gender differential. Recall the Winogender evaluation set is gender-balanced for each occupation; thus the horizontal dotted black line $\\scriptstyle ( \\mathrm { y = 0 } )$ in both plots represents a hypothetical system with $100 \\%$ accuracy. Regression lines with $9 5 \\%$ confidence intervals are shown.", "figure_type": "plot", "visual_anchor": "three colored regression lines with different slopes and 95% confidence interval bands", "text_evidence": "By multiple measures, the Winogender schemas reveal varying degrees of gender bias in all three systems.", "query_type": "value_context"}
{"query_id": "l1_1804.09301_1804.09301_fig_4_396", "query": "Why do several occupations cluster at -100 on the y-axis despite testing the deep reinforcement neural system?", "answer": "The clustering at -100 (maximum male bias) indicates that for certain occupations, the Clark and Manning (2016a) deep reinforcement NEURAL system always resolved male pronouns to those occupations and never female pronouns, showing complete gender bias despite using modern neural architecture.", "doc_id": "1804.09301", "figure_id": "1804.09301_fig_4", "figure_number": 4, "image_path": "data/mineru_output/1804.09301/1804.09301/hybrid_auto/images/1804.09301_page0_fig3.jpg", "caption": "Figure 4: These two plots show how gender bias in coreference systems corresponds with occupational gender statistics from the U.S Bureau of Labor Statistics (left) and from text as computed by Bergsma and Lin (2006) (right); each point represents one occupation. The y-axes measure the extent to which a coref system prefers to match female pronouns with a given occupation over male pronouns, as tested by our Winogender schemas. A value of 100 (maximum female bias) means the system always resolved female pronouns to the given occupation and never male pronouns $( 1 0 0 \\% - 0 \\% )$ ; a score of -100 (maximum male bias) is the reverse; and a value of 0 indicates no gender differential. Recall the Winogender evaluation set is gender-balanced for each occupation; thus the horizontal dotted black line $\\scriptstyle ( \\mathrm { y = 0 } )$ in both plots represents a hypothetical system with $100 \\%$ accuracy. Regression lines with $9 5 \\%$ confidence intervals are shown.", "figure_type": "plot", "visual_anchor": "multiple green triangles and other markers clustered at y=-100", "text_evidence": "We evaluate examples of each of the three coreference system architectures described in 2: the Lee et al. (2011) sieve system from the rulebased paradigm (referred to as RULE), Durrett and Klein (2013) from the statistical paradigm (STAT), and the Clark and Manning (2016a) deep reinforcement system from the neural paradigm (NEURAL).", "query_type": "anomaly_cause"}
{"query_id": "l1_1805.03094_1805.03094_fig_2_397", "query": "Why does the red topmost curve maintain acceptance probability near 0.45 while blue drops below 0.25 across answer positions?", "answer": "The horizontal bands correspond to different subgroups conditioned on total number of answers the user has written, showing performance variation across experience levels as answer position increases.", "doc_id": "1805.03094", "figure_id": "1805.03094_fig_2", "figure_number": 1, "image_path": "data/mineru_output/1805.03094/1805.03094/hybrid_auto/images/1805.03094_page0_fig1.jpg", "caption": "Figure 1: Disaggregation of Stack Exchange data. (a) The heat map shows the probability the answer is accepted as a function of its answer position within a session, with the horizontal bands corresponding to the different subgroups, conditioned on total number of answers the user has written. (b) Number of data samples within each bin of the heat map. Note that the outcome becomes noisy when there are few samples. The trends in performance as a function of answer position in (c) disaggregated data and (d) aggregate data. Error bars in (c) and (d) show $9 5 \\%$ confidence interval.", "figure_type": "plot", "visual_anchor": "red topmost curve at ~0.45 probability versus blue bottom curve at ~0.25 probability", "text_evidence": "the horizontal bands corresponding to the different subgroups, conditioned on total number of answers the user has written", "query_type": "comparison_explanation"}
{"query_id": "l1_1805.03094_1805.03094_fig_2_398", "query": "Do the declining trends from position 0 to 20 in all colored curves support disaggregation by answer count?", "answer": "Yes, the visualization shows disaggregated data with each curve representing a different subgroup based on number of answers written, revealing distinct declining performance patterns.", "doc_id": "1805.03094", "figure_id": "1805.03094_fig_2", "figure_number": 1, "image_path": "data/mineru_output/1805.03094/1805.03094/hybrid_auto/images/1805.03094_page0_fig1.jpg", "caption": "Figure 1: Disaggregation of Stack Exchange data. (a) The heat map shows the probability the answer is accepted as a function of its answer position within a session, with the horizontal bands corresponding to the different subgroups, conditioned on total number of answers the user has written. (b) Number of data samples within each bin of the heat map. Note that the outcome becomes noisy when there are few samples. The trends in performance as a function of answer position in (c) disaggregated data and (d) aggregate data. Error bars in (c) and (d) show $9 5 \\%$ confidence interval.", "figure_type": "plot", "visual_anchor": "declining trends in all curves from answer position 0 to 20", "text_evidence": "Figure 1 visualizes the data, disaggregated on the number of answers", "query_type": "comparison_explanation"}
{"query_id": "l1_1805.03094_1805.03094_fig_2_399", "query": "Why do the 95% confidence interval error bars increase at higher answer positions around 15-20?", "answer": "The outcome becomes noisy when there are few samples at higher answer positions, causing larger confidence intervals despite the stated 95% level.", "doc_id": "1805.03094", "figure_id": "1805.03094_fig_2", "figure_number": 1, "image_path": "data/mineru_output/1805.03094/1805.03094/hybrid_auto/images/1805.03094_page0_fig1.jpg", "caption": "Figure 1: Disaggregation of Stack Exchange data. (a) The heat map shows the probability the answer is accepted as a function of its answer position within a session, with the horizontal bands corresponding to the different subgroups, conditioned on total number of answers the user has written. (b) Number of data samples within each bin of the heat map. Note that the outcome becomes noisy when there are few samples. The trends in performance as a function of answer position in (c) disaggregated data and (d) aggregate data. Error bars in (c) and (d) show $9 5 \\%$ confidence interval.", "figure_type": "plot", "visual_anchor": "error bars increasing in size at answer positions 15-20", "text_evidence": "Note that the outcome becomes noisy when there are few samples. The trends in performance as a function of answer position in (c) disaggregated data and (d) aggregate data. Error bars in (c) and (d) show 95% confidence interval", "query_type": "anomaly_cause"}
{"query_id": "l1_1805.03094_1805.03094_fig_6_400", "query": "Why does the blue line remain near 0.2 acceptance probability across all answer positions when user reputation categories exist?", "answer": "The blue line represents users with the lowest reputation category. Lower reputation users experience consistently lower acceptance probabilities regardless of answer position, showing reputation-based stratification in the disaggregated Stack Exchange data.", "doc_id": "1805.03094", "figure_id": "1805.03094_fig_6", "figure_number": 2, "image_path": "data/mineru_output/1805.03094/1805.03094/hybrid_auto/images/1805.03094_page0_fig5.jpg", "caption": "Figure 2: Disaggregation of Stack Exchange data similar to Fig. 1, but instead disaggreagted on user reputation. (a) The heat map shows acceptance probability as a function of its answer position within a session. (b) Number of data samples within each bin of the heat map. Note that the outcome becomes noisy when there are few samples. The trends in (c) disaggregated data and (d) aggregate data.", "figure_type": "plot", "visual_anchor": "Blue line with data points consistently around 0.2 acceptance probability", "text_evidence": "Figure 2: Disaggregation of Stack Exchange data similar to Fig. 1, but instead disaggreagted on user reputation.", "query_type": "comparison_explanation"}
{"query_id": "l1_1805.03094_1805.03094_fig_6_401", "query": "Does the red line's maintenance near 0.5 acceptance probability through position 20 align with high-reputation user behavior patterns?", "answer": "Yes, the red line represents the highest reputation users who maintain approximately 0.5 acceptance probability across all answer positions, demonstrating that high reputation provides consistent acceptance advantages independent of answer timing.", "doc_id": "1805.03094", "figure_id": "1805.03094_fig_6", "figure_number": 2, "image_path": "data/mineru_output/1805.03094/1805.03094/hybrid_auto/images/1805.03094_page0_fig5.jpg", "caption": "Figure 2: Disaggregation of Stack Exchange data similar to Fig. 1, but instead disaggreagted on user reputation. (a) The heat map shows acceptance probability as a function of its answer position within a session. (b) Number of data samples within each bin of the heat map. Note that the outcome becomes noisy when there are few samples. The trends in (c) disaggregated data and (d) aggregate data.", "figure_type": "plot", "visual_anchor": "Red line with error bars maintaining around 0.5 acceptance probability from position 0 to 20", "text_evidence": "The heat map shows acceptance probability as a function of its answer position within a session. The trends in (c) disaggregated data and (d) aggregate data.", "query_type": "value_context"}
{"query_id": "l1_1805.03094_1805.03094_fig_6_402", "query": "What explains the sparse confidence intervals at later answer positions despite the platform's gamified learning environment?", "answer": "The sparse confidence intervals reflect fewer data samples in those bins, as noted in the caption. This occurs because fewer answers reach later positions in a session, making outcomes noisier regardless of Duolingo's gamification features.", "doc_id": "1805.03094", "figure_id": "1805.03094_fig_6", "figure_number": 2, "image_path": "data/mineru_output/1805.03094/1805.03094/hybrid_auto/images/1805.03094_page0_fig5.jpg", "caption": "Figure 2: Disaggregation of Stack Exchange data similar to Fig. 1, but instead disaggreagted on user reputation. (a) The heat map shows acceptance probability as a function of its answer position within a session. (b) Number of data samples within each bin of the heat map. Note that the outcome becomes noisy when there are few samples. The trends in (c) disaggregated data and (d) aggregate data.", "figure_type": "plot", "visual_anchor": "Wider error bars and vertical lines extending from data points at answer positions 15-20", "text_evidence": "Number of data samples within each bin of the heat map. Note that the outcome becomes noisy when there are few samples.", "query_type": "anomaly_cause"}
{"query_id": "l1_1805.03094_1805.03094_fig_13_403", "query": "Does the red region at month 5 for five first attempt 5 align with lesson index as a performance feature?", "answer": "Yes, the high performance (red, ~0.9-1.0) at month 5 for five first attempt 5 corresponds with the use of lesson index among all lessons for this user as one of the more than two dozen features describing performance.", "doc_id": "1805.03094", "figure_id": "1805.03094_fig_13", "figure_number": 3, "image_path": "data/mineru_output/1805.03094/1805.03094/hybrid_auto/images/1805.03094_page0_fig12.jpg", "caption": "Figure 3: A disaggregation of the Khan Academy data showing performance as a function of hour of day, conditioned on all first attempts. (a) The heat map shows average performance within a subgroup as a function of the hour of day. (b) Number of data samples within each subgroup. The trends in (c) the disaggregated data and in (d) aggregated data.", "figure_type": "plot", "visual_anchor": "red region at month 5, five first attempt value 5", "text_evidence": "These include the number of words seen and correctly answered during a lesson (lesson seen and lesson correct), the number of distinct words shown during a lesson, lesson index among all lessons for this user", "query_type": "value_context"}
{"query_id": "l1_1805.03094_1805.03094_fig_13_404", "query": "Why does performance shift from blue at month 6 to lighter colors at later months when tracking first attempts?", "answer": "The shift from blue (low performance, ~0.2) at month 6 to lighter colors reflects temporal learning patterns captured by features like time to next lesson and time since previous lesson, which track user engagement over time.", "doc_id": "1805.03094", "figure_id": "1805.03094_fig_13", "figure_number": 3, "image_path": "data/mineru_output/1805.03094/1805.03094/hybrid_auto/images/1805.03094_page0_fig12.jpg", "caption": "Figure 3: A disaggregation of the Khan Academy data showing performance as a function of hour of day, conditioned on all first attempts. (a) The heat map shows average performance within a subgroup as a function of the hour of day. (b) Number of data samples within each subgroup. The trends in (c) the disaggregated data and in (d) aggregated data.", "figure_type": "plot", "visual_anchor": "blue region at month 6 transitioning to pink/red at months 8-12", "text_evidence": "lesson index among all lessons for this user, time to next lesson, time since the previous lesson, lesson position within its session, session length in terms of the number of lessons and duration", "query_type": "comparison_explanation"}
{"query_id": "l1_1805.03094_1805.03094_fig_13_405", "query": "What explains the consistently high red performance for five first attempts 4-5 despite session position variability?", "answer": "The consistently high performance (red, 0.8-1.0) for five first attempts 4-5 suggests that completing more initial lessons, tracked as a user-related feature, strongly predicts success regardless of within-session positioning.", "doc_id": "1805.03094", "figure_id": "1805.03094_fig_13", "figure_number": 3, "image_path": "data/mineru_output/1805.03094/1805.03094/hybrid_auto/images/1805.03094_page0_fig12.jpg", "caption": "Figure 3: A disaggregation of the Khan Academy data showing performance as a function of hour of day, conditioned on all first attempts. (a) The heat map shows average performance within a subgroup as a function of the hour of day. (b) Number of data samples within each subgroup. The trends in (c) the disaggregated data and in (d) aggregated data.", "figure_type": "plot", "visual_anchor": "red horizontal bands at y-values 4 and 5 across most months", "text_evidence": "User-related features include the number of five first lessons", "query_type": "anomaly_cause"}
{"query_id": "l1_1805.03094_1805.03094_fig_14_406", "query": "Why do the pink dots remain stable around 0.8 across all months despite lessons introducing more distinct words?", "answer": "Users are conditioned on five first attempts, selecting only those who succeeded initially. However, as lessons introduce more words (after 20 new words), even these selected users can no longer maintain perfect performance.", "doc_id": "1805.03094", "figure_id": "1805.03094_fig_14", "figure_number": 4, "image_path": "data/mineru_output/1805.03094/1805.03094/hybrid_auto/images/1805.03094_page0_fig13.jpg", "caption": "Figure 4: Disaggregation of Khan Academy data showing performance as a function of month, conditioned on five first attempts. (a) The heat map shows average performance as a function of the month. (b) Number of data samples within each subgroup. The trends in (c) the disaggregated data and in (d) aggregated data.", "figure_type": "plot", "visual_anchor": "pink dots with error bars at approximately 0.8 performance level", "text_evidence": "After 20 new words are shown in a lesson, users can no longer answer all the words correctly.", "query_type": "anomaly_cause"}
{"query_id": "l1_1805.03094_1805.03094_fig_14_407", "query": "How does the vertical separation between pink, purple, and blue performance clusters relate to lesson difficulty patterns?", "answer": "The three distinct performance levels (0.8, 0.6, 0.3-0.4) show stratification based on lesson difficulty. The heat map diagonal shows perfect lessons, but as more distinct words are introduced, maintaining perfect performance becomes impossible.", "doc_id": "1805.03094", "figure_id": "1805.03094_fig_14", "figure_number": 4, "image_path": "data/mineru_output/1805.03094/1805.03094/hybrid_auto/images/1805.03094_page0_fig13.jpg", "caption": "Figure 4: Disaggregation of Khan Academy data showing performance as a function of month, conditioned on five first attempts. (a) The heat map shows average performance as a function of the month. (b) Number of data samples within each subgroup. The trends in (c) the disaggregated data and in (d) aggregated data.", "figure_type": "plot", "visual_anchor": "three horizontally separated groups of dots at different y-axis values with dashed lines between them", "text_evidence": "The trends along the diagonal show perfect lessons, where users answered all words they were shown correctly. However, as the lessons become more difficult—more distinct words are introduced—it becomes more difficult for users to have perfect performance.", "query_type": "comparison_explanation"}
{"query_id": "l1_1805.03094_1805.03094_fig_14_408", "query": "Do the blue dots at months 6-7 dropping to 0.2 performance align with the Khan Academy disaggregation methodology?", "answer": "Yes, the drop represents a specific subgroup in the disaggregated data conditioned on five first attempts. The disaggregation reveals month-dependent performance variations that would be masked in aggregated analysis.", "doc_id": "1805.03094", "figure_id": "1805.03094_fig_14", "figure_number": 4, "image_path": "data/mineru_output/1805.03094/1805.03094/hybrid_auto/images/1805.03094_page0_fig13.jpg", "caption": "Figure 4: Disaggregation of Khan Academy data showing performance as a function of month, conditioned on five first attempts. (a) The heat map shows average performance as a function of the month. (b) Number of data samples within each subgroup. The trends in (c) the disaggregated data and in (d) aggregated data.", "figure_type": "plot", "visual_anchor": "blue dots at months 6-7 showing lowest performance around 0.2-0.3", "text_evidence": "Figure 4 shows the disaggregation corresponding to the", "query_type": "value_context"}
{"query_id": "l1_1805.03094_1805.03094_fig_21_409", "query": "Why does the blue region transition to red near 20 distinct words, despite the diagonal showing perfect lessons?", "answer": "After 20 new words are shown in a lesson, users can no longer answer all words correctly, indicating that lesson difficulty increases with more distinct words introduced.", "doc_id": "1805.03094", "figure_id": "1805.03094_fig_21", "figure_number": 5, "image_path": "data/mineru_output/1805.03094/1805.03094/hybrid_auto/images/1805.03094_page0_fig20.jpg", "caption": "Figure 5: Disaggregation of Duolingo data. (a) The heat map shows performance, as a function of how many lessons the user completed, conditioned on how many of the five first lessons were answered correctly. (b) Number of data samples within each bin of the heat map. Trends in (c) the disaggregated data and in (d) aggregate data. Errors bars show $9 5 \\%$ confidence interval.", "figure_type": "plot", "visual_anchor": "transition from blue to red coloring around 20 distinct words on the y-axis", "text_evidence": "After 20 new words are shown in a lesson, users can no longer answer all the words correctly.", "query_type": "anomaly_cause"}
{"query_id": "l1_1805.03094_1805.03094_fig_21_410", "query": "Does the diagonal blue region with high performance values support that users answered all shown words correctly?", "answer": "Yes, the diagonal blue region with performance values near 0.6-0.7 corresponds to perfect lessons where users answered all words correctly.", "doc_id": "1805.03094", "figure_id": "1805.03094_fig_21", "figure_number": 5, "image_path": "data/mineru_output/1805.03094/1805.03094/hybrid_auto/images/1805.03094_page0_fig20.jpg", "caption": "Figure 5: Disaggregation of Duolingo data. (a) The heat map shows performance, as a function of how many lessons the user completed, conditioned on how many of the five first lessons were answered correctly. (b) Number of data samples within each bin of the heat map. Trends in (c) the disaggregated data and in (d) aggregate data. Errors bars show $9 5 \\%$ confidence interval.", "figure_type": "plot", "visual_anchor": "diagonal blue region with performance values 0.6-0.7 on the color scale", "text_evidence": "along the diagonal show perfect lessons, where users answered all words they were shown correctly.", "query_type": "value_context"}
{"query_id": "l1_1805.03094_1805.03094_fig_21_411", "query": "How does the red coloring at high distinct word counts relate to the stated impact of experience?", "answer": "The red coloring indicates declining performance as distinct words increase, demonstrating that experience with more lessons (x-axis) impacts how users handle vocabulary complexity.", "doc_id": "1805.03094", "figure_id": "1805.03094_fig_21", "figure_number": 5, "image_path": "data/mineru_output/1805.03094/1805.03094/hybrid_auto/images/1805.03094_page0_fig20.jpg", "caption": "Figure 5: Disaggregation of Duolingo data. (a) The heat map shows performance, as a function of how many lessons the user completed, conditioned on how many of the five first lessons were answered correctly. (b) Number of data samples within each bin of the heat map. Trends in (c) the disaggregated data and in (d) aggregate data. Errors bars show $9 5 \\%$ confidence interval.", "figure_type": "plot", "visual_anchor": "red coloring at high distinct word values (upper portion of heat map)", "text_evidence": "Figure 5 examines the impact of experience on performance.", "query_type": "comparison_explanation"}
{"query_id": "l1_1805.03094_1805.03094_fig_22_412", "query": "Does the dark green diagonal band in the heatmap reflect users who answer all words correctly?", "answer": "Yes, the dark green diagonal band represents high performance, specifically the probability to answer all words correctly in a lesson, as shown by the count scale reaching 10^4.", "doc_id": "1805.03094", "figure_id": "1805.03094_fig_22", "figure_number": 6, "image_path": "data/mineru_output/1805.03094/1805.03094/hybrid_auto/images/1805.03094_page0_fig21.jpg", "caption": "Figure 6: Disaggregation of Duolingo data showing performance as a function of lesson Correct (a) The heat map shows performance, i.e., probability to answer all the words correctly, conditioned on the number of distinct words in the lesson. (b) Number of data samples in each bin of the heat map. Trends in (c) the disaggregated data and in (d) aggregate data. Errors bars show $9 5 \\%$ confidence interval.", "figure_type": "plot", "visual_anchor": "dark green diagonal band from lower-left to upper-right with count values ~10^3 to 10^4", "text_evidence": "The heat map shows performance, i.e., probability to answer all the words correctly, conditioned on the number of distinct words in the lesson.", "query_type": "visual_definition"}
{"query_id": "l1_1805.03094_1805.03094_fig_22_413", "query": "Why does performance remain high along the diagonal green band despite increasing distinct words from 10^0 to 10^1?", "answer": "The diagonal pattern suggests that when lesson correct count scales proportionally with distinct words, users maintain high probability of answering all words correctly, indicating balanced difficulty progression.", "doc_id": "1805.03094", "figure_id": "1805.03094_fig_22", "figure_number": 6, "image_path": "data/mineru_output/1805.03094/1805.03094/hybrid_auto/images/1805.03094_page0_fig21.jpg", "caption": "Figure 6: Disaggregation of Duolingo data showing performance as a function of lesson Correct (a) The heat map shows performance, i.e., probability to answer all the words correctly, conditioned on the number of distinct words in the lesson. (b) Number of data samples in each bin of the heat map. Trends in (c) the disaggregated data and in (d) aggregate data. Errors bars show $9 5 \\%$ confidence interval.", "figure_type": "plot", "visual_anchor": "diagonal green band maintaining dark green color (~10^3 count) as both axes increase proportionally", "text_evidence": "Another disaggregation of DL data is shown in Figure 6.", "query_type": "comparison_explanation"}
{"query_id": "l1_1805.03094_1805.03094_fig_22_414", "query": "Does the yellow region at high Lesson Correct values support that initial performance differentiates user subgroups?", "answer": "Yes, the yellow low-count region at high Lesson Correct values indicates fewer users achieve very high correctness, which aligns with initial performance separating high from low performers over time.", "doc_id": "1805.03094", "figure_id": "1805.03094_fig_22", "figure_number": 6, "image_path": "data/mineru_output/1805.03094/1805.03094/hybrid_auto/images/1805.03094_page0_fig21.jpg", "caption": "Figure 6: Disaggregation of Duolingo data showing performance as a function of lesson Correct (a) The heat map shows performance, i.e., probability to answer all the words correctly, conditioned on the number of distinct words in the lesson. (b) Number of data samples in each bin of the heat map. Trends in (c) the disaggregated data and in (d) aggregate data. Errors bars show $9 5 \\%$ confidence interval.", "figure_type": "plot", "visual_anchor": "yellow region (~10^1 count) at Lesson Correct > 10^1.5", "text_evidence": "Those users who were initially high performers appear to be different from the low performers, especially when looking at how their performance changes over time.", "query_type": "value_context"}
{"query_id": "l1_1805.03677_1805.03677_fig_1_415", "query": "Why does the red arrow emphasize preprocessing when the pipeline begins with a question or goal?", "answer": "The red arrow highlights the Dataset Nutrition Label's role in lowering barriers to standardized data analysis before model development, addressing the costly and non-standardized nature of current data analysis methods.", "doc_id": "1805.03677", "figure_id": "1805.03677_fig_1", "figure_number": 1, "image_path": "data/mineru_output/1805.03677/1805.03677/hybrid_auto/images/1805.03677_page0_fig0.jpg", "caption": "Figure 1.​ Model Development Pipeline", "figure_type": "diagram", "visual_anchor": "red arrow pointing from Dataset to Dataset Preprocessing with text 'Interrogating data quality & generating nutrition label'", "text_evidence": "Current methods of data analysis, particularly before model development, are costly and not standardized. The Dataset Nutrition Label (the Label) is a diagnostic framework that1 lowers the barrier to standardized data analysis", "query_type": "comparison_explanation"}
{"query_id": "l1_1805.03677_1805.03677_fig_1_416", "query": "Does the curved arrow returning to Dataset address the timing issue of models coming under scrutiny?", "answer": "Yes, the feedback arrow enables iterative data refinement throughout the pipeline, allowing scrutiny to occur before deployment rather than only after models are built, trained, and deployed.", "doc_id": "1805.03677", "figure_id": "1805.03677_fig_1", "figure_number": 1, "image_path": "data/mineru_output/1805.03677/1805.03677/hybrid_auto/images/1805.03677_page0_fig0.jpg", "caption": "Figure 1.​ Model Development Pipeline", "figure_type": "diagram", "visual_anchor": "curved feedback arrow from Model Development stage back to Dataset icon", "text_evidence": "Models often come under scrutiny only after they are built, trained, and deployed.", "query_type": "value_context"}
{"query_id": "l1_1805.03677_1805.03677_fig_1_417", "query": "How do the wrench and screwdriver icons in Dataset Preprocessing relate to the Label's extensible design?", "answer": "The tools symbolize the modular construction approach of the Label, which is designed in an extensible fashion with multiple distinct components referred to as modules.", "doc_id": "1805.03677", "figure_id": "1805.03677_fig_1", "figure_number": 1, "image_path": "data/mineru_output/1805.03677/1805.03677/hybrid_auto/images/1805.03677_page0_fig0.jpg", "caption": "Figure 1.​ Model Development Pipeline", "figure_type": "diagram", "visual_anchor": "wrench and screwdriver icons in the Dataset Preprocessing stage box", "text_evidence": "The Label is designed in an extensible fashion with multiple distinct components that we refer to as \"modules\"", "query_type": "visual_definition"}
{"query_id": "l1_1805.03677_1805.03677_fig_2_418", "query": "Does the 44.1% majority lacking best practices support the claim that specialists need quicker viability assessment?", "answer": "Yes, the survey showing 44.1% (15 respondents) answered 'No' to having best practices demonstrates the problem that data specialists require better tools to assess AI system viability more quickly.", "doc_id": "1805.03677", "figure_id": "1805.03677_fig_2", "figure_number": 2, "image_path": "data/mineru_output/1805.03677/1805.03677/hybrid_auto/images/1805.03677_page0_fig1.jpg", "caption": "Figure 2.​ (A) Survey results about data analysis best practices in respondents’ organizations and (B) Survey results about how respondents learned to analyze data", "figure_type": "plot", "visual_anchor": "Blue segment at 44.1% (15 respondents) representing 'No'", "text_evidence": "To improve the accuracy and fairness of AI systems, it is imperative that data specialists are able to more quickly assess the viability", "query_type": "value_context"}
{"query_id": "l1_1805.03677_1805.03677_fig_2_419", "query": "Why do the combined 44.1% with no practices and 8.8% in development exceed those with established practices?", "answer": "The survey results showing 52.9% either lacking or developing best practices provide credence to the problem that organizations struggle with data interrogation practices before building AI models.", "doc_id": "1805.03677", "figure_id": "1805.03677_fig_2", "figure_number": 2, "image_path": "data/mineru_output/1805.03677/1805.03677/hybrid_auto/images/1805.03677_page0_fig1.jpg", "caption": "Figure 2.​ (A) Survey results about data analysis best practices in respondents’ organizations and (B) Survey results about how respondents learned to analyze data", "figure_type": "plot", "visual_anchor": "Blue segment (44.1%) and green segment (8.8%) compared to red segment (26.5%)", "text_evidence": "We conducted an anonymous online survey (Figure 2), ​the results of which further lend credence to this problem.", "query_type": "comparison_explanation"}
{"query_id": "l1_1805.03677_1805.03677_fig_2_420", "query": "How does the 26.5% affirmative response relate to the prototype's need for diverse information modules?", "answer": "The 26.5% with existing practices suggests organizations have varied approaches, which aligns with developing prototype modules that highlight potential diversity of information to accommodate different organizational needs.", "doc_id": "1805.03677", "figure_id": "1805.03677_fig_2", "figure_number": 2, "image_path": "data/mineru_output/1805.03677/1805.03677/hybrid_auto/images/1805.03677_page0_fig1.jpg", "caption": "Figure 2.​ (A) Survey results about data analysis best practices in respondents’ organizations and (B) Survey results about how respondents learned to analyze data", "figure_type": "plot", "visual_anchor": "Red segment showing 26.5% (9 respondents) for 'Yes'", "text_evidence": "Variability of attributes across prototype modules highlights the potential diversity of information included in a Label", "query_type": "value_context"}
{"query_id": "l1_1805.03677_1805.03677_fig_4_421", "query": "How does the blue arrow path from data spreadsheet to label generator support processing tabular datasets under 100K rows?", "answer": "The blue arrow shows data flowing directly to the label generator, which performs simple statistical analyses in the browser for small tabular datasets under 100K rows without requiring server-side processing.", "doc_id": "1805.03677", "figure_id": "1805.03677_fig_4", "figure_number": 3, "image_path": "data/mineru_output/1805.03677/1805.03677/hybrid_auto/images/1805.03677_page0_fig3.jpg", "caption": "Figure 3.​ Architecture of the proposed Data Nutrition Label ecosystem.", "figure_type": "architecture", "visual_anchor": "blue arrow from data spreadsheet to label generator", "text_evidence": "Simple statistical analyses involving the generation of histograms, distribution information, and linear correlations are carried out directly in the browser, given tabular datasets of ${ < } 1 0 0 \\mathrm { K }$ rows.", "query_type": "value_context"}
{"query_id": "l1_1805.03677_1805.03677_fig_4_422", "query": "Why does the architecture show both the label generator and label viewer as distinct components with separate icons?", "answer": "The ecosystem comprises two main components designed separately: a label maker (generator with gear icon) and a label viewer (with eye icon), reflecting their different functional roles in creating versus displaying labels.", "doc_id": "1805.03677", "figure_id": "1805.03677_fig_4", "figure_number": 3, "image_path": "data/mineru_output/1805.03677/1805.03677/hybrid_auto/images/1805.03677_page0_fig3.jpg", "caption": "Figure 3.​ Architecture of the proposed Data Nutrition Label ecosystem.", "figure_type": "architecture", "visual_anchor": "label generator (gear icon) and label viewer (eye icon) as separate browser windows", "text_evidence": "The label ecosystem comprises two main components: a label maker and a label viewer (Figure 3​).", "query_type": "visual_definition"}
{"query_id": "l1_1805.03677_1805.03677_fig_4_423", "query": "How do the red arrows from label repository enable the extensible module design mentioned for different dataset types?", "answer": "The red retrieval arrows allow flexible access to stored labels composed of stand-alone modules, enabling different module arrangements to be retrieved and viewed based on specific dataset requirements and available information.", "doc_id": "1805.03677", "figure_id": "1805.03677_fig_4", "figure_number": 3, "image_path": "data/mineru_output/1805.03677/1805.03677/hybrid_auto/images/1805.03677_page0_fig3.jpg", "caption": "Figure 3.​ Architecture of the proposed Data Nutrition Label ecosystem.", "figure_type": "architecture", "visual_anchor": "red arrows showing label retrieval flow from cloud repository to viewer", "text_evidence": "The Label is designed in an extensible fashion with multiple distinct components that we refer to as \"modules\" (Table 1​). The modules are stand-alone, allowing for greater flexibility as arrangements of different modules can be used for different types of datasets.", "query_type": "comparison_explanation"}
{"query_id": "l1_1805.03677_1805.03677_fig_5_424", "query": "Why can the Pair Plot module's dropdown menus select variables for datasets under 100K rows without server-side processing?", "answer": "Simple statistical analyses including histograms, distributions, and linear correlations are executed directly in the browser for datasets smaller than 100K rows, reserving server-side processing only for specialized analyses requiring additional computational power.", "doc_id": "1805.03677", "figure_id": "1805.03677_fig_5", "figure_number": 4, "image_path": "data/mineru_output/1805.03677/1805.03677/hybrid_auto/images/1805.03677_page0_fig4.jpg", "caption": "Figure 4.​ Prototype Label demonstrating the Pair Plot module and highlighting the interactive dropdown menus for selecting variables.", "figure_type": "plot", "visual_anchor": "Dropdown menus at bottom showing 'Variable A: product_name' and 'Variable B: recipient_state'", "text_evidence": "Simple statistical analyses involving the generation of histograms, distribution information, and linear correlations are carried out directly in the browser, given tabular datasets of ${ < } 1 0 0 \\mathrm { K }$ rows. Server-side processing is thus reserved for more specialized and sophisticated analyses requiring additional computational power.", "query_type": "value_context"}
{"query_id": "l1_1805.03677_1805.03677_fig_5_425", "query": "How does the CA state bar at ~55 in the histogram demonstrate viewer choice enabled by the interactive dropdown?", "answer": "The Pair Plot module introduces interactivity where viewers can choose which variable pairs to compare. The histogram displays the distribution for the selected recipient_state variable, with CA showing the highest frequency around 55.", "doc_id": "1805.03677", "figure_id": "1805.03677_fig_5", "figure_number": 4, "image_path": "data/mineru_output/1805.03677/1805.03677/hybrid_auto/images/1805.03677_page0_fig4.jpg", "caption": "Figure 4.​ Prototype Label demonstrating the Pair Plot module and highlighting the interactive dropdown menus for selecting variables.", "figure_type": "plot", "visual_anchor": "Histogram in bottom right panel showing CA bar reaching approximately 55", "text_evidence": "The Pair Plot module (Figure 4​) starts to introduce interactivity into the label where the viewer is able to choose the variable pair being compared to one another.", "query_type": "comparison_explanation"}
{"query_id": "l1_1805.03677_1805.03677_fig_5_426", "query": "What processing approach supports the grayscale heatmap intensity scale ranging from 0 to 30 for state-product correlations?", "answer": "Browser-based processing handles linear correlations and distribution information for datasets under 100K rows, enabling the heatmap visualization. More sophisticated analyses requiring additional computational power would utilize server-side processing with multiple backends.", "doc_id": "1805.03677", "figure_id": "1805.03677_fig_5", "figure_number": 4, "image_path": "data/mineru_output/1805.03677/1805.03677/hybrid_auto/images/1805.03677_page0_fig4.jpg", "caption": "Figure 4.​ Prototype Label demonstrating the Pair Plot module and highlighting the interactive dropdown menus for selecting variables.", "figure_type": "plot", "visual_anchor": "Grayscale intensity scale on right side of heatmap ranging from 0 to 30", "text_evidence": "Simple statistical analyses involving the generation of histograms, distribution information, and linear correlations are carried out directly in the browser, given tabular datasets of ${ < } 1 0 0 \\mathrm { K }$ rows. Server-side processing is thus reserved for more specialized and sophisticated analyses requiring additional computational power. Such processing could run multiple backends", "query_type": "visual_definition"}
{"query_id": "l1_1805.03677_1805.03677_fig_6_427", "query": "Does the CT bar reaching 0.1 represent synthetic data generated by the BayesDB backend?", "answer": "Yes, the Probabilistic Model module attempts to generate synthetic data by utilizing the BayesDB backend, and the CT bar at 0.1 represents the highest probability in this generated distribution for Eliquis payments.", "doc_id": "1805.03677", "figure_id": "1805.03677_fig_6", "figure_number": 5, "image_path": "data/mineru_output/1805.03677/1805.03677/hybrid_auto/images/1805.03677_page0_fig5.jpg", "caption": "Figure 5.​ Prototype Label demonstrating the Probabilistic Model module and showcasing a hypothetical distribution for payments made towards the drug \"Eliquis\" across different states.", "figure_type": "plot", "visual_anchor": "CT bar at approximately 0.1, tallest bar in the chart", "text_evidence": "While all modules thus far investigate the dataset itself, the Probabilistic Model module (Figure 5​) attempts to generate synthetic data by utilizing the aforementioned BayesDB backend.", "query_type": "value_context"}
{"query_id": "l1_1805.03677_1805.03677_fig_6_428", "query": "Why does the bar height decrease from CT at 0.1 to SD near zero in this hypothetical distribution?", "answer": "The descending pattern represents a hypothetical probability distribution for Eliquis payments across states, with CT having the highest probability and states like SD having minimal probability in the synthetic data model.", "doc_id": "1805.03677", "figure_id": "1805.03677_fig_6", "figure_number": 5, "image_path": "data/mineru_output/1805.03677/1805.03677/hybrid_auto/images/1805.03677_page0_fig5.jpg", "caption": "Figure 5.​ Prototype Label demonstrating the Probabilistic Model module and showcasing a hypothetical distribution for payments made towards the drug \"Eliquis\" across different states.", "figure_type": "plot", "visual_anchor": "Descending bar heights from CT (0.1) to SD (near 0)", "text_evidence": "Prototype Label demonstrating the Probabilistic Model module and showcasing a hypothetical distribution for payments made towards the drug 'Eliquis' across different states.", "query_type": "comparison_explanation"}
{"query_id": "l1_1805.03677_1805.03677_fig_6_429", "query": "How does the dropdown selection of eliquis determine which drug's payment distribution appears in the bars?", "answer": "The dropdown menu allows users to select which drug to analyze, with 'eliquis' currently selected, causing the bars to display the hypothetical distribution specifically for Eliquis payments across different states.", "doc_id": "1805.03677", "figure_id": "1805.03677_fig_6", "figure_number": 5, "image_path": "data/mineru_output/1805.03677/1805.03677/hybrid_auto/images/1805.03677_page0_fig5.jpg", "caption": "Figure 5.​ Prototype Label demonstrating the Probabilistic Model module and showcasing a hypothetical distribution for payments made towards the drug \"Eliquis\" across different states.", "figure_type": "plot", "visual_anchor": "Dropdown menu displaying 'eliquis' above the bar chart", "text_evidence": "showcasing a hypothetical distribution for payments made towards the drug 'Eliquis' across different states", "query_type": "visual_definition"}
{"query_id": "l1_1805.03677_1805.03677_fig_7_430", "query": "Does the darkest cell in the spend_per_person heatmap at Rural_fraction warrant further analysis as suggested for likely relationships?", "answer": "Yes, the Ground Truth Correlation module provides initial evidence of a strong positive correlation between spend_per_person and Rural_fraction, indicating this relationship is likely and warrants further analysis.", "doc_id": "1805.03677", "figure_id": "1805.03677_fig_7", "figure_number": 6, "image_path": "data/mineru_output/1805.03677/1805.03677/hybrid_auto/images/1805.03677_page0_fig6.jpg", "caption": "Figure 6.​ The negative (top) and positive (bottom) correlations to demographics produced by the Ground Truth Correlations module.", "figure_type": "plot", "visual_anchor": "darkest cell (correlation value ~1) in bottom heatmap at Rural_fraction column", "text_evidence": "The Ground Truth Correlation module (Figure 6​) provides the data specialist initial evidence as to whether such relationships are likely, thus warranting further analysis.", "query_type": "value_context"}
{"query_id": "l1_1805.03677_1805.03677_fig_7_431", "query": "Why does the module show mostly light gray cells for total_amount_of_payment_usdollars when it prompts critical questions during preprocessing?", "answer": "The light gray cells indicate weak or near-zero correlations between total_amount_of_payment_usdollars and demographic variables, prompting critical questions about whether meaningful demographic relationships exist during the preprocessing phase.", "doc_id": "1805.03677", "figure_id": "1805.03677_fig_7", "figure_number": 6, "image_path": "data/mineru_output/1805.03677/1805.03677/hybrid_auto/images/1805.03677_page0_fig6.jpg", "caption": "Figure 6.​ The negative (top) and positive (bottom) correlations to demographics produced by the Ground Truth Correlations module.", "figure_type": "plot", "visual_anchor": "light gray cells in top heatmap for total_amount_of_payment_usdollars row", "text_evidence": "Overall, it prompts critical questions and interrogation in the preprocessing phase of model development.", "query_type": "comparison_explanation"}
{"query_id": "l1_1805.03677_1805.03677_fig_7_432", "query": "How does visualizing both positive and negative correlation heatmaps expedite decision making without sacrificing quality in model development?", "answer": "The dual heatmap visualization allows data specialists to quickly identify strong correlations (dark cells) versus weak ones (light cells) across all demographic variables simultaneously, enabling faster pattern recognition while maintaining thorough data interrogation.", "doc_id": "1805.03677", "figure_id": "1805.03677_fig_7", "figure_number": 6, "image_path": "data/mineru_output/1805.03677/1805.03677/hybrid_auto/images/1805.03677_page0_fig6.jpg", "caption": "Figure 6.​ The negative (top) and positive (bottom) correlations to demographics produced by the Ground Truth Correlations module.", "figure_type": "plot", "visual_anchor": "two separate heatmaps with grayscale gradients showing correlation strength", "text_evidence": "It also expedites decision making, which saves time in the overall model development phase without sacrificing the quality or thoroughness of the data interrogation itself", "query_type": "comparison_explanation"}
{"query_id": "l1_1805.05859_1805.05859_fig_1_433", "query": "Does the arrow from A to Y in the graph represent a structural equation that gets overridden during intervention?", "answer": "Yes, a perfect intervention on variable A at value v corresponds to overriding the structural equation f_i with V_i = v, which would affect the arrow from A to Y in the causal graph.", "doc_id": "1805.05859", "figure_id": "1805.05859_fig_1", "figure_number": 1, "image_path": "data/mineru_output/1805.05859/1805.05859/hybrid_auto/images/1805.05859_page0_fig0.jpg", "caption": "Figure 1: (a) A causal graph for three observed variables $A , Y , Z$ . (b) A joint representation with explicit background variables, and two counterfactual alternatives where $A$ is intervened at two different levels. (c) Similar to (b), where the interventions take place on $Y$ .", "figure_type": "diagram", "visual_anchor": "Arrow from node A to node Y", "text_evidence": "This notion says that a perfect intervention on a variable V_i, at value v, corresponds to overriding f_i(·) with the equation V_i = v.", "query_type": "value_context"}
{"query_id": "l1_1805.05859_1805.05859_fig_1_434", "query": "Why does node Z receive arrows from both U_Z and A if interventions modify remaining variable distributions?", "answer": "After intervening on A, the joint distribution of remaining variables V\\i (including Z) is determined by the causal model. Z retains its incoming arrow from U_Z while A's influence is overridden by the intervention value.", "doc_id": "1805.05859", "figure_id": "1805.05859_fig_1", "figure_number": 1, "image_path": "data/mineru_output/1805.05859/1805.05859/hybrid_auto/images/1805.05859_page0_fig0.jpg", "caption": "Figure 1: (a) A causal graph for three observed variables $A , Y , Z$ . (b) A joint representation with explicit background variables, and two counterfactual alternatives where $A$ is intervened at two different levels. (c) Similar to (b), where the interventions take place on $Y$ .", "figure_type": "diagram", "visual_anchor": "Node Z with incoming arrows from U_Z and A", "text_evidence": "Once this is done, the joint distribution of the remaining variables V_{\\i} ≡ V \\ {V_i} is given by the causal model.", "query_type": "comparison_explanation"}
{"query_id": "l1_1805.05859_1805.05859_fig_1_435", "query": "Can the structural equation represented by the arrow from U_A to A be tested for correctness observationally?", "answer": "No, unless structural equations depend on observed variables only, they cannot be tested for correctness without imposing other untestable assumptions. Since U_A is an unobserved background variable, this equation cannot be verified.", "doc_id": "1805.05859", "figure_id": "1805.05859_fig_1", "figure_number": 1, "image_path": "data/mineru_output/1805.05859/1805.05859/hybrid_auto/images/1805.05859_page0_fig0.jpg", "caption": "Figure 1: (a) A causal graph for three observed variables $A , Y , Z$ . (b) A joint representation with explicit background variables, and two counterfactual alternatives where $A$ is intervened at two different levels. (c) Similar to (b), where the interventions take place on $Y$ .", "figure_type": "diagram", "visual_anchor": "Arrow from U_A to node A", "text_evidence": "Unless structural equations depend on observed variables only, they cannot be tested for correctness (unless other untestable assumptions are imposed).", "query_type": "value_context"}
{"query_id": "l1_1805.05859_1805.05859_fig_4_436", "query": "Why do only some edges from A carry counterfactual values when the goal is minimizing untestable assumptions?", "answer": "When minimizing untestable assumptions, explicit structural equations are avoided but directed edges and independence constraints among counterfactuals are allowed. This selective propagation prevents violations of path-specific counterfactual fairness constraints.", "doc_id": "1805.05859", "figure_id": "1805.05859_fig_4", "figure_number": 2, "image_path": "data/mineru_output/1805.05859/1805.05859/hybrid_auto/images/1805.05859_page0_fig3.jpg", "caption": "Figure 2: (a) A causal graph linking protected attribute $A$ to predictor $\\hat { Y }$ , where only a subset of edges will “carry” counterfactual values of $A$ in order to represent the constraints of path-specific counterfactual fairness. (b) This diagram, inspired by [30], is a representation of how counterfactuals are propagated only through some edges. For other edges, inputs are based on the baseline value $a$ of an individual.", "figure_type": "diagram", "visual_anchor": "Subset of directed edges from node A to X1, X2, and through to Ŷ", "text_evidence": "When the goal is to minimise the number of untestable assumptions, one direction is to avoid making any explicit assumptions about the structural equations of the causal model. In this line of work, assumptions about the directed edges in the causal graph and a parametric formulation for the observed distributions are allowed, as well as independence constraints among counterfactuals.", "query_type": "comparison_explanation"}
{"query_id": "l1_1805.05859_1805.05859_fig_4_437", "query": "How does the edge from X1 to Ŷ relate to the constraint preventing predictor dependence on certain counterfactuals?", "answer": "The edge from X1 to Ŷ must respect constraints where inputs to Ŷ depend only on factuals that won't violate path-specific fairness. For instance, setting Ŷ ≡ X1 would violate the constraint because it would create improper counterfactual dependencies.", "doc_id": "1805.05859", "figure_id": "1805.05859_fig_4", "figure_number": 2, "image_path": "data/mineru_output/1805.05859/1805.05859/hybrid_auto/images/1805.05859_page0_fig3.jpg", "caption": "Figure 2: (a) A causal graph linking protected attribute $A$ to predictor $\\hat { Y }$ , where only a subset of edges will “carry” counterfactual values of $A$ in order to represent the constraints of path-specific counterfactual fairness. (b) This diagram, inspired by [30], is a representation of how counterfactuals are propagated only through some edges. For other edges, inputs are based on the baseline value $a$ of an individual.", "figure_type": "diagram", "visual_anchor": "Direct arrow from node X1 to node Ŷ", "text_evidence": "To enforce the constraint in (9), the input to $\\hat { Y }$ can be made to depend only on the factuals which will not violate the above. For instance, $\\hat { Y } \\equiv X _ { 1 }$ will violate the above, as $\\dot { Y } ( a , X _ { 1 } ( a , X _ { 2 } ( a ) ) , X _ { 2 } ( a ) ) = X _ { 1 } ( a , X _ { 2 } ( a ) ) = x _ { 1 }$ while", "query_type": "value_context"}
{"query_id": "l1_1805.05859_1805.05859_fig_4_438", "query": "What role do the three variables A, X1, X2 play in demonstrating path-specific counterfactual propagation to predictor Ŷ?", "answer": "The three variables A, X1, X2 form a causal structure that illustrates how counterfactual values of protected attribute A are selectively propagated through specific paths to the predictor Ŷ, representing path-specific counterfactual fairness constraints.", "doc_id": "1805.05859", "figure_id": "1805.05859_fig_4", "figure_number": 2, "image_path": "data/mineru_output/1805.05859/1805.05859/hybrid_auto/images/1805.05859_page0_fig3.jpg", "caption": "Figure 2: (a) A causal graph linking protected attribute $A$ to predictor $\\hat { Y }$ , where only a subset of edges will “carry” counterfactual values of $A$ in order to represent the constraints of path-specific counterfactual fairness. (b) This diagram, inspired by [30], is a representation of how counterfactuals are propagated only through some edges. For other edges, inputs are based on the baseline value $a$ of an individual.", "figure_type": "diagram", "visual_anchor": "Nodes A, X1, X2 connected to Ŷ through multiple directed paths", "text_evidence": "Figure 2(a) shows an example with three variables $A , X _ { 1 } , X _ { 2 }$ and the predictor $\\hat { Y }$ .", "query_type": "visual_definition"}
{"query_id": "l1_1805.09458_1805.09458_fig_1_439", "query": "Does the VFAE salmon bar at 0.75 adversarial loss indicate the encoder-decoder pair uses Gaussian reparameterization?", "answer": "Yes, VFAE's modified loss (Eq. 18) requires learning an encoder-decoder pair q(z|x) and p(x|z,c) where the encoder q(z|x) uses the Gaussian reparameterization trick, as shown by the 0-layer result.", "doc_id": "1805.09458", "figure_id": "1805.09458_fig_1", "figure_number": 1, "image_path": "data/mineru_output/1805.09458/1805.09458/hybrid_auto/images/1805.09458_page0_fig0.jpg", "caption": "Figure 1: On the left we display the adversarial loss (the accuracy of the adversary on $c$ ) and predictive accurracy on $y$ for three methods, plus the majority-class baseline, on both Adult and German datasets. For adv. loss lower is better, while for pred. acc. higher is better. On the right we plot adversarial loss by varying adversarial strength (indicated by color), parameterized by the number of layers from zero (logistic regression) to three. All evaluations are performed on the hold-out test sets.", "figure_type": "plot", "visual_anchor": "VFAE group salmon/red bar at approximately 0.75 adversarial loss", "text_evidence": "We have two modified VAE loss (Eq. 18) and modified VIB loss (Eq. 31). In both we have to learn an encoder and decoder pair $q ( z | x )$ and $p ( x | z , c )$ . We use feed forward networks to approximate these functions. For $q ( z | x )$ we use the Gaussian reparameterization trick", "query_type": "value_context"}
{"query_id": "l1_1805.09458_1805.09458_fig_1_440", "query": "Why does Xie's olive bar reach 0.95 while VFAE remains at 0.90, given both methods have comparable predictive accuracy?", "answer": "The methods are roughly equivalent in performance. All methods achieve comparable predictive accuracy despite differences in adversarial loss, suggesting that higher adversarial loss does not necessarily compromise prediction quality.", "doc_id": "1805.09458", "figure_id": "1805.09458_fig_1", "figure_number": 1, "image_path": "data/mineru_output/1805.09458/1805.09458/hybrid_auto/images/1805.09458_page0_fig0.jpg", "caption": "Figure 1: On the left we display the adversarial loss (the accuracy of the adversary on $c$ ) and predictive accurracy on $y$ for three methods, plus the majority-class baseline, on both Adult and German datasets. For adv. loss lower is better, while for pred. acc. higher is better. On the right we plot adversarial loss by varying adversarial strength (indicated by color), parameterized by the number of layers from zero (logistic regression) to three. All evaluations are performed on the hold-out test sets.", "figure_type": "plot", "visual_anchor": "Xie olive bar at ~0.95 compared to VFAE olive bar at ~0.90", "text_evidence": "For the German dataset shown on top table of Figure 1, the methods are roughly equivalent. All methods have comparable predictive accuracy, while the VFAE and the proposed method have", "query_type": "comparison_explanation"}
{"query_id": "l1_1805.09458_1805.09458_fig_1_441", "query": "Does the Proposed method's purple bar at 0.78 reflect the predictor branch concatenating c onto z as features?", "answer": "Yes, the Proposed method's modified VIB includes a predictor branch p(y|z) implemented with feed forward networks. The decoder p(x|z,c) concatenates c onto z as extra input features, contributing to the adversarial loss shown.", "doc_id": "1805.09458", "figure_id": "1805.09458_fig_1", "figure_number": 1, "image_path": "data/mineru_output/1805.09458/1805.09458/hybrid_auto/images/1805.09458_page0_fig0.jpg", "caption": "Figure 1: On the left we display the adversarial loss (the accuracy of the adversary on $c$ ) and predictive accurracy on $y$ for three methods, plus the majority-class baseline, on both Adult and German datasets. For adv. loss lower is better, while for pred. acc. higher is better. On the right we plot adversarial loss by varying adversarial strength (indicated by color), parameterized by the number of layers from zero (logistic regression) to three. All evaluations are performed on the hold-out test sets.", "figure_type": "plot", "visual_anchor": "Proposed group purple bar (3 layers) at approximately 0.78 adversarial loss", "text_evidence": "In the modified VIB we also have a predictor branch $p ( y | z )$ , which we also use a feed forward network to p", "query_type": "value_context"}
{"query_id": "l1_1805.09458_1805.09458_fig_2_442", "query": "Why do the red and cyan points overlap extensively in VFAE Baseline despite covariate class separation goals?", "answer": "The VFAE Baseline fails to effectively remove covariate information from the latent encodings, resulting in poor separation between the majority class (red) and minority class (cyan) in the t-SNE visualization.", "doc_id": "1805.09458", "figure_id": "1805.09458_fig_2", "figure_number": 2, "image_path": "data/mineru_output/1805.09458/1805.09458/hybrid_auto/images/1805.09458_page0_fig1.jpg", "caption": "Figure 2: t-SNE plots for the latent encodings of (Left to Right) the VFAE, Xie et al., and our proposed method on the Adult dataset (first 1000 pts., test split). The value of the $c$ variable is provided as color, where red is the majority class.", "figure_type": "plot", "visual_anchor": "overlapping red and cyan point clusters throughout the plot space", "text_evidence": "t-SNE plots for the latent encodings of (Left to Right) the VFAE, Xie et al., and our proposed method on the Adult dataset (first 1000 pts., test split). The value of the $c$ variable is provided as color, where red is the majority class.", "query_type": "anomaly_cause"}
{"query_id": "l1_1805.09458_1805.09458_fig_2_443", "query": "How does the VFAE Baseline red majority class distribution relate to pushing stylistic information into latent space?", "answer": "The VFAE Baseline shows poor separation of the red majority class from other points, suggesting it fails to effectively push non-class stylistic information into the latent space while removing covariate information as intended.", "doc_id": "1805.09458", "figure_id": "1805.09458_fig_2", "figure_number": 2, "image_path": "data/mineru_output/1805.09458/1805.09458/hybrid_auto/images/1805.09458_page0_fig1.jpg", "caption": "Figure 2: t-SNE plots for the latent encodings of (Left to Right) the VFAE, Xie et al., and our proposed method on the Adult dataset (first 1000 pts., test split). The value of the $c$ variable is provided as color, where red is the majority class.", "figure_type": "plot", "visual_anchor": "red points (majority class) mixed throughout the scatter plot", "text_evidence": "which pushes all non-class stylistic information into the latent space while attempting to remove information about the exact digit being written", "query_type": "comparison_explanation"}
{"query_id": "l1_1805.09458_1805.09458_fig_2_444", "query": "Does the cyan triangular cluster distribution at X1=50 support successful unsupervised manipulation of the decoder?", "answer": "No, the cyan points' scattered distribution, including the cluster at X1=50, indicates poor latent separation in VFAE Baseline, which would hinder the decoder's ability to manipulate outputs based on style as demonstrated in the unsupervised learning approach.", "doc_id": "1805.09458", "figure_id": "1805.09458_fig_2", "figure_number": 2, "image_path": "data/mineru_output/1805.09458/1805.09458/hybrid_auto/images/1805.09458_page0_fig1.jpg", "caption": "Figure 2: t-SNE plots for the latent encodings of (Left to Right) the VFAE, Xie et al., and our proposed method on the Adult dataset (first 1000 pts., test split). The value of the $c$ variable is provided as color, where red is the majority class.", "figure_type": "plot", "visual_anchor": "cyan triangular points scattered across plot, including cluster near X1=50", "text_evidence": "We demonstrate a form of unsupervised image manipulation inspired by Fader Networks [14] on the MNIST dataset. We use the digit label as the covariate class $c$ , which pushes all non-class stylistic information into the latent space", "query_type": "value_context"}
{"query_id": "l1_1805.09458_1805.09458_fig_5_445", "query": "Why do the generated digits in row 6 maintain the bold stroke style of the real digit 0 in that row?", "answer": "The encoder maps the real digit into latent space z that is invariant to digit label but captures stylistic information like stroke boldness. The decoder then generates different digits (0-9) while preserving the extracted bold stroke style from the original digit.", "doc_id": "1805.09458", "figure_id": "1805.09458_fig_5", "figure_number": 3, "image_path": "data/mineru_output/1805.09458/1805.09458/hybrid_auto/images/1805.09458_page0_fig4.jpg", "caption": "Figure 3: We demonstrate the ability to generate stylistically similar images of varying classes using the MNIST dataset. The left column is mapped into $z$ that is invariant to its digit label c. We then can generate an image using $z$ and any other specified digit, $c ^ { \\prime }$ , as show on the right.", "figure_type": "example", "visual_anchor": "Bold row 6 in the figure with thick strokes in both real (left) and generated (right) digits", "text_evidence": "The left column is mapped into z that is invariant to its digit label c. We then can generate an image using z and any other specified digit, c′, as show on the right.", "query_type": "comparison_explanation"}
{"query_id": "l1_1805.09458_1805.09458_fig_5_446", "query": "How does using digit label as covariate enable the leftmost real 9 to generate all ten digits rightward?", "answer": "Using digit label as covariate class c pushes all non-class stylistic information into latent space while removing information about the exact digit. This separation allows the decoder to recombine the extracted style with any specified digit label c′ to produce all ten digits.", "doc_id": "1805.09458", "figure_id": "1805.09458_fig_5", "figure_number": 3, "image_path": "data/mineru_output/1805.09458/1805.09458/hybrid_auto/images/1805.09458_page0_fig4.jpg", "caption": "Figure 3: We demonstrate the ability to generate stylistically similar images of varying classes using the MNIST dataset. The left column is mapped into $z$ that is invariant to its digit label c. We then can generate an image using $z$ and any other specified digit, $c ^ { \\prime }$ , as show on the right.", "figure_type": "example", "visual_anchor": "Top row showing real digit 9 on left and generated sequence 0123456789 on right", "text_evidence": "We use the digit label as the covariate class c, which pushes all non-class stylistic information into the latent space while attempting to remove information about the exact digit being written.", "query_type": "visual_definition"}
{"query_id": "l1_1805.09458_1805.09458_fig_5_447", "query": "What allows the real digit 1 in the bottom row to produce generated digits with its slanted handwriting style?", "answer": "The system performs unsupervised image manipulation by separating style from class information. The encoder captures the slanted writing style in z while being invariant to the digit identity, enabling the decoder to apply this style to any target digit during generation.", "doc_id": "1805.09458", "figure_id": "1805.09458_fig_5", "figure_number": 3, "image_path": "data/mineru_output/1805.09458/1805.09458/hybrid_auto/images/1805.09458_page0_fig4.jpg", "caption": "Figure 3: We demonstrate the ability to generate stylistically similar images of varying classes using the MNIST dataset. The left column is mapped into $z$ that is invariant to its digit label c. We then can generate an image using $z$ and any other specified digit, $c ^ { \\prime }$ , as show on the right.", "figure_type": "example", "visual_anchor": "Bottom row showing real slanted digit 1 on left and corresponding slanted generated digits on right", "text_evidence": "We demonstrate a form of unsupervised image manipulation inspired by Fader Networks [14] on the MNIST dataset.", "query_type": "comparison_explanation"}
{"query_id": "l1_1805.11202_1805.11202_fig_1_448", "query": "Why does the discriminator receive input from both the real dataset box and generator output G(z)?", "answer": "The discriminator must distinguish between real data (labeled 1) and generated synthetic data (labeled 0) to train the GAN adversarially, as it evaluates whether samples come from the real distribution P_data or the generator.", "doc_id": "1805.11202", "figure_id": "1805.11202_fig_1", "figure_number": 1, "image_path": "data/mineru_output/1805.11202/1805.11202/hybrid_auto/images/1805.11202_page0_fig0.jpg", "caption": "Figure 1: Illustration of generative adversarial networks", "figure_type": "diagram", "visual_anchor": "D Discriminator box with two input arrows and output labels 0: G(z) and 1: x", "text_evidence": "In fairness-aware learning, the literature has studied notions of group fairness on dataset and classification", "query_type": "comparison_explanation"}
{"query_id": "l1_1805.11202_1805.11202_fig_1_449", "query": "Does the P_Z noise input feeding into generator G relate to the synthetic datasets SYN1-GAN through SYN4-FairGAN?", "answer": "Yes, the noise P_Z is the random input that the generator G transforms into synthetic data, which corresponds to the synthetic datasets like SYN1-GAN and SYN4-FairGAN that are generated and compared against real data in the table.", "doc_id": "1805.11202", "figure_id": "1805.11202_fig_1", "figure_number": 1, "image_path": "data/mineru_output/1805.11202/1805.11202/hybrid_auto/images/1805.11202_page0_fig0.jpg", "caption": "Figure 1: Illustration of generative adversarial networks", "figure_type": "diagram", "visual_anchor": "P_Z Noise box connected to G Generator box", "text_evidence": "Table 1: Risk differences of real and synthetic datasets with Real Data, SYN1-GAN, SYN2-NFGANI, SYN3-NFGANII, SYN4-FairGAN columns showing disk(D) values", "query_type": "value_context"}
{"query_id": "l1_1805.11202_1805.11202_fig_1_450", "query": "How does the discriminator's output label 1: x connect to the statistical parity fairness definition?", "answer": "The label 1: x indicates real data from P_data, which is the dataset D where statistical parity is evaluated through the risk difference disc(D) = P(y=1|s=1) - P(y=1|s=0) to measure discrimination with respect to protected attributes.", "doc_id": "1805.11202", "figure_id": "1805.11202_fig_1", "figure_number": 1, "image_path": "data/mineru_output/1805.11202/1805.11202/hybrid_auto/images/1805.11202_page0_fig0.jpg", "caption": "Figure 1: Illustration of generative adversarial networks", "figure_type": "diagram", "visual_anchor": "Output label 1: x from discriminator", "text_evidence": "The discrimination in a labeled dataset w.r.t the protected attribute s is evaluated by the risk difference: disc(D) = P(y = 1|s = 1) - P(y = 1|s = 0)", "query_type": "visual_definition"}
{"query_id": "l1_1805.11202_1805.11202_fig_2_451", "query": "Why does discriminator D1 receive both P_data and P_G inputs when its objective function uses D(·) probability?", "answer": "D1 must distinguish real data from generated fake data by outputting probabilities, so it receives both P_data(x,y|s) and P_G(x,y|s) to learn this discrimination. The objective function D(·) outputs the probability that input is from real data rather than generated fake data.", "doc_id": "1805.11202", "figure_id": "1805.11202_fig_2", "figure_number": 2, "image_path": "data/mineru_output/1805.11202/1805.11202/hybrid_auto/images/1805.11202_page0_fig1.jpg", "caption": "Figure 2: The Structure of FairGAN", "figure_type": "architecture", "visual_anchor": "D1 discriminator box with two input arrows from P_data(x,y|s) and P_G(x,y|s)", "text_evidence": "where $D ( \\cdot )$ outputs the probability that $\\cdot$ is from the real data rather Dthan the generated fake data", "query_type": "comparison_explanation"}
{"query_id": "l1_1805.11202_1805.11202_fig_2_452", "query": "How does the Generator G_Dec fooling discriminators relate to minimizing Equation 2 for the objective function of G?", "answer": "The generator G_Dec is trained to fool the discriminator by making generated data indistinguishable from real data. Minimization of Equation 2 ensures that the discriminator is fooled by G(z), achieving the objective function of G.", "doc_id": "1805.11202", "figure_id": "1805.11202_fig_2", "figure_number": 2, "image_path": "data/mineru_output/1805.11202/1805.11202/hybrid_auto/images/1805.11202_page0_fig1.jpg", "caption": "Figure 2: The Structure of FairGAN", "figure_type": "architecture", "visual_anchor": "G_Dec Generator box with arrows pointing upward to both discriminators D1 and D2", "text_evidence": "In order to make the generative distribution $P _ { G }$ close to the real data distribution $P _ { \\mathrm { d a t a } }$ , $G$ is trained by PG P Gfooling the discriminator unable to distinguish the generated data from the real data. Thus, the objective function of $G$ is defined as: Minimization of Equation 2 ensures that the discriminator is fooled by $G ( \\mathbf { z } )$ an", "query_type": "value_context"}
{"query_id": "l1_1805.11202_1805.11202_fig_2_453", "query": "Why does the structure show G_Dec receiving hidden representations when x'=Dec(Enc(x)) captures salient input information?", "answer": "The generator G_Dec consists of a generator G that produces salient representations and a decoder Dec that reconstructs data. Because the hidden representation x'=Dec(Enc(x)) can reconstruct the original input, it captures the salient information needed for generation.", "doc_id": "1805.11202", "figure_id": "1805.11202_fig_2", "figure_number": 2, "image_path": "data/mineru_output/1805.11202/1805.11202/hybrid_auto/images/1805.11202_page0_fig1.jpg", "caption": "Figure 2: The Structure of FairGAN", "figure_type": "architecture", "visual_anchor": "G_Dec Generator box at bottom center receiving inputs from Ps and Pz", "text_evidence": "where $\\mathbf { x } ^ { \\prime } = D e c ( E n c ( \\mathbf { x } ) )$ . Because the hidden representation can Dec Encbe used to reconstruct the original input, it captures the salient information of the input. To generate the dataset which contains discrete attributes, the generator $G _ { D e c }$ in medGAN consists of two components, the generator $G$ GDecand the decoder", "query_type": "visual_definition"}
{"query_id": "l1_1805.11202_1805.11202_fig_5_454", "query": "Why does the black curve reach approximately 0.10 at x≈3.5 while both conditional distributions peak below 0.05?", "answer": "The black curve represents the overall data distribution P_data(x), which combines both conditional distributions P_data(x|s=1) and P_data(x|s=0), resulting in higher probability density than either individual conditional distribution alone.", "doc_id": "1805.11202", "figure_id": "1805.11202_fig_5", "figure_number": 3, "image_path": "data/mineru_output/1805.11202/1805.11202/hybrid_auto/images/1805.11202_page0_fig4.jpg", "caption": "Figure 3: Comparing FairGAN, NaïveFairGAN-I and NaïveFairGAN-II on a toy dataset. (a) shows the distributions $P _ { \\mathbf { d a t a } } ( x )$ (black), $P _ { \\bf d a t a } ( x | s = 1 )$ (green) and $P _ { \\bf d a t a } ( x | s = 0 )$ (red) of real data; (b), (c) and (d) are distributions $P _ { G } ( x ) , P _ { G } ( x | s = 1 )$ and $P _ { G } ( x | s = 0 )$ P x s P x s Pof synthetic datasets generated by NaïveFairGAN-I, NaïveFairGAN-II and FairGAN separately.", "figure_type": "plot", "visual_anchor": "Black solid curve peak at 0.10 versus green and red dashed curves peaking below 0.05", "text_evidence": "shows the distributions P_data(x) (black), P_data(x|s=1) (green) and P_data(x|s=0) (red) of real data", "query_type": "comparison_explanation"}
{"query_id": "l1_1805.11202_1805.11202_fig_5_455", "query": "Do the green and red conditional distributions exhibit similar bimodal patterns as required for evaluating FairGAN versus NaïveFairGAN variants?", "answer": "Yes, both the green (s=1) and red (s=0) conditional distributions show bimodal patterns with peaks around x=1 and x=3.5, mirroring the overall distribution structure. This toy dataset structure enables comparison of FairGAN, NaïveFairGAN-I and NaïveFairGAN-II performance.", "doc_id": "1805.11202", "figure_id": "1805.11202_fig_5", "figure_number": 3, "image_path": "data/mineru_output/1805.11202/1805.11202/hybrid_auto/images/1805.11202_page0_fig4.jpg", "caption": "Figure 3: Comparing FairGAN, NaïveFairGAN-I and NaïveFairGAN-II on a toy dataset. (a) shows the distributions $P _ { \\mathbf { d a t a } } ( x )$ (black), $P _ { \\bf d a t a } ( x | s = 1 )$ (green) and $P _ { \\bf d a t a } ( x | s = 0 )$ (red) of real data; (b), (c) and (d) are distributions $P _ { G } ( x ) , P _ { G } ( x | s = 1 )$ and $P _ { G } ( x | s = 0 )$ P x s P x s Pof synthetic datasets generated by NaïveFairGAN-I, NaïveFairGAN-II and FairGAN separately.", "figure_type": "plot", "visual_anchor": "Green and red dashed curves both showing two peaks at similar x-locations", "text_evidence": "Comparing FairGAN, NaïveFairGAN-I and NaïveFairGAN-II on a toy dataset", "query_type": "value_context"}
{"query_id": "l1_1805.11202_1805.11202_fig_5_456", "query": "How do the red and green curve amplitudes relate to the experimental setup testing classifiers on real versus synthetic datasets?", "answer": "The red (s=0) and green (s=1) curves show the real data's conditional distributions that FairGAN aims to replicate in synthetic data. The experimental setup evaluates whether classifiers trained on synthetic datasets (REAL2REAL setting) can match performance on these real conditional distributions.", "doc_id": "1805.11202", "figure_id": "1805.11202_fig_5", "figure_number": 3, "image_path": "data/mineru_output/1805.11202/1805.11202/hybrid_auto/images/1805.11202_page0_fig4.jpg", "caption": "Figure 3: Comparing FairGAN, NaïveFairGAN-I and NaïveFairGAN-II on a toy dataset. (a) shows the distributions $P _ { \\mathbf { d a t a } } ( x )$ (black), $P _ { \\bf d a t a } ( x | s = 1 )$ (green) and $P _ { \\bf d a t a } ( x | s = 0 )$ (red) of real data; (b), (c) and (d) are distributions $P _ { G } ( x ) , P _ { G } ( x | s = 1 )$ and $P _ { G } ( x | s = 0 )$ P x s P x s Pof synthetic datasets generated by NaïveFairGAN-I, NaïveFairGAN-II and FairGAN separately.", "figure_type": "plot", "visual_anchor": "Red and green dashed curves with amplitudes around 0.02-0.05", "text_evidence": "We evaluate the classifiers with three settings: 1) the classifiers are trained and tested on the real dataset, called REAL2REAL; 2) the classifiers are trained and tested on the synthetic datasets", "query_type": "comparison_explanation"}
{"query_id": "l1_1805.11202_1805.11202_fig_10_457", "query": "Do the dots clustering along the diagonal line indicate that FairGAN achieves identical conditional probability distributions for fairness?", "answer": "Yes, the dots closely follow the diagonal line, which represents the ideal fairness where data have identical conditional probability distributions given different sensitive attribute values s.", "doc_id": "1805.11202", "figure_id": "1805.11202_fig_10", "figure_number": 4, "image_path": "data/mineru_output/1805.11202/1805.11202/hybrid_auto/images/1805.11202_page0_fig9.jpg", "caption": "Figure 4: Dimension-wise conditional probability distributions $P ( \\mathbf { x } , y | s = 1 )$ vs. $P ( \\mathbf { x } , y | s = 0 )$ ). Each dot represents one attribute. The $\\mathbf { x }$ -axis represents the conditional probability given $s = 1$ P ,y s P ,y s. The y-axis represents the conditional probability given $s = 0$ . sThe diagonal line indicates the ideal fairness, where data have identical conditional probability distributions given .", "figure_type": "plot", "visual_anchor": "diagonal line and dots clustered along it", "text_evidence": "The diagonal line indicates the ideal fairness, where data have identical conditional probability distributions given .", "query_type": "visual_definition"}
{"query_id": "l1_1805.11202_1805.11202_fig_10_458", "query": "Why does each dot in the scatter plot represent one attribute when evaluating FairGAN on fair data generation?", "answer": "Each dot represents the dimension-wise conditional probability for one attribute, allowing comparison between P(x,y|s=1) and P(x,y|s=0) to assess whether FairGAN generates fair data.", "doc_id": "1805.11202", "figure_id": "1805.11202_fig_10", "figure_number": 4, "image_path": "data/mineru_output/1805.11202/1805.11202/hybrid_auto/images/1805.11202_page0_fig9.jpg", "caption": "Figure 4: Dimension-wise conditional probability distributions $P ( \\mathbf { x } , y | s = 1 )$ vs. $P ( \\mathbf { x } , y | s = 0 )$ ). Each dot represents one attribute. The $\\mathbf { x }$ -axis represents the conditional probability given $s = 1$ P ,y s P ,y s. The y-axis represents the conditional probability given $s = 0$ . sThe diagonal line indicates the ideal fairness, where data have identical conditional probability distributions given .", "figure_type": "plot", "visual_anchor": "individual dots in scatter plot", "text_evidence": "We evaluate FairGAN on data generation from two perspectives, fairness and utility. Fairness is to check whether FairGAN can generate fair data", "query_type": "value_context"}
{"query_id": "l1_1805.11202_1805.11202_fig_10_459", "query": "How does the scatter pattern along the diagonal support the claim that risk difference disc measures fairness performance?", "answer": "The dots aligned along the diagonal show that P(y=1|s=1) and P(y=1|s=0) are nearly equal, minimizing the risk difference disc(D) = P(y=1|s=1) - P(y=1|s=0), which is the fairness metric used.", "doc_id": "1805.11202", "figure_id": "1805.11202_fig_10", "figure_number": 4, "image_path": "data/mineru_output/1805.11202/1805.11202/hybrid_auto/images/1805.11202_page0_fig9.jpg", "caption": "Figure 4: Dimension-wise conditional probability distributions $P ( \\mathbf { x } , y | s = 1 )$ vs. $P ( \\mathbf { x } , y | s = 0 )$ ). Each dot represents one attribute. The $\\mathbf { x }$ -axis represents the conditional probability given $s = 1$ P ,y s P ,y s. The y-axis represents the conditional probability given $s = 0$ . sThe diagonal line indicates the ideal fairness, where data have identical conditional probability distributions given .", "figure_type": "plot", "visual_anchor": "dots aligned along diagonal line from (0,0) to (1,1)", "text_evidence": "We adopt the risk difference in a labeled dataset $( d i s c ( \\mathscr { D } ) =$ $P ( y = 1 | s = 1 ) - P ( y = 1 | s = 0 ) ,$ disc as the metric to compare the per-P y s P y sformance of different GAN models", "query_type": "comparison_explanation"}
{"query_id": "l1_1805.11202_1805.11202_fig_22_460", "query": "Does the clustering of dots along the diagonal line demonstrate the ideal case where real and synthetic data show identical quality?", "answer": "Yes, the diagonal line indicates the ideal case where real and synthetic datasets have identical quality. The close clustering of dots along this line suggests the synthetic data successfully replicates the probability distributions of the real dataset.", "doc_id": "1805.11202", "figure_id": "1805.11202_fig_22", "figure_number": 5, "image_path": "data/mineru_output/1805.11202/1805.11202/hybrid_auto/images/1805.11202_page0_fig21.jpg", "caption": "Figure 5: Dimension-wise probability distributions. Each dot represents one attribute. The $\\mathbf { x }$ -axis represents the Bernoulli success probability for the real dataset. The y-axis represents the probability for the synthetic dataset generated by each model. The diagonal line indicates the ideal case, where the real and synthetic data show identical quality.", "figure_type": "plot", "visual_anchor": "diagonal dashed line and dots clustered near it", "text_evidence": "The diagonal line indicates the ideal case, where the real and synthetic data show identical quality.", "query_type": "value_context"}
{"query_id": "l1_1805.11202_1805.11202_fig_22_461", "query": "How do the Bernoulli success probabilities on the x-axis relate to the evaluation of classifiers trained in REAL2REAL setting?", "answer": "The x-axis shows Bernoulli success probabilities from the real dataset, which is used both for training and testing in the REAL2REAL setting. This establishes the baseline probability distribution against which synthetic data generation quality is measured.", "doc_id": "1805.11202", "figure_id": "1805.11202_fig_22", "figure_number": 5, "image_path": "data/mineru_output/1805.11202/1805.11202/hybrid_auto/images/1805.11202_page0_fig21.jpg", "caption": "Figure 5: Dimension-wise probability distributions. Each dot represents one attribute. The $\\mathbf { x }$ -axis represents the Bernoulli success probability for the real dataset. The y-axis represents the probability for the synthetic dataset generated by each model. The diagonal line indicates the ideal case, where the real and synthetic data show identical quality.", "figure_type": "plot", "visual_anchor": "x-axis labeled P_real(x_d) with values from 0.0 to 1.0", "text_evidence": "We evaluate the classifiers with three settings: 1) the classifiers are trained and tested on the real dataset, called REAL2REAL", "query_type": "comparison_explanation"}
{"query_id": "l1_1805.11202_1805.11202_fig_22_462", "query": "Why must each dot's y-axis position match its x-axis value for fair classification training to succeed in subsection 5.3?", "answer": "For fair classification, the synthetic dataset must preserve the same attribute probability distributions as the real data. Each dot represents one attribute, and alignment along the diagonal ensures the synthetic probability (y-axis) matches the real probability (x-axis), maintaining fairness properties.", "doc_id": "1805.11202", "figure_id": "1805.11202_fig_22", "figure_number": 5, "image_path": "data/mineru_output/1805.11202/1805.11202/hybrid_auto/images/1805.11202_page0_fig21.jpg", "caption": "Figure 5: Dimension-wise probability distributions. Each dot represents one attribute. The $\\mathbf { x }$ -axis represents the Bernoulli success probability for the real dataset. The y-axis represents the probability for the synthetic dataset generated by each model. The diagonal line indicates the ideal case, where the real and synthetic data show identical quality.", "figure_type": "plot", "visual_anchor": "dots positioned along diagonal with matching x and y coordinates", "text_evidence": "In this subsection, we adopt the real and synthetic datasets to train several classifiers and check whether the classifiers can achieve fairness.", "query_type": "comparison_explanation"}
{"query_id": "l1_1805.11202_1805.11202_fig_25_463", "query": "Why does the blue risk difference curve decline from 0.18 to near 0.03 as lambda increases to 2.0?", "answer": "FairGAN is designed to generate fair data free from disparate treatment and disparate impact. Increasing lambda strengthens fairness constraints, reducing risk difference while maintaining data utility measured by Euclidean distance.", "doc_id": "1805.11202", "figure_id": "1805.11202_fig_25", "figure_number": 6, "image_path": "data/mineru_output/1805.11202/1805.11202/hybrid_auto/images/1805.11202_page0_fig24.jpg", "caption": "Figure 6: The sensitivity analysis of FairGAN with various", "figure_type": "plot", "visual_anchor": "blue line with square markers showing decline from 0.18 at lambda=0.0 to approximately 0.03 at lambda=2.0", "text_evidence": "we have developed FairGAN to generate fair data, which is free from disparate treatment and disparate impact in terms of the real protected attribute, while retaining high data utility", "query_type": "comparison_explanation"}
{"query_id": "l1_1805.11202_1805.11202_fig_25_464", "query": "Does the flat orange Euclidean distance curve near 0.02 confirm that data utility is retained during fairness optimization?", "answer": "Yes, the orange curve remains stable around 0.02 across all lambda values, demonstrating that FairGAN retains high data utility while the risk difference decreases, confirming the dual objective is achieved.", "doc_id": "1805.11202", "figure_id": "1805.11202_fig_25", "figure_number": 6, "image_path": "data/mineru_output/1805.11202/1805.11202/hybrid_auto/images/1805.11202_page0_fig24.jpg", "caption": "Figure 6: The sensitivity analysis of FairGAN with various", "figure_type": "plot", "visual_anchor": "orange line with circle markers remaining flat near 0.02 across lambda values from 0.0 to 2.0", "text_evidence": "We evaluate how the  in FairGAN affects the synthetic datasets λfor fair data generation and fair classification.", "query_type": "value_context"}
{"query_id": "l1_1805.11202_1805.11202_fig_25_465", "query": "What causes the large error bars on the blue curve at lambda values 0.6 to 1.8 compared to earlier points?", "answer": "The larger error bars at intermediate lambda values suggest increased variability in fairness metrics during training. This occurs as FairGAN balances between the generator and two discriminators, where training stability varies with different fairness constraint strengths.", "doc_id": "1805.11202", "figure_id": "1805.11202_fig_25", "figure_number": 6, "image_path": "data/mineru_output/1805.11202/1805.11202/hybrid_auto/images/1805.11202_page0_fig24.jpg", "caption": "Figure 6: The sensitivity analysis of FairGAN with various", "figure_type": "plot", "visual_anchor": "error bars on blue risk difference line, notably larger at lambda 0.6-1.8 compared to lambda 0.0-0.4", "text_evidence": "FairGAN consists of one generator and two discriminators. In particular, the generator generates fake samples conditioned on", "query_type": "anomaly_cause"}
{"query_id": "l1_1808.08166_1808.08166_fig_1_466", "query": "Do the brown-circled accepted individuals in the left Male quadrants support the claim about bridging statistical and individual fairness?", "answer": "Yes, the brown circles show selected individuals across both blue and green male subgroups, illustrating how rich subgroup fairness applies statistical constraints (equalizing false positive rates) to exponentially large collections of subgroups defined by bounded VC dimension classes.", "doc_id": "1808.08166", "figure_id": "1808.08166_fig_1", "figure_number": 1, "image_path": "data/mineru_output/1808.08166/1808.08166/hybrid_auto/images/1808.08166_page0_fig0.jpg", "caption": "Figure 1: Fairness Gerrymandering: A Toy Example [Kearns et al., 2018]", "figure_type": "diagram", "visual_anchor": "Brown circles marking accepted individuals in the left Male column, distributed across both Blue and Green rows", "text_evidence": "Rich subgroup fairness picks a statistical fairness constraint (say, equalizing false positive rates across protected groups), but then asks that this constraint hold over an exponentially or infinitely large collection of subgroups defined by a class of functions with bounded VC dimension.", "query_type": "value_context"}
{"query_id": "l1_1808.08166_1808.08166_fig_1_467", "query": "Why are more green dots than blue dots circled in the right Female quadrants, given the two sensitive attributes described?", "answer": "The asymmetric acceptance pattern across Female quadrants demonstrates the fairness gerrymandering problem, where individuals have two sensitive attributes (race as blue/green and gender as male/female), allowing for complex subgroup definitions that can manipulate fairness constraints.", "doc_id": "1808.08166", "figure_id": "1808.08166_fig_1", "figure_number": 1, "image_path": "data/mineru_output/1808.08166/1808.08166/hybrid_auto/images/1808.08166_page0_fig0.jpg", "caption": "Figure 1: Fairness Gerrymandering: A Toy Example [Kearns et al., 2018]", "figure_type": "diagram", "visual_anchor": "Female column showing four brown circles among green dots versus two circles among blue dots", "text_evidence": "Suppose individuals each have two sensitive attributes: race (say blue and green) and gender (say male and female).", "query_type": "comparison_explanation"}
{"query_id": "l1_1808.08166_1808.08166_fig_1_468", "query": "How do the circled blue individuals in the Male-Blue quadrant relate to the linear threshold function mentioned for this heuristic?", "answer": "The five brown-circled blue dots in the Male-Blue quadrant represent individuals selected by the linear threshold function heuristic, demonstrating how the algorithm applies decision boundaries to achieve rich subgroup fairness across the combinatorially defined subgroups.", "doc_id": "1808.08166", "figure_id": "1808.08166_fig_1", "figure_number": 1, "image_path": "data/mineru_output/1808.08166/1808.08166/hybrid_auto/images/1808.08166_page0_fig0.jpg", "caption": "Figure 1: Fairness Gerrymandering: A Toy Example [Kearns et al., 2018]", "figure_type": "diagram", "visual_anchor": "Five brown circles among blue dots in the top-left Male-Blue quadrant", "text_evidence": "The heuristic finds a linear threshold function as follows:", "query_type": "visual_definition"}
{"query_id": "l1_1808.08166_1808.08166_fig_3_469", "query": "Do the dashed horizontal lines representing varying input gamma values align with where the colored curves stabilize?", "answer": "Yes, the dashed horizontal lines correspond to the input gamma values where the colored curves converge, demonstrating that the algorithm achieves the specified fairness violation targets as iterations progress.", "doc_id": "1808.08166", "figure_id": "1808.08166_fig_3", "figure_number": 2, "image_path": "data/mineru_output/1808.08166/1808.08166/hybrid_auto/images/1808.08166_page0_fig2.jpg", "caption": "Figure 2: Error $\\varepsilon _ { t }$ and fairness violation $\\gamma _ { t }$ for Law School dataset (panels (a) and (b)) and Adult data set (panels (c) and (d)), for values of input $\\gamma$ ranging from 0 to 0.03. Dashed horizontal lines on $\\gamma _ { t }$ plots correspond to varying values of $\\gamma$ .", "figure_type": "plot", "visual_anchor": "Dashed horizontal lines at multiple y-values between 0 and 0.03", "text_evidence": "Dashed horizontal lines on $\\gamma _ { t }$ plots correspond to varying values of $\\gamma$", "query_type": "comparison_explanation"}
{"query_id": "l1_1808.08166_1808.08166_fig_3_470", "query": "Why do most colored curves show rapid descent from high gamma values before iteration 2000 for input gamma ranging 0 to 0.03?", "answer": "The rapid initial descent demonstrates the quick convergence properties of the SUBGROUP algorithm, consistent with findings that the algorithm converges quickly while providing an appealing trade-off between error and fairness.", "doc_id": "1808.08166", "figure_id": "1808.08166_fig_3", "figure_number": 2, "image_path": "data/mineru_output/1808.08166/1808.08166/hybrid_auto/images/1808.08166_page0_fig2.jpg", "caption": "Figure 2: Error $\\varepsilon _ { t }$ and fairness violation $\\gamma _ { t }$ for Law School dataset (panels (a) and (b)) and Adult data set (panels (c) and (d)), for values of input $\\gamma$ ranging from 0 to 0.03. Dashed horizontal lines on $\\gamma _ { t }$ plots correspond to varying values of $\\gamma$ .", "figure_type": "plot", "visual_anchor": "Steep decline in colored curves between iteration 0 and 2000", "text_evidence": "Kearns et al. [2018] had already reported preliminary convergence results for the Communities and Crime dataset, showing that their algorithm converges quickly, and that varying the input $\\gamma$ provides an appealing trade-off between error and fairness", "query_type": "anomaly_cause"}
{"query_id": "l1_1808.08166_1808.08166_fig_3_471", "query": "Does the stabilization pattern of curves after iteration 4000 support plotting epsilon-gamma pairs across all iterations and input values?", "answer": "Yes, the stabilization of curves after iteration 4000 provides meaningful epsilon-gamma pairs across iterations and input values, which naturally enables computation of the undominated Pareto frontier of these pairs.", "doc_id": "1808.08166", "figure_id": "1808.08166_fig_3", "figure_number": 2, "image_path": "data/mineru_output/1808.08166/1808.08166/hybrid_auto/images/1808.08166_page0_fig2.jpg", "caption": "Figure 2: Error $\\varepsilon _ { t }$ and fairness violation $\\gamma _ { t }$ for Law School dataset (panels (a) and (b)) and Adult data set (panels (c) and (d)), for values of input $\\gamma$ ranging from 0 to 0.03. Dashed horizontal lines on $\\gamma _ { t }$ plots correspond to varying values of $\\gamma$ .", "figure_type": "plot", "visual_anchor": "Relatively flat curves after iteration 4000 converging to dashed horizontal lines", "text_evidence": "Regardless of convergence, for plots such as those in Figure 2, it is natural to take the $\\langle \\varepsilon _ { t } , \\gamma _ { t } \\rangle$ pairs across all $t$ and all input $\\gamma$ , and compute the undominated or Pareto frontier of these pairs", "query_type": "value_context"}
{"query_id": "l1_1808.08166_1808.08166_fig_11_472", "query": "Does the blue points' downward trend from y=0.25 to y=0 demonstrate the MARGINAL algorithm's error-fairness pairs?", "answer": "Yes, the blue points show the error and marginal fairness violation pairs achieved by the MARGINAL algorithm, displaying a clear trade-off as error increases from left to right.", "doc_id": "1808.08166", "figure_id": "1808.08166_fig_11", "figure_number": 3, "image_path": "data/mineru_output/1808.08166/1808.08166/hybrid_auto/images/1808.08166_page0_fig10.jpg", "caption": "Figure 3: Left column: The red points show the Pareto frontier of error (x axis) and subgroup fairness violation (y axis) for the SUBGROUP algorithm across all four data sets, while the blue points show the error and subgroup fairness violation for the models achieved by the MARGINAL algorithm. Right column: The error and marginal fairness violation for the MARGINAL algorithm across all four data sets. Ordering of datasets is Communities and Crime, Law School, Adult, and Student.", "figure_type": "plot", "visual_anchor": "blue points forming downward curve from upper-left (error ~0.16, fairness ~0.25) to lower-right (error ~0.23, fairness ~0)", "text_evidence": "The error and marginal fairness violation for the MARGINAL algorithm across all four data sets.", "query_type": "value_context"}
{"query_id": "l1_1808.08166_1808.08166_fig_11_473", "query": "Why are blue points shown in the right column when the SUBGROUP algorithm's Pareto frontier is red in the left column?", "answer": "The right column specifically displays the MARGINAL algorithm's performance on marginal fairness, while the left column compares SUBGROUP (red) versus MARGINAL (blue) on subgroup fairness.", "doc_id": "1808.08166", "figure_id": "1808.08166_fig_11", "figure_number": 3, "image_path": "data/mineru_output/1808.08166/1808.08166/hybrid_auto/images/1808.08166_page0_fig10.jpg", "caption": "Figure 3: Left column: The red points show the Pareto frontier of error (x axis) and subgroup fairness violation (y axis) for the SUBGROUP algorithm across all four data sets, while the blue points show the error and subgroup fairness violation for the models achieved by the MARGINAL algorithm. Right column: The error and marginal fairness violation for the MARGINAL algorithm across all four data sets. Ordering of datasets is Communities and Crime, Law School, Adult, and Student.", "figure_type": "plot", "visual_anchor": "blue points in right column plot versus red/blue distinction mentioned for left column", "text_evidence": "In the left column of Figure 3, we show the SUBGROUP algorithm Pareto frontiers for subgroup fairness on all four datasets, and also the pairs achieved by the MARGINAL algorithm.", "query_type": "comparison_explanation"}
{"query_id": "l1_1808.08166_1808.08166_fig_11_474", "query": "Does the clustering of blue points near error 0.18 reflect the heuristic Auditor's detection limitations for subgroup unfairness?", "answer": "The clustering reflects the MARGINAL algorithm's performance trade-offs, not the Auditor's limitations. The Auditor's potential failure to detect larger discrimination is a separate concern about measurement accuracy.", "doc_id": "1808.08166", "figure_id": "1808.08166_fig_11", "figure_number": 3, "image_path": "data/mineru_output/1808.08166/1808.08166/hybrid_auto/images/1808.08166_page0_fig10.jpg", "caption": "Figure 3: Left column: The red points show the Pareto frontier of error (x axis) and subgroup fairness violation (y axis) for the SUBGROUP algorithm across all four data sets, while the blue points show the error and subgroup fairness violation for the models achieved by the MARGINAL algorithm. Right column: The error and marginal fairness violation for the MARGINAL algorithm across all four data sets. Ordering of datasets is Communities and Crime, Law School, Adult, and Student.", "figure_type": "plot", "visual_anchor": "dense clustering of blue points around x=0.18 on error axis", "text_evidence": "This means that while any detected unfairness is a lower bound on the true subgroup unfairness, it could be the case that the heuristic Auditor is simply failing to detect a larger dis", "query_type": "anomaly_cause"}
{"query_id": "l1_1808.08166_1808.08166_fig_19_475", "query": "Does the yellow peak at t=31 represent positive false positive discrepancy for a specific subgroup over two protected attributes?", "answer": "Yes, each point in the plane corresponds to a different subgroup over two protected attributes, and the z-value represents the current false positive discrepancy for that subgroup, with yellow indicating positive values.", "doc_id": "1808.08166", "figure_id": "1808.08166_fig_19", "figure_number": 4, "image_path": "data/mineru_output/1808.08166/1808.08166/hybrid_auto/images/1808.08166_page0_fig18.jpg", "caption": "Figure 4: Evolution of discrimination surface for the SUBGROUP algorithm from $t = 1 \\ldots 1 3 0 1$ . Each point in the plane corresponds to a different subgroup over two protected attributes, and the corresponding $z$ value is the current false positive discrepancy for the subgroup. 12", "figure_type": "plot", "visual_anchor": "yellow peak in the surface plot with positive z-values around 0.02-0.03", "text_evidence": "Each point in the plane corresponds to a different subgroup over two protected attributes, and the corresponding $z$ value is the current false positive discrepancy for the subgroup.", "query_type": "value_context"}
{"query_id": "l1_1808.08166_1808.08166_fig_19_476", "query": "Why does the discrimination surface at t=31 show both yellow peaks and blue valleys across the coefficient plane?", "answer": "The surface visualizes the evolution of the SUBGROUP algorithm with input γ=0, where different subgroups defined by white and black coefficient combinations exhibit varying false positive discrepancies, creating peaks and valleys.", "doc_id": "1808.08166", "figure_id": "1808.08166_fig_19", "figure_number": 4, "image_path": "data/mineru_output/1808.08166/1808.08166/hybrid_auto/images/1808.08166_page0_fig18.jpg", "caption": "Figure 4: Evolution of discrimination surface for the SUBGROUP algorithm from $t = 1 \\ldots 1 3 0 1$ . Each point in the plane corresponds to a different subgroup over two protected attributes, and the corresponding $z$ value is the current false positive discrepancy for the subgroup. 12", "figure_type": "plot", "visual_anchor": "contrasting yellow peaks and blue valleys distributed across the white coefficient and black coefficient plane", "text_evidence": "we show a sequence of \"discrimination surfaces\" for the SUBGROUP algorithm over the 2 protected features, with input $\\gamma = 0$", "query_type": "comparison_explanation"}
{"query_id": "l1_1808.08166_1808.08166_fig_19_477", "query": "What temporal range does the snapshot at t=31 belong to in the discrimination surface evolution sequence?", "answer": "The snapshot at t=31 is an early frame in the evolution sequence that runs from t=1 to t=1301, capturing the algorithm's discrimination surface during its initial iterations.", "doc_id": "1808.08166", "figure_id": "1808.08166_fig_19", "figure_number": 4, "image_path": "data/mineru_output/1808.08166/1808.08166/hybrid_auto/images/1808.08166_page0_fig18.jpg", "caption": "Figure 4: Evolution of discrimination surface for the SUBGROUP algorithm from $t = 1 \\ldots 1 3 0 1$ . Each point in the plane corresponds to a different subgroup over two protected attributes, and the corresponding $z$ value is the current false positive discrepancy for the subgroup. 12", "figure_type": "plot", "visual_anchor": "time stamp t=31 displayed at the top of the 3D surface plot", "text_evidence": "Evolution of discrimination surface for the SUBGROUP algorithm from $t = 1 \\ldots 1 3 0 1$", "query_type": "value_context"}
{"query_id": "l1_1808.08166_1808.08166_fig_23_478", "query": "Why does the trajectory starting at error 0.1 move upward past the dashed line at gamma=0.005 for Communities and Crime?", "answer": "The trajectory explores different gamma values from the set {0.001, 0.005, 0.009, 0.022}, moving upward as gamma increases beyond 0.005 to test subgroup fairness constraints on the Communities and Crime dataset.", "doc_id": "1808.08166", "figure_id": "1808.08166_fig_23", "figure_number": 5, "image_path": "data/mineru_output/1808.08166/1808.08166/hybrid_auto/images/1808.08166_page0_fig22.jpg", "caption": "Figure 5: $\\langle \\varepsilon _ { t } , \\gamma _ { t } \\rangle$ trajectories for Communities and Crime, for $\\gamma \\in \\{ 0 . 0 0 1 , 0 . 0 0 5 , 0 . 0 0 9 , 0 . 0 2 2 \\}$ .", "figure_type": "plot", "visual_anchor": "trajectory starting at (0.1, 0.027) crossing above the horizontal dashed line at gamma=0.005", "text_evidence": "$\\langle \\varepsilon _ { t } , \\gamma _ { t } \\rangle$ trajectories for Communities and Crime, for $\\gamma \\in \\{ 0 . 0 0 1 , 0 . 0 0 5 , 0 . 0 0 9 , 0 . 0 2 2 \\}$", "query_type": "comparison_explanation"}
{"query_id": "l1_1808.08166_1808.08166_fig_23_479", "query": "Do the colored markers near error 0.2 and gamma 0.006 demonstrate the empirical efficacy of rich subgroup fairness on Communities and Crime?", "answer": "Yes, the colored markers show the algorithm's trajectory through the error-fairness tradeoff space, empirically demonstrating how subgroup fairness constraints are enforced during training on this fairness-sensitive dataset.", "doc_id": "1808.08166", "figure_id": "1808.08166_fig_23", "figure_number": 5, "image_path": "data/mineru_output/1808.08166/1808.08166/hybrid_auto/images/1808.08166_page0_fig22.jpg", "caption": "Figure 5: $\\langle \\varepsilon _ { t } , \\gamma _ { t } \\rangle$ trajectories for Communities and Crime, for $\\gamma \\in \\{ 0 . 0 0 1 , 0 . 0 0 5 , 0 . 0 0 9 , 0 . 0 2 2 \\}$ .", "figure_type": "plot", "visual_anchor": "red to yellow gradient markers clustered near coordinates (0.2, 0.006)", "text_evidence": "In this work we have established the empirical efficacy of the notion of rich subgroup fairness and the algorithm of Kearns et al. [2018] on four fairness-sensitive datasets", "query_type": "value_context"}
{"query_id": "l1_1808.08166_1808.08166_fig_23_480", "query": "What causes the peak at error 0.35 and gamma 0.047 despite testing only four discrete gamma values?", "answer": "The peak represents an intermediate state in the iterative algorithm trajectory where the Auditor identifies subgroups with high unfairness, forcing gamma_t to temporarily spike before the Learner adjusts to satisfy the subgroup fairness constraint.", "doc_id": "1808.08166", "figure_id": "1808.08166_fig_23", "figure_number": 5, "image_path": "data/mineru_output/1808.08166/1808.08166/hybrid_auto/images/1808.08166_page0_fig22.jpg", "caption": "Figure 5: $\\langle \\varepsilon _ { t } , \\gamma _ { t } \\rangle$ trajectories for Communities and Crime, for $\\gamma \\in \\{ 0 . 0 0 1 , 0 . 0 0 5 , 0 . 0 0 9 , 0 . 0 2 2 \\}$ .", "figure_type": "plot", "visual_anchor": "peak point at approximately (0.35, 0.047)", "text_evidence": "the necessity of explicitly enforcing subgroup (as opposed to only marginal) fairness", "query_type": "anomaly_cause"}
{"query_id": "l1_1809.01496_1809.01496_fig_1_481", "query": "Why are waitress and nun positioned at negative x-values while waiter and salesman appear at positive x-values?", "answer": "The plot shows cosine similarity with the gender direction, where negative values represent bias towards female and positive values towards male, as defined by the gender direction vector.", "doc_id": "1809.01496", "figure_id": "1809.01496_fig_1", "figure_number": 1, "image_path": "data/mineru_output/1809.01496/1809.01496/hybrid_auto/images/1809.01496_page0_fig0.jpg", "caption": "Figure 1: Cosine similarity between the gender direction and the embeddings of gender-neutral words. In each figure, negative values represent a bias towards female, otherwise male.", "figure_type": "plot", "visual_anchor": "red triangles (waitress, nun) at x < 0 and green triangles (waiter, salesman) at x > 0", "text_evidence": "where $\\Omega ^ { \\prime }$ is a set of predefined gender word pairs", "query_type": "visual_definition"}
{"query_id": "l1_1809.01496_1809.01496_fig_1_482", "query": "Does the clustering of blue x markers near the origin support the fixed vector assumption during gradient descent?", "answer": "Yes, the concentration of gender-neutral words near zero cosine similarity indicates reduced gender bias, consistent with assuming v_g is fixed during training to optimize the embedding space efficiently.", "doc_id": "1809.01496", "figure_id": "1809.01496_fig_1", "figure_number": 1, "image_path": "data/mineru_output/1809.01496/1809.01496/hybrid_auto/images/1809.01496_page0_fig0.jpg", "caption": "Figure 1: Cosine similarity between the gender direction and the embeddings of gender-neutral words. In each figure, negative values represent a bias towards female, otherwise male.", "figure_type": "plot", "visual_anchor": "dense cluster of blue x markers centered around (0,0)", "text_evidence": "To reduce the computational complexity in training the wording embedding, we assume $v _ { g }$ is a fixed vector (i.e., we do not derive gradient w.r.t $v _ { g }$ in updating $w ^ { ( a ) } , \\forall w \\in \\Omega ^ { \\prime } )$ and estimate $v _ { g }$ only at the beginning of each epoch", "query_type": "comparison_explanation"}
{"query_id": "l1_1809.01496_1809.01496_fig_1_483", "query": "How do the positions of colonel and captain demonstrate the method's generalization beyond binary gender pronouns?", "answer": "Colonel and captain appear near the center with low gender bias, showing that the method successfully neutralizes gender in occupation words even though the approach can extend beyond binary gender as noted in future directions.", "doc_id": "1809.01496", "figure_id": "1809.01496_fig_1", "figure_number": 1, "image_path": "data/mineru_output/1809.01496/1809.01496/hybrid_auto/images/1809.01496_page0_fig0.jpg", "caption": "Figure 1: Cosine similarity between the gender direction and the embeddings of gender-neutral words. In each figure, negative values represent a bias towards female, otherwise male.", "figure_type": "plot", "visual_anchor": "colonel and captain labels positioned near x=0.5-1.0, y=0 to -1.0", "text_evidence": "Future directions include extending the proposed approach to model other properties of words such as sentiment and generalizing our analysis beyond binary gender", "query_type": "value_context"}
{"query_id": "l1_1809.02208_1809.02208_fig_1_484", "query": "Why does translating 'ő egy ápoló' yield 'she's a nurse' while 'ő egy tudós' produces 'he is a scientist'?", "answer": "The translation tool reflects gender bias by associating stereotypical female roles like nurse with female pronouns and male-dominated fields like scientist with male pronouns, consistent with societal inequality.", "doc_id": "1809.02208", "figure_id": "1809.02208_fig_1", "figure_number": 1, "image_path": "data/mineru_output/1809.02208/1809.02208/hybrid_auto/images/1809.02208_page0_fig0.jpg", "caption": "Figure 1: Translating sentences from a gender neutral language such as Hungarian to English provides a glimpse into the phenomenon of gender bias in machine translation. This screenshot from Google Translate shows how occupations from traditionally male-dominated fields [40] such as scholar, engineer and CEO are interpreted as male, while occupations such as nurse, baker and wedding organizer are interpreted as female.", "figure_type": "example", "visual_anchor": "First line 'ő egy ápoló' translating to 'she's a nurse' versus second line 'ő egy tudós' translating to 'he is a scientist'", "text_evidence": "In this paper we assume that a statistical translation tool should reflect at most the inequality existent in society – it is only logical that a translation tool will poll from examples that society produced and, as such, will inevitably retain some of that bias.", "query_type": "comparison_explanation"}
{"query_id": "l1_1809.02208_1809.02208_fig_1_485", "query": "Does the male pronoun 'he is' appearing for engineer support claims about bias in male-dominated field translations?", "answer": "Yes, the translation of 'ő egy mérnök' as 'he is an engineer' demonstrates that occupations from traditionally male-dominated fields are interpreted as male by the statistical translation model.", "doc_id": "1809.02208", "figure_id": "1809.02208_fig_1", "figure_number": 1, "image_path": "data/mineru_output/1809.02208/1809.02208/hybrid_auto/images/1809.02208_page0_fig0.jpg", "caption": "Figure 1: Translating sentences from a gender neutral language such as Hungarian to English provides a glimpse into the phenomenon of gender bias in machine translation. This screenshot from Google Translate shows how occupations from traditionally male-dominated fields [40] such as scholar, engineer and CEO are interpreted as male, while occupations such as nurse, baker and wedding organizer are interpreted as female.", "figure_type": "example", "visual_anchor": "Third line showing 'ő egy mérnök' translated as 'he is an engineer'", "text_evidence": "This screenshot from Google Translate shows how occupations from traditionally male-dominated fields [40] such as scholar, engineer and CEO are interpreted as male, while occupations such as nurse, baker and wedding organizer are interpreted as female.", "query_type": "value_context"}
{"query_id": "l1_1809.02208_1809.02208_fig_1_486", "query": "How does the female pronoun for wedding organizer versus male for CEO illustrate the hypothesis about stereotypical role translation?", "answer": "The contrast between 'She is a wedding organizer' and 'he's a CEO' demonstrates high probability translation of sentences about stereotypical gender roles, with traditional female roles receiving female pronouns and leadership positions receiving male pronouns.", "doc_id": "1809.02208", "figure_id": "1809.02208_fig_1", "figure_number": 1, "image_path": "data/mineru_output/1809.02208/1809.02208/hybrid_auto/images/1809.02208_page0_fig0.jpg", "caption": "Figure 1: Translating sentences from a gender neutral language such as Hungarian to English provides a glimpse into the phenomenon of gender bias in machine translation. This screenshot from Google Translate shows how occupations from traditionally male-dominated fields [40] such as scholar, engineer and CEO are interpreted as male, while occupations such as nurse, baker and wedding organizer are interpreted as female.", "figure_type": "example", "visual_anchor": "Sixth line 'She is a wedding organizer' contrasted with seventh line 'he's a CEO'", "text_evidence": "this approach produces results consistent with the hypothesis that sentences about stereotypical gender roles are translated accordingly with high probability: nurse and baker are translated with female pronouns while engineer and CEO are translated with male ones.", "query_type": "comparison_explanation"}
{"query_id": "l1_1809.02208_1809.02208_fig_2_487", "query": "Why does the tallest bar at X=0 reach approximately 680 occupations when STEM fields are nearly exclusively concentrated there?", "answer": "The concentration of STEM fields at X=0 indicates minimal translation to female pronouns, contributing significantly to the tall bar representing occupations with zero female pronoun translations across languages.", "doc_id": "1809.02208", "figure_id": "1809.02208_fig_2", "figure_number": 2, "image_path": "data/mineru_output/1809.02208/1809.02208/hybrid_auto/images/1809.02208_page0_fig1.jpg", "caption": "Figure 2: The data for the number of translated female pronouns per merged occupation category totaled among languages suggests and inverse distribution. STEM fields are nearly exclusively concentrated at $X = 0$ , while more evenly distributed in fields such as production and healthcare (See Table", "figure_type": "plot", "visual_anchor": "Tallest stacked bar at X=0 reaching approximately 680 occupations with STEM (olive) visible in upper portion", "text_evidence": "STEM fields are nearly exclusively concentrated at $X = 0$ , while more evenly distributed in fields such as production and healthcare", "query_type": "comparison_explanation"}
{"query_id": "l1_1809.02208_1809.02208_fig_2_488", "query": "Does the wider horizontal spread of Production turquoise bars compared to STEM olive bars support an inverse distribution claim?", "answer": "Yes, Production occupations extend across multiple X values (0-5) while STEM remains concentrated at X=0, demonstrating the inverse distribution where some categories have broader female pronoun translation while others have minimal representation.", "doc_id": "1809.02208", "figure_id": "1809.02208_fig_2", "figure_number": 2, "image_path": "data/mineru_output/1809.02208/1809.02208/hybrid_auto/images/1809.02208_page0_fig1.jpg", "caption": "Figure 2: The data for the number of translated female pronouns per merged occupation category totaled among languages suggests and inverse distribution. STEM fields are nearly exclusively concentrated at $X = 0$ , while more evenly distributed in fields such as production and healthcare (See Table", "figure_type": "plot", "visual_anchor": "Production category (turquoise) bars visible from X=0 to approximately X=5, contrasted with STEM (olive) concentrated at X=0", "text_evidence": "The histogram in Figure 2 suggests that the number of female pronouns is inversely distributed – which is mirrored in the data for gender-neutral pronouns in Figure 4", "query_type": "comparison_explanation"}
{"query_id": "l1_1809.02208_1809.02208_fig_2_489", "query": "How does Healthcare purple segment visibility across multiple X positions relate to the 73.0% female participation in Education?", "answer": "Healthcare's distribution across X values suggests more balanced female pronoun translation, similar to Education's high female participation rate, indicating that occupations with higher female representation show less concentrated distribution at X=0.", "doc_id": "1809.02208", "figure_id": "1809.02208_fig_2", "figure_number": 2, "image_path": "data/mineru_output/1809.02208/1809.02208/hybrid_auto/images/1809.02208_page0_fig1.jpg", "caption": "Figure 2: The data for the number of translated female pronouns per merged occupation category totaled among languages suggests and inverse distribution. STEM fields are nearly exclusively concentrated at $X = 0$ , while more evenly distributed in fields such as production and healthcare (See Table", "figure_type": "plot", "visual_anchor": "Healthcare (purple) segments visible across X=0 to X=4 positions in stacked bars", "text_evidence": "Education, training, and library\tEducation\t22\t73.0%", "query_type": "value_context"}
{"query_id": "l1_1809.02208_1809.02208_fig_3_490", "query": "Why do STEM fields shown in yellow-green concentrate mainly at x≥6 in the distribution?", "answer": "STEM fields concentrate to the right (x≥6) because male pronouns in these occupations show a skew normal distribution with a peak at x=6, indicating higher male pronoun usage in STEM translations across languages.", "doc_id": "1809.02208", "figure_id": "1809.02208_fig_3", "figure_number": 3, "image_path": "data/mineru_output/1809.02208/1809.02208/hybrid_auto/images/1809.02208_page0_fig2.jpg", "caption": "Figure 3: In contrast to Figure   ", "figure_type": "plot", "visual_anchor": "yellow-green STEM bars concentrated to the right at x≥6", "text_evidence": "One can see how STEM fields concentrate mainly to the right ( $X \\geq 6$ ).", "query_type": "comparison_explanation"}
{"query_id": "l1_1809.02208_1809.02208_fig_3_491", "query": "Does the peak at x=6 in the stacked bars support the skew normal distribution claim?", "answer": "Yes, the visible peak around x=6 in the stacked bar heights supports the skew normal distribution pattern described for male pronouns, with occupations clustering at this value before tapering off.", "doc_id": "1809.02208", "figure_id": "1809.02208_fig_3", "figure_number": 3, "image_path": "data/mineru_output/1809.02208/1809.02208/hybrid_auto/images/1809.02208_page0_fig2.jpg", "caption": "Figure 3: In contrast to Figure   ", "figure_type": "plot", "visual_anchor": "peak in total bar height at x=6", "text_evidence": "male pronouns are seemingly skew normally distributed, with a peak at $X = 6$", "query_type": "value_context"}
{"query_id": "l1_1809.02208_1809.02208_fig_3_492", "query": "How does the male pronoun distribution pattern extending rightward contrast with the female pronoun pattern mentioned?", "answer": "The male pronoun distribution shows a skew normal pattern extending to higher values (x≥6), while female pronouns follow an inversely distributed pattern, suggesting opposite gender bias trends in translation across occupations.", "doc_id": "1809.02208", "figure_id": "1809.02208_fig_3", "figure_number": 3, "image_path": "data/mineru_output/1809.02208/1809.02208/hybrid_auto/images/1809.02208_page0_fig2.jpg", "caption": "Figure 3: In contrast to Figure   ", "figure_type": "plot", "visual_anchor": "distribution extending to x=10 with substantial bars at higher values", "text_evidence": "The histogram in Figure 2 suggests that the number of female pronouns is inversely distributed – which is mirrored in the data for gender-neutral pronouns in Figure 4 –, while the same data for male pronouns (shown in Figure 3) suggests a skew normal distribution.", "query_type": "comparison_explanation"}
{"query_id": "l1_1809.02208_1809.02208_fig_4_493", "query": "Why does the tallest bar at X=0 reaching approximately 465 occupations validate that STEM fields predominantly concentrate at X=0?", "answer": "The histogram shows the highest concentration of occupations at X=0, and STEM (yellow-green) is visibly prominent in this tallest bar, directly supporting the observation that STEM fields have minimal gender-neutral pronoun usage.", "doc_id": "1809.02208", "figure_id": "1809.02208_fig_4", "figure_number": 4, "image_path": "data/mineru_output/1809.02208/1809.02208/hybrid_auto/images/1809.02208_page0_fig3.jpg", "caption": "Figure 4: The scarcity of gender-neutral pronouns is manifest in their histogram. Once again, STEM fields are predominantly concentrated at $X = 0$ .", "figure_type": "plot", "visual_anchor": "Tallest bar at X=0 reaching approximately 465 occupations with visible STEM (yellow-green) segment", "text_evidence": "The scarcity of gender-neutral pronouns is manifest in their histogram. Once again, STEM fields are predominantly concentrated at $X = 0$", "query_type": "value_context"}
{"query_id": "l1_1809.02208_1809.02208_fig_4_494", "query": "How does the rapid decrease from 465 occupations at X=0 to under 50 at X=4 illustrate the inverse distribution compared to male pronouns?", "answer": "The steep decline in occupation counts as X increases demonstrates an inverse pattern, where gender-neutral pronouns are scarce (concentrated at low X values), contrasting with male pronouns which peak at X=6 in a skew normal distribution.", "doc_id": "1809.02208", "figure_id": "1809.02208_fig_4", "figure_number": 4, "image_path": "data/mineru_output/1809.02208/1809.02208/hybrid_auto/images/1809.02208_page0_fig3.jpg", "caption": "Figure 4: The scarcity of gender-neutral pronouns is manifest in their histogram. Once again, STEM fields are predominantly concentrated at $X = 0$ .", "figure_type": "plot", "visual_anchor": "Decreasing bar heights from ~465 at X=0 to approximately 50 at X=4", "text_evidence": "The histogram in Figure 2 suggests that the number of female pronouns is inversely distributed – which is mirrored in the data for gender-neutral pronouns in Figure 4 –, while the same data for male pronouns (shown in Figure 3) suggests a skew normal distribution", "query_type": "comparison_explanation"}
{"query_id": "l1_1809.02208_1809.02208_fig_5_495", "query": "Why does the cyan bar at x=0 reach 75 occupations when male defaults are second-most prominent in STEM?", "answer": "The histogram shows translated pronouns grouped among languages, not individual language translations. The high female count at x=0 indicates many STEM occupations where no languages produced female pronouns, consistent with male defaults being prominent.", "doc_id": "1809.02208", "figure_id": "1809.02208_fig_5", "figure_number": 5, "image_path": "data/mineru_output/1809.02208/1809.02208/hybrid_auto/images/1809.02208_page0_fig4.jpg", "caption": "Figure 5: Histograms for the distribution of the number of translated female, male and gender neutral pronouns totaled among languages are plotted side by side for job occupations in the STEM (Science, Technology, Engineering and Mathematics) field, in which male defaults are the second-to-most prominent (after Legal).", "figure_type": "plot", "visual_anchor": "Cyan bar at x=0 reaching y≈75", "text_evidence": "job occupations in the STEM (Science, Technology, Engineering and Mathematics) field, in which male defaults are the second-to-most prominent", "query_type": "anomaly_cause"}
{"query_id": "l1_1809.02208_1809.02208_fig_5_496", "query": "How does the pink bar height of 57 at x=0 compare to the olive distribution, given Legal has more male defaults?", "answer": "The pink neutral bar at 57 is lower than the cyan female bar at 75, while olive male bars are distributed across higher x-values. This suggests STEM has fewer neutral translations but still substantial gender-neutral defaults, though less male-biased than Legal.", "doc_id": "1809.02208", "figure_id": "1809.02208_fig_5", "figure_number": 5, "image_path": "data/mineru_output/1809.02208/1809.02208/hybrid_auto/images/1809.02208_page0_fig4.jpg", "caption": "Figure 5: Histograms for the distribution of the number of translated female, male and gender neutral pronouns totaled among languages are plotted side by side for job occupations in the STEM (Science, Technology, Engineering and Mathematics) field, in which male defaults are the second-to-most prominent (after Legal).", "figure_type": "plot", "visual_anchor": "Pink bar at x=0 reaching y≈57", "text_evidence": "in which male defaults are the second-to-most prominent (after Legal)", "query_type": "comparison_explanation"}
{"query_id": "l1_1809.02208_1809.02208_fig_5_497", "query": "Does the olive bar peak at x=7 with height 23 align with the claim about translated male pronouns totaled among languages?", "answer": "Yes, the olive bar at x=7 shows 23 occupations received male pronouns in 7 languages. This distribution pattern confirms the histogram represents pronouns totaled among languages, with most male translations clustering in the 6-9 language range.", "doc_id": "1809.02208", "figure_id": "1809.02208_fig_5", "figure_number": 5, "image_path": "data/mineru_output/1809.02208/1809.02208/hybrid_auto/images/1809.02208_page0_fig4.jpg", "caption": "Figure 5: Histograms for the distribution of the number of translated female, male and gender neutral pronouns totaled among languages are plotted side by side for job occupations in the STEM (Science, Technology, Engineering and Mathematics) field, in which male defaults are the second-to-most prominent (after Legal).", "figure_type": "plot", "visual_anchor": "Olive bar at x=7 reaching y≈23", "text_evidence": "Histograms for the distribution of the number of translated female, male and gender neutral pronouns totaled among languages", "query_type": "value_context"}
{"query_id": "l1_1809.02208_1809.02208_fig_6_498", "query": "Why does the pink neutral bar peak at 28 occupations near x=1 in Healthcare where male defaults are least prominent?", "answer": "In Healthcare, where male defaults are least prominent, most occupations translate to gender-neutral pronouns across languages rather than defaulting to male pronouns, resulting in the highest concentration of neutral translations.", "doc_id": "1809.02208", "figure_id": "1809.02208_fig_6", "figure_number": 6, "image_path": "data/mineru_output/1809.02208/1809.02208/hybrid_auto/images/1809.02208_page0_fig5.jpg", "caption": "Figure 6: Histograms for the distribution of the number of translated female, male and gender neutral pronouns totaled among languages are plotted side by side for job occupations in the Healthcare field, in which male defaults are least prominent.", "figure_type": "plot", "visual_anchor": "pink bar at x=1 reaching approximately 28 occupations", "text_evidence": "Histograms for the distribution of the number of translated female, male and gender neutral pronouns totaled among languages are plotted side by side for job occupations in the Healthcare field, in which male defaults are least prominent.", "query_type": "comparison_explanation"}
{"query_id": "l1_1809.02208_1809.02208_fig_6_499", "query": "How does the yellow male pronoun distribution extending to x=13 relate to broader occupation category patterns summarized in tables?", "answer": "The male pronoun distribution spans the widest range to x=13, indicating that even in Healthcare with least male defaults, some occupations still translate to male pronouns across many languages. This variation is captured in the coalesced broader occupation groups.", "doc_id": "1809.02208", "figure_id": "1809.02208_fig_6", "figure_number": 6, "image_path": "data/mineru_output/1809.02208/1809.02208/hybrid_auto/images/1809.02208_page0_fig5.jpg", "caption": "Figure 6: Histograms for the distribution of the number of translated female, male and gender neutral pronouns totaled among languages are plotted side by side for job occupations in the Healthcare field, in which male defaults are least prominent.", "figure_type": "plot", "visual_anchor": "yellow bars extending from x=3 to x=13", "text_evidence": "Table 7 summarizes it even further by coalescing occupation categories into broader groups to ease interpretation.", "query_type": "value_context"}
{"query_id": "l1_1809.02208_1809.02208_fig_6_500", "query": "Does the blue female bar reaching only 19 at x=1 versus pink at 28 support detailed occupation-level data summarization?", "answer": "Yes, the lower female pronoun peak (19) compared to neutral (28) at x=1 shows gender-specific translation patterns vary by occupation within Healthcare, justifying the detailed summarization approach for individual occupations.", "doc_id": "1809.02208", "figure_id": "1809.02208_fig_6", "figure_number": 6, "image_path": "data/mineru_output/1809.02208/1809.02208/hybrid_auto/images/1809.02208_page0_fig5.jpg", "caption": "Figure 6: Histograms for the distribution of the number of translated female, male and gender neutral pronouns totaled among languages are plotted side by side for job occupations in the Healthcare field, in which male defaults are least prominent.", "figure_type": "plot", "visual_anchor": "blue bar at x=1 reaching approximately 19 occupations compared to pink bar at 28", "text_evidence": "Table 6 summarizes these data, and Table 7 summarizes it even further by coalescing occupation categories into broader groups to ease interpretation.", "query_type": "comparison_explanation"}
{"query_id": "l1_1809.02208_1809.02208_fig_7_501", "query": "Why do Legal and STEM bars show taller pink segments than Healthcare's pink segment, confirming predominance patterns?", "answer": "Legal and STEM fields exhibit a predominance of male defaults, while Healthcare shows a larger proportion of female and neutral pronouns, resulting in shorter pink (male) segments for Healthcare compared to Legal and STEM.", "doc_id": "1809.02208", "figure_id": "1809.02208_fig_7", "figure_number": 7, "image_path": "data/mineru_output/1809.02208/1809.02208/hybrid_auto/images/1809.02208_page0_fig6.jpg", "caption": "Figure 7: Bar plots show how much of the distribution of translated gender pronouns for each occupation category (grouped as in Table 7) is composed of female, male and neutral terms. Legal and STEM fields exhibit a predominance of male defaults and contrast with Healthcare and Education, with a larger proportion of female and neutral pronouns. Note that in general the bars do not add up to $1 0 0 \\%$ , as there is a fair amount of translated sentences for which we cannot obtain a gender pronoun. Categories are sorted with respect to the proportions of male, female and neutral translated pronouns respectively", "figure_type": "plot", "visual_anchor": "Pink (Male) segment height comparison between Legal/STEM bars versus Healthcare bar", "text_evidence": "Legal and STEM fields exhibit a predominance of male defaults and contrast with Healthcare and Education, with a larger proportion of female and neutral pronouns.", "query_type": "comparison_explanation"}
{"query_id": "l1_1809.02208_1809.02208_fig_7_502", "query": "Do the bar heights reaching near 100% validate the observation that categories were sorted by male, female, neutral proportions?", "answer": "The bars do not reach exactly 100% because there is a fair amount of translated sentences for which no gender pronoun could be obtained. Categories are indeed sorted with respect to the proportions of male, female and neutral translated pronouns respectively.", "doc_id": "1809.02208", "figure_id": "1809.02208_fig_7", "figure_number": 7, "image_path": "data/mineru_output/1809.02208/1809.02208/hybrid_auto/images/1809.02208_page0_fig6.jpg", "caption": "Figure 7: Bar plots show how much of the distribution of translated gender pronouns for each occupation category (grouped as in Table 7) is composed of female, male and neutral terms. Legal and STEM fields exhibit a predominance of male defaults and contrast with Healthcare and Education, with a larger proportion of female and neutral pronouns. Note that in general the bars do not add up to $1 0 0 \\%$ , as there is a fair amount of translated sentences for which we cannot obtain a gender pronoun. Categories are sorted with respect to the proportions of male, female and neutral translated pronouns respectively", "figure_type": "plot", "visual_anchor": "Bar heights stopping below the 100% gridline across all categories", "text_evidence": "Note that in general the bars do not add up to $1 0 0 \\%$ , as there is a fair amount of translated sentences for which we cannot obtain a gender pronoun. Categories are sorted with respect to the proportions of male, female and neutral translated pronouns respectively", "query_type": "anomaly_cause"}
{"query_id": "l1_1809.02208_1809.02208_fig_7_503", "query": "How do the yellow and cyan segments in Education's bar help visualize the broader occupation groupings mentioned?", "answer": "The bar plots visualize how much of each occupation category's distribution is composed of female (yellow), male (pink), and gender-neutral (cyan) pronouns. Education's larger yellow and cyan segments illustrate the coalesced groupings that ease interpretation of gender pronoun distributions.", "doc_id": "1809.02208", "figure_id": "1809.02208_fig_7", "figure_number": 7, "image_path": "data/mineru_output/1809.02208/1809.02208/hybrid_auto/images/1809.02208_page0_fig6.jpg", "caption": "Figure 7: Bar plots show how much of the distribution of translated gender pronouns for each occupation category (grouped as in Table 7) is composed of female, male and neutral terms. Legal and STEM fields exhibit a predominance of male defaults and contrast with Healthcare and Education, with a larger proportion of female and neutral pronouns. Note that in general the bars do not add up to $1 0 0 \\%$ , as there is a fair amount of translated sentences for which we cannot obtain a gender pronoun. Categories are sorted with respect to the proportions of male, female and neutral translated pronouns respectively", "figure_type": "plot", "visual_anchor": "Yellow (Female) and cyan (Neutral) segment sizes in the Education category bar", "text_evidence": "The bar plots in Figure 7 help us visualize how much of the distribution of each occupation category is composed of female, male and gender-neutral pronouns.", "query_type": "visual_definition"}
{"query_id": "l1_1809.02208_1809.02208_fig_8_504", "query": "Why does the bottom right corner show darker colors despite probabilities being sorted to concentrate there?", "answer": "Both axes are sorted to concentrate higher female pronoun translation probabilities toward the bottom right corner, creating a gradient pattern from blue (low) in the top left to darker colors in the bottom right.", "doc_id": "1809.02208", "figure_id": "1809.02208_fig_8", "figure_number": 8, "image_path": "data/mineru_output/1809.02208/1809.02208/hybrid_auto/images/1809.02208_page0_fig7.jpg", "caption": "Figure 8: Heatmap for the translation probability into female pronouns for each pair of language and occupation category. Probabilities range from 0% (blue) to $1 0 0 \\%$ (red), and both axes are sorted in such a way that higher probabilities concentrate on the bottom right corner.", "figure_type": "plot", "visual_anchor": "bottom right corner with darker red/orange colors (40-80% probability range)", "text_evidence": "both axes are sorted in such a way that higher probabilities concentrate on the bottom right corner", "query_type": "visual_definition"}
{"query_id": "l1_1809.02208_1809.02208_fig_8_505", "query": "Does the blue color in Japanese-STEM cell indicate rejection of the complementary null hypothesis for that pair?", "answer": "Yes, darker shades of blue indicate language-category pairs where the complementary null hypothesis is rejected, and Japanese-STEM shows a dark blue color near 0% probability.", "doc_id": "1809.02208", "figure_id": "1809.02208_fig_8", "figure_number": 8, "image_path": "data/mineru_output/1809.02208/1809.02208/hybrid_auto/images/1809.02208_page0_fig7.jpg", "caption": "Figure 8: Heatmap for the translation probability into female pronouns for each pair of language and occupation category. Probabilities range from 0% (blue) to $1 0 0 \\%$ (red), and both axes are sorted in such a way that higher probabilities concentrate on the bottom right corner.", "figure_type": "plot", "visual_anchor": "Japanese-STEM cell in top-left region showing dark blue color (~0% probability)", "text_evidence": "Language-Category pairs for which the complementary null hypothesis is rejected are painted in a darker shade of blue", "query_type": "value_context"}
{"query_id": "l1_1809.02208_1809.02208_fig_8_506", "query": "Which language-category pairs reach the red end of the scale showing close to one hundred percent female pronoun translation?", "answer": "Finnish-Healthcare and Estonian-Healthcare show the darkest red colors, indicating probabilities near 100% for female pronoun translation in those language-occupation pairs.", "doc_id": "1809.02208", "figure_id": "1809.02208_fig_8", "figure_number": 8, "image_path": "data/mineru_output/1809.02208/1809.02208/hybrid_auto/images/1809.02208_page0_fig7.jpg", "caption": "Figure 8: Heatmap for the translation probability into female pronouns for each pair of language and occupation category. Probabilities range from 0% (blue) to $1 0 0 \\%$ (red), and both axes are sorted in such a way that higher probabilities concentrate on the bottom right corner.", "figure_type": "plot", "visual_anchor": "red cells in bottom right corner, particularly Finnish-Healthcare and Estonian-Healthcare (~60-80% range)", "text_evidence": "Probabilities range from 0% (blue) to $1 0 0 \\%$ (red)", "query_type": "comparison_explanation"}
{"query_id": "l1_1809.02208_1809.02208_fig_9_507", "query": "Why does Malay show deep red across most categories if probabilities concentrate on the bottom right corner?", "answer": "Malay is positioned at the bottom of the sorted y-axis, which is arranged to concentrate higher male pronoun translation probabilities toward the bottom right, explaining its consistently dark red coloration across categories.", "doc_id": "1809.02208", "figure_id": "1809.02208_fig_9", "figure_number": 9, "image_path": "data/mineru_output/1809.02208/1809.02208/hybrid_auto/images/1809.02208_page0_fig8.jpg", "caption": "Figure 9: Heatmap for the translation probability into male pronouns for each pair of language and occupation category. Probabilities range from 0% (blue) to $1 0 0 \\%$ (red), and both axes are sorted in such a way that higher probabilities concentrate on the bottom right corner.", "figure_type": "plot", "visual_anchor": "Malay row showing deep red coloration across Healthcare, Production, Education, Service, and Legal categories", "text_evidence": "both axes are sorted in such a way that higher probabilities concentrate on the bottom right corner", "query_type": "comparison_explanation"}
{"query_id": "l1_1809.02208_1809.02208_fig_9_508", "query": "Does the light blue cell for Basque-Healthcare align with the sorting strategy for translation probability ranges?", "answer": "Yes, the light blue indicates near 0% male pronoun translation probability for Basque in Healthcare, consistent with the sorting that places lower probabilities away from the bottom right corner concentration area.", "doc_id": "1809.02208", "figure_id": "1809.02208_fig_9", "figure_number": 9, "image_path": "data/mineru_output/1809.02208/1809.02208/hybrid_auto/images/1809.02208_page0_fig8.jpg", "caption": "Figure 9: Heatmap for the translation probability into male pronouns for each pair of language and occupation category. Probabilities range from 0% (blue) to $1 0 0 \\%$ (red), and both axes are sorted in such a way that higher probabilities concentrate on the bottom right corner.", "figure_type": "plot", "visual_anchor": "Light blue cell at Basque-Healthcare intersection in top left region", "text_evidence": "Probabilities range from 0% (blue) to $1 0 0 \\%$ (red)", "query_type": "value_context"}
{"query_id": "l1_1809.02208_1809.02208_fig_9_509", "query": "How does the deep red Chinese-Legal cell relate to the male pronoun translation probability heatmap design?", "answer": "The deep red indicates close to 100% male pronoun translation probability for Chinese in Legal occupations, positioned in the bottom right corner where the highest probabilities are concentrated by design.", "doc_id": "1809.02208", "figure_id": "1809.02208_fig_9", "figure_number": 9, "image_path": "data/mineru_output/1809.02208/1809.02208/hybrid_auto/images/1809.02208_page0_fig8.jpg", "caption": "Figure 9: Heatmap for the translation probability into male pronouns for each pair of language and occupation category. Probabilities range from 0% (blue) to $1 0 0 \\%$ (red), and both axes are sorted in such a way that higher probabilities concentrate on the bottom right corner.", "figure_type": "plot", "visual_anchor": "Deep red cell at Chinese-Legal intersection in bottom right corner", "text_evidence": "Heatmap for the translation probability into male pronouns for each pair of language and occupation category", "query_type": "value_context"}
{"query_id": "l1_1809.02208_1809.02208_fig_10_510", "query": "Why does Basque show red coloring across Entertainment through Production categories when other languages display blue?", "answer": "The heatmap is sorted to concentrate higher probabilities in the bottom right corner, so Basque's red coloring (near 100%) reflects its tendency to translate into gender neutral pronouns across those occupation categories, contrasting with other languages' low probabilities (blue, near 0%).", "doc_id": "1809.02208", "figure_id": "1809.02208_fig_10", "figure_number": 10, "image_path": "data/mineru_output/1809.02208/1809.02208/hybrid_auto/images/1809.02208_page0_fig9.jpg", "caption": "Figure 10: Heatmap for the translation probability into gender neutral pronouns for each pair of language and occupation category. Probabilities range from 0% (blue) to 100% (red), and both axes are sorted in such a way that higher probabilities concentrate on the bottom right corner.", "figure_type": "plot", "visual_anchor": "Basque row with red/dark red cells from Entertainment to Production versus blue cells in upper rows", "text_evidence": "Probabilities range from 0% (blue) to 100% (red), and both axes are sorted in such a way that higher probabilities concentrate on the bottom right corner.", "query_type": "comparison_explanation"}
{"query_id": "l1_1809.02208_1809.02208_fig_10_511", "query": "Does the blue concentration in Malay through Armenian rows support experimenting with a fair amount of different gender neutral languages?", "answer": "Yes, the predominantly blue coloring in those language rows indicates very low probabilities of gender neutral pronoun translation, demonstrating variation across the multiple gender neutral languages tested in the experiment.", "doc_id": "1809.02208", "figure_id": "1809.02208_fig_10", "figure_number": 10, "image_path": "data/mineru_output/1809.02208/1809.02208/hybrid_auto/images/1809.02208_page0_fig9.jpg", "caption": "Figure 10: Heatmap for the translation probability into gender neutral pronouns for each pair of language and occupation category. Probabilities range from 0% (blue) to 100% (red), and both axes are sorted in such a way that higher probabilities concentrate on the bottom right corner.", "figure_type": "plot", "visual_anchor": "Blue-dominated rows for Malay, Finnish, Hungarian, Swahili, Estonian, and Armenian", "text_evidence": "We have taken the care of experimenting with a fair amount of different gender neutral languages.", "query_type": "value_context"}
{"query_id": "l1_1809.02208_1809.02208_fig_10_512", "query": "Why does Turkish show teal-to-light blue gradient from Education to Production if data is coalesced by language groups?", "answer": "The gradient indicates varying translation probabilities across occupation categories for Turkish. Coalescing data by language groups helps visualize the effect of different cultures on gender bias genesis, explaining why Turkish shows moderate probabilities (teal/light blue) rather than uniform values.", "doc_id": "1809.02208", "figure_id": "1809.02208_fig_10", "figure_number": 10, "image_path": "data/mineru_output/1809.02208/1809.02208/hybrid_auto/images/1809.02208_page0_fig9.jpg", "caption": "Figure 10: Heatmap for the translation probability into gender neutral pronouns for each pair of language and occupation category. Probabilities range from 0% (blue) to 100% (red), and both axes are sorted in such a way that higher probabilities concentrate on the bottom right corner.", "figure_type": "plot", "visual_anchor": "Turkish row displaying gradient from darker teal on left to lighter blue/gray on right", "text_evidence": "another sensible way of coalescing our data is by language groups, as shown in Table 11. This can help us visualize the effect of different cultures in the genesis – or lack thereof – of gender bias.", "query_type": "comparison_explanation"}
{"query_id": "l1_1809.02208_1809.02208_fig_11_513", "query": "Why does Japanese show only ~0.2% yellow female pronouns when female pronouns reach as low as 0.196% for this language?", "answer": "The yellow segment in the Japanese bar visually confirms the extremely low female pronoun proportion of 0.196% mentioned, demonstrating the severe male default bias in Japanese translation outputs.", "doc_id": "1809.02208", "figure_id": "1809.02208_fig_11", "figure_number": 11, "image_path": "data/mineru_output/1809.02208/1809.02208/hybrid_auto/images/1809.02208_page0_fig10.jpg", "caption": "Figure 11: The distribution of pronominal genders per language also suggests a tendency towards male defaults, with female pronouns reaching as low as 0.196% and 1.865% for Japanese and Chinese respectively. Once again not all bars add up to 100% , as there is a fair amount of translated sentences for which we cannot obtain a gender pronoun, particularly in Basque. Among all tested languages, Basque was the only one to yield more gender neutral than male pronouns, with Bengali and Yoruba following after in this order. Languages are sorted with respect to the proportions of male, female and neutral translated pronouns respectively.", "figure_type": "plot", "visual_anchor": "Yellow (Female) segment in Japanese bar, approximately 0.2% of total height", "text_evidence": "The distribution of pronominal genders per language also suggests a tendency towards male defaults, with female pronouns reaching as low as 0.196% and 1.865% for Japanese and Chinese respectively.", "query_type": "value_context"}
{"query_id": "l1_1809.02208_1809.02208_fig_11_514", "query": "Does Basque's larger cyan neutral bar compared to pink male bar support its status as yielding more gender neutral than male pronouns?", "answer": "Yes, Basque's cyan neutral segment (~56%) visually exceeds its pink male segment (~8%), directly confirming that it was the only tested language to yield more gender neutral than male pronouns.", "doc_id": "1809.02208", "figure_id": "1809.02208_fig_11", "figure_number": 11, "image_path": "data/mineru_output/1809.02208/1809.02208/hybrid_auto/images/1809.02208_page0_fig10.jpg", "caption": "Figure 11: The distribution of pronominal genders per language also suggests a tendency towards male defaults, with female pronouns reaching as low as 0.196% and 1.865% for Japanese and Chinese respectively. Once again not all bars add up to 100% , as there is a fair amount of translated sentences for which we cannot obtain a gender pronoun, particularly in Basque. Among all tested languages, Basque was the only one to yield more gender neutral than male pronouns, with Bengali and Yoruba following after in this order. Languages are sorted with respect to the proportions of male, female and neutral translated pronouns respectively.", "figure_type": "plot", "visual_anchor": "Basque bar showing cyan (Neutral) segment larger than pink (Male) segment", "text_evidence": "Among all tested languages, Basque was the only one to yield more gender neutral than male pronouns, with Bengali and Yoruba following after in this order.", "query_type": "comparison_explanation"}
{"query_id": "l1_1809.02208_1809.02208_fig_11_515", "query": "Why do bars not reach exactly 100% height, particularly for Basque's shorter bar compared to Japanese or Malay?", "answer": "Not all bars add up to 100% because there are translated sentences for which no gender pronoun could be obtained, particularly in Basque. This extraction difficulty varies by language, making some bars shorter than others.", "doc_id": "1809.02208", "figure_id": "1809.02208_fig_11", "figure_number": 11, "image_path": "data/mineru_output/1809.02208/1809.02208/hybrid_auto/images/1809.02208_page0_fig10.jpg", "caption": "Figure 11: The distribution of pronominal genders per language also suggests a tendency towards male defaults, with female pronouns reaching as low as 0.196% and 1.865% for Japanese and Chinese respectively. Once again not all bars add up to 100% , as there is a fair amount of translated sentences for which we cannot obtain a gender pronoun, particularly in Basque. Among all tested languages, Basque was the only one to yield more gender neutral than male pronouns, with Bengali and Yoruba following after in this order. Languages are sorted with respect to the proportions of male, female and neutral translated pronouns respectively.", "figure_type": "plot", "visual_anchor": "Basque bar height significantly below 100% gridline, shorter than Japanese and Malay bars", "text_evidence": "Once again not all bars add up to 100% , as there is a fair amount of translated sentences for which we cannot obtain a gender pronoun, particularly in Basque.", "query_type": "anomaly_cause"}
{"query_id": "l1_1809.02208_1809.02208_fig_12_516", "query": "Why do Shy and Desirable show over 45% yellow bars while Proud shows over 85% pink?", "answer": "Adjectives such as Shy and Desirable amass at the female side of the spectrum, contrasting with Proud which is almost exclusively translated with male pronouns. This demonstrates how stereotypical gender roles influence automatic translation of simple adjectives.", "doc_id": "1809.02208", "figure_id": "1809.02208_fig_12", "figure_number": 12, "image_path": "data/mineru_output/1809.02208/1809.02208/hybrid_auto/images/1809.02208_page0_fig11.jpg", "caption": "Figure 12: The distribution of pronominal genders for each word in Table 5 shows how stereotypical gender roles can play a part on the automatic translation of simple adjectives. One can see that adjectives such as Shy and Desirable, Sad and Dumb amass at the female side of the spectrum, contrasting with Proud, Guilty, Cruel and Brave which are almost exclusively translated with male pronouns.", "figure_type": "plot", "visual_anchor": "Yellow bars (Female) dominating Shy and Desirable vs pink bars (Male) dominating Proud", "text_evidence": "adjectives such as Shy and Desirable, Sad and Dumb amass at the female side of the spectrum, contrasting with Proud, Guilty, Cruel and Brave which are almost exclusively translated with male pronouns", "query_type": "comparison_explanation"}
{"query_id": "l1_1809.02208_1809.02208_fig_12_517", "query": "Do the cyan bars for Successful and Right exceeding 90% confirm filtering for human-applicable adjectives?", "answer": "Yes, the high neutral pronoun percentages for these adjectives are consistent with the filtering process. The researchers filtered adjectives from the COCA corpus that would fit templates and make sense for describing a human being.", "doc_id": "1809.02208", "figure_id": "1809.02208_fig_12", "figure_number": 12, "image_path": "data/mineru_output/1809.02208/1809.02208/hybrid_auto/images/1809.02208_page0_fig11.jpg", "caption": "Figure 12: The distribution of pronominal genders for each word in Table 5 shows how stereotypical gender roles can play a part on the automatic translation of simple adjectives. One can see that adjectives such as Shy and Desirable, Sad and Dumb amass at the female side of the spectrum, contrasting with Proud, Guilty, Cruel and Brave which are almost exclusively translated with male pronouns.", "figure_type": "plot", "visual_anchor": "Cyan bars (Neutral) at approximately 90% for Successful and Right adjectives", "text_evidence": "we queried the 1000 most frequently used adjectives in English, as classified in the COCA corpus, but since not all of them were readily applicable to the sentence template we used, we filtered the N adjectives that would fit the templates and made sense for describing a human being", "query_type": "value_context"}
{"query_id": "l1_1809.02208_1809.02208_fig_12_518", "query": "Could the pink bars exceeding 80% for Brave and Guilty reflect male-dominated field demographics rather than bias?", "answer": "This possibility is explicitly addressed as a sensible objection. The researchers acknowledge they must account for the possibility that gender pronoun statistics in Google Translate outputs merely reflect demographics of male-dominated fields.", "doc_id": "1809.02208", "figure_id": "1809.02208_fig_12", "figure_number": 12, "image_path": "data/mineru_output/1809.02208/1809.02208/hybrid_auto/images/1809.02208_page0_fig11.jpg", "caption": "Figure 12: The distribution of pronominal genders for each word in Table 5 shows how stereotypical gender roles can play a part on the automatic translation of simple adjectives. One can see that adjectives such as Shy and Desirable, Sad and Dumb amass at the female side of the spectrum, contrasting with Proud, Guilty, Cruel and Brave which are almost exclusively translated with male pronouns.", "figure_type": "plot", "visual_anchor": "Pink bars (Male) at approximately 80-85% for Brave and Guilty adjectives", "text_evidence": "A sensible objection to the conclusions we draw from our study is that the perceived gender bias in Google Translate results stems from the fact that possibly female participation in some job positions is itself low. We must account for the possibility that the statistics of gender pronouns in Google Translate outputs merely reflects the demographics of maledominated fields", "query_type": "anomaly_cause"}
{"query_id": "l1_1809.02208_1809.02208_fig_13_519", "query": "Do the higher yellow bars in quantiles 3-4 correspond to increased blue bar heights, challenging the demographics hypothesis?", "answer": "No, the blue bars in quantiles 3-4 show approximately 19% and 8.5% respectively, which do not increase proportionally with the yellow bars showing 10% and 23%. This lack of correlation does not support the hypothesis that Google Translate bias reflects female worker demographics.", "doc_id": "1809.02208", "figure_id": "1809.02208_fig_13", "figure_number": 13, "image_path": "data/mineru_output/1809.02208/1809.02208/hybrid_auto/images/1809.02208_page0_fig12.jpg", "caption": "Figure 13: Women participation ( $\\%$ ) data obtained from the U.S. Bureau of Labor Statistics allows us to assess whether the Google Translate bias towards male defaults is at least to some extent explained by small frequencies of female workers in some job positions. Our data does not make a very good case for that hypothesis: the total frequency of translated female pronouns (in blue) for each 12-quantile does not seem to respond to the higher proportion of female workers (in yellow) in the last quantiles.", "figure_type": "plot", "visual_anchor": "Yellow bars at quantiles 3-4 (~10% and ~23%) compared to blue bars (~19% and ~8.5%)", "text_evidence": "A sensible objection to the conclusions we draw from our study is that the perceived gender bias in Google Translate results stems from the fact that possibly female participation in some job positions is itself low.", "query_type": "comparison_explanation"}
{"query_id": "l1_1809.02208_1809.02208_fig_13_520", "query": "Why do quantiles 9-12 show near-zero blue bars despite yellow bars indicating 5-6% female participation rates?", "answer": "The blue bars in quantiles 9-12 are nearly zero (less than 1%) while yellow bars show 5-6% female participation, demonstrating that Google Translate's female pronoun frequency does not respond to actual female worker proportions. This contradicts the hypothesis that translation outputs merely reflect job demographics.", "doc_id": "1809.02208", "figure_id": "1809.02208_fig_13", "figure_number": 13, "image_path": "data/mineru_output/1809.02208/1809.02208/hybrid_auto/images/1809.02208_page0_fig12.jpg", "caption": "Figure 13: Women participation ( $\\%$ ) data obtained from the U.S. Bureau of Labor Statistics allows us to assess whether the Google Translate bias towards male defaults is at least to some extent explained by small frequencies of female workers in some job positions. Our data does not make a very good case for that hypothesis: the total frequency of translated female pronouns (in blue) for each 12-quantile does not seem to respond to the higher proportion of female workers (in yellow) in the last quantiles.", "figure_type": "plot", "visual_anchor": "Blue bars at quantiles 9-12 (near 0%) versus yellow bars (5-6%)", "text_evidence": "the total frequency of translated female pronouns (in blue) for each 12-quantile does not seem to respond to the higher proportion of female workers (in yellow) in the last quantiles", "query_type": "anomaly_cause"}
{"query_id": "l1_1809.02208_1809.02208_fig_13_521", "query": "Does the 38% blue peak at quantile 1 versus 18% yellow bar validate that male-dominated field statistics explain translation outputs?", "answer": "No, the blue bar at quantile 1 reaches approximately 38% while the yellow bar shows only 18% female participation, indicating Google Translate over-represents female pronouns in this quantile. This inverse relationship contradicts the hypothesis that translation statistics merely reflect male-dominated field demographics.", "doc_id": "1809.02208", "figure_id": "1809.02208_fig_13", "figure_number": 13, "image_path": "data/mineru_output/1809.02208/1809.02208/hybrid_auto/images/1809.02208_page0_fig12.jpg", "caption": "Figure 13: Women participation ( $\\%$ ) data obtained from the U.S. Bureau of Labor Statistics allows us to assess whether the Google Translate bias towards male defaults is at least to some extent explained by small frequencies of female workers in some job positions. Our data does not make a very good case for that hypothesis: the total frequency of translated female pronouns (in blue) for each 12-quantile does not seem to respond to the higher proportion of female workers (in yellow) in the last quantiles.", "figure_type": "plot", "visual_anchor": "Quantile 1 showing blue bar at ~38% and yellow bar at ~18%", "text_evidence": "We must account for the possibility that the statistics of gender pronouns in Google Translate outputs merely reflects the demographics of maledominated fields (male-dominated fields can be considered those", "query_type": "value_context"}
{"query_id": "l1_1809.02208_1809.02208_fig_14_522", "query": "Why does the left interface showing 'he is a doctor' represent the version analyzed in August 2018 experiments?", "answer": "The experiments reported in the study were carried out in August 2018, capturing a snapshot of Google Translate at that time when it offered only one official translation for each input word.", "doc_id": "1809.02208", "figure_id": "1809.02208_fig_14", "figure_number": 14, "image_path": "data/mineru_output/1809.02208/1809.02208/hybrid_auto/images/1809.02208_page0_fig13.jpg", "caption": "Figure 14: Comparison between the GUI of Google Translate before (left) and after (right) the introduction of the new feature intended to promote gender fairness in translation. The results described in this paper relate to the older version.", "figure_type": "example", "visual_anchor": "Left interface labeled 'Before' with single translation 'he is a doctor' in blue box", "text_evidence": "In this context, all experiments reported here offer an analysis of a \"screenshot\" of that tool as of August 2018, the moment they were carried out.", "query_type": "value_context"}
{"query_id": "l1_1809.02208_1809.02208_fig_14_523", "query": "How does the single translation 'he is a doctor' in the left interface demonstrate the male default bias?", "answer": "The interface shows only the masculine pronoun 'he' for the gender-neutral Turkish input, exemplifying the strong tendency towards male defaults that the study provided evidence for in statistical translation tools.", "doc_id": "1809.02208", "figure_id": "1809.02208_fig_14", "figure_number": 14, "image_path": "data/mineru_output/1809.02208/1809.02208/hybrid_auto/images/1809.02208_page0_fig13.jpg", "caption": "Figure 14: Comparison between the GUI of Google Translate before (left) and after (right) the introduction of the new feature intended to promote gender fairness in translation. The results described in this paper relate to the older version.", "figure_type": "example", "visual_anchor": "Blue box displaying 'he is a doctor' as the sole translation result", "text_evidence": "In this paper, we have provided evidence that statistical translation tools such as Google Translate can exhibit gender biases and a strong tendency towards male defaults.", "query_type": "comparison_explanation"}
{"query_id": "l1_1809.02208_1809.02208_fig_14_524", "query": "What policy change occurred on December 6, 2018 that would alter the left interface display?", "answer": "Google released a new feature presenting users with both feminine and masculine official translations, moving away from the single translation shown in the 'Before' interface to reduce gender bias.", "doc_id": "1809.02208", "figure_id": "1809.02208_fig_14", "figure_number": 14, "image_path": "data/mineru_output/1809.02208/1809.02208/hybrid_auto/images/1809.02208_page0_fig13.jpg", "caption": "Figure 14: Comparison between the GUI of Google Translate before (left) and after (right) the introduction of the new feature intended to promote gender fairness in translation. The results described in this paper relate to the older version.", "figure_type": "example", "visual_anchor": "Left interface labeled 'Before' showing only one translation option", "text_evidence": "On December 6, 2018 the company's policy changed, and a statement was released detailing their efforts to reduce gender bias on Google Translate, which included a new feature presenting the user with a feminine as well as a masculine official translation", "query_type": "value_context"}
{"query_id": "l1_1809.02244_1809.02244_fig_1_525", "query": "Why does the arrow from X to M enable identification of the distribution sum over x,m in the response formula?", "answer": "The arrow from X to M represents the conditional probability p(M|a=f_A(x), X=x), which is one of the three multiplicative terms in the identified distribution formula that sums over x and m to compute Y(a=f_A(X)).", "doc_id": "1809.02244", "figure_id": "1809.02244_fig_1", "figure_number": 1, "image_path": "data/mineru_output/1809.02244/1809.02244/hybrid_auto/images/1809.02244_page0_fig0.jpg", "caption": "Figure 1. (a) A simple causal DAG, with a single treatment $A$ , a single outcome $Y$ , a vector $X$ of baseline variables, and a single mediator $M$ . (b) A causal DAG corresponding to our (simplified) child welfare example with baseline factors $X$ , sensitive feature $S$ , action $A$ , vector of mediators (including e.g. socioeconomic variables, histories of drug treatment) $M$ , an indicator $Y_{1}$ of whether a child is separated from their parents, and an indicator of child hospitalization $Y_{2}$ . (d) A multistage decision problem, which corresponds to a complete DAG over vertices $X, S, M, A_{1}, Y_{1}, \\dots, A_{K}, Y_{K}$ .", "figure_type": "diagram", "visual_anchor": "Blue arrow from node X to node M", "text_evidence": "its distribution is identified as $\\sum_{x,m} p(Y|a = f_A(x), M = m, X = x)p(M|a = f_A(x), X = x)p(X = x)$", "query_type": "comparison_explanation"}
{"query_id": "l1_1809.02244_1809.02244_fig_1_526", "query": "Does the direct arrow from A to Y allow defining Y(a=f_A(X)) without including the mediator M in the counterfactual?", "answer": "No, despite the direct arrow from A to Y, the counterfactual Y(a=f_A(X)) must be defined as Y(a=f_A(X), M(a=f_A(X), X), X), explicitly including the mediator M because M lies on a causal path from A to Y.", "doc_id": "1809.02244", "figure_id": "1809.02244_fig_1", "figure_number": 1, "image_path": "data/mineru_output/1809.02244/1809.02244/hybrid_auto/images/1809.02244_page0_fig0.jpg", "caption": "Figure 1. (a) A simple causal DAG, with a single treatment $A$ , a single outcome $Y$ , a vector $X$ of baseline variables, and a single mediator $M$ . (b) A causal DAG corresponding to our (simplified) child welfare example with baseline factors $X$ , sensitive feature $S$ , action $A$ , vector of mediators (including e.g. socioeconomic variables, histories of drug treatment) $M$ , an indicator $Y_{1}$ of whether a child is separated from their parents, and an indicator of child hospitalization $Y_{2}$ . (d) A multistage decision problem, which corresponds to a complete DAG over vertices $X, S, M, A_{1}, Y_{1}, \\dots, A_{K}, Y_{K}$ .", "figure_type": "diagram", "visual_anchor": "Blue arrow from node A directly to node Y", "text_evidence": "$Y(a = f_A(X))$ in Fig. 1(a) is defined as $Y(a = f_A(X), M(a = f_A(X), X), X)$", "query_type": "anomaly_cause"}
{"query_id": "l1_1809.02244_1809.02244_fig_1_527", "query": "How does node M's position between A and Y relate to the difficulty of validating optimal policy sets?", "answer": "Node M as a mediator between A and Y makes Y(f_A) a counterfactual quantity involving unobserved pathways through M, which creates difficulty in validating the optimal policy f_A* that maximizes E[Y(f_A)] using only observed data.", "doc_id": "1809.02244", "figure_id": "1809.02244_fig_1", "figure_number": 1, "image_path": "data/mineru_output/1809.02244/1809.02244/hybrid_auto/images/1809.02244_page0_fig0.jpg", "caption": "Figure 1. (a) A simple causal DAG, with a single treatment $A$ , a single outcome $Y$ , a vector $X$ of baseline variables, and a single mediator $M$ . (b) A causal DAG corresponding to our (simplified) child welfare example with baseline factors $X$ , sensitive feature $S$ , action $A$ , vector of mediators (including e.g. socioeconomic variables, histories of drug treatment) $M$ , an indicator $Y_{1}$ of whether a child is separated from their parents, and an indicator of child hospitalization $Y_{2}$ . (d) A multistage decision problem, which corresponds to a complete DAG over vertices $X, S, M, A_{1}, Y_{1}, \\dots, A_{K}, Y_{K}$ .", "figure_type": "diagram", "visual_anchor": "Node M positioned between nodes A and Y with incoming and outgoing blue arrows", "text_evidence": "Since $Y(f_{A})$ is a counterfactual quantity, validating the found set of policies is difficult given only", "query_type": "comparison_explanation"}
{"query_id": "l1_1809.02244_1809.02244_fig_4_528", "query": "Why does the blue solid curve remain near 0.8 across all utility parameters despite the dataset size of 5,000?", "answer": "The synthetic data was generated with sensitive variable S as an informative covariate in estimating Y, which maintains the African-American incarceration rate at elevated levels regardless of the utility parameter value.", "doc_id": "1809.02244", "figure_id": "1809.02244_fig_4", "figure_number": 2, "image_path": "data/mineru_output/1809.02244/1809.02244/hybrid_auto/images/1809.02244_page0_fig3.jpg", "caption": "Figure 2. Group-level incarceration rates for the COMPAS data as a function of the utility parameter $\\theta$ .", "figure_type": "plot", "visual_anchor": "Blue solid line staying approximately constant at 0.8 from utility parameter 1 to 4", "text_evidence": "We generated a dataset of size 5,000, with 100 bootstrap replications, where the sensitive variable $S$ is randomly assigned and where $S$ is chosen to be an informative covariate in estimating $Y$.", "query_type": "comparison_explanation"}
{"query_id": "l1_1809.02244_1809.02244_fig_4_529", "query": "Does the red solid line's rise from 0.0 to 0.3 between utility parameters 2.5 and 3.5 align with the two-stage decision structure?", "answer": "Yes, the rise reflects the causal model in Fig. 1(c) with K=2 stages, where the utility parameter θ influences the decision policy, causing changes in Caucasian incarceration rates as the policy optimizes the continuous response utility Y.", "doc_id": "1809.02244", "figure_id": "1809.02244_fig_4", "figure_number": 2, "image_path": "data/mineru_output/1809.02244/1809.02244/hybrid_auto/images/1809.02244_page0_fig3.jpg", "caption": "Figure 2. Group-level incarceration rates for the COMPAS data as a function of the utility parameter $\\theta$ .", "figure_type": "plot", "visual_anchor": "Red solid line increasing from approximately 0.0 to 0.3 between utility parameter 2.5 and 3.5", "text_evidence": "We generated synthetic data for a two-stage decision problem according to the causal model shown in Fig. 1(c) $(K = 2)$, where all variables are binary except for the continuous response utility $Y \\equiv Y_{2}$.", "query_type": "value_context"}
{"query_id": "l1_1809.02244_1809.02244_fig_4_530", "query": "How do the dashed and dotted blue curves demonstrate the formalization extended from Nabi and Shpitser 2018 for utility parameter above 2.5?", "answer": "The gap between the dashed (optimal unrestricted) and dotted (optimal fair) blue curves shows the tradeoff between unconstrained policy optimization and fairness-constrained policy learning, illustrating how fairness constraints modify decision-making to induce more equitable outcomes.", "doc_id": "1809.02244", "figure_id": "1809.02244_fig_4", "figure_number": 2, "image_path": "data/mineru_output/1809.02244/1809.02244/hybrid_auto/images/1809.02244_page0_fig3.jpg", "caption": "Figure 2. Group-level incarceration rates for the COMPAS data as a function of the utility parameter $\\theta$ .", "figure_type": "plot", "visual_anchor": "Separation between blue dashed line (~1.0) and blue dotted line (~0.8) for utility parameter values above 2.5", "text_evidence": "We have extended a formalization of algorithmic fairness from Nabi & Shpitser (2018) to the setting of learning optimal policies under fairness constraints. We show how to constrain a set of statistical models and learn a policy such that subsequent decision making given new observations from the \"unfair world\" induces high-quality outcomes while satisfying the specified fairness constraints", "query_type": "comparison_explanation"}
{"query_id": "l1_1809.02244_1809.02244_fig_5_531", "query": "Why does the dashed Fair policy line drop to near zero around utility parameter 2.5 when PSE constraints lie between negative 0.05 and 0.05?", "answer": "The PSE constraints between -0.05 and 0.05 enforce fairness by limiting predictive score differences between groups, which at this utility parameter value requires minimizing incarceration rates to satisfy the fairness bounds.", "doc_id": "1809.02244", "figure_id": "1809.02244_fig_5", "figure_number": 3, "image_path": "data/mineru_output/1809.02244/1809.02244/hybrid_auto/images/1809.02244_page0_fig4.jpg", "caption": "Figure 3. Overall incarceration rates for the COMPAS data as a function of the utility parameter $\\theta$ .", "figure_type": "plot", "visual_anchor": "Dashed line dropping to approximately 0.0 at utility parameter 2.5", "text_evidence": "We constrained the PSEs to lie between $-0.05$ and 0.05 and 0.95 and 1.05, respectively.", "query_type": "anomaly_cause"}
{"query_id": "l1_1809.02244_1809.02244_fig_5_532", "query": "How does the solid unfair policy line behavior starting at utility parameter 2.5 relate to the IPW estimators used for PSE estimation?", "answer": "The solid unfair policy line shows an unconstrained optimization that sharply increases incarceration rates starting at utility parameter 2.5, reflecting utility maximization without the PSE fairness constraints that the IPW estimators would enforce.", "doc_id": "1809.02244", "figure_id": "1809.02244_fig_5", "figure_number": 3, "image_path": "data/mineru_output/1809.02244/1809.02244/hybrid_auto/images/1809.02244_page0_fig4.jpg", "caption": "Figure 3. Overall incarceration rates for the COMPAS data as a function of the utility parameter $\\theta$ .", "figure_type": "plot", "visual_anchor": "Solid line showing sharp increase from approximately 0.0 to 0.2 starting at utility parameter 2.5", "text_evidence": "For estimating the PSEs which we constrain, we used the same IPW estimators described in the main paper and reproduced in the theorem below.", "query_type": "comparison_explanation"}
{"query_id": "l1_1809.02244_1809.02244_fig_5_533", "query": "Does the dotted observed policy line remaining constant at 0.28 validate the regression model specifications mentioned for COMPAS data analysis?", "answer": "Yes, the constant observed policy line at 0.28 represents the baseline empirical incarceration rate from the actual COMPAS data, serving as a reference point against which the regression models evaluate optimal fair and unfair policies.", "doc_id": "1809.02244", "figure_id": "1809.02244_fig_5", "figure_number": 3, "image_path": "data/mineru_output/1809.02244/1809.02244/hybrid_auto/images/1809.02244_page0_fig4.jpg", "caption": "Figure 3. Overall incarceration rates for the COMPAS data as a function of the utility parameter $\\theta$ .", "figure_type": "plot", "visual_anchor": "Dotted horizontal line at incarceration rate approximately 0.28", "text_evidence": "The regression models we used in the COMPAS data analysis were specified as follows:", "query_type": "value_context"}
{"query_id": "l1_1809.02244_1809.02244_fig_6_534", "query": "Why does the solid line remain above both other curves across all utility parameter values despite all three descending?", "answer": "The solid line represents the optimal unfair policy, which maximizes expected utility without fairness constraints, allowing it to achieve higher utility values than the constrained fair policy or the observed policy across all parameter settings.", "doc_id": "1809.02244", "figure_id": "1809.02244_fig_6", "figure_number": 4, "image_path": "data/mineru_output/1809.02244/1809.02244/hybrid_auto/images/1809.02244_page0_fig5.jpg", "caption": "Figure 4. The relative utility of policies for the COMPAS data as a function of the utility parameter $\\theta$ .", "figure_type": "plot", "visual_anchor": "solid line (Optimal unfair policy) positioned above dashed and dotted lines from utility parameter 1 to 4", "text_evidence": "The relative utility of policies for the COMPAS data as a function of the utility parameter θ", "query_type": "comparison_explanation"}
{"query_id": "l1_1809.02244_1809.02244_fig_6_535", "query": "Do the dashed and dotted lines converging near utility parameter 3 support that fairness constraints reduce optimality gaps?", "answer": "Yes, the convergence of the optimal fair policy (dashed) and observed policy (dotted) around utility parameter 3 suggests that at higher parameter values, fairness constraints become less costly, narrowing the gap between optimal and observed fair policies.", "doc_id": "1809.02244", "figure_id": "1809.02244_fig_6", "figure_number": 4, "image_path": "data/mineru_output/1809.02244/1809.02244/hybrid_auto/images/1809.02244_page0_fig5.jpg", "caption": "Figure 4. The relative utility of policies for the COMPAS data as a function of the utility parameter $\\theta$ .", "figure_type": "plot", "visual_anchor": "dashed line (Optimal Fair policy) and dotted line (Observed policy) converging around utility parameter 3", "text_evidence": "Assume S is binary. Under the causal model above, the following are consistent estimators of PSE^{sy} and PSE^{sak_k}, assuming all models are correctly specified", "query_type": "comparison_explanation"}
{"query_id": "l1_1809.02244_1809.02244_fig_6_536", "query": "What explains the steeper decline of all three curves between utility parameter 1 and 2 compared to later regions?", "answer": "The steeper initial decline reflects higher sensitivity of expected utility to the utility parameter θ at lower values, where policy differences have greater impact. As θ increases beyond 2, the rate of utility decrease moderates across all policies.", "doc_id": "1809.02244", "figure_id": "1809.02244_fig_6", "figure_number": 4, "image_path": "data/mineru_output/1809.02244/1809.02244/hybrid_auto/images/1809.02244_page0_fig5.jpg", "caption": "Figure 4. The relative utility of policies for the COMPAS data as a function of the utility parameter $\\theta$ .", "figure_type": "plot", "visual_anchor": "all three curves showing steeper negative slope between utility parameter 1 and 2 on x-axis", "text_evidence": "The latent projection (Verma and Pearl, 1990) of any K stage DAG onto X, S, M, A, Y suffices to identify and estimate the two path-specific effects in question", "query_type": "anomaly_cause"}
{"query_id": "l1_1809.04737_1809.04737_fig_1_537", "query": "Why do the dotted black κ=hinge and dash-dot green κ=square curves diverge after x=0.5 despite both being surrogate functions?", "answer": "Different surrogate functions approximate the indicator function differently. The hinge function grows linearly while the square function grows quadratically in the positive region, leading to divergent behavior after x=0.5.", "doc_id": "1809.04737", "figure_id": "1809.04737_fig_1", "figure_number": 1, "image_path": "data/mineru_output/1809.04737/1809.04737/hybrid_auto/images/1809.04737_page0_fig0.jpg", "caption": "Figure 1: Examples of $\\kappa ( \\cdot )$ and $\\delta ( \\cdot )$ .", "figure_type": "plot", "visual_anchor": "dotted black κ=hinge and dash-dot green κ=square curves diverging after x=0.5", "text_evidence": "Similarly, the indicator function in above equation can be replaced with the surrogate function. The issue here is, two constraints $\\mathbb { R D } ( f ) \\leq \\tau$ and $- \\mathbb { R } \\mathbb { D } ( f ) \\leq \\tau$ are opposite to each other.", "query_type": "comparison_explanation"}
{"query_id": "l1_1809.04737_1809.04737_fig_1_538", "query": "How does the solid red δ=logistic curve's plateau near y=1.0 for x>0.5 relate to fairness constraint satisfaction?", "answer": "The plateau demonstrates the logistic surrogate's bounded growth, which helps in satisfying fairness constraints without extreme penalty values when optimizing the fairness-aware classification objective.", "doc_id": "1809.04737", "figure_id": "1809.04737_fig_1", "figure_number": 1, "image_path": "data/mineru_output/1809.04737/1809.04737/hybrid_auto/images/1809.04737_page0_fig0.jpg", "caption": "Figure 1: Examples of $\\kappa ( \\cdot )$ and $\\delta ( \\cdot )$ .", "figure_type": "plot", "visual_anchor": "solid red δ=logistic curve plateauing near y=1.0 for x>0.5", "text_evidence": "The fairness-aware classification aims to find a classifier that minimizes the empirical loss while satisfying certain fairness constraints. Several fairness notions or definitions are proposed in the literature", "query_type": "value_context"}
{"query_id": "l1_1809.04737_1809.04737_fig_1_539", "query": "What causes the dashed red κ=logistic curve to reach only y≈0.5 at x=2.0 while dotted κ=hinge exceeds y=2.0?", "answer": "The logistic function has a gentler growth rate due to its sigmoid-based nature, while the hinge function grows linearly without bound, resulting in the dramatic difference at x=2.0.", "doc_id": "1809.04737", "figure_id": "1809.04737_fig_1", "figure_number": 1, "image_path": "data/mineru_output/1809.04737/1809.04737/hybrid_auto/images/1809.04737_page0_fig0.jpg", "caption": "Figure 1: Examples of $\\kappa ( \\cdot )$ and $\\delta ( \\cdot )$ .", "figure_type": "plot", "visual_anchor": "dashed red κ=logistic at y≈0.5 versus dotted black κ=hinge above y=2.0 at x=2.0", "text_evidence": "For simplicity, we may want to denote $P ( S = s ^ { + } | \\mathbf { x } )$ by $\\eta ( \\mathbf { x } )$ and $P ( \\bar { S } = s ^ { + } )$ by $p$", "query_type": "comparison_explanation"}
{"query_id": "l1_1809.04737_1809.04737_fig_2_540", "query": "Do the yellow circles at y=1.5 and blue circles at y=-0.5 illustrate maximal conditional risk difference H⁺(η)?", "answer": "Yes, the yellow and blue circles at different y-values demonstrate two classifiers making predictions with different conditional risks. This visualization supports the definition of maximal conditional risk difference H⁺(η) between gender groups.", "doc_id": "1809.04737", "figure_id": "1809.04737_fig_2", "figure_number": 2, "image_path": "data/mineru_output/1809.04737/1809.04737/hybrid_auto/images/1809.04737_page0_fig1.jpg", "caption": "Figure 2: Two classifiers and their predictions.", "figure_type": "plot", "visual_anchor": "yellow circles at y=1.5 and blue circles at y=-0.5", "text_evidence": "We similarly define $H ^ { + } ( \\eta )$ the maximal conditional risk difference, $H _ { \\delta } ^ { + } ( \\eta )$ the maximal conditional $\\delta$ -risk difference, $\\mathbb { R } \\mathbb { D } _ { \\delta } ^ { + }$ the maximal $\\delta$ -risk difference", "query_type": "visual_definition"}
{"query_id": "l1_1809.04737_1809.04737_fig_2_541", "query": "Why do female predictions shift from y=1.5 to y=-0.5 while male predictions remain constant across the two classifiers?", "answer": "The shift demonstrates how different classifiers can produce varying risk differences between gender groups. This pattern is relevant to the constrained optimization problem that seeks fair classifiers satisfying -c₂ ≤ RD(f) ≤ c₁.", "doc_id": "1809.04737", "figure_id": "1809.04737_fig_2", "figure_number": 2, "image_path": "data/mineru_output/1809.04737/1809.04737/hybrid_auto/images/1809.04737_page0_fig1.jpg", "caption": "Figure 2: Two classifiers and their predictions.", "figure_type": "plot", "visual_anchor": "blue circles transitioning from y=1.5 to y=-0.5 across classifiers", "text_evidence": "A fair classifier $f = s i g n ( h )$ that achieves fairness constraint $- c _ { 2 } \\leq \\mathbb { R D } ( f ) \\leq c _ { 1 }$ can be obtained by solving the following constrained optimization", "query_type": "comparison_explanation"}
{"query_id": "l1_1809.04737_1809.04737_fig_2_542", "query": "How does the constant male prediction at y=1.5 relate to the constraint RDκ(h) ≤ ψκ(c₁ - RD⁻) + RDκ⁻?", "answer": "The constant male predictions suggest one group maintains fixed risk while the other varies, which is captured by the RHS constraint being constant for a given dataset. This allows the constrained optimization to remain convex despite varying predictions.", "doc_id": "1809.04737", "figure_id": "1809.04737_fig_2", "figure_number": 2, "image_path": "data/mineru_output/1809.04737/1809.04737/hybrid_auto/images/1809.04737_page0_fig1.jpg", "caption": "Figure 2: Two classifiers and their predictions.", "figure_type": "plot", "visual_anchor": "yellow circles consistently at y=1.5 across both classifiers", "text_evidence": "subject to $\\mathbb { R } \\mathbb { D } _ { \\kappa } ( h ) \\leq \\psi _ { \\kappa } \\big ( c _ { 1 } - \\mathbb { R } \\mathbb { D } ^ { - } \\big ) + \\mathbb { R } \\mathbb { D } _ { \\kappa } ^ { - } ,$ Note that the RHS of above two inequalities are constants for a given dataset. Therefore, the constrained optimization problem is still convex.", "query_type": "value_context"}
{"query_id": "l1_1809.04737_1809.04737_fig_4_543", "query": "Why does the red dotted Our Method curve start at higher empirical loss near 0.295 compared to other methods?", "answer": "Our Method begins at the minimal risk difference point (RD^-), which corresponds to the maximal/minimal risk difference classifiers measured for the datasets. This represents the lower bound of achievable risk differences with corresponding empirical loss.", "doc_id": "1809.04737", "figure_id": "1809.04737_fig_4", "figure_number": 3, "image_path": "data/mineru_output/1809.04737/1809.04737/hybrid_auto/images/1809.04737_page0_fig3.jpg", "caption": "Figure 3: Comparison of fair classifiers.", "figure_type": "plot", "visual_anchor": "Red dotted line starting at approximately 0.295 empirical loss at -0.1 risk difference", "text_evidence": "we build the maximal/minimal risk difference classifiers $f _ { \\mathrm { m i n } }$ , $f _ { \\mathrm { m a x } }$ for both Adult and Dutch datasets, and measure the risk differences they produce, i.e., $\\mathbb { R D } ^ { - } , \\mathbb { R D } ^ { + }$", "query_type": "comparison_explanation"}
{"query_id": "l1_1809.04737_1809.04737_fig_4_544", "query": "Do the converging trajectories of all three methods near 0.195 loss support the claim about large maximal risk differences?", "answer": "Yes, all methods converge near 0.3 risk difference at approximately 0.195 empirical loss, demonstrating the large span between minimal and maximal risk differences. This confirms the measured RD^+ values shown in Table 3.", "doc_id": "1809.04737", "figure_id": "1809.04737_fig_4", "figure_number": 3, "image_path": "data/mineru_output/1809.04737/1809.04737/hybrid_auto/images/1809.04737_page0_fig3.jpg", "caption": "Figure 3: Comparison of fair classifiers.", "figure_type": "plot", "visual_anchor": "Convergence point of solid blue, dashed green, and dotted red lines at approximately 0.195 loss and 0.3 risk difference", "text_evidence": "The results are shown in the first two rows in Table 3. As can be seen, in both datasets we have large maximal and minimal risk differences.", "query_type": "value_context"}
{"query_id": "l1_1809.04737_1809.04737_fig_4_545", "query": "How does the blue X Unconstrained marker at 0.193 loss relate to fairness-aware classifier construction methods?", "answer": "The Unconstrained marker represents a baseline without fairness constraints, achieving slightly lower loss but much higher risk difference (0.37). This illustrates why methods for constructing fairness-aware classifiers are needed to balance accuracy and fairness.", "doc_id": "1809.04737", "figure_id": "1809.04737_fig_4", "figure_number": 3, "image_path": "data/mineru_output/1809.04737/1809.04737/hybrid_auto/images/1809.04737_page0_fig3.jpg", "caption": "Figure 3: Comparison of fair classifiers.", "figure_type": "plot", "visual_anchor": "Blue X marker labeled Unconstrained at approximately 0.193 empirical loss and 0.37 risk difference", "text_evidence": "Many methods have been proposed for constructing fairnessaware classifiers, which can be broadly classified into pre/post-processing and in-processing methods.", "query_type": "value_context"}
{"query_id": "l1_1809.10083_1809.10083_fig_1_546", "query": "Why does the dashed disentangle line connect e2 to ψ if Dec tries to extract all information into e2?", "answer": "The disentanglement term enforces separation between e1 and e2, while ψ makes ẽ1 unreliable for reconstruction, forcing Dec to rely on e2 for extracting information about x.", "doc_id": "1809.10083", "figure_id": "1809.10083_fig_1", "figure_number": 1, "image_path": "data/mineru_output/1809.10083/1809.10083/hybrid_auto/images/1809.10083_page0_fig0.jpg", "caption": "Figure 1: (a) Unsupervised Invariance Induction Framework and (b) Adversarial Model Design", "figure_type": "diagram", "visual_anchor": "dashed line labeled 'disentangle' between e2 and ψ block", "text_evidence": "This is made possible by $\\psi$ , which makes $\\tilde { e } _ { 1 }$ an unreliable source of information for reconstructing $x$ . Moreover, a version of this framework without $\\psi$ can converge to a degenerate solution where $e _ { 1 }$ contains all the information about $x$", "query_type": "comparison_explanation"}
{"query_id": "l1_1809.10083_1809.10083_fig_1_547", "query": "How does the green Pred block receiving e1 support the claim that Pred pulls y-relevant information into e1?", "answer": "The predictor Pred directly receives e1 as input to predict y, creating competitive pressure to pull information relevant to y into e1 while Dec competes to extract all x information into e2.", "doc_id": "1809.10083", "figure_id": "1809.10083_fig_1", "figure_number": 1, "image_path": "data/mineru_output/1809.10083/1809.10083/hybrid_auto/images/1809.10083_page0_fig0.jpg", "caption": "Figure 1: (a) Unsupervised Invariance Induction Framework and (b) Adversarial Model Design", "figure_type": "diagram", "visual_anchor": "green Pred block with arrow from e1 circle", "text_evidence": "The predictor and the decoder are designed to enter into a competition, where P red tries to pull information relevant to $y$ into $e _ { 1 }$ while $D e c$ tries to extract all the information about $x$ into $e _ { 2 }$", "query_type": "visual_definition"}
{"query_id": "l1_1809.10083_1809.10083_fig_1_548", "query": "What role does the split representation e=[e1 e2] shown as gray circles play in achieving unsupervised invariance induction?", "answer": "The split representation disentangles information required for predicting y (in e1) from other unrelated information (in e2), enabling invariance to nuisance factors through competing reconstruction and prediction tasks.", "doc_id": "1809.10083", "figure_id": "1809.10083_fig_1", "figure_number": 1, "image_path": "data/mineru_output/1809.10083/1809.10083/hybrid_auto/images/1809.10083_page0_fig0.jpg", "caption": "Figure 1: (a) Unsupervised Invariance Induction Framework and (b) Adversarial Model Design", "figure_type": "diagram", "visual_anchor": "two gray circles labeled e1 and e2", "text_evidence": "This is achieved by learning a split representation of data as $\\boldsymbol { e } = \\left[ e _ { 1 } e _ { 2 } \\right]$ , su", "query_type": "value_context"}
{"query_id": "l1_1809.10083_1809.10083_fig_5_549", "query": "Why do the blue circles representing front lighting remain clustered together in this t-SNE projection?", "answer": "This is a t-SNE visualization of embedding e₁ labeled by lighting condition, showing that the model successfully groups images with the same front lighting into a coherent cluster in the embedding space.", "doc_id": "1809.10083", "figure_id": "1809.10083_fig_5", "figure_number": 2, "image_path": "data/mineru_output/1809.10083/1809.10083/hybrid_auto/images/1809.10083_page0_fig4.jpg", "caption": "Figure 2: Extended Yale-B – t-SNE visualization of (a) raw data, (b) $e _ { 2 }$ labeled by lighting condition, (c) $e _ { 1 }$ labeled by lighting condition, and (d) $e _ { 1 }$ labeled by subject-ID (numerical markers, not colors).", "figure_type": "plot", "visual_anchor": "blue circles (front lighting category) forming a distinct cluster", "text_evidence": "Figure 2: Extended Yale-B – t-SNE visualization of (a) raw data, (b) $e _ { 2 }$ labeled by lighting condition, (c) $e _ { 1 }$ labeled by lighting condition, and (d) $e _ { 1 }$ labeled by subject-ID (numerical markers, not colors).", "query_type": "visual_definition"}
{"query_id": "l1_1809.10083_1809.10083_fig_5_550", "query": "Does the separation between orange lower-right markers and cyan upper-right markers confirm that embedding e₁ captures lighting variations?", "answer": "Yes, the clear spatial separation between different lighting conditions (lower-right vs upper-right) in the t-SNE visualization demonstrates that embedding e₁ successfully disentangles and represents lighting condition as a distinct factor.", "doc_id": "1809.10083", "figure_id": "1809.10083_fig_5", "figure_number": 2, "image_path": "data/mineru_output/1809.10083/1809.10083/hybrid_auto/images/1809.10083_page0_fig4.jpg", "caption": "Figure 2: Extended Yale-B – t-SNE visualization of (a) raw data, (b) $e _ { 2 }$ labeled by lighting condition, (c) $e _ { 1 }$ labeled by lighting condition, and (d) $e _ { 1 }$ labeled by subject-ID (numerical markers, not colors).", "figure_type": "plot", "visual_anchor": "orange plus signs (lower-right) and cyan stars (upper-right) spatially separated", "text_evidence": "Figure 2 shows t-SNE [15] visualization of raw data and embeddings $e _ { 1 }$ and $e _ { 2 }$ for our model.", "query_type": "comparison_explanation"}
{"query_id": "l1_1809.10083_1809.10083_fig_5_551", "query": "How does the clustering pattern of lime lower-left circles compare to the model's 0.74 accuracy on Chairs?", "answer": "The tight clustering of lower-left lighting conditions demonstrates successful disentanglement in the Yale-B dataset, which is consistent with the model achieving improved performance (Ay=0.74) on the separate Chairs dataset.", "doc_id": "1809.10083", "figure_id": "1809.10083_fig_5", "figure_number": 2, "image_path": "data/mineru_output/1809.10083/1809.10083/hybrid_auto/images/1809.10083_page0_fig4.jpg", "caption": "Figure 2: Extended Yale-B – t-SNE visualization of (a) raw data, (b) $e _ { 2 }$ labeled by lighting condition, (c) $e _ { 1 }$ labeled by lighting condition, and (d) $e _ { 1 }$ labeled by subject-ID (numerical markers, not colors).", "figure_type": "plot", "visual_anchor": "lime circles (lower-left category) showing clustering pattern", "text_evidence": "Table 2: Results on Chairs. High $A _ { y }$ and low $A _ { z }$ are desired. Metric CAI Ours Ay 0.68 0.74 Az 0.69 0.34", "query_type": "value_context"}
{"query_id": "l1_1809.10083_1809.10083_fig_13_552", "query": "Why do the middle column reconstructions from e1 appear blurred compared to the sharp left column originals?", "answer": "The reconstructions from e1 demonstrate the model's ability to handle expected variations that are unobserved or infrequently seen during training, which is achieved through the unsupervised adversarial approach rather than requiring supervised rotation angle information.", "doc_id": "1809.10083", "figure_id": "1809.10083_fig_13", "figure_number": 3, "image_path": "data/mineru_output/1809.10083/1809.10083/hybrid_auto/images/1809.10083_page0_fig12.jpg", "caption": "Figure 3: Reconstruction from $e _ { 1 }$ and $e _ { 2 }$ for (a) Extended Yale B and (b) Chairs. Columns in each block reflect (left to right): real, reconstruction from $e _ { 1 }$ and that from $e _ { 2 }$ .", "figure_type": "example", "visual_anchor": "middle column showing blurred chair reconstructions from e1", "text_evidence": "A popular approach to learn models robust to such expected yet unobserved or infrequently seen (during training) variations is data augmentation through synthetic generation", "query_type": "comparison_explanation"}
{"query_id": "l1_1809.10083_1809.10083_fig_13_553", "query": "Do the rightmost column e2 reconstructions support claims that unsupervised adversarial models outperform baseline ablation versions?", "answer": "Yes, the e2 reconstructions demonstrate the model's learned representation capabilities. The results show that the unsupervised adversarial model performs better than baseline ablations and even outperforms CAI which uses supervised information.", "doc_id": "1809.10083", "figure_id": "1809.10083_fig_13", "figure_number": 3, "image_path": "data/mineru_output/1809.10083/1809.10083/hybrid_auto/images/1809.10083_page0_fig12.jpg", "caption": "Figure 3: Reconstruction from $e _ { 1 }$ and $e _ { 2 }$ for (a) Extended Yale B and (b) Chairs. Columns in each block reflect (left to right): real, reconstruction from $e _ { 1 }$ and that from $e _ { 2 }$ .", "figure_type": "example", "visual_anchor": "rightmost column showing e2 reconstructions with preserved chair structure", "text_evidence": "our unsupervised adversarial model not only performs better than the baseline ablation versions but also outperforms CAI, which uses supervised information about the rotation angle", "query_type": "value_context"}
{"query_id": "l1_1809.10083_1809.10083_fig_13_554", "query": "How do the blurred middle and right columns relate to synthetic generation methods for handling rotation variations?", "answer": "The blurred reconstructions from e1 and e2 represent learned latent representations that capture rotation and viewpoint variations. These demonstrate synthetic generation capabilities ranging from simple operations to more sophisticated approaches for handling expected transformations.", "doc_id": "1809.10083", "figure_id": "1809.10083_fig_13", "figure_number": 3, "image_path": "data/mineru_output/1809.10083/1809.10083/hybrid_auto/images/1809.10083_page0_fig12.jpg", "caption": "Figure 3: Reconstruction from $e _ { 1 }$ and $e _ { 2 }$ for (a) Extended Yale B and (b) Chairs. Columns in each block reflect (left to right): real, reconstruction from $e _ { 1 }$ and that from $e _ { 2 }$ .", "figure_type": "example", "visual_anchor": "middle and right columns showing progressively blurred reconstructions", "text_evidence": "data augmentation through synthetic generation using methods ranging from simple operations [10] like rotation and translation to Generative Adversarial Networks", "query_type": "visual_definition"}
{"query_id": "l1_1809.10083_1809.10083_fig_16_555", "query": "Why does the blue cluster at x=-45.0 show tighter grouping than the red cluster at x=45.0 in this embedding?", "answer": "The visualization shows e₁ labeled by subject-ID with numerical markers. The tighter blue clustering likely reflects better subject-ID separation in this region of the embedding space compared to the more dispersed red region.", "doc_id": "1809.10083", "figure_id": "1809.10083_fig_16", "figure_number": 2, "image_path": "data/mineru_output/1809.10083/1809.10083/hybrid_auto/images/1809.10083_page0_fig15.jpg", "caption": "Table 2: Results on Chairs. High $A _ { y }$ and low $A _ { z }$ are desired.   ", "figure_type": "plot", "visual_anchor": "Blue cluster at x=-45.0 versus red cluster at x=45.0", "text_evidence": "Figure 2 shows t-SNE [15] visualization of raw data and embeddings e₁ and e₂ for our model.", "query_type": "comparison_explanation"}
{"query_id": "l1_1809.10083_1809.10083_fig_16_556", "query": "Does the three-cluster separation at positions -45.0, 0.0, and 45.0 align with the desired high Ay metric of 0.74?", "answer": "Yes, the clear spatial separation of the three colored clusters along the horizontal axis demonstrates effective disentanglement of the y-axis rotation factor, which directly corresponds to achieving the high Ay accuracy of 0.74.", "doc_id": "1809.10083", "figure_id": "1809.10083_fig_16", "figure_number": 2, "image_path": "data/mineru_output/1809.10083/1809.10083/hybrid_auto/images/1809.10083_page0_fig15.jpg", "caption": "Table 2: Results on Chairs. High $A _ { y }$ and low $A _ { z }$ are desired.   ", "figure_type": "plot", "visual_anchor": "Three distinct clusters positioned at -45.0 (blue), 0.0 (green), and 45.0 (red)", "text_evidence": "Table 2: Results on Chairs. High Ay and low Az are desired. Ay: 0.68 (CAI), 0.74 (Ours). Az: 0.69 (CAI), 0.34 (Ours).", "query_type": "value_context"}
{"query_id": "l1_1809.10083_1809.10083_fig_16_557", "query": "How does the green cluster's central position at x=0.0 relate to lighting condition versus subject-ID labeling distinctions?", "answer": "The green cluster represents an intermediate embedding state. Since e₂ is labeled by lighting condition and e₁ by subject-ID, the central positioning at x=0.0 suggests this group captures a distinct categorical feature in the disentangled representation.", "doc_id": "1809.10083", "figure_id": "1809.10083_fig_16", "figure_number": 2, "image_path": "data/mineru_output/1809.10083/1809.10083/hybrid_auto/images/1809.10083_page0_fig15.jpg", "caption": "Table 2: Results on Chairs. High $A _ { y }$ and low $A _ { z }$ are desired.   ", "figure_type": "plot", "visual_anchor": "Green cluster centered at x=0.0", "text_evidence": "Extended Yale-B – t-SNE visualization of (a) raw data, (b) e₂ labeled by lighting condition, (c) e₁ labeled by lighting condition, and (d) e₁ labeled by subject-ID (numerical markers, not colors).", "query_type": "visual_definition"}
{"query_id": "l1_1809.10083_1809.10083_fig_18_558", "query": "Do the well-separated clusters in the visualization of e₁ demonstrate improved representation learning compared to raw data?", "answer": "Yes, the e₁ visualization shows clearer cluster separation compared to the raw data visualization, demonstrating that the model learned more discriminative features for the MNIST-ROT dataset.", "doc_id": "1809.10083", "figure_id": "1809.10083_fig_18", "figure_number": 4, "image_path": "data/mineru_output/1809.10083/1809.10083/hybrid_auto/images/1809.10083_page0_fig17.jpg", "caption": "Figure 4: MNIST-ROT – t-SNE visualization of (a) raw data and (b) $e _ { 1 }$   ", "figure_type": "plot", "visual_anchor": "comparison between overlapping clusters in raw data versus distinct separated clusters in e₁ visualization", "text_evidence": "Figure 4 shows t-SNE visualization of raw MNIST-ROT images and $e _ { 1 }$ learned by our model.", "query_type": "comparison_explanation"}
{"query_id": "l1_1809.10083_1809.10083_fig_18_559", "query": "How does the 0.977 accuracy achieved by the model relate to the cluster quality visible in the e₁ t-SNE plot?", "answer": "The 0.977 accuracy at Θ angle reflects the well-separated and distinct clusters visible in the e₁ t-SNE visualization, indicating the learned representation effectively captures digit identity despite rotation.", "doc_id": "1809.10083", "figure_id": "1809.10083_fig_18", "figure_number": 4, "image_path": "data/mineru_output/1809.10083/1809.10083/hybrid_auto/images/1809.10083_page0_fig17.jpg", "caption": "Figure 4: MNIST-ROT – t-SNE visualization of (a) raw data and (b) $e _ { 1 }$   ", "figure_type": "plot", "visual_anchor": "well-separated colored clusters in e₁ visualization corresponding to digits 0-9", "text_evidence": "Metric Angle CAI Ours B0 B1 Ay Θ 0.958 0.977 0.974 0.972 ±55° 0.826 0.856 0.826 0.829 ±65° 0.662", "query_type": "value_context"}
{"query_id": "l1_1809.10083_1809.10083_fig_18_560", "query": "Why do certain digit clusters like red and purple remain partially overlapped in the e₁ visualization despite rotation invariance training?", "answer": "Some digits share similar structural features even after learning rotation-invariant representations, leading to partial overlap. This reflects inherent visual similarities between certain digit classes that the model cannot fully disambiguate.", "doc_id": "1809.10083", "figure_id": "1809.10083_fig_18", "figure_number": 4, "image_path": "data/mineru_output/1809.10083/1809.10083/hybrid_auto/images/1809.10083_page0_fig17.jpg", "caption": "Figure 4: MNIST-ROT – t-SNE visualization of (a) raw data and (b) $e _ { 1 }$   ", "figure_type": "plot", "visual_anchor": "overlapping red (digit 2) and purple (digit 3) clusters in the central region of e₁ plot", "text_evidence": "Figure 4: MNIST-ROT – t-SNE visualization of (a) raw data and (b) $e _ { 1 }$", "query_type": "anomaly_cause"}
{"query_id": "l1_1809.10083_1809.10083_fig_19_561", "query": "Why do the purple and orange clusters overlap in the top-center despite training on rotations from -45 to +45 degrees?", "answer": "The proposed Unsupervised Adversarial Invariance model was trained on Θ = {0, ±22.5, ±45} but visualized on unseen rotations Θ = {±55}, testing generalization to variations not present in training data.", "doc_id": "1809.10083", "figure_id": "1809.10083_fig_19", "figure_number": 5, "image_path": "data/mineru_output/1809.10083/1809.10083/hybrid_auto/images/1809.10083_page0_fig18.jpg", "caption": "Figure 5: t-SNE visualization of MNIST-ROT $e _ { 1 }$ embedding for the proposed Unsupervised Adversarial Invariance model (a) & (c), and baseline model $B _ { 0 }$ (b) & (d). Models trained on $\\Theta = \\{ 0 , \\pm 2 2 . 5 , \\pm 4 5 \\}$ . Visualization generated for $\\Theta = \\{ \\pm 5 5 \\}$ .", "figure_type": "plot", "visual_anchor": "overlapping purple (label 5) and orange (label 4) clusters in top-center region", "text_evidence": "Models trained on Θ = {0, ±22.5, ±45}. Visualization generated for Θ = {±55}.", "query_type": "anomaly_cause"}
{"query_id": "l1_1809.10083_1809.10083_fig_19_562", "query": "Does the distinct separation of the green cluster in the bottom-left support learning invariance to unobserved rotation angles?", "answer": "Yes, the clear separation demonstrates that the model effectively uses synthetic data augmentation to learn robustness to expected yet unobserved variations, as the visualization shows generalization to ±55 degrees despite training only up to ±45 degrees.", "doc_id": "1809.10083", "figure_id": "1809.10083_fig_19", "figure_number": 5, "image_path": "data/mineru_output/1809.10083/1809.10083/hybrid_auto/images/1809.10083_page0_fig18.jpg", "caption": "Figure 5: t-SNE visualization of MNIST-ROT $e _ { 1 }$ embedding for the proposed Unsupervised Adversarial Invariance model (a) & (c), and baseline model $B _ { 0 }$ (b) & (d). Models trained on $\\Theta = \\{ 0 , \\pm 2 2 . 5 , \\pm 4 5 \\}$ . Visualization generated for $\\Theta = \\{ \\pm 5 5 \\}$ .", "figure_type": "plot", "visual_anchor": "green cluster (label 0) distinctly separated in bottom-left region", "text_evidence": "A popular approach to learn models robust to such expected yet unobserved or infrequently seen (during training) variations is data augmentation through synthetic generation", "query_type": "value_context"}
{"query_id": "l1_1809.10083_1809.10083_fig_19_563", "query": "How does the black cluster's tight grouping in the far right compare to baseline model performance on rotated digits?", "answer": "The tight black cluster (label 9) in the proposed model's embedding suggests superior invariance learning compared to baseline model B₀, demonstrating effective adversarial training for rotation robustness on the MNIST-ROT dataset.", "doc_id": "1809.10083", "figure_id": "1809.10083_fig_19", "figure_number": 5, "image_path": "data/mineru_output/1809.10083/1809.10083/hybrid_auto/images/1809.10083_page0_fig18.jpg", "caption": "Figure 5: t-SNE visualization of MNIST-ROT $e _ { 1 }$ embedding for the proposed Unsupervised Adversarial Invariance model (a) & (c), and baseline model $B _ { 0 }$ (b) & (d). Models trained on $\\Theta = \\{ 0 , \\pm 2 2 . 5 , \\pm 4 5 \\}$ . Visualization generated for $\\Theta = \\{ \\pm 5 5 \\}$ .", "figure_type": "plot", "visual_anchor": "tightly grouped black cluster (label 9) in far right region", "text_evidence": "t-SNE visualization of MNIST-ROT e₁ embedding for the proposed Unsupervised Adversarial Invariance model (a) & (c), and baseline model B₀ (b) & (d)", "query_type": "comparison_explanation"}
{"query_id": "l1_1809.10083_1809.10083_fig_22_564", "query": "Why do the middle column reconstructions preserve digit identity better than the right column reconstructions for rotated inputs?", "answer": "The middle column shows reconstructions from e₁, which likely captures class-relevant features invariant to rotation. The right column from e₂ appears to encode nuisance factors like rotation, making digit identity less clear.", "doc_id": "1809.10083", "figure_id": "1809.10083_fig_22", "figure_number": 6, "image_path": "data/mineru_output/1809.10083/1809.10083/hybrid_auto/images/1809.10083_page0_fig21.jpg", "caption": "Figure 6: MNIST-ROT – reconstruction from $e _ { 1 }$ and $e _ { 2 }$ , (c) e. Columns in each block reflect (left to right): real, reconstruction from $e _ { 1 }$ and that from $e _ { 2 }$ .", "figure_type": "example", "visual_anchor": "middle column reconstructions showing clear digit shapes versus right column showing degraded representations", "text_evidence": "Data is often not available for all possible variations of nuisance factors. A popular approach to learn models robust to such expected yet unobserved or infrequently seen (during training) variations is data augmentation through synthetic generation using methods ranging from simple operations [10] like rotation", "query_type": "comparison_explanation"}
{"query_id": "l1_1809.10083_1809.10083_fig_22_565", "query": "Do the left column rotated originals demonstrate the type of synthetic variation needed when training data lacks rotational diversity?", "answer": "Yes, the left column displays rotated MNIST digits that exemplify synthetic augmentation through rotation operations. This addresses the problem of learning robustness to expected but unobserved variations during training.", "doc_id": "1809.10083", "figure_id": "1809.10083_fig_22", "figure_number": 6, "image_path": "data/mineru_output/1809.10083/1809.10083/hybrid_auto/images/1809.10083_page0_fig21.jpg", "caption": "Figure 6: MNIST-ROT – reconstruction from $e _ { 1 }$ and $e _ { 2 }$ , (c) e. Columns in each block reflect (left to right): real, reconstruction from $e _ { 1 }$ and that from $e _ { 2 }$ .", "figure_type": "example", "visual_anchor": "left column showing real rotated digit samples across all ten blocks", "text_evidence": "A popular approach to learn models robust to such expected yet unobserved or infrequently seen (during training) variations is data augmentation through synthetic generation using methods ranging from simple operations [10] like rotation and translation to Generative Adversarial Networks", "query_type": "value_context"}
{"query_id": "l1_1809.10083_1809.10083_fig_22_566", "query": "How does the three-column reconstruction structure relate to inducing invariance by separating domain information from prediction tasks?", "answer": "The structure separates class-relevant features (e₁, middle column) from nuisance factors (e₂, right column), similar to domain adaptation where prediction must be invariant to domain. This disentanglement enables learning invariant representations.", "doc_id": "1809.10083", "figure_id": "1809.10083_fig_22", "figure_number": 6, "image_path": "data/mineru_output/1809.10083/1809.10083/hybrid_auto/images/1809.10083_page0_fig21.jpg", "caption": "Figure 6: MNIST-ROT – reconstruction from $e _ { 1 }$ and $e _ { 2 }$ , (c) e. Columns in each block reflect (left to right): real, reconstruction from $e _ { 1 }$ and that from $e _ { 2 }$ .", "figure_type": "example", "visual_anchor": "three-column organization showing real, e₁ reconstruction, and e₂ reconstruction across all digit blocks", "text_evidence": "Domain adaptation has been treated as an invariance induction task in recent literature [6, 14] where the goal is to make the prediction task invariant to the \"domain\" information.", "query_type": "comparison_explanation"}
{"query_id": "l1_1810.01943_1810.01943_fig_1_567", "query": "Why does the fair pre-processor trapezoid receive input from training, validation, and testing boxes simultaneously?", "answer": "The fair pre-processor learns from all three dataset splits (training, validation, testing) to transform the original dataset into a fairer version, as fairness transformations require understanding patterns across all data partitions.", "doc_id": "1810.01943", "figure_id": "1810.01943_fig_1", "figure_number": 1, "image_path": "data/mineru_output/1810.01943/1810.01943/hybrid_auto/images/1810.01943_page0_fig0.jpg", "caption": "Figure 1. The fairness pipeline. An example instantiation of this generic pipeline consists of loading data into a dataset object, transforming it into a fairer dataset using a fair pre-processing algorithm, learning a classifier from this transformed dataset, and obtaining predictions from this classifier. Metrics can be calculated on the original, transformed, and predicted datasets as well as between the transformed and predicted datasets. Many other instantiations are also possible.", "figure_type": "diagram", "visual_anchor": "fair pre-processor trapezoid with three incoming arrows from training, validation, and testing boxes", "text_evidence": "An example instantiation of this generic pipeline consists of loading data into a dataset object, transforming it into a fairer dataset using a fair pre-processing algorithm", "query_type": "comparison_explanation"}
{"query_id": "l1_1810.01943_1810.01943_fig_1_568", "query": "Does the fair classifier trapezoid placement between Transformed Dataset and Fair Predicted Dataset support in-processing bias mitigation?", "answer": "Yes, the fair classifier trapezoid positioned in the center receives 'learn' arrows from multiple dataset boxes and applies fairness constraints during model training, which aligns with in-processing techniques that operate during the learning phase.", "doc_id": "1810.01943", "figure_id": "1810.01943_fig_1", "figure_number": 1, "image_path": "data/mineru_output/1810.01943/1810.01943/hybrid_auto/images/1810.01943_page0_fig0.jpg", "caption": "Figure 1. The fairness pipeline. An example instantiation of this generic pipeline consists of loading data into a dataset object, transforming it into a fairer dataset using a fair pre-processing algorithm, learning a classifier from this transformed dataset, and obtaining predictions from this classifier. Metrics can be calculated on the original, transformed, and predicted datasets as well as between the transformed and predicted datasets. Many other instantiations are also possible.", "figure_type": "diagram", "visual_anchor": "fair classifier trapezoid labeled 'fair in-processing' in center of diagram", "text_evidence": "BinaryLabelDatasetMetric examine a single dataset as input (StructuredDataset and BinaryLabelDataset, respectively) and are typically applied in the left half of Figure 1 to either the original dataset or the transformed dataset.", "query_type": "visual_definition"}
{"query_id": "l1_1810.01943_1810.01943_fig_2_569", "query": "Why does the bar at 32-37 exceed 0.20 when the localization approach targets the 43-58 range in older groups?", "answer": "The figure shows the younger (unprivileged) group where the 17-27 range is localized. The 43-58 range applies to the separate older (privileged) group analysis, not shown in this panel.", "doc_id": "1810.01943", "figure_id": "1810.01943_fig_2", "figure_number": 2, "image_path": "data/mineru_output/1810.01943/1810.01943/hybrid_auto/images/1810.01943_page0_fig1.jpg", "caption": "Figure 2. Protected attribute bias localization in (a) younger (unprivileged), and (b) older (privileged) groups in the German Credit dataset. The 17–27 range in the younger group and the 43– 58 range in the older group would be localized by the approach.", "figure_type": "plot", "visual_anchor": "tallest blue bar at age 32-37 reaching approximately 0.25", "text_evidence": "The 17–27 range in the younger group and the 43–58 range in the older group would be localized by the approach.", "query_type": "comparison_explanation"}
{"query_id": "l1_1810.01943_1810.01943_fig_2_570", "query": "Does the horizontal line at 0.14 represent the threshold separating privileged from unprivileged age groups in German Credit?", "answer": "No, the horizontal line likely represents a reference threshold for fairness metrics. The caption indicates this panel shows the younger (unprivileged) group specifically, while privileged refers to the separate older group.", "doc_id": "1810.01943", "figure_id": "1810.01943_fig_2", "figure_number": 2, "image_path": "data/mineru_output/1810.01943/1810.01943/hybrid_auto/images/1810.01943_page0_fig1.jpg", "caption": "Figure 2. Protected attribute bias localization in (a) younger (unprivileged), and (b) older (privileged) groups in the German Credit dataset. The 17–27 range in the younger group and the 43– 58 range in the older group would be localized by the approach.", "figure_type": "plot", "visual_anchor": "horizontal line intersecting at approximately 0.14 on the y-axis", "text_evidence": "Protected attribute bias localization in (a) younger (unprivileged), and (b) older (privileged) groups in the German Credit dataset.", "query_type": "visual_definition"}
{"query_id": "l1_1810.01943_1810.01943_fig_2_571", "query": "How does the increasing trend from 0.05 at 22-27 to 0.25 at 32-37 relate to dataset fairness checks?", "answer": "The increasing ratio of favorable outcomes across age ranges demonstrates bias patterns that the DatasetMetric class computes to check for bias in datasets and models, as part of the fairness analysis framework.", "doc_id": "1810.01943", "figure_id": "1810.01943_fig_2", "figure_number": 2, "image_path": "data/mineru_output/1810.01943/1810.01943/hybrid_auto/images/1810.01943_page0_fig1.jpg", "caption": "Figure 2. Protected attribute bias localization in (a) younger (unprivileged), and (b) older (privileged) groups in the German Credit dataset. The 17–27 range in the younger group and the 43– 58 range in the older group would be localized by the approach.", "figure_type": "plot", "visual_anchor": "blue bars increasing from approximately 0.05 at age 22-27 to 0.25 at age 32-37", "text_evidence": "The Metric class and its subclasses compute various individual and group fairness metrics to check for bias in datasets and models.", "query_type": "comparison_explanation"}
{"query_id": "l1_1810.01943_1810.01943_fig_4_572", "query": "Why does Hartford County's green bar at 0.043 make its DI ratio higher than Middlesex County's?", "answer": "Hartford's Hispanic search rate (0.043) is much closer to its white rate (0.013), creating a smaller disparity ratio. Middlesex's Hispanic rate (0.012) compared to its white rate (0.006) shows different proportions, affecting the DI fairness metric differently.", "doc_id": "1810.01943", "figure_id": "1810.01943_fig_4", "figure_number": 3, "image_path": "data/mineru_output/1810.01943/1810.01943/hybrid_auto/images/1810.01943_page0_fig3.jpg", "caption": "Figure 3. Feature bias localization in the Stanford Open Policing dataset for Connecticut, with county name as the feature and race as the protected attribute. In Hartford County, the ratio of search rates for the unprivileged groups (black and Hispanic) in proportion to the search rate for the privileged group (this ratio is the DI fairness metric) is higher than the same metric in Middlesex County and others. The approach would localize Hartford County.", "figure_type": "plot", "visual_anchor": "Hartford County green bar at 0.043 versus Middlesex County green bar at 0.012", "text_evidence": "In Hartford County, the ratio of search rates for the unprivileged groups (black and Hispanic) in proportion to the search rate for the privileged group (this ratio is the DI fairness metric) is higher than the same metric in Middlesex County and others.", "query_type": "comparison_explanation"}
{"query_id": "l1_1810.01943_1810.01943_fig_4_573", "query": "Does Hartford's orange bar at 0.042 compared to its blue bar support localization as a biased feature?", "answer": "Yes, Hartford's black search rate (0.042) is significantly higher than its white rate (0.013), demonstrating substantial disparity. This disparity pattern makes county_name a feature where bias can be localized, as Hartford exhibits different fairness characteristics.", "doc_id": "1810.01943", "figure_id": "1810.01943_fig_4", "figure_number": 3, "image_path": "data/mineru_output/1810.01943/1810.01943/hybrid_auto/images/1810.01943_page0_fig3.jpg", "caption": "Figure 3. Feature bias localization in the Stanford Open Policing dataset for Connecticut, with county name as the feature and race as the protected attribute. In Hartford County, the ratio of search rates for the unprivileged groups (black and Hispanic) in proportion to the search rate for the privileged group (this ratio is the DI fairness metric) is higher than the same metric in Middlesex County and others. The approach would localize Hartford County.", "figure_type": "plot", "visual_anchor": "Hartford County orange bar at 0.042 versus blue bar at 0.013", "text_evidence": "Feature bias localization in the Stanford Open Policing dataset for Connecticut, with county name as the feature and race as the protected attribute. The approach would localize Hartford County.", "query_type": "value_context"}
{"query_id": "l1_1810.01943_1810.01943_fig_4_574", "query": "How do Middlesex's three bars at 0.006, 0.021, and 0.012 relate to unprivileged group search rates?", "answer": "Middlesex shows black (0.021) and Hispanic (0.012) search rates that are both higher than white rates (0.006), representing the unprivileged groups. These rates demonstrate the disparity between privileged and unprivileged groups that the DI fairness metric captures.", "doc_id": "1810.01943", "figure_id": "1810.01943_fig_4", "figure_number": 3, "image_path": "data/mineru_output/1810.01943/1810.01943/hybrid_auto/images/1810.01943_page0_fig3.jpg", "caption": "Figure 3. Feature bias localization in the Stanford Open Policing dataset for Connecticut, with county name as the feature and race as the protected attribute. In Hartford County, the ratio of search rates for the unprivileged groups (black and Hispanic) in proportion to the search rate for the privileged group (this ratio is the DI fairness metric) is higher than the same metric in Middlesex County and others. The approach would localize Hartford County.", "figure_type": "plot", "visual_anchor": "Middlesex County three bars showing blue at 0.006, orange at 0.021, green at 0.012", "text_evidence": "the ratio of search rates for the unprivileged groups (black and Hispanic) in proportion to the search rate for the privileged group (this ratio is the DI fairness metric)", "query_type": "visual_definition"}
{"query_id": "l1_1810.01943_1810.01943_fig_7_575", "query": "Do the orange bars showing post-transformation values support that the original dataset itself gets transformed?", "answer": "Yes, the orange bars consistently show values closer to the ideal fair values (SPD=0, DI=1) compared to blue bars, demonstrating that pre-processing algorithms transform the original dataset to improve fairness metrics.", "doc_id": "1810.01943", "figure_id": "1810.01943_fig_7", "figure_number": 4, "image_path": "data/mineru_output/1810.01943/1810.01943/hybrid_auto/images/1810.01943_page0_fig6.jpg", "caption": "Figure 4. Statistical Parity Difference (SPD) and Disparate Impact (DI) before (blue bar) and after (orange bar) applying pre-processing algorithms on various datasets for different protected attributes. The dark gray bars indicate the extent of $\\pm 1$ standard deviation. The ideal fair value of SPD is 0 and DI is 1.   ", "figure_type": "plot", "visual_anchor": "Orange bars positioned after blue bars for each dataset-attribute combination", "text_evidence": "For fair pre-processing algorithms, since the original dataset itself gets transformed (see Figure 1), we compute fairness metrics before and after this transformation", "query_type": "value_context"}
{"query_id": "l1_1810.01943_1810.01943_fig_7_576", "query": "Why do the dark gray bars extending from orange bars remain small despite the ideal SPD value being 0?", "answer": "The small dark gray bars indicate low variance (±1 standard deviation) in the fairness metrics after pre-processing, suggesting consistent algorithm performance across runs even when achieving near-ideal SPD values close to 0.", "doc_id": "1810.01943", "figure_id": "1810.01943_fig_7", "figure_number": 4, "image_path": "data/mineru_output/1810.01943/1810.01943/hybrid_auto/images/1810.01943_page0_fig6.jpg", "caption": "Figure 4. Statistical Parity Difference (SPD) and Disparate Impact (DI) before (blue bar) and after (orange bar) applying pre-processing algorithms on various datasets for different protected attributes. The dark gray bars indicate the extent of $\\pm 1$ standard deviation. The ideal fair value of SPD is 0 and DI is 1.   ", "figure_type": "plot", "visual_anchor": "Dark gray bars showing standard deviation extent on orange bars", "text_evidence": "The dark gray bars indicate the extent of $\\pm 1$ standard deviation. The ideal fair value of SPD is 0 and DI is 1.", "query_type": "comparison_explanation"}
{"query_id": "l1_1810.01943_1810.01943_fig_7_577", "query": "How do the blue bar values before transformation relate to computing fairness metrics for pre-processing algorithms?", "answer": "The blue bars represent baseline fairness metrics computed on the original untransformed data, providing the comparison point to evaluate improvement after applying pre-processing algorithms shown by the orange bars.", "doc_id": "1810.01943", "figure_id": "1810.01943_fig_7", "figure_number": 4, "image_path": "data/mineru_output/1810.01943/1810.01943/hybrid_auto/images/1810.01943_page0_fig6.jpg", "caption": "Figure 4. Statistical Parity Difference (SPD) and Disparate Impact (DI) before (blue bar) and after (orange bar) applying pre-processing algorithms on various datasets for different protected attributes. The dark gray bars indicate the extent of $\\pm 1$ standard deviation. The ideal fair value of SPD is 0 and DI is 1.   ", "figure_type": "plot", "visual_anchor": "Blue bars showing initial metric values ranging from approximately 0.2 to 0.8", "text_evidence": "we compute fairness metrics before and after this transformation and present results in Figure 4", "query_type": "value_context"}
{"query_id": "l1_1810.01943_1810.01943_fig_10_578", "query": "Do the green circles at disparate impact 0.8 in the top panel support that optimized pre-processing improves fairness?", "answer": "Yes, the green circles (RF,Optimized pre-processing) show disparate impact closest to the ideal value of 1 in the top panel before mitigation, indicating that optimized pre-processing achieves better fairness metrics.", "doc_id": "1810.01943", "figure_id": "1810.01943_fig_10", "figure_number": 5, "image_path": "data/mineru_output/1810.01943/1810.01943/hybrid_auto/images/1810.01943_page0_fig9.jpg", "caption": "Figure 5. Fairness vs. Balanced Accuracy before (top panel) and after (bottom panel) applying various bias mitigation algorithms. Four different fairness metrics are shown. In most cases two classifiers (Logistic regression - LR or Random forest classifier - RF) were used. The ideal fair value of disparate impact is 1, whereas for all other metrics it is 0. The circles indicate the mean value and bars indicate the extent of $\\pm 1$ standard deviation. Dataset: Adult, Protected attribute: race.", "figure_type": "plot", "visual_anchor": "green circles at disparate impact ~0.8 in top panel", "text_evidence": "The ideal fair value of disparate impact is 1, whereas for all other metrics it is 0.", "query_type": "comparison_explanation"}
{"query_id": "l1_1810.01943_1810.01943_fig_10_579", "query": "How does standard deviation shown by horizontal bars compare between logistic regression and random forest classifiers?", "answer": "The horizontal bars show that standard deviation varies across methods, with some configurations showing wider bars (greater uncertainty) than others. Both LR and RF classifiers were used to evaluate the extent of variation in fairness metrics.", "doc_id": "1810.01943", "figure_id": "1810.01943_fig_10", "figure_number": 5, "image_path": "data/mineru_output/1810.01943/1810.01943/hybrid_auto/images/1810.01943_page0_fig9.jpg", "caption": "Figure 5. Fairness vs. Balanced Accuracy before (top panel) and after (bottom panel) applying various bias mitigation algorithms. Four different fairness metrics are shown. In most cases two classifiers (Logistic regression - LR or Random forest classifier - RF) were used. The ideal fair value of disparate impact is 1, whereas for all other metrics it is 0. The circles indicate the mean value and bars indicate the extent of $\\pm 1$ standard deviation. Dataset: Adult, Protected attribute: race.", "figure_type": "plot", "visual_anchor": "horizontal bars representing ±1 standard deviation across different colored circles", "text_evidence": "In most cases two classifiers (Logistic regression - LR or Random forest classifier - RF) were used.", "query_type": "comparison_explanation"}
{"query_id": "l1_1810.01943_1810.01943_fig_14_580", "query": "Does the gray bar value of 0.59 improve to 0.85 after the adversarial debiasing mitigation described?", "answer": "Yes, the gray 'original' bar shows 0.59, while the teal 'mitigated' bar shows 0.85, indicating improvement after adversarial debiasing mitigation.", "doc_id": "1810.01943", "figure_id": "1810.01943_fig_14", "figure_number": 6, "image_path": "data/mineru_output/1810.01943/1810.01943/hybrid_auto/images/1810.01943_page0_fig13.jpg", "caption": "Figure 6. Graphs from the interactive web experience showing one of the metrics, for one of the datasets, before and after mitigation.", "figure_type": "plot", "visual_anchor": "Gray bar at 0.59 and teal bar at 0.85", "text_evidence": "After adversarial debiasing mitigation   \nFigure 6. Graphs from the interactive web experience showing one of the metrics, for one of the datasets, before and after mitigation.", "query_type": "comparison_explanation"}
{"query_id": "l1_1810.01943_1810.01943_fig_14_581", "query": "How close does the teal mitigated bar at 0.85 come to the Fair threshold shown on the graph?", "answer": "The teal mitigated bar at 0.85 is 0.15 units below the Fair threshold marked at 1.0 on the graph.", "doc_id": "1810.01943", "figure_id": "1810.01943_fig_14", "figure_number": 6, "image_path": "data/mineru_output/1810.01943/1810.01943/hybrid_auto/images/1810.01943_page0_fig13.jpg", "caption": "Figure 6. Graphs from the interactive web experience showing one of the metrics, for one of the datasets, before and after mitigation.", "figure_type": "plot", "visual_anchor": "Teal bar at 0.85 relative to 'Fair' label at y=1.0", "text_evidence": "Graphs from the interactive web experience showing one of the metrics, for one of the datasets, before and after mitigation", "query_type": "value_context"}
{"query_id": "l1_1810.01943_1810.01943_fig_14_582", "query": "Why does the graph display both 0.59 and 0.85 values for the same Disparate Impact metric?", "answer": "The graph shows both values to compare performance before (0.59, gray) and after (0.85, teal) mitigation, demonstrating the effectiveness of the toolkit's mitigation approach.", "doc_id": "1810.01943", "figure_id": "1810.01943_fig_14", "figure_number": 6, "image_path": "data/mineru_output/1810.01943/1810.01943/hybrid_auto/images/1810.01943_page0_fig13.jpg", "caption": "Figure 6. Graphs from the interactive web experience showing one of the metrics, for one of the datasets, before and after mitigation.", "figure_type": "plot", "visual_anchor": "Two bars showing 0.59 (gray) and 0.85 (teal) for same metric", "text_evidence": "convey toolkit richness while still being approachable. In the final design, a short textual introduction to the content of the site, along with direct links to the API documentation and code repository", "query_type": "comparison_explanation"}
{"query_id": "l1_1810.01943_1810.01943_fig_15_583", "query": "How does the BinaryLabelDataset's dual aggregation arrows from StructuredDataset and StandardDataset support the toolbox's transformation workflow?", "answer": "The dual aggregation enables BinaryLabelDataset to accept inputs from both structured and standard formats, allowing the Transformer class to operate on either type. This supports the common task of loading a dataset, understanding outcome disparity, and transforming it to mitigate disparity.", "doc_id": "1810.01943", "figure_id": "1810.01943_fig_15", "figure_number": 7, "image_path": "data/mineru_output/1810.01943/1810.01943/hybrid_auto/images/1810.01943_page0_fig14.jpg", "caption": "Figure 7. Class abstractions for a fair machine learning pipeline, as implemented in AIF360. This figure is meant to provide a visual sense of the class hierarchy, many details and some methods are omitted. For brevity, inherited members and methods are not shown (but overridden ones are) nor are aliases such as recall() for true positive rate(). Some methods are “metametrics” — such as difference(), ratio(), total(), average(), maximum() — that act on other metrics to get, e.g. true positive rate difference(). The metric explainer classes use the same method signatures as the metric classes (not enumerated) but provide further description for the values. The GenericPreProcessing, GenericInProcessing, and GenericPostProcessing are not actual classes but serve as placeholders here for the real bias mitigation algorithms we implemented. Finally, memoize and addmetadata are Python decorator functions that are automatically applied to every function in their respective classes.", "figure_type": "diagram", "visual_anchor": "Two aggregation arrows labeled '1' and '2' pointing from StructuredDataset and StandardDataset to BinaryLabelDataset", "text_evidence": "The example involves the user loading a dataset, splitting it into training and testing partitions, understanding the outcome disparity between two demographic groups, and transforming the dataset to mitigate this disparity.", "query_type": "comparison_explanation"}
{"query_id": "l1_1810.01943_1810.01943_fig_15_584", "query": "Why are GenericPreProcessing, GenericInProcessing, and GenericPostProcessing shown as separate classes when real bias mitigation algorithms exist?", "answer": "These are placeholder classes representing the actual bias mitigation algorithms implemented in AIF360. They serve to illustrate the three-stage processing architecture without enumerating all specific algorithm implementations.", "doc_id": "1810.01943", "figure_id": "1810.01943_fig_15", "figure_number": 7, "image_path": "data/mineru_output/1810.01943/1810.01943/hybrid_auto/images/1810.01943_page0_fig14.jpg", "caption": "Figure 7. Class abstractions for a fair machine learning pipeline, as implemented in AIF360. This figure is meant to provide a visual sense of the class hierarchy, many details and some methods are omitted. For brevity, inherited members and methods are not shown (but overridden ones are) nor are aliases such as recall() for true positive rate(). Some methods are “metametrics” — such as difference(), ratio(), total(), average(), maximum() — that act on other metrics to get, e.g. true positive rate difference(). The metric explainer classes use the same method signatures as the metric classes (not enumerated) but provide further description for the values. The GenericPreProcessing, GenericInProcessing, and GenericPostProcessing are not actual classes but serve as placeholders here for the real bias mitigation algorithms we implemented. Finally, memoize and addmetadata are Python decorator functions that are automatically applied to every function in their respective classes.", "figure_type": "diagram", "visual_anchor": "Three boxes labeled GenericPreProcessing, GenericInProcessing, and GenericPostProcessing with params... notation", "text_evidence": "The GenericPreProcessing, GenericInProcessing, and GenericPostProcessing are not actual classes but serve as placeholders here for the real bias mitigation algorithms we implemented.", "query_type": "visual_definition"}
{"query_id": "l1_1810.01943_1810.01943_fig_15_585", "query": "Does the memoize decorator applied to Metric class methods explain why metric computation efficiency matters for metametrics?", "answer": "Yes, the memoize decorator automatically caches metric results, which is crucial for metametrics like difference() and ratio() that repeatedly call other metrics. This avoids redundant computations when calculating aggregate fairness measures.", "doc_id": "1810.01943", "figure_id": "1810.01943_fig_15", "figure_number": 7, "image_path": "data/mineru_output/1810.01943/1810.01943/hybrid_auto/images/1810.01943_page0_fig14.jpg", "caption": "Figure 7. Class abstractions for a fair machine learning pipeline, as implemented in AIF360. This figure is meant to provide a visual sense of the class hierarchy, many details and some methods are omitted. For brevity, inherited members and methods are not shown (but overridden ones are) nor are aliases such as recall() for true positive rate(). Some methods are “metametrics” — such as difference(), ratio(), total(), average(), maximum() — that act on other metrics to get, e.g. true positive rate difference(). The metric explainer classes use the same method signatures as the metric classes (not enumerated) but provide further description for the values. The GenericPreProcessing, GenericInProcessing, and GenericPostProcessing are not actual classes but serve as placeholders here for the real bias mitigation algorithms we implemented. Finally, memoize and addmetadata are Python decorator functions that are automatically applied to every function in their respective classes.", "figure_type": "diagram", "visual_anchor": "Metric class box showing -memoize() method, connected to DatasetMetric with methods like difference() and ratio()", "text_evidence": "memoize and addmetadata are Python decorator functions that are automatically applied to every function in their respective classes", "query_type": "value_context"}
{"query_id": "l1_1810.01943_1810.01943_fig_16_586", "query": "Why do the green circles at disparate impact 0.75 in both panels maintain balanced accuracy above 0.73 despite the ideal fair value being 1?", "answer": "The green circles represent the Disparate impact remover method, which achieves high balanced accuracy while approaching the ideal fair value of 1 for disparate impact. The method balances fairness and accuracy trade-offs effectively.", "doc_id": "1810.01943", "figure_id": "1810.01943_fig_16", "figure_number": 8, "image_path": "data/mineru_output/1810.01943/1810.01943/hybrid_auto/images/1810.01943_page0_fig15.jpg", "caption": "Figure 8. Fairness vs. Balanced Accuracy before (top panel) and after (bottom panel) applying various bias mitigation algorithms. Four different fairness metrics are shown. In most cases two classifiers (Logistic regression - LR or Random forest classifier - RF) were used. The ideal fair value of disparate impact is 1, whereas for all other metrics it is 0. The circles indicate the mean value and bars indicate the extent of $\\pm 1$ standard deviation. Data set: Adult, Protected attribute: sex.   ", "figure_type": "plot", "visual_anchor": "Green circles positioned at disparate impact ~0.75 in both top and bottom panels, with balanced accuracy above 0.73", "text_evidence": "The ideal fair value of disparate impact is 1, whereas for all other metrics it is 0.", "query_type": "comparison_explanation"}
{"query_id": "l1_1810.01943_1810.01943_fig_16_587", "query": "Do the brown circles near balanced accuracy 0.6 in all bottom panel plots support higher benefit for privileged or unprivileged groups?", "answer": "The brown circles represent Reject option classification, showing balanced accuracy around 0.6. The positive values in equal opportunity difference suggest higher benefit for the unprivileged group, as values greater than 0 indicate this pattern.", "doc_id": "1810.01943", "figure_id": "1810.01943_fig_16", "figure_number": 8, "image_path": "data/mineru_output/1810.01943/1810.01943/hybrid_auto/images/1810.01943_page0_fig15.jpg", "caption": "Figure 8. Fairness vs. Balanced Accuracy before (top panel) and after (bottom panel) applying various bias mitigation algorithms. Four different fairness metrics are shown. In most cases two classifiers (Logistic regression - LR or Random forest classifier - RF) were used. The ideal fair value of disparate impact is 1, whereas for all other metrics it is 0. The circles indicate the mean value and bars indicate the extent of $\\pm 1$ standard deviation. Data set: Adult, Protected attribute: sex.   ", "figure_type": "plot", "visual_anchor": "Brown circles positioned at balanced accuracy ~0.6 across all four bottom panel plots", "text_evidence": "A value of 0 implies both groups have equal benefit, a value less than 0 implies higher benefit for the privileged group and a value greater than 0 implies higher benefit for the unprivileged group.", "query_type": "value_context"}
{"query_id": "l1_1810.01943_1810.01943_fig_16_588", "query": "Why does the purple circle shift from statistical parity difference -0.35 to near 0.3 between top and bottom panels?", "answer": "The purple circle represents Calibrated equal odds postprocessing applied to the classifier. The shift reflects the bias mitigation algorithm's effect on statistical parity, moving the metric closer to the ideal fair value of 0 after applying the postprocessing method.", "doc_id": "1810.01943", "figure_id": "1810.01943_fig_16", "figure_number": 8, "image_path": "data/mineru_output/1810.01943/1810.01943/hybrid_auto/images/1810.01943_page0_fig15.jpg", "caption": "Figure 8. Fairness vs. Balanced Accuracy before (top panel) and after (bottom panel) applying various bias mitigation algorithms. Four different fairness metrics are shown. In most cases two classifiers (Logistic regression - LR or Random forest classifier - RF) were used. The ideal fair value of disparate impact is 1, whereas for all other metrics it is 0. The circles indicate the mean value and bars indicate the extent of $\\pm 1$ standard deviation. Data set: Adult, Protected attribute: sex.   ", "figure_type": "plot", "visual_anchor": "Purple circles in the leftmost plots, positioned at statistical parity difference ~-0.35 (top) and ~0.3 (bottom)", "text_evidence": "Fairness vs. Balanced Accuracy before (top panel) and after (bottom panel) applying various bias mitigation algorithms.", "query_type": "comparison_explanation"}
{"query_id": "l1_1810.01943_1810.01943_fig_18_589", "query": "Why does the green Disparate impact remover achieve balanced accuracy near 0.57 in the bottom panel for the german dataset with age as protected attribute?", "answer": "After applying bias mitigation, the Disparate impact remover trades off accuracy for fairness, reducing from ~0.58 in the top panel to ~0.57 in the bottom panel. This demonstrates the fairness-accuracy tradeoff when mitigating bias on the german dataset with age as the protected attribute.", "doc_id": "1810.01943", "figure_id": "1810.01943_fig_18", "figure_number": 10, "image_path": "data/mineru_output/1810.01943/1810.01943/hybrid_auto/images/1810.01943_page0_fig17.jpg", "caption": "Figure 10. Fairness vs. Balanced Accuracy before (top panel) and after (bottom panel) applying various bias mitigation algorithms. Four different fairness metrics are shown. In most cases two classifiers (Logistic regression - LR or Random forest classifier - RF) were used. The ideal fair value of disparate impact is 1, whereas for all other metrics it is 0. The circles indicate the mean value and bars indicate the extent of $\\pm 1$ standard deviation. Data set: german, Protected attribute: age.   ", "figure_type": "plot", "visual_anchor": "Green circles in bottom panel at balanced accuracy ~0.57", "text_evidence": "Fairness vs. Balanced Accuracy before (top panel) and after (bottom panel) applying various bias mitigation algorithms. Data set: german, Protected attribute: age.", "query_type": "comparison_explanation"}
{"query_id": "l1_1810.01943_1810.01943_fig_18_590", "query": "Do the brown circles at balanced accuracy 0.60 in the bottom right panel support that Reject option classification maintains accuracy after mitigation?", "answer": "Yes, the brown circles representing Reject option classification remain at approximately 0.60 balanced accuracy in the bottom panel, similar to their position in the top panel. This indicates the method maintains relatively stable accuracy after bias mitigation is applied.", "doc_id": "1810.01943", "figure_id": "1810.01943_fig_18", "figure_number": 10, "image_path": "data/mineru_output/1810.01943/1810.01943/hybrid_auto/images/1810.01943_page0_fig17.jpg", "caption": "Figure 10. Fairness vs. Balanced Accuracy before (top panel) and after (bottom panel) applying various bias mitigation algorithms. Four different fairness metrics are shown. In most cases two classifiers (Logistic regression - LR or Random forest classifier - RF) were used. The ideal fair value of disparate impact is 1, whereas for all other metrics it is 0. The circles indicate the mean value and bars indicate the extent of $\\pm 1$ standard deviation. Data set: german, Protected attribute: age.   ", "figure_type": "plot", "visual_anchor": "Brown circles at balanced accuracy ~0.60 in bottom right panel", "text_evidence": "The circles indicate the mean value and bars indicate the extent of ±1 standard deviation.", "query_type": "value_context"}
{"query_id": "l1_1810.01943_1810.01943_fig_18_591", "query": "Why do most methods cluster near disparate impact 0.8 in the top panel when the ideal fair value is 1?", "answer": "Most methods show disparate impact around 0.8 before mitigation, deviating from the ideal value of 1, indicating initial bias. The clustering demonstrates that without bias mitigation, these algorithms exhibit similar levels of unfairness on this fairness metric.", "doc_id": "1810.01943", "figure_id": "1810.01943_fig_18", "figure_number": 10, "image_path": "data/mineru_output/1810.01943/1810.01943/hybrid_auto/images/1810.01943_page0_fig17.jpg", "caption": "Figure 10. Fairness vs. Balanced Accuracy before (top panel) and after (bottom panel) applying various bias mitigation algorithms. Four different fairness metrics are shown. In most cases two classifiers (Logistic regression - LR or Random forest classifier - RF) were used. The ideal fair value of disparate impact is 1, whereas for all other metrics it is 0. The circles indicate the mean value and bars indicate the extent of $\\pm 1$ standard deviation. Data set: german, Protected attribute: age.   ", "figure_type": "plot", "visual_anchor": "Cluster of circles near disparate impact 0.8 in top panel, second column", "text_evidence": "The ideal fair value of disparate impact is 1, whereas for all other metrics it is 0.", "query_type": "anomaly_cause"}
{"query_id": "l1_1810.01943_1810.01943_fig_20_592", "query": "Do the green Disparate impact remover circles at balanced accuracy 0.67-0.68 align with reducing bias to acceptable levels for race?", "answer": "Yes, the green circles show high balanced accuracy (0.67-0.68) across metrics, consistent with the claim that bias against the unprivileged group was reduced to acceptable levels for 1 of 2 previously biased metrics when Optimized Pre-processing was applied to the race attribute.", "doc_id": "1810.01943", "figure_id": "1810.01943_fig_20", "figure_number": 12, "image_path": "data/mineru_output/1810.01943/1810.01943/hybrid_auto/images/1810.01943_page0_fig19.jpg", "caption": "Figure 12. Fairness vs. Balanced Accuracy before (top panel) and after (bottom panel) applying various bias mitigation algorithms. Four different fairness metrics are shown. In most cases two classifiers (Logistic regression - LR or Random forest classifier - RF) were used. The ideal fair value of disparate impact is 1, whereas for all other metrics it is 0. The circles indicate the mean value and bars indicate the extent of $\\pm 1$ standard deviation. Data set: compas, Protected attribute: race.", "figure_type": "plot", "visual_anchor": "Green circles (Disparate impact remover methods) positioned at balanced accuracy 0.67-0.68 in top panel", "text_evidence": "Bias against unprivileged group was reduced to acceptable levels*for1 of 2 previously biased metrics", "query_type": "value_context"}
{"query_id": "l1_1810.01943_1810.01943_fig_20_593", "query": "Why do bottom panel methods cluster around balanced accuracy 0.65 despite the accuracy drop from 82% to 74% after mitigation?", "answer": "The bottom panel shows post-mitigation methods maintain similar balanced accuracy (0.65) because balanced accuracy accounts for both privileged and unprivileged group performance, while the overall accuracy drop reflects the trade-off between fairness and raw prediction performance.", "doc_id": "1810.01943", "figure_id": "1810.01943_fig_20", "figure_number": 12, "image_path": "data/mineru_output/1810.01943/1810.01943/hybrid_auto/images/1810.01943_page0_fig19.jpg", "caption": "Figure 12. Fairness vs. Balanced Accuracy before (top panel) and after (bottom panel) applying various bias mitigation algorithms. Four different fairness metrics are shown. In most cases two classifiers (Logistic regression - LR or Random forest classifier - RF) were used. The ideal fair value of disparate impact is 1, whereas for all other metrics it is 0. The circles indicate the mean value and bars indicate the extent of $\\pm 1$ standard deviation. Data set: compas, Protected attribute: race.", "figure_type": "plot", "visual_anchor": "Bottom panel clustering of multiple colored circles around balanced accuracy 0.65", "text_evidence": "Accuracy after mitigation changed from $8 2 \\%$ to $7 4 \\%$", "query_type": "comparison_explanation"}
{"query_id": "l1_1810.01943_1810.01943_fig_20_594", "query": "Do the four fairness metrics with ideal values at disparate impact 1 and others at 0 explain the horizontal positioning differences?", "answer": "Yes, the horizontal positioning reflects different fairness metric scales: disparate impact centers around 1 (ideal fairness) with values 0.6-1.2, while statistical parity, average odds, and equal opportunity center around 0 (ideal) with ranges approximately -0.4 to 0.1.", "doc_id": "1810.01943", "figure_id": "1810.01943_fig_20", "figure_number": 12, "image_path": "data/mineru_output/1810.01943/1810.01943/hybrid_auto/images/1810.01943_page0_fig19.jpg", "caption": "Figure 12. Fairness vs. Balanced Accuracy before (top panel) and after (bottom panel) applying various bias mitigation algorithms. Four different fairness metrics are shown. In most cases two classifiers (Logistic regression - LR or Random forest classifier - RF) were used. The ideal fair value of disparate impact is 1, whereas for all other metrics it is 0. The circles indicate the mean value and bars indicate the extent of $\\pm 1$ standard deviation. Data set: compas, Protected attribute: race.", "figure_type": "plot", "visual_anchor": "Four columns with different x-axis scales: disparate impact at 0.6-1.2, others at -0.4 to 0.1", "text_evidence": "The ideal fair value of disparate impact is 1, whereas for all other metrics it is 0.", "query_type": "visual_definition"}
{"query_id": "l1_1810.03611_1810.03611_fig_1_595", "query": "Why does the peak at 0.0% contain 10^6 documents when targeted perturbation sets use documents from histogram tails?", "answer": "The targeted (increase, decrease) perturbation sets are constructed from documents whose removals cause the greatest differential bias, located in the tails. The peak at 0.0% represents documents with minimal bias impact, which are not selected for perturbation.", "doc_id": "1810.03611", "figure_id": "1810.03611_fig_1", "figure_number": 1, "image_path": "data/mineru_output/1810.03611/1810.03611/hybrid_auto/images/1810.03611_page0_fig0.jpg", "caption": "Figure 1. Histogram of the approximated differential bias of removal for every document in our NYT setup, considering WEAT1, measured in percent change from the baseline mean.", "figure_type": "plot", "visual_anchor": "Peak at 0.0% reaching 10^6 documents and histogram tails at -0.6% and +0.4%", "text_evidence": "The targeted (increase, decrease) perturbation sets are constructed from the documents whose removals were predicted (by our method) to cause the greatest differential bias, e.g., the documents located in the tails of the histograms in Figure 1.", "query_type": "comparison_explanation"}
{"query_id": "l1_1810.03611_1810.03611_fig_1_596", "query": "How does the standard deviation σ: 0.00430 relate to the visible spread of bars between -0.2% and +0.2%?", "answer": "The σ: 0.00430 (0.430%) indicates most documents fall within approximately ±0.4% differential bias. The histogram confirms this, with the majority of bars concentrated between -0.2% and +0.2%, representing roughly one standard deviation from the mean μ: 0.00001.", "doc_id": "1810.03611", "figure_id": "1810.03611_fig_1", "figure_number": 1, "image_path": "data/mineru_output/1810.03611/1810.03611/hybrid_auto/images/1810.03611_page0_fig0.jpg", "caption": "Figure 1. Histogram of the approximated differential bias of removal for every document in our NYT setup, considering WEAT1, measured in percent change from the baseline mean.", "figure_type": "plot", "visual_anchor": "Text box showing σ: 0.00430 and histogram bars concentrated between -0.2% and +0.2%", "text_evidence": "Histogram of the approximated differential bias of removal for every document in our NYT setup, considering WEAT1, measured in percent change from the baseline mean.", "query_type": "value_context"}
{"query_id": "l1_1810.03611_1810.03611_fig_1_597", "query": "Does the near-zero mean μ: 0.00001 align with the corpus using 75-dimensional vectors for semantic meaning?", "answer": "Yes, the near-zero mean indicates that most document removals have minimal impact on bias in the NYT corpus. The 75-dimensional GloVe vectors are sufficient to capture syntactic and semantic meaning, allowing the method to identify which specific documents in the tails actually influence bias.", "doc_id": "1810.03611", "figure_id": "1810.03611_fig_1", "figure_number": 1, "image_path": "data/mineru_output/1810.03611/1810.03611/hybrid_auto/images/1810.03611_page0_fig0.jpg", "caption": "Figure 1. Histogram of the approximated differential bias of removal for every document in our NYT setup, considering WEAT1, measured in percent change from the baseline mean.", "figure_type": "plot", "visual_anchor": "Text box showing μ: 0.00001 and symmetric distribution centered at 0.0%", "text_evidence": "This first setup consists of a corpus constructed from a Simple English Wikipedia dump (2017- 11-03) (Wikimedia, 2018) using 75-dimensional word vectors. These dimensions are small by the standards of a typical word embedding, but sufficient to start capturing syntactic and semantic meaning.", "query_type": "value_context"}
{"query_id": "l1_1810.03611_1810.03611_fig_2_598", "query": "Why does the vertical dotted line at x≈1.3 represent the baseline unperturbed mean for WEAT1 bias?", "answer": "The vertical dotted line marks the baseline (unperturbed) mean effect size before any perturbation sets were removed from the NYT corpus, serving as the reference point for measuring bias changes.", "doc_id": "1810.03611", "figure_id": "1810.03611_fig_2", "figure_number": 2, "image_path": "data/mineru_output/1810.03611/1810.03611/hybrid_auto/images/1810.03611_page0_fig1.jpg", "caption": "Figure 2. Approximated and ground truth WEAT bias effect size due to the removal of various perturbation sets for our NYT corpus, considering WEAT1. Each point describes the mean effect size of one set; error bars depict one standard deviation; the baseline (unperturbed) mean is shown with a vertical dotted line.", "figure_type": "plot", "visual_anchor": "vertical dotted line at x≈1.3", "text_evidence": "the baseline (unperturbed) mean is shown with a vertical dotted line", "query_type": "visual_definition"}
{"query_id": "l1_1810.03611_1810.03611_fig_2_599", "query": "Does the strong alignment of blue points along the red dashed diagonal support that approximation accuracy exceeds PPMI baseline?", "answer": "The close alignment of data points along the diagonal trend line demonstrates high correlation between approximated and validated effect sizes, confirming the method's accuracy. This visual validation directly relates to the comparison with the PPMI baseline discussed in the subsequent section.", "doc_id": "1810.03611", "figure_id": "1810.03611_fig_2", "figure_number": 2, "image_path": "data/mineru_output/1810.03611/1810.03611/hybrid_auto/images/1810.03611_page0_fig1.jpg", "caption": "Figure 2. Approximated and ground truth WEAT bias effect size due to the removal of various perturbation sets for our NYT corpus, considering WEAT1. Each point describes the mean effect size of one set; error bars depict one standard deviation; the baseline (unperturbed) mean is shown with a vertical dotted line.", "figure_type": "plot", "visual_anchor": "blue data points aligned with red dashed diagonal line", "text_evidence": "We have shown that our method can be used to identify bias-influencing documents and accurately approximate the impact of their removal, but how does it compare to a more", "query_type": "comparison_explanation"}
{"query_id": "l1_1810.03611_1810.03611_fig_2_600", "query": "What explains the error bars varying in length across points from bottom-left to top-right on the plot?", "answer": "Each point describes the mean effect size of one perturbation set, and error bars depict one standard deviation, indicating that different perturbation sets have varying levels of consistency in their bias impact across multiple measurements.", "doc_id": "1810.03611", "figure_id": "1810.03611_fig_2", "figure_number": 2, "image_path": "data/mineru_output/1810.03611/1810.03611/hybrid_auto/images/1810.03611_page0_fig1.jpg", "caption": "Figure 2. Approximated and ground truth WEAT bias effect size due to the removal of various perturbation sets for our NYT corpus, considering WEAT1. Each point describes the mean effect size of one set; error bars depict one standard deviation; the baseline (unperturbed) mean is shown with a vertical dotted line.", "figure_type": "plot", "visual_anchor": "error bars of varying lengths on blue data points", "text_evidence": "Each point describes the mean effect size of one set; error bars depict one standard deviation", "query_type": "anomaly_cause"}
{"query_id": "l1_1810.03611_1810.03611_fig_3_601", "query": "Why do the red and blue diamond means at decrease-1000 align so closely despite the method being an approximation?", "answer": "The method accurately approximates the impact of removing bias-influencing documents. The close alignment demonstrates that the approximation method successfully identifies and predicts the effect of document removal on WEAT effect size.", "doc_id": "1810.03611", "figure_id": "1810.03611_fig_3", "figure_number": 3, "image_path": "data/mineru_output/1810.03611/1810.03611/hybrid_auto/images/1810.03611_page0_fig2.jpg", "caption": "Figure 3. Approximated and ground truth differential bias of removal for every perturbation set. Results for different perturbation sets arranged vertically, named as type - size (number of documents removed). (NYT - WEAT1)", "figure_type": "plot", "visual_anchor": "Red and blue diamond markers at decrease-1000 row, both positioned near x=-0.5", "text_evidence": "We have shown that our method can be used to identify bias-influencing documents and accurately approximate the impact of their removal", "query_type": "comparison_explanation"}
{"query_id": "l1_1810.03611_1810.03611_fig_3_602", "query": "Does the reversal of WEAT effect size from positive to negative at decrease-1000 demonstrate the document removal impact?", "answer": "Yes, the decrease-1000 perturbation shows mean values shifting from baseline-0 (near x=1.0) to negative values (near x=-0.5), confirming that removing a small percentage of articles can reverse the WEAT effect size as stated.", "doc_id": "1810.03611", "figure_id": "1810.03611_fig_3", "figure_number": 3, "image_path": "data/mineru_output/1810.03611/1810.03611/hybrid_auto/images/1810.03611_page0_fig2.jpg", "caption": "Figure 3. Approximated and ground truth differential bias of removal for every perturbation set. Results for different perturbation sets arranged vertically, named as type - size (number of documents removed). (NYT - WEAT1)", "figure_type": "plot", "visual_anchor": "Blue and red diamond markers at decrease-1000 positioned at negative x-values, contrasting with baseline-0 at x≈1.0", "text_evidence": "0 7 % of articles can reverse the WEAT effect size in the New York Times, as is shown in Figure 3, decrease-1000", "query_type": "value_context"}
{"query_id": "l1_1810.03611_1810.03611_fig_3_603", "query": "Do increase perturbation sets above baseline-0 show larger WEAT effect sizes than the baseline near the vertical dotted line?", "answer": "Yes, all increase perturbation sets (increase-100 through increase-10000) show mean values positioned to the right of baseline-0, indicating larger positive WEAT effect sizes ranging from approximately 1.5 to 2.0.", "doc_id": "1810.03611", "figure_id": "1810.03611_fig_3", "figure_number": 3, "image_path": "data/mineru_output/1810.03611/1810.03611/hybrid_auto/images/1810.03611_page0_fig2.jpg", "caption": "Figure 3. Approximated and ground truth differential bias of removal for every perturbation set. Results for different perturbation sets arranged vertically, named as type - size (number of documents removed). (NYT - WEAT1)", "figure_type": "plot", "visual_anchor": "Blue and red diamond markers for increase-100, increase-300, increase-1000, increase-3000, and increase-10000, all positioned at x>1.5, right of baseline-0", "text_evidence": "Results for different perturbation sets arranged vertically, named as type - size (number of documents removed). (NYT - WEAT1)", "query_type": "comparison_explanation"}
{"query_id": "l1_1810.03611_1810.03611_fig_4_604", "query": "Do the black circles showing GloVe effects confirm the strong impact claimed for documents identified by the method?", "answer": "Yes, the black circles show substantial changes in WEAT effect size across perturbation sets, ranging from approximately -1.0 to 2.0, demonstrating the strong impact of identified influential documents on GloVe embeddings.", "doc_id": "1810.03611", "figure_id": "1810.03611_fig_4", "figure_number": 4, "image_path": "data/mineru_output/1810.03611/1810.03611/hybrid_auto/images/1810.03611_page0_fig3.jpg", "caption": "Figure 4. The effects of removing the different perturbation sets (most impactful documents as identified by our method) on the WEAT bias in: our GloVe embeddings, the PPMI representation, and word2vec embeddings with comparable hyper-parameters; error bars represent one standard deviation. (NYT - WEAT1)", "figure_type": "plot", "visual_anchor": "black circles with connecting line spanning from approximately -1.0 to 2.0 WEAT effect size", "text_evidence": "The documents identified as influential by our method clearly have a strong impact on the WEAT effect size in GloVe embeddings.", "query_type": "value_context"}
{"query_id": "l1_1810.03611_1810.03611_fig_4_605", "query": "Why do blue diamonds for PPMI and black circles for GloVe show similar trends when testing identical perturbation sets?", "answer": "Both representations respond to the same influential documents identified by the methodology, showing that these documents affect bias across different embedding types. The similar patterns validate that the identified documents have consistent impact on word association bias.", "doc_id": "1810.03611", "figure_id": "1810.03611_fig_4", "figure_number": 4, "image_path": "data/mineru_output/1810.03611/1810.03611/hybrid_auto/images/1810.03611_page0_fig3.jpg", "caption": "Figure 4. The effects of removing the different perturbation sets (most impactful documents as identified by our method) on the WEAT bias in: our GloVe embeddings, the PPMI representation, and word2vec embeddings with comparable hyper-parameters; error bars represent one standard deviation. (NYT - WEAT1)", "figure_type": "plot", "visual_anchor": "parallel trends of blue diamonds (PPMI) and black circles (GloVe) across all perturbation levels", "text_evidence": "Here we explore how those same documents impact the bias in word2vec embeddings, as well as other bias metrics.", "query_type": "comparison_explanation"}
{"query_id": "l1_1810.03611_1810.03611_fig_4_606", "query": "Does the red line for word2vec at baseline-0 align with comparable hyperparameters used in training these embeddings?", "answer": "Yes, the red line shows word2vec results at baseline-0 near zero WEAT effect, consistent with training using comparable hyperparameters. This provides a controlled comparison point for evaluating how the same perturbation sets affect different embedding methods.", "doc_id": "1810.03611", "figure_id": "1810.03611_fig_4", "figure_number": 4, "image_path": "data/mineru_output/1810.03611/1810.03611/hybrid_auto/images/1810.03611_page0_fig3.jpg", "caption": "Figure 4. The effects of removing the different perturbation sets (most impactful documents as identified by our method) on the WEAT bias in: our GloVe embeddings, the PPMI representation, and word2vec embeddings with comparable hyper-parameters; error bars represent one standard deviation. (NYT - WEAT1)", "figure_type": "plot", "visual_anchor": "red line at baseline-0 position showing WEAT effect size near zero", "text_evidence": "We start by training five word2vec emebeddings with comparable hyperparameters3 for each perturbation set, and measure how their removals affect the bias.", "query_type": "value_context"}
{"query_id": "l1_1810.03611_1810.03611_fig_5_607", "query": "Does the red triangle for astronomy at -0.2 support that removing bias-decreasing documents increases male-associated bias?", "answer": "Yes, the red triangle (bias decreasing removal) shifts astronomy leftward toward the male end compared to baseline, demonstrating that removing these documents increases male association as predicted by the methodology.", "doc_id": "1810.03611", "figure_id": "1810.03611_fig_5", "figure_number": 5, "image_path": "data/mineru_output/1810.03611/1810.03611/hybrid_auto/images/1810.03611_page0_fig4.jpg", "caption": "Figure 5. The effect of removing the 10k most bias increasing and bias decreasing documents as identified by our method on the projection of the target words onto the gender axis vs. unperturbed corpus (base); error bars show one std dev; corpus word frequency noted in parentheses. (NYT - WEAT1)", "figure_type": "plot", "visual_anchor": "red triangle marker for astronomy positioned at approximately -0.2 on the gender axis", "text_evidence": "removing the most influential documents identified by our methodology significantly impacts the WEAT, a metric that has been shown to correlate with known human biases", "query_type": "value_context"}
{"query_id": "l1_1810.03611_1810.03611_fig_5_608", "query": "Why do green markers for poetry and art shift rightward beyond 0.2 while their red counterparts remain near baseline?", "answer": "Removing the 10k most bias-increasing documents (green markers) shifts these arts terms more toward female association, while removing bias-decreasing documents (red) keeps them near baseline, showing opposite directional effects of the two document sets.", "doc_id": "1810.03611", "figure_id": "1810.03611_fig_5", "figure_number": 5, "image_path": "data/mineru_output/1810.03611/1810.03611/hybrid_auto/images/1810.03611_page0_fig4.jpg", "caption": "Figure 5. The effect of removing the 10k most bias increasing and bias decreasing documents as identified by our method on the projection of the target words onto the gender axis vs. unperturbed corpus (base); error bars show one std dev; corpus word frequency noted in parentheses. (NYT - WEAT1)", "figure_type": "plot", "visual_anchor": "green inverted triangle markers for poetry and art positioned beyond 0.2, compared to red triangles near 0.0", "text_evidence": "In Figure 5 we show the baseline projections and compare them to the projections after having removed the 10k most bias increasing and bias decreasing documents", "query_type": "comparison_explanation"}
{"query_id": "l1_1810.03611_1810.03611_fig_5_609", "query": "What explains the contrasting green marker positions for dance at 0.15 versus experiment at 0.4 despite both removing bias-increasing documents?", "answer": "The methodology traces bias origins by measuring change when removing documents. Dance and experiment have different baseline biases and corpus frequencies (136k vs 22k), so removing bias-increasing documents produces different magnitudes of shift toward female association.", "doc_id": "1810.03611", "figure_id": "1810.03611_fig_5", "figure_number": 5, "image_path": "data/mineru_output/1810.03611/1810.03611/hybrid_auto/images/1810.03611_page0_fig4.jpg", "caption": "Figure 5. The effect of removing the 10k most bias increasing and bias decreasing documents as identified by our method on the projection of the target words onto the gender axis vs. unperturbed corpus (base); error bars show one std dev; corpus word frequency noted in parentheses. (NYT - WEAT1)", "figure_type": "plot", "visual_anchor": "green inverted triangle at 0.15 for dance versus 0.4 for experiment", "text_evidence": "We conceptualize the problem as measuring the resulting change in bias when we remove a training document (or small subset of the training corpus), and interpret this as the amount of bias contributed by the document to the overall embedding bias", "query_type": "comparison_explanation"}
{"query_id": "l1_1810.03611_1810.03611_fig_7_610", "query": "Does the peak near 0% and μ value of -0.00236 confirm that most documents have minimal differential bias of removal?", "answer": "Yes, the histogram shows the highest concentration of documents clustered around 0% differential bias, with the mean μ of -0.00236 indicating the distribution is centered very close to zero, confirming minimal average bias impact per document.", "doc_id": "1810.03611", "figure_id": "1810.03611_fig_7", "figure_number": 6, "image_path": "data/mineru_output/1810.03611/1810.03611/hybrid_auto/images/1810.03611_page0_fig6.jpg", "caption": "Figure 6. Histogram of the approximated differential bias of removal for every document in our Wiki setup (top) and NYT setup (bottom), considering WEAT1 (left) and WEAT2 (right), measured in percent change from the corresponding mean baseline bias.", "figure_type": "plot", "visual_anchor": "peak near 0% on x-axis and μ: -0.00236 label in the legend box", "text_evidence": "Histogram of the approximated differential bias of removal for every document in our Wiki setup (top) and NYT setup (bottom), considering WEAT1 (left) and WEAT2 (right), measured in percent change from the corresponding mean baseline bias.", "query_type": "value_context"}
{"query_id": "l1_1810.03611_1810.03611_fig_7_611", "query": "Why does the σ value of 0.96661 indicate low variance despite the histogram extending from -30% to 30%?", "answer": "The σ of 0.96661 represents standard deviation in the original scale, not percentage points. The histogram's spread to ±30% represents extreme outliers, while most documents cluster tightly near zero, resulting in the relatively small standard deviation.", "doc_id": "1810.03611", "figure_id": "1810.03611_fig_7", "figure_number": 6, "image_path": "data/mineru_output/1810.03611/1810.03611/hybrid_auto/images/1810.03611_page0_fig6.jpg", "caption": "Figure 6. Histogram of the approximated differential bias of removal for every document in our Wiki setup (top) and NYT setup (bottom), considering WEAT1 (left) and WEAT2 (right), measured in percent change from the corresponding mean baseline bias.", "figure_type": "plot", "visual_anchor": "σ: 0.96661 label and histogram bars extending to -30% and 30% on x-axis", "text_evidence": "measured in percent change from the corresponding mean baseline bias", "query_type": "anomaly_cause"}
{"query_id": "l1_1810.03611_1810.03611_fig_7_612", "query": "How does the logarithmic y-axis scaling reveal the distribution pattern of the 29344 documents across differential bias ranges?", "answer": "The logarithmic scale shows that while thousands of documents concentrate near 0% bias (peak reaching ~10^4), only single-digit or tens of documents exist at extreme values (±20-30%), revealing a highly concentrated central distribution with rare outliers.", "doc_id": "1810.03611", "figure_id": "1810.03611_fig_7", "figure_number": 6, "image_path": "data/mineru_output/1810.03611/1810.03611/hybrid_auto/images/1810.03611_page0_fig6.jpg", "caption": "Figure 6. Histogram of the approximated differential bias of removal for every document in our Wiki setup (top) and NYT setup (bottom), considering WEAT1 (left) and WEAT2 (right), measured in percent change from the corresponding mean baseline bias.", "figure_type": "plot", "visual_anchor": "y-axis labeled 'Number of Documents' with logarithmic scale from 10^0 to 10^4", "text_evidence": "Histogram of the approximated differential bias of removal for every document in our Wiki setup", "query_type": "visual_definition"}
{"query_id": "l1_1810.03611_1810.03611_fig_13_613", "query": "Does the positive correlation shown by the red dashed line validate that approximated effect sizes predict ground truth WEAT bias changes?", "answer": "Yes, the strong positive correlation indicated by the red dashed line confirms that approximated effect sizes reliably predict ground truth WEAT bias effect sizes due to perturbation set removal.", "doc_id": "1810.03611", "figure_id": "1810.03611_fig_13", "figure_number": 7, "image_path": "data/mineru_output/1810.03611/1810.03611/hybrid_auto/images/1810.03611_page0_fig12.jpg", "caption": "Figure 7. Approximated vs. ground truth WEAT bias effect size due to the removal of each (non-random) perturbation set in Wiki setup (top) and NYT setup (bottom), considering WEAT1 (left) and WEAT2 (right); points plot the means; error bars depict one standard deviation; dashed line shows least squares; the baseline means are shown with vertical dotted lines; correlations in Table 4.", "figure_type": "plot", "visual_anchor": "red dashed least squares line showing positive correlation", "text_evidence": "Approximated vs. ground truth WEAT bias effect size due to the removal of each (non-random) perturbation set", "query_type": "comparison_explanation"}
{"query_id": "l1_1810.03611_1810.03611_fig_13_614", "query": "Why do the rightmost blue points at x≈1.5-1.8 show increased vertical spread compared to the baseline vertical dotted line position?", "answer": "The increased spread at higher validated effect sizes indicates greater variability in approximation accuracy when perturbation removal has larger ground truth impact, as shown by the error bars depicting one standard deviation.", "doc_id": "1810.03611", "figure_id": "1810.03611_fig_13", "figure_number": 7, "image_path": "data/mineru_output/1810.03611/1810.03611/hybrid_auto/images/1810.03611_page0_fig12.jpg", "caption": "Figure 7. Approximated vs. ground truth WEAT bias effect size due to the removal of each (non-random) perturbation set in Wiki setup (top) and NYT setup (bottom), considering WEAT1 (left) and WEAT2 (right); points plot the means; error bars depict one standard deviation; dashed line shows least squares; the baseline means are shown with vertical dotted lines; correlations in Table 4.", "figure_type": "plot", "visual_anchor": "rightmost blue points with larger error bars near vertical dotted baseline line", "text_evidence": "points plot the means; error bars depict one standard deviation; dashed line shows least squares; the baseline means are shown with vertical dotted lines", "query_type": "anomaly_cause"}
{"query_id": "l1_1810.03611_1810.03611_fig_13_615", "query": "How does the leftmost blue point at approximately (-0.5, -0.5) relate to WEAT1 and WEAT2 measurements in both Wiki and NYT setups?", "answer": "This point represents one of the non-random perturbation sets tested across both Wiki and NYT setups for either WEAT1 or WEAT2, showing negative bias effect sizes in both approximated and validated measurements.", "doc_id": "1810.03611", "figure_id": "1810.03611_fig_13", "figure_number": 7, "image_path": "data/mineru_output/1810.03611/1810.03611/hybrid_auto/images/1810.03611_page0_fig12.jpg", "caption": "Figure 7. Approximated vs. ground truth WEAT bias effect size due to the removal of each (non-random) perturbation set in Wiki setup (top) and NYT setup (bottom), considering WEAT1 (left) and WEAT2 (right); points plot the means; error bars depict one standard deviation; dashed line shows least squares; the baseline means are shown with vertical dotted lines; correlations in Table 4.", "figure_type": "plot", "visual_anchor": "leftmost blue point at coordinates approximately (-0.5, -0.5)", "text_evidence": "WEAT bias effect size due to the removal of each (non-random) perturbation set in Wiki setup (top) and NYT setup (bottom), considering WEAT1 (left) and WEAT2 (right)", "query_type": "value_context"}
{"query_id": "l1_1810.03611_1810.03611_fig_15_616", "query": "Do the blue and red diamonds cluster near the vertical dotted baseline for every non-random perturbation set?", "answer": "No, the approximation means (blue diamonds) and ground truth means (red diamonds) span the entire range from decrease-1000 to increase-1000, showing substantial deviation from the baseline dotted line at x≈0 across different perturbation sets.", "doc_id": "1810.03611", "figure_id": "1810.03611_fig_15", "figure_number": 8, "image_path": "data/mineru_output/1810.03611/1810.03611/hybrid_auto/images/1810.03611_page0_fig14.jpg", "caption": "Figure 8. Approximated and ground truth differential bias of removal for every (non-random) perturbation set in Wiki setup (top) and NYT setup (bottom), considering WEAT1 (left) and WEAT2 (right); the baseline means are shown with vertical dotted lines", "figure_type": "plot", "visual_anchor": "vertical dotted line at x≈0 and colored diamonds spanning all y-axis levels", "text_evidence": "Approximated and ground truth differential bias of removal for every (non-random) perturbation set", "query_type": "comparison_explanation"}
{"query_id": "l1_1810.03611_1810.03611_fig_15_617", "query": "Why do red ground truth dots scatter more widely than blue approximation dots at the increase-1000 level?", "answer": "At the increase-1000 level, the red ground truth points extend from approximately x=1.0 to x=2.0, showing greater horizontal dispersion than the more tightly clustered blue approximation points, suggesting approximation methods may underestimate variance in extreme perturbation scenarios.", "doc_id": "1810.03611", "figure_id": "1810.03611_fig_15", "figure_number": 8, "image_path": "data/mineru_output/1810.03611/1810.03611/hybrid_auto/images/1810.03611_page0_fig14.jpg", "caption": "Figure 8. Approximated and ground truth differential bias of removal for every (non-random) perturbation set in Wiki setup (top) and NYT setup (bottom), considering WEAT1 (left) and WEAT2 (right); the baseline means are shown with vertical dotted lines", "figure_type": "plot", "visual_anchor": "red and blue dots at increase-1000 y-axis level", "text_evidence": "considering WEAT1 (left) and WEAT2 (right); the baseline means are shown with vertical dotted lines", "query_type": "anomaly_cause"}
{"query_id": "l1_1810.03611_1810.03611_fig_15_618", "query": "Does the concentration of both blue and red points near x=0 at baseline-0 validate the WEAT effect size measurement?", "answer": "Yes, at the baseline-0 level, both approximation (blue) and ground truth (red) points cluster tightly around x=0, aligning with the vertical dotted baseline mean and confirming minimal bias change when no perturbation is applied.", "doc_id": "1810.03611", "figure_id": "1810.03611_fig_15", "figure_number": 8, "image_path": "data/mineru_output/1810.03611/1810.03611/hybrid_auto/images/1810.03611_page0_fig14.jpg", "caption": "Figure 8. Approximated and ground truth differential bias of removal for every (non-random) perturbation set in Wiki setup (top) and NYT setup (bottom), considering WEAT1 (left) and WEAT2 (right); the baseline means are shown with vertical dotted lines", "figure_type": "plot", "visual_anchor": "clustering of blue and red dots at baseline-0 row near x=0", "text_evidence": "Approximated and ground truth differential bias of removal for every (non-random) perturbation set in Wiki setup", "query_type": "value_context"}
{"query_id": "l1_1810.03611_1810.03611_fig_18_619", "query": "Does the diagonal scatter pattern from bottom-left to top-right with r²=0.725 validate WEAT measurement across NYT GloVe embeddings?", "answer": "Yes, the strong positive correlation (r²=0.725) demonstrates that WEAT bias measured in NYT GloVe embeddings correlates consistently with the corpus' PPMI representation across 2000 randomly generated word sets.", "doc_id": "1810.03611", "figure_id": "1810.03611_fig_18", "figure_number": 9, "image_path": "data/mineru_output/1810.03611/1810.03611/hybrid_auto/images/1810.03611_page0_fig17.jpg", "caption": "Figure 9. The correlation of the WEAT as measured in our NYT GloVe embeddings versus the corpus’ PPMI representation in 2000 randomly generated word sets, $r ^ { 2 } = 0 . 7 2 5$ (left); versus when measured in word2vec embeddings with comparable hyper-parameters, $r ^ { 2 } = 0 . 8 0 { \\dot { 3 } }$ (right).", "figure_type": "plot", "visual_anchor": "diagonal scatter pattern with r²=0.725", "text_evidence": "The correlation of the WEAT as measured in our NYT GloVe embeddings versus the corpus' PPMI representation in 2000 randomly generated word sets, r²=0.725", "query_type": "value_context"}
{"query_id": "l1_1810.03611_1810.03611_fig_18_620", "query": "Why would the scatter correlation at r²=0.725 be considered weaker than word2vec's comparable measurement?", "answer": "The r²=0.725 correlation is weaker than word2vec's r²=0.803 when measured with comparable hyper-parameters, suggesting that word2vec embeddings show more consistent bias patterns with the underlying PPMI representation than GloVe embeddings.", "doc_id": "1810.03611", "figure_id": "1810.03611_fig_18", "figure_number": 9, "image_path": "data/mineru_output/1810.03611/1810.03611/hybrid_auto/images/1810.03611_page0_fig17.jpg", "caption": "Figure 9. The correlation of the WEAT as measured in our NYT GloVe embeddings versus the corpus’ PPMI representation in 2000 randomly generated word sets, $r ^ { 2 } = 0 . 7 2 5$ (left); versus when measured in word2vec embeddings with comparable hyper-parameters, $r ^ { 2 } = 0 . 8 0 { \\dot { 3 } }$ (right).", "figure_type": "plot", "visual_anchor": "r²=0.725 correlation coefficient", "text_evidence": "versus when measured in word2vec embeddings with comparable hyper-parameters, r²=0.803", "query_type": "comparison_explanation"}
{"query_id": "l1_1810.03611_1810.03611_fig_18_621", "query": "How does the positive correlation in the scatter plot relate to identifying bias-influencing documents dated January 1987 to June 2007?", "answer": "The positive correlation validates that WEAT can reliably measure bias in embeddings trained on the corpus, enabling the identification of the 50 most WEAT1 bias-influencing documents from the NYT dataset spanning those dates.", "doc_id": "1810.03611", "figure_id": "1810.03611_fig_18", "figure_number": 9, "image_path": "data/mineru_output/1810.03611/1810.03611/hybrid_auto/images/1810.03611_page0_fig17.jpg", "caption": "Figure 9. The correlation of the WEAT as measured in our NYT GloVe embeddings versus the corpus’ PPMI representation in 2000 randomly generated word sets, $r ^ { 2 } = 0 . 7 2 5$ (left); versus when measured in word2vec embeddings with comparable hyper-parameters, $r ^ { 2 } = 0 . 8 0 { \\dot { 3 } }$ (right).", "figure_type": "plot", "visual_anchor": "positive correlation scatter pattern", "text_evidence": "The below documents were identified to be the 50 most WEAT1 bias influencing documents in our NYT setup. Publication dates range from January 1, 1987 to June 19, 2007.", "query_type": "value_context"}
{"query_id": "l1_1810.03993_1810.03993_fig_1_622", "query": "Why does old-male reach the highest false positive rate near 0.08 when analyzing public figures only?", "answer": "The dataset is based on faces and annotations of public figures (celebrities), which may introduce demographic biases. The old-male group shows the highest FPR, suggesting potential bias in how the model performs across different demographic segments.", "doc_id": "1810.03993", "figure_id": "1810.03993_fig_1", "figure_number": null, "image_path": "data/mineru_output/1810.03993/1810.03993/hybrid_auto/images/1810.03993_page0_fig0.jpg", "caption": "", "figure_type": "plot", "visual_anchor": "Green dot at old-male row positioned at approximately 0.08 on x-axis", "text_evidence": "Faces and annotations based on public figures (celebrities). No new information is inferred or annotated.", "query_type": "anomaly_cause"}
{"query_id": "l1_1810.03993_1810.03993_fig_1_623", "query": "Does the 0.02 difference between old-male and young-female false positive rates indicate demographic performance variation?", "answer": "Yes, the approximately 0.02 difference between old-male (around 0.08) and young-female (around 0.06) suggests notable demographic performance variation, which is an important ethical consideration when using celebrity-based annotations.", "doc_id": "1810.03993", "figure_id": "1810.03993_fig_1", "figure_number": null, "image_path": "data/mineru_output/1810.03993/1810.03993/hybrid_auto/images/1810.03993_page0_fig0.jpg", "caption": "", "figure_type": "plot", "visual_anchor": "Comparison between old-male green dot at 0.08 and young-female green dot at 0.06", "text_evidence": "Faces and annotations based on public figures (celebrities). No new information is inferred or annotated.", "query_type": "comparison_explanation"}
{"query_id": "l1_1810.03993_1810.03993_fig_2_624", "query": "Why do the error bars for old-male extend nearly to 0.12 despite the model card goal of highlighting disproportionate errors?", "answer": "The wide error bars for old-male demonstrate metric variation that the model card uses to highlight errors falling disproportionately on some groups, consistent with mathematical fairness notions.", "doc_id": "1810.03993", "figure_id": "1810.03993_fig_2", "figure_number": 2, "image_path": "data/mineru_output/1810.03993/1810.03993/hybrid_auto/images/1810.03993_page0_fig1.jpg", "caption": "Figure 2: Example Model Card for a smile detector trained and evaluated on the CelebA dataset.", "figure_type": "plot", "visual_anchor": "old-male error bars extending from approximately 0.09 to 0.12", "text_evidence": "Quantitative analyses should demonstrate the metric variation (e.g., with error bars), as discussed in Section 4.4 and visualized in Figure 2.", "query_type": "comparison_explanation"}
{"query_id": "l1_1810.03993_1810.03993_fig_2_625", "query": "Does the absence of race-specific categories in the y-axis labels reflect the evaluation dataset limitation about skin type annotations?", "answer": "Yes, the model card explicitly notes it does not capture race or skin type, which has been reported as a source of disproportionate errors, indicating a limitation in the evaluation approach.", "doc_id": "1810.03993", "figure_id": "1810.03993_fig_2", "figure_number": 2, "image_path": "data/mineru_output/1810.03993/1810.03993/hybrid_auto/images/1810.03993_page0_fig1.jpg", "caption": "Figure 2: Example Model Card for a smile detector trained and evaluated on the CelebA dataset.", "figure_type": "plot", "visual_anchor": "y-axis showing only gender and age categories without race or skin type", "text_evidence": "Does not capture race or skin type, which has been reported as a source of disproportionate errors [5].", "query_type": "anomaly_cause"}
{"query_id": "l1_1810.03993_1810.03993_fig_2_626", "query": "How does the binary male-female categorization visible in the demographic groups relate to the stated evaluation spectrum concern?", "answer": "The binary gender classes shown as male/female reflect a limitation, as further work is needed to evaluate across a spectrum of genders beyond this binary representation.", "doc_id": "1810.03993", "figure_id": "1810.03993_fig_2", "figure_number": 2, "image_path": "data/mineru_output/1810.03993/1810.03993/hybrid_auto/images/1810.03993_page0_fig1.jpg", "caption": "Figure 2: Example Model Card for a smile detector trained and evaluated on the CelebA dataset.", "figure_type": "plot", "visual_anchor": "demographic labels showing male, female, old-male, old-female, young-male, young-female categories", "text_evidence": "Given gender classes are binary (male/not male), which we include as male/female. Further work needed to evaluate across a spectrum of genders.", "query_type": "value_context"}
{"query_id": "l1_1810.03993_1810.03993_fig_6_627", "query": "Do the bisexual, homosexual, and heterosexual bars all reaching approximately 0.97 AUC demonstrate Perspective API's toxicity detection performance?", "answer": "Yes, the bars show Version 5 of Perspective API's TOXICITY classifier achieves consistently high AUC scores across different sexual orientation identity groups, indicating relatively balanced performance for toxicity detection across these demographic categories.", "doc_id": "1810.03993", "figure_id": "1810.03993_fig_6", "figure_number": 3, "image_path": "data/mineru_output/1810.03993/1810.03993/hybrid_auto/images/1810.03993_page0_fig5.jpg", "caption": "Figure 3: Example Model Card for two versions of Perspective API’s toxicity detector.", "figure_type": "plot", "visual_anchor": "Middle group of three bars (bisexual, homosexual, heterosexual) all at approximately 0.97 AUC", "text_evidence": "Our second example provides a model card for Perspective API's TOXICITY classifier built to detect 'toxicity' in text [32], and is presented in Figure 3.", "query_type": "value_context"}
{"query_id": "l1_1810.03993_1810.03993_fig_6_628", "query": "Why do the lesbian, gay, queer, and straight bars show nearly identical heights around 0.97 despite representing different identity groups?", "answer": "The uniform performance across these identity groups suggests Version 5 of the toxicity detector was designed or trained to maintain consistent detection capabilities across different sexual orientations, reflecting the model card's documentation of performance across demographic slices.", "doc_id": "1810.03993", "figure_id": "1810.03993_fig_6", "figure_number": 3, "image_path": "data/mineru_output/1810.03993/1810.03993/hybrid_auto/images/1810.03993_page0_fig5.jpg", "caption": "Figure 3: Example Model Card for two versions of Perspective API’s toxicity detector.", "figure_type": "plot", "visual_anchor": "First group of four bars (lesbian, gay, queer, straight) all at approximately 0.97 height", "text_evidence": "We provide two example model cards in Section 5: A smiling detection model trained on the CelebA dataset [36] (Figure 2), and a public toxicity detection model [32] (Figure 3).", "query_type": "comparison_explanation"}
{"query_id": "l1_1810.03993_1810.03993_fig_6_629", "query": "Does the black bar reaching approximately 0.97 while white reaches 0.96 indicate racial performance parity in toxicity classification?", "answer": "The minimal difference between the black and white bars suggests near-parity in the toxicity detector's performance across these racial groups, demonstrating relatively balanced model behavior as documented in the model card example.", "doc_id": "1810.03993", "figure_id": "1810.03993_fig_6", "figure_number": 3, "image_path": "data/mineru_output/1810.03993/1810.03993/hybrid_auto/images/1810.03993_page0_fig5.jpg", "caption": "Figure 3: Example Model Card for two versions of Perspective API’s toxicity detector.", "figure_type": "plot", "visual_anchor": "Fourth group showing black bar at ~0.97 and white bar at ~0.96", "text_evidence": "Example Model Card for two versions of Perspective API's toxicity detector.", "query_type": "comparison_explanation"}
{"query_id": "l1_1810.08683_1810.08683_fig_1_630", "query": "Why do MTL red circles cluster near zero Normalized EOI despite the Adult dataset containing 45222 instances?", "answer": "The MTL approach appears to achieve lower normalized EOI values across different configurations, suggesting multi-task learning may reduce bias in the income prediction task across the large Adult dataset population.", "doc_id": "1810.08683", "figure_id": "1810.08683_fig_1", "figure_number": 2, "image_path": "data/mineru_output/1810.08683/1810.08683/hybrid_auto/images/1810.08683_page0_fig0.jpg", "caption": "Figure 2: Adult dataset: complete results set for Gender (text close to the symbols in plot are P, D, F, and S).", "figure_type": "plot", "visual_anchor": "Red circle markers (MTL) positioned at bottom of plot with Normalized EOI values near 0-0.2", "text_evidence": "The Adult dataset contains 14 features concerning demographic characteristics of 45222 instances (32561 for training and 12661 for testing), 2 features, Gender (G) and Race (R), can be considered sensitive.", "query_type": "anomaly_cause"}
{"query_id": "l1_1810.08683_1810.08683_fig_1_631", "query": "Does the blue square at coordinates approximately (0.05, 0.73) support that single-task learning handles Gender sensitivity differently?", "answer": "Yes, the high Normalized EOI value of approximately 0.73 for this STL/ITL configuration suggests single-task approaches may exhibit greater bias when Gender is the sensitive feature being analyzed.", "doc_id": "1810.08683", "figure_id": "1810.08683_fig_1", "figure_number": 2, "image_path": "data/mineru_output/1810.08683/1810.08683/hybrid_auto/images/1810.08683_page0_fig0.jpg", "caption": "Figure 2: Adult dataset: complete results set for Gender (text close to the symbols in plot are P, D, F, and S).", "figure_type": "plot", "visual_anchor": "Blue square marker (STL/ITL) at top-left region with Normalized EOI ~0.73 and Averaged Group Error ~0.05", "text_evidence": "2 features, Gender (G) and Race (R), can be considered sensitive. The task is to predict if a person has an income per year that is more (or less) than 50 000$.", "query_type": "value_context"}
{"query_id": "l1_1810.08683_1810.08683_fig_1_632", "query": "Which approach shows lower Averaged Group Error at high Normalized EOI, given that text labels indicate P, D, F, S configurations?", "answer": "STL/ITL (blue squares) shows lower Averaged Group Error values (0.05-0.2) at high Normalized EOI (0.7-1.0) compared to MTL, suggesting single-task methods achieve better group accuracy when normalized EOI is high.", "doc_id": "1810.08683", "figure_id": "1810.08683_fig_1", "figure_number": 2, "image_path": "data/mineru_output/1810.08683/1810.08683/hybrid_auto/images/1810.08683_page0_fig0.jpg", "caption": "Figure 2: Adult dataset: complete results set for Gender (text close to the symbols in plot are P, D, F, and S).", "figure_type": "plot", "visual_anchor": "Blue squares in upper-left region (low x-axis, high y-axis) versus red circles distributed across higher x-axis values", "text_evidence": "complete results set for Gender (text close to the symbols in plot are P, D, F, and S)", "query_type": "comparison_explanation"}
{"query_id": "l1_1810.08683_1810.08683_fig_2_633", "query": "Do the red MTL circles at labels 0001 and 0000 confirm the confusion matrices computed on the test set?", "answer": "Yes, the red MTL circles at 0001 (Normalized EOd ~0.9) and 0000 (Normalized EOd ~0.7) represent specific race prediction configurations that align with the confusion matrix results reported for the test set.", "doc_id": "1810.08683", "figure_id": "1810.08683_fig_2", "figure_number": 3, "image_path": "data/mineru_output/1810.08683/1810.08683/hybrid_auto/images/1810.08683_page0_fig1.jpg", "caption": "Figure 3: Adult dataset: complete results set for Race (text close to the symbols in plot are P, D, F, and S).", "figure_type": "plot", "visual_anchor": "Red circles labeled MTL at positions 0001 (Normalized EOd ~0.9, Averaged Group Error ~0.65) and 0000 (Normalized EOd ~0.7, Averaged Group Error ~0.8)", "text_evidence": "Table 3 and Table 4 report the confusion matrices computed on the test set.", "query_type": "value_context"}
{"query_id": "l1_1810.08683_1810.08683_fig_2_634", "query": "Why do blue STL/ITL squares at 0111 cluster lower in Normalized EOd than red MTL circles at 1001?", "answer": "The blue STL/ITL squares at 0111 (Normalized EOd ~0.4) show better fairness metrics than red MTL circles at 1001 (Normalized EOd ~0.55), likely because Random Forests predict Race differently depending on the combination of sensitive features P, D, F, and S.", "doc_id": "1810.08683", "figure_id": "1810.08683_fig_2", "figure_number": 3, "image_path": "data/mineru_output/1810.08683/1810.08683/hybrid_auto/images/1810.08683_page0_fig1.jpg", "caption": "Figure 3: Adult dataset: complete results set for Race (text close to the symbols in plot are P, D, F, and S).", "figure_type": "plot", "visual_anchor": "Blue squares labeled STL/ITL at 0111 (Normalized EOd ~0.4) versus red circles labeled MTL at 1001 (Normalized EOd ~0.55)", "text_evidence": "Adult dataset: complete results set for Race (text close to the symbols in plot are P, D, F, and S).", "query_type": "comparison_explanation"}
{"query_id": "l1_1810.08683_1810.08683_fig_2_635", "query": "What explains the high Normalized EOd of red circle 0001 given Random Forests predict Race from nonsensitive features?", "answer": "The red MTL circle at 0001 shows high Normalized EOd (~0.9) because this specific combination of protected attributes (P, D, F, S) leads to higher equalized odds disparity when Random Forests predict Race from other nonsensitive features in the Adult dataset.", "doc_id": "1810.08683", "figure_id": "1810.08683_fig_2", "figure_number": 3, "image_path": "data/mineru_output/1810.08683/1810.08683/hybrid_auto/images/1810.08683_page0_fig1.jpg", "caption": "Figure 3: Adult dataset: complete results set for Race (text close to the symbols in plot are P, D, F, and S).", "figure_type": "plot", "visual_anchor": "Red circle labeled MTL at 0001 with Normalized EOd ~0.9 and Averaged Group Error ~0.65", "text_evidence": "confusion matrices in percentage (true class in columns and predicted classes in rows) obtained by predicting Gender and Race from the other nonsensitive features using Random Forests.", "query_type": "anomaly_cause"}
{"query_id": "l1_1810.08683_1810.08683_fig_3_636", "query": "Do the red MTL circles at coordinates near 0.8 averaged error support the Random Forests method used for predicting from nonsensitive features?", "answer": "The red MTL circles at high averaged error (around 0.8-1.0) reflect poor group fairness, consistent with using Random Forests to predict Gender and Race from other features, as shown in the confusion matrices.", "doc_id": "1810.08683", "figure_id": "1810.08683_fig_3", "figure_number": 4, "image_path": "data/mineru_output/1810.08683/1810.08683/hybrid_auto/images/1810.08683_page0_fig2.jpg", "caption": "Figure 4: Adult dataset: complete results set for Gender+Race (text close to the symbols in plot are P, D, F, and S).", "figure_type": "plot", "visual_anchor": "Red MTL circles at x≈0.8-1.0", "text_evidence": "confusion matrices in percentage (true class in columns and predicted classes in rows) obtained by predicting Gender and Race from the other nonsensitive features using Random Forests", "query_type": "value_context"}
{"query_id": "l1_1810.08683_1810.08683_fig_3_637", "query": "Why do blue STL/ITL squares cluster at lower normalized EOd values than red MTL circles for the same binary label patterns?", "answer": "Blue STL/ITL squares achieve lower normalized EOd (better fairness) than red MTL circles, suggesting single-task or independent learning approaches yield more equitable outcomes across protected groups than multi-task learning for Gender+Race combinations.", "doc_id": "1810.08683", "figure_id": "1810.08683_fig_3", "figure_number": 4, "image_path": "data/mineru_output/1810.08683/1810.08683/hybrid_auto/images/1810.08683_page0_fig2.jpg", "caption": "Figure 4: Adult dataset: complete results set for Gender+Race (text close to the symbols in plot are P, D, F, and S).", "figure_type": "plot", "visual_anchor": "Blue squares at lower y-values compared to red circles with similar x-coordinates", "text_evidence": "complete results set for Gender+Race (text close to the symbols in plot are P, D, F, and S)", "query_type": "comparison_explanation"}
{"query_id": "l1_1810.08683_1810.08683_fig_3_638", "query": "What explains the red circle at 1011 having near-zero normalized EOd despite the test set confusion matrices showing prediction errors?", "answer": "The 1011 red circle at normalized EOd≈0 indicates equal error rates across groups for that specific Gender+Race combination, even though overall prediction accuracy may be poor as evidenced by the confusion matrices on the test set.", "doc_id": "1810.08683", "figure_id": "1810.08683_fig_3", "figure_number": 4, "image_path": "data/mineru_output/1810.08683/1810.08683/hybrid_auto/images/1810.08683_page0_fig2.jpg", "caption": "Figure 4: Adult dataset: complete results set for Gender+Race (text close to the symbols in plot are P, D, F, and S).", "figure_type": "plot", "visual_anchor": "Red circle labeled 1011 at y≈0", "text_evidence": "Table 3 and Table 4 report the confusion matrices computed on the test set", "query_type": "anomaly_cause"}
{"query_id": "l1_1810.08683_1810.08683_fig_4_639", "query": "Why does ACC peak at 91 near λ=2e-10 in the bottom panel when θ and ρ are fixed?", "answer": "The figure shows results when θ and ρ are fixed to best validation values while varying λ with P=0, F=1, S=0. The peak at λ=2e-10 represents the optimal balance for D=1 under these specific parameter constraints.", "doc_id": "1810.08683", "figure_id": "1810.08683_fig_4", "figure_number": 5, "image_path": "data/mineru_output/1810.08683/1810.08683/hybrid_auto/images/1810.08683_page0_fig3.jpg", "caption": "Figure 5: Adult Dataset: ACC and EOd of MTL, when we fix $\\theta$ and $\\rho$ to be the best values found during the validation procedure and we vary  with $P { = } 0$ , $F { = } 1$ , and $S { = } 0$ .", "figure_type": "plot", "visual_anchor": "blue line peak at approximately 91 ACC at λ=2e-10 in bottom panel (D=1)", "text_evidence": "we fix $\\theta$ and $\\rho$ to be the best values found during the validation procedure and we vary  with $P { = } 0$ , $F { = } 1$ , and $S { = } 0$", "query_type": "value_context"}
{"query_id": "l1_1810.08683_1810.08683_fig_4_640", "query": "Does the inverse relationship between blue ACC and orange EOd curves in the bottom panel support varying λ conclusions?", "answer": "Yes, as λ increases from 2e-10 to 1, ACC drops from ~91 to ~67 while EOd rises from ~0.05 to ~0.13, demonstrating the accuracy-fairness trade-off when varying λ as described in the analysis.", "doc_id": "1810.08683", "figure_id": "1810.08683_fig_4", "figure_number": 5, "image_path": "data/mineru_output/1810.08683/1810.08683/hybrid_auto/images/1810.08683_page0_fig3.jpg", "caption": "Figure 5: Adult Dataset: ACC and EOd of MTL, when we fix $\\theta$ and $\\rho$ to be the best values found during the validation procedure and we vary  with $P { = } 0$ , $F { = } 1$ , and $S { = } 0$ .", "figure_type": "plot", "visual_anchor": "opposing trends of blue ACC (declining) and orange EOd (rising) curves from left to right in bottom panel", "text_evidence": "This hypothesis is also supported by the results of Figure 5, in which we check how the accuracy and fairness, as measured with the EOd, varies by varying", "query_type": "comparison_explanation"}
{"query_id": "l1_1810.08683_1810.08683_fig_4_641", "query": "Why does the top panel show stable EOd around 0.07 while bottom panel EOd varies from 0.05 to 0.13?", "answer": "The difference between D=0 (top) and D=1 (bottom) conditions creates distinct fairness behavior. D=1 allows the sensitive feature prediction to affect fairness metrics more dynamically across λ values, while D=0 maintains more stable EOd.", "doc_id": "1810.08683", "figure_id": "1810.08683_fig_4", "figure_number": 5, "image_path": "data/mineru_output/1810.08683/1810.08683/hybrid_auto/images/1810.08683_page0_fig3.jpg", "caption": "Figure 5: Adult Dataset: ACC and EOd of MTL, when we fix $\\theta$ and $\\rho$ to be the best values found during the validation procedure and we vary  with $P { = } 0$ , $F { = } 1$ , and $S { = } 0$ .", "figure_type": "plot", "visual_anchor": "flat orange EOd line (~0.07) in top panel versus curved orange EOd line (0.05 to 0.13) in bottom panel", "text_evidence": "to first predict the sensitive feature based on the non-sensitive features, and then use the predicted value in the functional form of a model, allowing to treat people belonging to different groups, but having similar non-se", "query_type": "anomaly_cause"}
{"query_id": "l1_1811.00103_1811.00103_fig_1_642", "query": "Does the purple Male curve starting at 35 reconstruction error illustrate higher error on population A versus B?", "answer": "Yes, the purple Male curve consistently shows higher reconstruction error than the blue Female curve across all feature counts, demonstrating that PCA has higher reconstruction error on one population than the other.", "doc_id": "1811.00103", "figure_id": "1811.00103_fig_1", "figure_number": null, "image_path": "data/mineru_output/1811.00103/1811.00103/hybrid_auto/images/1811.00103_page0_fig0.jpg", "caption": "", "figure_type": "plot", "visual_anchor": "purple line labeled Male RE PCA starting at reconstruction error 35", "text_evidence": "We show on several real-world data sets, PCA has higher reconstruction error on population $A$ than on $B$ (for example, women versus men or lower- versus higher-educated individuals).", "query_type": "value_context"}
{"query_id": "l1_1811.00103_1811.00103_fig_1_643", "query": "Why do both curves converge from their initial 3-point gap to nearly overlapping at 20 features?", "answer": "As the number of features increases, PCA retains more information for both populations, reducing the reconstruction error disparity. The convergence shows that dimensionality reduction amplifies representation fidelity differences most severely at low feature counts.", "doc_id": "1811.00103", "figure_id": "1811.00103_fig_1", "figure_number": null, "image_path": "data/mineru_output/1811.00103/1811.00103/hybrid_auto/images/1811.00103_page0_fig0.jpg", "caption": "", "figure_type": "plot", "visual_anchor": "convergence of purple and blue curves from ~3-point gap at 1 feature to near-overlap at 20 features", "text_evidence": "We investigate whether the standard dimensionality reduction technique of PCA inadvertently produces data representations with different fidelity for two different populations.", "query_type": "comparison_explanation"}
{"query_id": "l1_1811.00103_1811.00103_fig_2_644", "query": "Why does the blue curve start higher at approximately 36 compared to the purple curve at lower dimensions?", "answer": "At lower dimensions, there is a noticeable gap between PCA's average reconstruction error for men and women on the LFW data set, with women (blue) showing higher reconstruction error initially.", "doc_id": "1811.00103", "figure_id": "1811.00103_fig_2", "figure_number": 1, "image_path": "data/mineru_output/1811.00103/1811.00103/hybrid_auto/images/1811.00103_page0_fig1.jpg", "caption": "Figure 1: Left: Average reconstruction error of PCA on labeled faces in the wild data set (LFW), separated by gender. Right: The same, but sampling 1000 faces with men and women equiprobably (mean over 20 samples).", "figure_type": "plot", "visual_anchor": "blue Female RE PCA curve starting at approximately 36 versus purple Male RE PCA curve starting at approximately 32", "text_evidence": "As we already saw in Figure 1 left, at lower dimensions, there is a noticeable gap between PCA's average reconstruction error for men and women on the LFW data set.", "query_type": "comparison_explanation"}
{"query_id": "l1_1811.00103_1811.00103_fig_2_645", "query": "Does the convergence of both curves to approximately 11 at 20 features indicate fairness in dimensionality reduction?", "answer": "The convergence suggests reduced disparity at higher dimensions, but the work analyzes when dimensionality reduction introduces unfairness, indicating this is part of examining where unfairness enters the ML pipeline.", "doc_id": "1811.00103", "figure_id": "1811.00103_fig_2", "figure_number": 1, "image_path": "data/mineru_output/1811.00103/1811.00103/hybrid_auto/images/1811.00103_page0_fig1.jpg", "caption": "Figure 1: Left: Average reconstruction error of PCA on labeled faces in the wild data set (LFW), separated by gender. Right: The same, but sampling 1000 faces with men and women equiprobably (mean over 20 samples).", "figure_type": "plot", "visual_anchor": "both purple and blue curves converging to approximately 11-12 reconstruction error at 20 features", "text_evidence": "Our work takes another step in fleshing out this picture by analyzing when dimensi", "query_type": "value_context"}
{"query_id": "l1_1811.00103_1811.00103_fig_2_646", "query": "What sampling strategy would equalize the purple and blue reconstruction error curves shown in the left panel?", "answer": "Sampling 1000 faces with men and women equiprobably (mean over 20 samples) would equalize the curves, as shown in the right panel of the figure.", "doc_id": "1811.00103", "figure_id": "1811.00103_fig_2", "figure_number": 1, "image_path": "data/mineru_output/1811.00103/1811.00103/hybrid_auto/images/1811.00103_page0_fig1.jpg", "caption": "Figure 1: Left: Average reconstruction error of PCA on labeled faces in the wild data set (LFW), separated by gender. Right: The same, but sampling 1000 faces with men and women equiprobably (mean over 20 samples).", "figure_type": "plot", "visual_anchor": "purple Male RE PCA and blue Female RE PCA curves showing different trajectories", "text_evidence": "Right: The same, but sampling 1000 faces with men and women equiprobably (mean over 20 samples).", "query_type": "comparison_explanation"}
{"query_id": "l1_1811.00103_1811.00103_fig_6_647", "query": "Why does the red triangular Female RE Fair PCA line start at approximately 35 when optimal fairness for dimension d requires at most d+k-1 dimensions?", "answer": "The polynomial-time algorithm achieves optimal fairness objective value for dimension d using at most d+k-1 dimensions, but at very low feature counts (near 1), the reconstruction error is inherently high before Fair PCA's fairness constraints take full effect.", "doc_id": "1811.00103", "figure_id": "1811.00103_fig_6", "figure_number": 3, "image_path": "data/mineru_output/1811.00103/1811.00103/hybrid_auto/images/1811.00103_page0_fig5.jpg", "caption": "Figure 3: Reconstruction error of PCA/Fair PCA on LFW and the Default Credit data set.", "figure_type": "plot", "visual_anchor": "red triangular line starting at reconstruction error ~35 at lowest feature count", "text_evidence": "There is a polynomial-time algorithm to find a projection such that it is of dimension at most $d + k - 1$ and achieves the optimal fairness objective value for dimension $d$", "query_type": "value_context"}
{"query_id": "l1_1811.00103_1811.00103_fig_6_648", "query": "Does the convergence of all four lines to approximately 12 at 20 features support the claim that Fair PCA assigns equal loss across populations?", "answer": "Yes, the convergence demonstrates that with sufficient features, Fair PCA achieves near-equal reconstruction error across Male and Female groups, consistent with optimal fairness solutions that minimize loss disparity between groups.", "doc_id": "1811.00103", "figure_id": "1811.00103_fig_6", "figure_number": 3, "image_path": "data/mineru_output/1811.00103/1811.00103/hybrid_auto/images/1811.00103_page0_fig5.jpg", "caption": "Figure 3: Reconstruction error of PCA/Fair PCA on LFW and the Default Credit data set.", "figure_type": "plot", "visual_anchor": "convergence of purple, blue, green, and red lines to ~12 at x=20", "text_evidence": "Figure 3 shows the average reconstruction error of each population (Male/Female, Higher/Lower education) as the result of running vanilla PCA and Fair PCA on LFW and Credit data", "query_type": "comparison_explanation"}
{"query_id": "l1_1811.00103_1811.00103_fig_6_649", "query": "Why do all curves show steeper descent between 2.5 and 7.5 features compared to the gradual decline after 10 features, given that k groups exist?", "answer": "With more than two groups, optimal solutions may not assign identical loss initially, but as features increase beyond k groups, the fairness constraints stabilize and reconstruction error improvements plateau, explaining the transition from steep to gradual decline.", "doc_id": "1811.00103", "figure_id": "1811.00103_fig_6", "figure_number": 3, "image_path": "data/mineru_output/1811.00103/1811.00103/hybrid_auto/images/1811.00103_page0_fig5.jpg", "caption": "Figure 3: Reconstruction error of PCA/Fair PCA on LFW and the Default Credit data set.", "figure_type": "plot", "visual_anchor": "steep slope of all four lines between x=2.5 and x=7.5 versus shallow slope after x=10", "text_evidence": "when there are more than two groups in the data, it is possible that all optimal solutions to fair PCA will not assign the same loss to all groups. However, with $k -", "query_type": "anomaly_cause"}
{"query_id": "l1_1811.00103_1811.00103_fig_8_650", "query": "Why does the blue Female loss PCA curve reach 0.63 at 7 features while eigenvector basis is discussed for optimization?", "answer": "The peak at 0.63 represents a local maximum in the PCA loss landscape. The eigenvector basis with eigenvalues λ₁ ≥ λ₂ ≥ ... ≥ λₙ defines the global minimum structure that the optimization seeks.", "doc_id": "1811.00103", "figure_id": "1811.00103_fig_8", "figure_number": 4, "image_path": "data/mineru_output/1811.00103/1811.00103/hybrid_auto/images/1811.00103_page0_fig7.jpg", "caption": "Figure 4: Loss of PCA/Fair PCA on LFW and the Default Credit data set.", "figure_type": "plot", "visual_anchor": "blue line peak at approximately 0.63 loss when number of features equals 7", "text_evidence": "Let {v₁, ..., vₙ} be an orthonormal basis of eigenvectors of AᵀA with corresponding eigenvalues λ₁ ≥ λ₂ ≥ ... ≥ λₙ", "query_type": "comparison_explanation"}
{"query_id": "l1_1811.00103_1811.00103_fig_8_651", "query": "Does the purple Male loss PCA staying flat near 0.06 across all features support proving local minima equal global minimum?", "answer": "Yes, the consistently low and flat male loss suggests convergence to a stable minimum. This aligns with proving that local minima values equal the global minimum value at the top d eigenvector subspace.", "doc_id": "1811.00103", "figure_id": "1811.00103_fig_8", "figure_number": 4, "image_path": "data/mineru_output/1811.00103/1811.00103/hybrid_auto/images/1811.00103_page0_fig7.jpg", "caption": "Figure 4: Loss of PCA/Fair PCA on LFW and the Default Credit data set.", "figure_type": "plot", "visual_anchor": "purple line remaining approximately constant at 0.06 loss from features 1 to 20", "text_evidence": "We prove that the value of function gₐ at its local minima is equal to its value at its global minimum, which we know is the subspace spanned by a top d eigenvectors of AᵀA", "query_type": "value_context"}
{"query_id": "l1_1811.00103_1811.00103_fig_8_652", "query": "What causes the green Fair loss curve to stabilize around 0.22 after 10 features when analyzing PCA loss on LFW?", "answer": "The stabilization indicates that adding more features beyond 10 does not significantly reduce the fair loss, suggesting the optimal subspace dimensionality has been reached for balancing fairness and reconstruction on the LFW dataset.", "doc_id": "1811.00103", "figure_id": "1811.00103_fig_8", "figure_number": 4, "image_path": "data/mineru_output/1811.00103/1811.00103/hybrid_auto/images/1811.00103_page0_fig7.jpg", "caption": "Figure 4: Loss of PCA/Fair PCA on LFW and the Default Credit data set.", "figure_type": "plot", "visual_anchor": "green line with dots plateauing at approximately 0.22 loss from 10 to 20 features", "text_evidence": "Loss of PCA/Fair PCA on LFW and the Default Credit data set", "query_type": "anomaly_cause"}
{"query_id": "l1_1811.03654_1811.03654_fig_1_653", "query": "Why does the Ratio condition reach 7.03 in Treatment 1, supporting the hypothesis about proportional loan repayment rate allocation?", "answer": "The Ratio condition's high rating of 7.03 demonstrates that participants consistently rated dividing the $50,000 in proportion to loan repayment rates as more fair than equal splitting, providing evidence for hypothesis H1A.", "doc_id": "1811.03654", "figure_id": "1811.03654_fig_1", "figure_number": 1, "image_path": "data/mineru_output/1811.03654/1811.03654/hybrid_auto/images/1811.03654_page0_fig0.jpg", "caption": "Figure 1: Comparison of means (with $9 5 \\%$ CI) for Study 1. Where * signifies p $< 0 . 0 5$ , $^ { \\ast \\ast } \\mathrm { p } < 0 . 0 1$ , and ** $\\mathfrak { i } \\mathrm { p } < 0 . 0 0 1$ .", "figure_type": "plot", "visual_anchor": "Mean value 7.03 labeled at Ratio condition in Treatment 1 panel", "text_evidence": "We found evidence in support of H1A in all treatments: participants consistently rated dividing the $50,000 between the two individuals in proportion of their loan repayment rates (the 'Ratio' decision) as more fair than splitting the $50,000 equally (the 'Equal' decision)", "query_type": "value_context"}
{"query_id": "l1_1811.03654_1811.03654_fig_1_654", "query": "How does the consistent Ratio superiority marked with asterisks across all four treatments relate to task-specific feature influence?", "answer": "The statistical significance markers above Ratio conditions in all treatments demonstrate that information on the candidates' loan repayment rates systematically influenced fairness perceptions, supporting the study's motivation to investigate task-specific feature effects.", "doc_id": "1811.03654", "figure_id": "1811.03654_fig_1", "figure_number": 1, "image_path": "data/mineru_output/1811.03654/1811.03654/hybrid_auto/images/1811.03654_page0_fig0.jpg", "caption": "Figure 1: Comparison of means (with $9 5 \\%$ CI) for Study 1. Where * signifies p $< 0 . 0 5$ , $^ { \\ast \\ast } \\mathrm { p } < 0 . 0 1$ , and ** $\\mathfrak { i } \\mathrm { p } < 0 . 0 0 1$ .", "figure_type": "plot", "visual_anchor": "Significance brackets with asterisks (**, ***) connecting Ratio to other conditions across all four treatment panels", "text_evidence": "In this study, our motivation is to investigate how information on an individual task-specific feature (i.e., the candidates' loan repayment rate) influences perceptions of fairness", "query_type": "comparison_explanation"}
{"query_id": "l1_1811.03654_1811.03654_fig_1_655", "query": "What explains the three-level progression from 5.09 to 6.34 to 7.03 in Treatment 1 given the allocation rules tested?", "answer": "The ascending fairness ratings from All A (5.09) through Equal (6.34) to Ratio (7.03) reflect participants' preferences across the three allocation rules formulated to test qualitative judgments about loan distribution fairness.", "doc_id": "1811.03654", "figure_id": "1811.03654_fig_1", "figure_number": 1, "image_path": "data/mineru_output/1811.03654/1811.03654/hybrid_auto/images/1811.03654_page0_fig0.jpg", "caption": "Figure 1: Comparison of means (with $9 5 \\%$ CI) for Study 1. Where * signifies p $< 0 . 0 5$ , $^ { \\ast \\ast } \\mathrm { p } < 0 . 0 1$ , and ** $\\mathfrak { i } \\mathrm { p } < 0 . 0 0 1$ .", "figure_type": "plot", "visual_anchor": "Sequential mean values 5.09, 6.34, and 7.03 in Treatment 1 panel from left to right", "text_evidence": "We choose three allocation rules, described in the following paragraphs, that allow us to formulate qualitative judgments", "query_type": "comparison_explanation"}
{"query_id": "l1_1811.03654_1811.03654_fig_2_656", "query": "Why does Treatment 2's Equal condition rating of 5.85 exceed All A's 4.00 when the higher-repayment candidate is white?", "answer": "Participants rated the \"Equal\" decision as more fair than the \"All A\" decision in Treatment 2, but only when the candidate with the higher repayment rate was white, as shown by the statistical significance markers.", "doc_id": "1811.03654", "figure_id": "1811.03654_fig_2", "figure_number": 2, "image_path": "data/mineru_output/1811.03654/1811.03654/hybrid_auto/images/1811.03654_page0_fig1.jpg", "caption": "Figure 2: Comparison of means (with $9 5 \\%$ CI) for Study 2 (when the individual with the higher loan repayment rate is white). Where * signifies p $< 0 . 0 5$ , ** p $< 0 . 0 1$ , and $\\ast \\ast \\ast _ { \\mathrm { ~ p ~ } }$ ${ < } 0 . 0 0 1$ .", "figure_type": "plot", "visual_anchor": "Treatment 2 subplot showing Equal at 5.85 and All A at 4.00 with *** and * significance markers", "text_evidence": "Participants also rated the \"Equal\" decision as more fair than the \"All A\" decision in Treatment 2, but only when the candidate with the higher repayment rate was white", "query_type": "comparison_explanation"}
{"query_id": "l1_1811.03654_1811.03654_fig_2_657", "query": "Does the Ratio condition's highest rating of 6.74 in Treatment 1 support fairness preference when sensitive information includes loan repayment rates?", "answer": "Yes, participants consistently gave most support to the decision to divide the money between individuals in proportion to their repayment rates, viewing the \"Ratio\" decision as more fair than both \"Equal\" and \"All A\" decisions across all treatments.", "doc_id": "1811.03654", "figure_id": "1811.03654_fig_2", "figure_number": 2, "image_path": "data/mineru_output/1811.03654/1811.03654/hybrid_auto/images/1811.03654_page0_fig1.jpg", "caption": "Figure 2: Comparison of means (with $9 5 \\%$ CI) for Study 2 (when the individual with the higher loan repayment rate is white). Where * signifies p $< 0 . 0 5$ , ** p $< 0 . 0 1$ , and $\\ast \\ast \\ast _ { \\mathrm { ~ p ~ } }$ ${ < } 0 . 0 0 1$ .", "figure_type": "plot", "visual_anchor": "Treatment 1 Ratio value of 6.74 with *** significance compared to Equal and All A conditions", "text_evidence": "participants in Study 2 consistently gave most support to the decision to divide the $50,000 between the two individuals in prop", "query_type": "value_context"}
{"query_id": "l1_1811.03654_1811.03654_fig_2_658", "query": "How does Ratio's 5.98 rating in Treatment 3 compare to Equal's 5.28 given the experimental paradigm presenting two loan applicants?", "answer": "The Ratio decision (5.98) was viewed as significantly more fair than the Equal decision (5.28) in Treatment 3, consistent with participants viewing proportional allocation based on task-specific features as fairer across all treatments.", "doc_id": "1811.03654", "figure_id": "1811.03654_fig_2", "figure_number": 2, "image_path": "data/mineru_output/1811.03654/1811.03654/hybrid_auto/images/1811.03654_page0_fig1.jpg", "caption": "Figure 2: Comparison of means (with $9 5 \\%$ CI) for Study 2 (when the individual with the higher loan repayment rate is white). Where * signifies p $< 0 . 0 5$ , ** p $< 0 . 0 1$ , and $\\ast \\ast \\ast _ { \\mathrm { ~ p ~ } }$ ${ < } 0 . 0 0 1$ .", "figure_type": "plot", "visual_anchor": "Treatment 3 subplot showing Ratio at 5.98 and Equal at 5.28 with * significance marker", "text_evidence": "We employed the same experimental paradigm as in Study 1, presenting participants with the scenario of two individuals applying for a loan, and three possible ways of allocating the loan money", "query_type": "comparison_explanation"}
{"query_id": "l1_1811.03654_1811.03654_fig_3_659", "query": "Does the Ratio condition's 6.00 rating in Treatment 2 support that participants consistently favored proportional division?", "answer": "Yes, participants in Study 2 consistently gave most support to the decision to divide the $50,000 between the two individuals in proportion (Ratio condition), as shown by the highest ratings across all four treatments.", "doc_id": "1811.03654", "figure_id": "1811.03654_fig_3", "figure_number": 3, "image_path": "data/mineru_output/1811.03654/1811.03654/hybrid_auto/images/1811.03654_page0_fig2.jpg", "caption": "Figure 3: Comparison of means (with $9 5 \\%$ CI) for Study 2 (when the individual with the higher loan repayment rate is black). Where * signifies p $< 0 . 0 5$ , $^ { * * } \\mathrm {  ~ p ~ } { < } 0 . 0 1$ , and $\\ast \\ast \\ast _ { \\mathrm { ~ p ~ } }$ ${ < } 0 . 0 0 1$ .", "figure_type": "plot", "visual_anchor": "Ratio condition value of 6.00 with error bars in Treatment 2 upper-right panel", "text_evidence": "Thus, participants in Study 2 consistently gave most support to the decision to divide the $50,000 between the two individuals in prop", "query_type": "value_context"}
{"query_id": "l1_1811.03654_1811.03654_fig_3_660", "query": "Why is the All A rating 5.55 significantly higher than Equal's 4.83 in Treatment 3 when both lack proportional allocation?", "answer": "When the difference between repayment rates was larger (Treatments 3 and 4), participants viewed the 'All A' decision as more fair than the 'Equal' decision specifically when the candidate with the higher repayment rate was black, reflecting race-influenced fairness perceptions under larger performance disparities.", "doc_id": "1811.03654", "figure_id": "1811.03654_fig_3", "figure_number": 3, "image_path": "data/mineru_output/1811.03654/1811.03654/hybrid_auto/images/1811.03654_page0_fig2.jpg", "caption": "Figure 3: Comparison of means (with $9 5 \\%$ CI) for Study 2 (when the individual with the higher loan repayment rate is black). Where * signifies p $< 0 . 0 5$ , $^ { * * } \\mathrm {  ~ p ~ } { < } 0 . 0 1$ , and $\\ast \\ast \\ast _ { \\mathrm { ~ p ~ } }$ ${ < } 0 . 0 0 1$ .", "figure_type": "plot", "visual_anchor": "Treatment 3 lower-left panel showing All A at 5.55 versus Equal at 4.83 with significant difference markers", "text_evidence": "When the difference between the two candidates' repayment rates was larger (Treatments 3 and 4), participants viewed the 'All A' decision as more fair than the 'Equal' decision but only when the candidate with the higher repayment rate was black", "query_type": "comparison_explanation"}
{"query_id": "l1_1811.03654_1811.03654_fig_3_661", "query": "Do the Ratio values 6.09 and 5.98 in Treatments 3-4 remain highest despite race attributes?", "answer": "Yes, participants perceived the 'Ratio' decision to be more fair than the other two options regardless of race attribute. These results are not dependent on the race attribute, showing consistent preference for proportional fairness.", "doc_id": "1811.03654", "figure_id": "1811.03654_fig_3", "figure_number": 3, "image_path": "data/mineru_output/1811.03654/1811.03654/hybrid_auto/images/1811.03654_page0_fig2.jpg", "caption": "Figure 3: Comparison of means (with $9 5 \\%$ CI) for Study 2 (when the individual with the higher loan repayment rate is black). Where * signifies p $< 0 . 0 5$ , $^ { * * } \\mathrm {  ~ p ~ } { < } 0 . 0 1$ , and $\\ast \\ast \\ast _ { \\mathrm { ~ p ~ } }$ ${ < } 0 . 0 0 1$ .", "figure_type": "plot", "visual_anchor": "Ratio condition showing highest values (6.09 in Treatment 3, 5.98 in Treatment 4) compared to All A and Equal", "text_evidence": "Our results show that participants perceived the 'Ratio' decision to be more fair than the other two, hence supporting the results from Study 1 and the related discussion. These results are not dependent on the race attribute.", "query_type": "value_context"}
{"query_id": "l1_1811.03654_1811.03654_fig_4_662", "query": "Does the 30% largest cyan segment contradict the eligibility requirement that workers be located in the United States?", "answer": "No, the 30-39 age group comprising 30% does not contradict the U.S. location requirement, as age distribution is independent of geographic eligibility criteria.", "doc_id": "1811.03654", "figure_id": "1811.03654_fig_4", "figure_number": 6, "image_path": "data/mineru_output/1811.03654/1811.03654/hybrid_auto/images/1811.03654_page0_fig3.jpg", "caption": "Figure 6: Age distribution of the participants in Study 1.", "figure_type": "plot", "visual_anchor": "30-39 segment at 30% (largest cyan portion)", "text_evidence": "To be eligible to take our surveys, the Amazon Mechanical Turk workers had to be located in the United States of America.", "query_type": "value_context"}
{"query_id": "l1_1811.03654_1811.03654_fig_4_663", "query": "Why does the 28% in the 20-29 cyan segment nearly match the 30% segment despite participation restrictions?", "answer": "The similar percentages (28% vs 30%) reflect natural age distribution among MTurk workers, not participation restrictions, which only prevented workers from taking both studies.", "doc_id": "1811.03654", "figure_id": "1811.03654_fig_4", "figure_number": 6, "image_path": "data/mineru_output/1811.03654/1811.03654/hybrid_auto/images/1811.03654_page0_fig3.jpg", "caption": "Figure 6: Age distribution of the participants in Study 1.", "figure_type": "plot", "visual_anchor": "20-29 segment at 28% compared to 30-39 at 30%", "text_evidence": "Amazon Mechanical Turk workers ('MTurker') could only participate in on one of the two studies, and not both.", "query_type": "comparison_explanation"}
{"query_id": "l1_1811.03654_1811.03654_fig_4_664", "query": "Does the 1% Under 20 dark blue segment indicate incomplete demographic data collection in the first section?", "answer": "No, the 1% Under 20 representation does not indicate incomplete data collection; it simply reflects the actual age distribution of participants who completed the demographic questions.", "doc_id": "1811.03654", "figure_id": "1811.03654_fig_4", "figure_number": 6, "image_path": "data/mineru_output/1811.03654/1811.03654/hybrid_auto/images/1811.03654_page0_fig3.jpg", "caption": "Figure 6: Age distribution of the participants in Study 1.", "figure_type": "plot", "visual_anchor": "Under 20 segment at 1% (smallest dark blue slice)", "text_evidence": "The first section contains all questions the workers were asked in the studies. The second section contains the demographics dat", "query_type": "anomaly_cause"}
{"query_id": "l1_1811.03654_1811.03654_fig_5_665", "query": "What percentage of Study 1 participants hold the Bachelor's degree shown as the largest 30% light blue segment?", "answer": "30% of Study 1 participants hold a Bachelor's degree, representing the largest educational attainment group in the study.", "doc_id": "1811.03654", "figure_id": "1811.03654_fig_5", "figure_number": 7, "image_path": "data/mineru_output/1811.03654/1811.03654/hybrid_auto/images/1811.03654_page0_fig4.jpg", "caption": "Figure 7: Education of the participants in Study 1.", "figure_type": "plot", "visual_anchor": "Bachelor's deg. 30% (largest light blue segment)", "text_evidence": "Study 1: Demographic information of the participants", "query_type": "value_context"}
{"query_id": "l1_1811.03654_1811.03654_fig_5_666", "query": "How does the 27% turquoise Some college segment compare to the 14% cyan Associate's degree segment among participants?", "answer": "The Some college segment at 27% is nearly double the Associate's degree segment at 14%, indicating more participants attended college without completing a four-year degree than those who earned an Associate's degree.", "doc_id": "1811.03654", "figure_id": "1811.03654_fig_5", "figure_number": 7, "image_path": "data/mineru_output/1811.03654/1811.03654/hybrid_auto/images/1811.03654_page0_fig4.jpg", "caption": "Figure 7: Education of the participants in Study 1.", "figure_type": "plot", "visual_anchor": "Some college 27% (turquoise) vs Associate's deg. 14% (cyan)", "text_evidence": "Figure 6: Age distribution of the participants in Study 1.", "query_type": "comparison_explanation"}
{"query_id": "l1_1811.03654_1811.03654_fig_5_667", "query": "Why does the 20% dark blue Graduate degree segment rank third in size for education levels documented?", "answer": "The Graduate degree segment at 20% ranks third because Bachelor's degree (30%) and Some college (27%) have higher representation, reflecting that fewer participants pursued advanced degrees beyond undergraduate education.", "doc_id": "1811.03654", "figure_id": "1811.03654_fig_5", "figure_number": 7, "image_path": "data/mineru_output/1811.03654/1811.03654/hybrid_auto/images/1811.03654_page0_fig4.jpg", "caption": "Figure 7: Education of the participants in Study 1.", "figure_type": "plot", "visual_anchor": "Graduate degree 20% (dark blue segment, third largest)", "text_evidence": "Figure 7: Education of the participants in Study 1.", "query_type": "comparison_explanation"}
{"query_id": "l1_1811.03654_1811.03654_fig_6_668", "query": "Does the turquoise segment representing 56% Female align with the stated gender breakdown for Study 1 participants?", "answer": "Yes, the turquoise segment showing 56% Female directly corresponds to the gender breakdown of participants in Study 1 as stated.", "doc_id": "1811.03654", "figure_id": "1811.03654_fig_6", "figure_number": 8, "image_path": "data/mineru_output/1811.03654/1811.03654/hybrid_auto/images/1811.03654_page0_fig5.jpg", "caption": "Figure 8: Gender breakdown of the participants in Study 1.", "figure_type": "plot", "visual_anchor": "Turquoise segment labeled Female 56%", "text_evidence": "Figure 8: Gender breakdown of the participants in Study 1.", "query_type": "value_context"}
{"query_id": "l1_1811.03654_1811.03654_fig_6_669", "query": "Why does the dark blue Male segment occupy 44% while the turquoise Female segment is larger at 56%?", "answer": "The difference reflects the actual gender composition of Study 1 participants, where females comprised a slight majority over males.", "doc_id": "1811.03654", "figure_id": "1811.03654_fig_6", "figure_number": 8, "image_path": "data/mineru_output/1811.03654/1811.03654/hybrid_auto/images/1811.03654_page0_fig5.jpg", "caption": "Figure 8: Gender breakdown of the participants in Study 1.", "figure_type": "plot", "visual_anchor": "Dark blue segment labeled Male 44% compared to turquoise segment at 56%", "text_evidence": "Gender   \nFigure 8: Gender breakdown of the participants in Study 1.", "query_type": "comparison_explanation"}
{"query_id": "l1_1811.03654_1811.03654_fig_6_670", "query": "What do the two contrasting segments in the pie chart reveal about participant demographics in Study 1?", "answer": "The two segments show a nearly balanced but female-majority gender distribution, with 56% female and 44% male participants in Study 1.", "doc_id": "1811.03654", "figure_id": "1811.03654_fig_6", "figure_number": 8, "image_path": "data/mineru_output/1811.03654/1811.03654/hybrid_auto/images/1811.03654_page0_fig5.jpg", "caption": "Figure 8: Gender breakdown of the participants in Study 1.", "figure_type": "plot", "visual_anchor": "Two contrasting colored segments (turquoise at 56%, dark blue at 44%)", "text_evidence": "- Gender   \nFigure 8: Gender breakdown of the participants in Study 1.", "query_type": "visual_definition"}
{"query_id": "l1_1811.03654_1811.03654_fig_7_671", "query": "Does the 34% Democratic Party segment in the pie chart represent the largest group in Study 1?", "answer": "Yes, the Democratic Party at 34% represents the largest political affiliation group among participants in Study 1, followed closely by Independent at 33%.", "doc_id": "1811.03654", "figure_id": "1811.03654_fig_7", "figure_number": 9, "image_path": "data/mineru_output/1811.03654/1811.03654/hybrid_auto/images/1811.03654_page0_fig6.jpg", "caption": "Figure 9: Political Affiliation of the participants in Study 1.", "figure_type": "plot", "visual_anchor": "cyan segment labeled Democratic Party 34%", "text_evidence": "Figure 9: Political Affiliation of the participants in Study 1.", "query_type": "value_context"}
{"query_id": "l1_1811.03654_1811.03654_fig_8_672", "query": "How does the participant count in FL around 15 compare to TX which also shows approximately 15 participants?", "answer": "Both FL and TX show similar participant counts at approximately 15, making them the second-highest contributing states after PA in Study 1.", "doc_id": "1811.03654", "figure_id": "1811.03654_fig_8", "figure_number": 11, "image_path": "data/mineru_output/1811.03654/1811.03654/hybrid_auto/images/1811.03654_page0_fig7.jpg", "caption": "Figure 11: Breakup by state of the participants in Study 1.", "figure_type": "plot", "visual_anchor": "FL and TX bars both at approximately 15 height", "text_evidence": "Figure 11: Breakup by state of the participants in Study 1.", "query_type": "comparison_explanation"}
{"query_id": "l1_1811.03654_1811.03654_fig_8_673", "query": "Why do most state bars remain below 10 participants despite the political affiliation focus of this study section?", "answer": "The majority of states contributed fewer than 10 participants, with only three states (PA, FL, TX) showing substantial participation above 13, indicating geographic concentration in Study 1 recruitment.", "doc_id": "1811.03654", "figure_id": "1811.03654_fig_8", "figure_number": 11, "image_path": "data/mineru_output/1811.03654/1811.03654/hybrid_auto/images/1811.03654_page0_fig7.jpg", "caption": "Figure 11: Breakup by state of the participants in Study 1.", "figure_type": "plot", "visual_anchor": "Majority of bars staying below the 10 mark on y-axis", "text_evidence": "State breakup   \nFigure 11: Breakup by state of the participants in Study 1.", "query_type": "anomaly_cause"}
{"query_id": "l1_1811.03654_1811.03654_fig_10_674", "query": "Does the 49% suburban community segment represent the largest residential category among Study 1 participants?", "answer": "Yes, the suburban community segment at 49% is the largest category, nearly double the city/urban and rural segments which are at 26% and 25% respectively.", "doc_id": "1811.03654", "figure_id": "1811.03654_fig_10", "figure_number": 12, "image_path": "data/mineru_output/1811.03654/1811.03654/hybrid_auto/images/1811.03654_page0_fig9.jpg", "caption": "Figure 12: Residential breakdown of the participants in Study 1.", "figure_type": "plot", "visual_anchor": "Dark blue segment labeled 'Suburban comm.' showing 49%", "text_evidence": "Figure 12: Residential breakdown of the participants in Study 1.", "query_type": "value_context"}
{"query_id": "l1_1811.03654_1811.03654_fig_10_675", "query": "Why is the cyan city or urban segment at 26% only slightly larger than the rural segment?", "answer": "The city or urban segment at 26% is nearly equal to the rural community segment at 25%, differing by only 1 percentage point, indicating a balanced distribution between these two residential categories in Study 1.", "doc_id": "1811.03654", "figure_id": "1811.03654_fig_10", "figure_number": 12, "image_path": "data/mineru_output/1811.03654/1811.03654/hybrid_auto/images/1811.03654_page0_fig9.jpg", "caption": "Figure 12: Residential breakdown of the participants in Study 1.", "figure_type": "plot", "visual_anchor": "Cyan segment at 26% compared to blue segment at 25%", "text_evidence": "Residential breakdown of the participants in Study 1.", "query_type": "comparison_explanation"}
{"query_id": "l1_1811.03654_1811.03654_fig_10_676", "query": "What explains the dominance of the 49% suburban segment over the combined 51% of other communities?", "answer": "The suburban community represents 49% of Study 1 participants, making it the single largest residential category though marginally less than the combined city/urban and rural categories together.", "doc_id": "1811.03654", "figure_id": "1811.03654_fig_10", "figure_number": 12, "image_path": "data/mineru_output/1811.03654/1811.03654/hybrid_auto/images/1811.03654_page0_fig9.jpg", "caption": "Figure 12: Residential breakdown of the participants in Study 1.", "figure_type": "plot", "visual_anchor": "Dark blue suburban segment occupying nearly half the pie chart at 49%", "text_evidence": "Residential Community Figure 12: Residential breakdown of the participants in Study 1.", "query_type": "anomaly_cause"}
{"query_id": "l1_1811.03654_1811.03654_fig_12_677", "query": "Does the 37% Bachelor's degree segment align with typical demographic patterns in Study 2 participants?", "answer": "Yes, the 37% Bachelor's degree representation is the largest educational segment among Study 2 participants, indicating a well-educated sample population.", "doc_id": "1811.03654", "figure_id": "1811.03654_fig_12", "figure_number": 13, "image_path": "data/mineru_output/1811.03654/1811.03654/hybrid_auto/images/1811.03654_page0_fig11.jpg", "caption": "Figure 13: Age distribution of the participants in Study 2.   ", "figure_type": "plot", "visual_anchor": "Bachelor's degree segment showing 37% (largest segment in cyan)", "text_evidence": "Study 2: Demographic information of the participants", "query_type": "value_context"}
{"query_id": "l1_1811.03654_1811.03654_fig_12_678", "query": "Why is the combined percentage of Bachelor's and Graduate degrees larger than the Some college segment showing 27%?", "answer": "Bachelor's degree (37%) and Graduate degree (13%) together represent 50% of participants, indicating Study 2 recruited more individuals with completed degrees than those with partial college education.", "doc_id": "1811.03654", "figure_id": "1811.03654_fig_12", "figure_number": 13, "image_path": "data/mineru_output/1811.03654/1811.03654/hybrid_auto/images/1811.03654_page0_fig11.jpg", "caption": "Figure 13: Age distribution of the participants in Study 2.   ", "figure_type": "plot", "visual_anchor": "Bachelor's degree (37%), Graduate degree (13%), and Some college (27%) segments", "text_evidence": "Figure 14: Education of the participants in Study 2.", "query_type": "comparison_explanation"}
{"query_id": "l1_1811.03654_1811.03654_fig_12_679", "query": "What explains the 0% representation in the No high school degree category for Study 2's age-related demographics?", "answer": "The 0% indicates that all Study 2 participants had at least a high school education or equivalent, likely reflecting recruitment criteria targeting educated populations for the age distribution analysis.", "doc_id": "1811.03654", "figure_id": "1811.03654_fig_12", "figure_number": 13, "image_path": "data/mineru_output/1811.03654/1811.03654/hybrid_auto/images/1811.03654_page0_fig11.jpg", "caption": "Figure 13: Age distribution of the participants in Study 2.   ", "figure_type": "plot", "visual_anchor": "No high school degree segment labeled 0% at the top of the pie chart", "text_evidence": "Figure 13: Age distribution of the participants in Study 2.", "query_type": "anomaly_cause"}
{"query_id": "l1_1811.03654_1811.03654_fig_13_680", "query": "How does the 42% male blue segment compare to the 58% female turquoise segment in Study 2?", "answer": "The male segment at 42% is 16 percentage points lower than the female segment at 58%, indicating females outnumber males by a moderate margin in Study 2.", "doc_id": "1811.03654", "figure_id": "1811.03654_fig_13", "figure_number": 15, "image_path": "data/mineru_output/1811.03654/1811.03654/hybrid_auto/images/1811.03654_page0_fig12.jpg", "caption": "Figure 15: Gender breakdown of the participants in Study 2.", "figure_type": "plot", "visual_anchor": "Blue segment labeled 'Male 42%' compared to turquoise segment labeled 'Female 58%'", "text_evidence": "Figure 15: Gender breakdown of the participants in Study 2.", "query_type": "comparison_explanation"}
{"query_id": "l1_1811.03654_1811.03654_fig_14_681", "query": "Does the 43% Democratic Party turquoise segment represent the political affiliation distribution in Study 2?", "answer": "Yes, the 43% turquoise segment represents Democratic Party affiliation in Study 2, as shown in the pie chart displaying political affiliation breakdown of participants.", "doc_id": "1811.03654", "figure_id": "1811.03654_fig_14", "figure_number": 16, "image_path": "data/mineru_output/1811.03654/1811.03654/hybrid_auto/images/1811.03654_page0_fig13.jpg", "caption": "Figure 16: Political Affiliation of the participants in Study 2.", "figure_type": "plot", "visual_anchor": "turquoise segment labeled Democratic Party at 43%", "text_evidence": "Figure 16: Political Affiliation of the participants in Study 2.", "query_type": "value_context"}
{"query_id": "l1_1811.03654_1811.03654_fig_14_682", "query": "Why is the dark blue Republican segment at 26% only slightly larger than the cyan Independent segment at 25%?", "answer": "The Republican and Independent segments are nearly equal because Study 2 participants were almost evenly distributed between these two affiliations, with only a 1 percentage point difference.", "doc_id": "1811.03654", "figure_id": "1811.03654_fig_14", "figure_number": 16, "image_path": "data/mineru_output/1811.03654/1811.03654/hybrid_auto/images/1811.03654_page0_fig13.jpg", "caption": "Figure 16: Political Affiliation of the participants in Study 2.", "figure_type": "plot", "visual_anchor": "dark blue Republican segment at 26% compared to cyan Independent segment at 25%", "text_evidence": "Political Affiliation   \nFigure 16: Political Affiliation of the participants in Study 2.", "query_type": "comparison_explanation"}
{"query_id": "l1_1811.03654_1811.03654_fig_14_683", "query": "Do the combined 1% purple Multiple and 1% Green Party segments represent minority affiliations among Study 2 participants?", "answer": "Yes, the Multiple and Green Party affiliations each represent only 1% of participants, making them the smallest groups compared to the dominant Democratic Party, Republican, and Independent categories.", "doc_id": "1811.03654", "figure_id": "1811.03654_fig_14", "figure_number": 16, "image_path": "data/mineru_output/1811.03654/1811.03654/hybrid_auto/images/1811.03654_page0_fig13.jpg", "caption": "Figure 16: Political Affiliation of the participants in Study 2.", "figure_type": "plot", "visual_anchor": "purple Multiple segment at 1% and light cyan Green Party segment at 1%", "text_evidence": "Gender   \nFigure 15: Gender breakdown of the participants in Study 2.\nText after: Political Affiliation   \nFigure 16: Political Affiliation of the participants in Study 2.", "query_type": "value_context"}
{"query_id": "l1_1811.03654_1811.03654_fig_15_684", "query": "What explains the 0% representation for both Native Hawaiian and American Indian segments in the racial composition pie chart?", "answer": "No participants in Study 2 identified as Native Hawaiian or American Indian, resulting in zero percentage allocation for these demographic categories in the participant pool.", "doc_id": "1811.03654", "figure_id": "1811.03654_fig_15", "figure_number": 17, "image_path": "data/mineru_output/1811.03654/1811.03654/hybrid_auto/images/1811.03654_page0_fig14.jpg", "caption": "Figure 17: Race of the participants in Study 2.   ", "figure_type": "plot", "visual_anchor": "Segments labeled Native Hawaiian 0% and American Indian 0%", "text_evidence": "Residential Community   \nFigure 18: Residential breakdown of the participants in Study 2.", "query_type": "anomaly_cause"}
{"query_id": "l1_1811.10104_1811.10104_fig_1_685", "query": "How do the π₁ and π₂ marginal distributions on the vertical axis relate to Cleary's bias definition for different student groups?", "answer": "The vertical marginal distributions show ground truth score distributions for subgroups π₁ and π₂, which Cleary used to define test bias quantitatively for black and white students by examining differences between these group distributions.", "doc_id": "1811.10104", "figure_id": "1811.10104_fig_1", "figure_number": 1, "image_path": "data/mineru_output/1811.10104/1811.10104/hybrid_auto/images/1811.10104_page0_fig0.jpg", "caption": "Figure 1: Petersen and Novick’s [52] original figures demonstrating fairness criteria. The marginal distributions of test scores and ground truth scores for subgroups $\\pi _ { 1 }$ and $\\pi _ { 2 }$ are shown by the axes.", "figure_type": "diagram", "visual_anchor": "Marginal distribution curves labeled π₁ and π₂ on the vertical y* axis", "text_evidence": "Concerned with the fairness of tests for black and white students, T. Anne Cleary defined a quantitative measure of test bias", "query_type": "value_context"}
{"query_id": "l1_1811.10104_1811.10104_fig_1_686", "query": "Why do the diagonal projection lines connect Test X and y* axes differently for π₁ versus π₂ subgroups?", "answer": "The different diagonal lines for π₁ and π₂ illustrate that test scores and ground truth scores relate differently across subgroups, demonstrating that available criterion scores may not be perfectly relevant, reliable, and unbiased measures as Thorndike critiqued.", "doc_id": "1811.10104", "figure_id": "1811.10104_fig_1", "figure_number": 1, "image_path": "data/mineru_output/1811.10104/1811.10104/hybrid_auto/images/1811.10104_page0_fig0.jpg", "caption": "Figure 1: Petersen and Novick’s [52] original figures demonstrating fairness criteria. The marginal distributions of test scores and ground truth scores for subgroups $\\pi _ { 1 }$ and $\\pi _ { 2 }$ are shown by the axes.", "figure_type": "diagram", "visual_anchor": "Two separate diagonal lines labeled π₁ and π₂ connecting horizontal and vertical axes", "text_evidence": "The discussion of 'fairness' in what has gone before is clearly oversimplified. In particular, it has been based upon the premise that the available criterion score is a perfectly relevant, reliable and unbiased measure...", "query_type": "comparison_explanation"}
{"query_id": "l1_1811.10104_1811.10104_fig_1_687", "query": "What shift in perspective does the shaded region under π₂ on the horizontal Test axis represent?", "answer": "The shaded region represents the evolution from 1960s bias analysis to 1970s fairness frameworks, marking the change in framing from unfairness to fairness that parallels recent ML fairness work as the decade transitioned.", "doc_id": "1811.10104", "figure_id": "1811.10104_fig_1", "figure_number": 1, "image_path": "data/mineru_output/1811.10104/1811.10104/hybrid_auto/images/1811.10104_page0_fig0.jpg", "caption": "Figure 1: Petersen and Novick’s [52] original figures demonstrating fairness criteria. The marginal distributions of test scores and ground truth scores for subgroups $\\pi _ { 1 }$ and $\\pi _ { 2 }$ are shown by the axes.", "figure_type": "diagram", "visual_anchor": "Shaded region under the π₂ curve on the horizontal Test (X) axis", "text_evidence": "As the 1960s turned to the 1970s, work began to arise that parallels the recent evolution of work in ML fairness, marking a change in framing from unfairness to fairness.", "query_type": "value_context"}
{"query_id": "l1_1811.10104_1811.10104_fig_3_688", "query": "Why does curve 1 show decreasing r_CX as r_XY increases, given Thorndike's premise about criterion scores?", "answer": "Thorndike questioned the oversimplified premise that criterion scores are perfectly relevant, reliable, and unbiased, which relates to how culture-test correlations should vary with test-truth correlations under different fairness definitions.", "doc_id": "1811.10104", "figure_id": "1811.10104_fig_3", "figure_number": 2, "image_path": "data/mineru_output/1811.10104/1811.10104/hybrid_auto/images/1811.10104_page0_fig2.jpg", "caption": "Figure 2: Darlington’s original graph of fair values of the correlation between culture and test score $r _ { C X }$ in Darlingrton’s notation), plotted against the correlation between test score and ground truth $( r _ { X Y } )$ , according to his definitions r(1–4). (The correlation between the demographic and target variables is assumed here to be fixed at 0.2.)", "figure_type": "plot", "visual_anchor": "curved line labeled 1 descending from top-left to bottom-right", "text_evidence": "Following Thorndike [62], 'The discussion of 'fairness' in what has gone before is clearly oversimplified. In particular, it has been based upon the premise that the available criterion score is a perfectly relevant, reliable and unbiased measure...'", "query_type": "comparison_explanation"}
{"query_id": "l1_1811.10104_1811.10104_fig_3_689", "query": "Do lines 2, 3, and 4 converging at the right support Cole's claim that research failed to indicate fairness unequivocally?", "answer": "Yes, the convergence of multiple fairness definitions (lines 2-4) at high r_XY values illustrates that different fairness criteria yield different fair values, supporting Cole's observation that research supplied no unequivocal fairness indication.", "doc_id": "1811.10104", "figure_id": "1811.10104_fig_3", "figure_number": 2, "image_path": "data/mineru_output/1811.10104/1811.10104/hybrid_auto/images/1811.10104_page0_fig2.jpg", "caption": "Figure 2: Darlington’s original graph of fair values of the correlation between culture and test score $r _ { C X }$ in Darlingrton’s notation), plotted against the correlation between test score and ground truth $( r _ { X Y } )$ , according to his definitions r(1–4). (The correlation between the demographic and target variables is assumed here to be fixed at 0.2.)", "figure_type": "plot", "visual_anchor": "lines labeled 2, 3, and 4 converging at the right side of the x-axis", "text_evidence": "The same Cole who in 1973 proposed equality of TPR, wrote in 2001 that [15]: In short, research over the last 30 or so years has not supplied any analyses to unequivocally indicate fairness or unfairness", "query_type": "value_context"}
{"query_id": "l1_1811.10104_1811.10104_fig_3_690", "query": "What explains the divergence among curves 1 through 4 at low r_XY values under the fixed 0.2 demographic-target correlation assumption?", "answer": "Darlington's four different fairness definitions yield distinct fair correlation values when test-truth correlation is low, reflecting fundamentally different conceptions of what constitutes fair testing under the same demographic conditions.", "doc_id": "1811.10104", "figure_id": "1811.10104_fig_3", "figure_number": 2, "image_path": "data/mineru_output/1811.10104/1811.10104/hybrid_auto/images/1811.10104_page0_fig2.jpg", "caption": "Figure 2: Darlington’s original graph of fair values of the correlation between culture and test score $r _ { C X }$ in Darlingrton’s notation), plotted against the correlation between test score and ground truth $( r _ { X Y } )$ , according to his definitions r(1–4). (The correlation between the demographic and target variables is assumed here to be fixed at 0.2.)", "figure_type": "plot", "visual_anchor": "separation of curves 1, 2, 3, and 4 at the left side where r_XY is low", "text_evidence": "fair values of the correlation between culture and test score $r _ { C X }$ in Darlingrton's notation), plotted against the correlation between test score and ground truth $( r _ { X Y } )$ , according to his definitions r(1–4). (The correlation between the demographic and target variables is assumed here to be fixed at 0.2.)", "query_type": "anomaly_cause"}
{"query_id": "l1_1811.10104_1811.10104_fig_4_691", "query": "Why do the filled circles remain above open circles across all SAT-VERBAL scores if DIF examines bias causes?", "answer": "DIF identifies items displaying differential functioning between groups, which test designers examine to identify the cause of bias and possibly remove from tests. The consistent separation between male and female performance curves illustrates such differential item functioning that would warrant further investigation.", "doc_id": "1811.10104", "figure_id": "1811.10104_fig_4", "figure_number": 3, "image_path": "data/mineru_output/1811.10104/1811.10104/hybrid_auto/images/1811.10104_page0_fig3.jpg", "caption": "Figure 3: Original graph from [22] illustrating DIF.", "figure_type": "plot", "visual_anchor": "Filled circles (Males) consistently positioned above open circles (Females) across the entire SAT-VERBAL score range from 200 to 800", "text_evidence": "Items displaying a DIF are ideally examined further to identify the cause of bias, and possibly removed from the test", "query_type": "comparison_explanation"}
{"query_id": "l1_1811.10104_1811.10104_fig_4_692", "query": "Does the 30-point gap between filled and open circles at SAT-VERBAL score 400 exemplify conditional probability fairness?", "answer": "Yes, the performance gap at matched SAT-VERBAL scores directly relates to conditional probability fairness, which modern ML fairness calls sufficiency or equalized odds. This criterion examines whether outcomes differ between groups given the same predictor score.", "doc_id": "1811.10104", "figure_id": "1811.10104_fig_4", "figure_number": 3, "image_path": "data/mineru_output/1811.10104/1811.10104/hybrid_auto/images/1811.10104_page0_fig3.jpg", "caption": "Figure 3: Original graph from [22] illustrating DIF.", "figure_type": "plot", "visual_anchor": "Vertical distance between filled circles (Males) and open circles (Females) at SAT-VERBAL score 400, showing approximately 30 percentage points difference", "text_evidence": "Peterson and Novick's \"conditional probability and its converse\" is equivalent to what in ML fairness is variously called sufficiency [4], equalized odds [32], or conditional procedure accuracy [5], sometimes expressed as the conditional independence A ⊥ D | Y", "query_type": "value_context"}
{"query_id": "l1_1811.10104_1811.10104_fig_4_693", "query": "How does the consistent vertical separation between filled and open circles relate to renewed 1980s intelligence debates?", "answer": "The persistent performance gap between males and females across all score levels illustrates the type of group differences that fueled 1980s public debates about racial differences in general intelligence and fair testing implications. Such visualizations of differential performance became central to discussions about test fairness.", "doc_id": "1811.10104", "figure_id": "1811.10104_fig_4", "figure_number": 3, "image_path": "data/mineru_output/1811.10104/1811.10104/hybrid_auto/images/1811.10104_page0_fig3.jpg", "caption": "Figure 3: Original graph from [22] illustrating DIF.", "figure_type": "plot", "visual_anchor": "Consistent vertical separation of approximately 10-20 percentage points between filled circles (Males) and open circles (Females) throughout the entire plot", "text_evidence": "With the start of the 1980s came renewed public debate about the existence of racial differences in general intelligence, and the implications for fair testing, following the publication of the controversial", "query_type": "comparison_explanation"}
{"query_id": "l1_1901.10436_1901.10436_fig_1_694", "query": "Why does the red bounding box in stage (a) need minimum 50x50 pixels before landmark extraction?", "answer": "Faces with region size less than 50x50 or inter-ocular distance of less than 30 pixels were discarded to ensure sufficient resolution for reliable processing and feature extraction.", "doc_id": "1901.10436", "figure_id": "1901.10436_fig_1", "figure_number": 1, "image_path": "data/mineru_output/1901.10436/1901.10436/hybrid_auto/images/1901.10436_page0_fig0.jpg", "caption": "Figure 1: Each candidate photo from YFCC-100M was processed by first detecting the depicted faces with a Convolutional Neural Network (CNN) using the Faster-RCNN based object detector [61]. Then each detected face as in (a) was processed using DLIB [62] to extract pose and landmark points as shown in (b) and subsequently assessed based on the width and height of the face region. Faces with region size less than 50x50 or inter-ocular distance of less than 30 pixels were discarded. Faces with non-frontal pose, or anything beyond being slightly tilted to the left or the right, were also discarded. Finally, an affine transformation was performed using center points of both eyes, and the face was rectified as shown in (c).", "figure_type": "diagram", "visual_anchor": "red bounding box in image (a) showing face detection region", "text_evidence": "Faces with region size less than 50x50 or inter-ocular distance of less than 30 pixels were discarded.", "query_type": "value_context"}
{"query_id": "l1_1901.10436_1901.10436_fig_1_695", "query": "How does the Faster-RCNN detector producing the red box relate to the YFCC-100M photo retrieval process?", "answer": "After downloading photos from active YFCC-100M URLs with Creative Commons licenses, the Faster-RCNN based CNN object detector was applied to detect all depicted faces in each retrieved photo.", "doc_id": "1901.10436", "figure_id": "1901.10436_fig_1", "figure_number": 1, "image_path": "data/mineru_output/1901.10436/1901.10436/hybrid_auto/images/1901.10436_page0_fig0.jpg", "caption": "Figure 1: Each candidate photo from YFCC-100M was processed by first detecting the depicted faces with a Convolutional Neural Network (CNN) using the Faster-RCNN based object detector [61]. Then each detected face as in (a) was processed using DLIB [62] to extract pose and landmark points as shown in (b) and subsequently assessed based on the width and height of the face region. Faces with region size less than 50x50 or inter-ocular distance of less than 30 pixels were discarded. Faces with non-frontal pose, or anything beyond being slightly tilted to the left or the right, were also discarded. Finally, an affine transformation was performed using center points of both eyes, and the face was rectified as shown in (c).", "figure_type": "diagram", "visual_anchor": "red bounding box from CNN face detection in image (a)", "text_evidence": "Once we retrieved the photo, we processed it using face detection to find all depicted faces. For the face detection step, we used a Convolutional Neural Network (CNN) object", "query_type": "comparison_explanation"}
{"query_id": "l1_1901.10436_1901.10436_fig_1_696", "query": "Why must faces progress from the red box detection to landmark points before reaching the rectified stage?", "answer": "The affine transformation for rectification requires center points of both eyes, which are obtained from DLIB landmark extraction performed on the detected face region.", "doc_id": "1901.10436", "figure_id": "1901.10436_fig_1", "figure_number": 1, "image_path": "data/mineru_output/1901.10436/1901.10436/hybrid_auto/images/1901.10436_page0_fig0.jpg", "caption": "Figure 1: Each candidate photo from YFCC-100M was processed by first detecting the depicted faces with a Convolutional Neural Network (CNN) using the Faster-RCNN based object detector [61]. Then each detected face as in (a) was processed using DLIB [62] to extract pose and landmark points as shown in (b) and subsequently assessed based on the width and height of the face region. Faces with region size less than 50x50 or inter-ocular distance of less than 30 pixels were discarded. Faces with non-frontal pose, or anything beyond being slightly tilted to the left or the right, were also discarded. Finally, an affine transformation was performed using center points of both eyes, and the face was rectified as shown in (c).", "figure_type": "diagram", "visual_anchor": "progression from red box (a) through landmark points (b) to rectified face (c)", "text_evidence": "Finally, an affine transformation was performed using center points of both eyes, and the face was rectified as shown in (c).", "query_type": "comparison_explanation"}
{"query_id": "l1_1901.10436_1901.10436_fig_4_697", "query": "Why are the 68 small dots used as the basis for localizing the 19 large yellow labeled landmarks?", "answer": "The 68 key-points extracted using DLIB from each face serve as the foundation to map and localize the 19 facial landmarks that were employed for extracting craniofacial measures for coding schemes 1-3.", "doc_id": "1901.10436", "figure_id": "1901.10436_fig_4", "figure_number": 2, "image_path": "data/mineru_output/1901.10436/1901.10436/hybrid_auto/images/1901.10436_page0_fig3.jpg", "caption": "Figure 2: We used the 68 key-points extracted using DLIB from each face (small dots) to localize 19 facial landmarks (large dots, labeled), out of the 47 introduced in [2]. Those 19 landmarks were employed as the basis for extraction of the craniofacial measures for coding schemes 1–3.", "figure_type": "diagram", "visual_anchor": "68 small dots and 19 large yellow dots with labels like tn, or, ps, ex, en", "text_evidence": "Those 19 landmarks were employed as the basis for extraction of the craniofacial measures for coding schemes 1–3.", "query_type": "value_context"}
{"query_id": "l1_1901.10436_1901.10436_fig_4_698", "query": "How do the vertical distance measures labeled n-sto and sto-gn relate to the eight characterizing measures?", "answer": "These vertical distances are part of the eight measures from coding scheme 1 that characterize all vertical distances between face elements: forehead top, eyes, nose, mouth, and chin.", "doc_id": "1901.10436", "figure_id": "1901.10436_fig_4", "figure_number": 2, "image_path": "data/mineru_output/1901.10436/1901.10436/hybrid_auto/images/1901.10436_page0_fig3.jpg", "caption": "Figure 2: We used the 68 key-points extracted using DLIB from each face (small dots) to localize 19 facial landmarks (large dots, labeled), out of the 47 introduced in [2]. Those 19 landmarks were employed as the basis for extraction of the craniofacial measures for coding schemes 1–3.", "figure_type": "diagram", "visual_anchor": "Vertical arrows labeled n-sto and sto-gn on the left side of the face", "text_evidence": "It comprises eight measures which characterize all the vertical distances between elements in a face: the top of the forehead, the eyes, the nose, the mouth and the chin.", "query_type": "comparison_explanation"}
{"query_id": "l1_1901.10436_1901.10436_fig_4_699", "query": "Why were the landmarks tn and sto marked despite DLIB providing 68 key-points for face detection?", "answer": "The landmarks tn and sto were required for the craniofacial coding scheme but were not part of the original 68 DLIB key-points set, necessitating additional localization.", "doc_id": "1901.10436", "figure_id": "1901.10436_fig_4", "figure_number": 2, "image_path": "data/mineru_output/1901.10436/1901.10436/hybrid_auto/images/1901.10436_page0_fig3.jpg", "caption": "Figure 2: We used the 68 key-points extracted using DLIB from each face (small dots) to localize 19 facial landmarks (large dots, labeled), out of the 47 introduced in [2]. Those 19 landmarks were employed as the basis for extraction of the craniofacial measures for coding schemes 1–3.", "figure_type": "diagram", "visual_anchor": "Large yellow dots labeled tn at the top and sto near the mouth", "text_evidence": "We note that two required points, tn and $s t o$ , were not part of the set of 68 DLIB key-points.", "query_type": "anomaly_cause"}
{"query_id": "l1_1901.10436_1901.10436_fig_5_700", "query": "How does the blue segment 'a' connecting yellow points C1 and C2 relate to craniofacial ratio features?", "answer": "The craniofacial ratio features used mapped DLIB key-points as facial landmarks, with eight dimensions summarized in Table 8, similar to the inner canthus points shown.", "doc_id": "1901.10436", "figure_id": "1901.10436_fig_5", "figure_number": 3, "image_path": "data/mineru_output/1901.10436/1901.10436/hybrid_auto/images/1901.10436_page0_fig4.jpg", "caption": "Figure 3: Process for extracting facial symmetry measures for coding scheme 4, starting with (a) rectified face showing face mid-line and reference points for inner canthus (C1 and C2) and philtrum (C3) and line segmented connecting them (point $a$ for C1-C2 and point $b$ connecting C3 to the midpoint of point $a$ ). Additionally, a Sobel filter is used to extract (b) edge magnitude and (c) orientation to derive the measure for edge orientation similarity.", "figure_type": "diagram", "visual_anchor": "Blue line segment 'a' connecting yellow points C1 and C2", "text_evidence": "Similar to the above features, the craniofacial ratios used the mapped DLIB key-points as facial landmarks. Table 8 summarizes the eight dimensions of the craniofacial ratio features.", "query_type": "value_context"}
{"query_id": "l1_1901.10436_1901.10436_fig_5_701", "query": "Why are reference points C1, C2, and C3 marked in yellow before applying the Sobel filter?", "answer": "These reference points for inner canthus and philtrum are extracted first to establish facial symmetry measures, which psychology and anthropology studies have found to be significant.", "doc_id": "1901.10436", "figure_id": "1901.10436_fig_5", "figure_number": 3, "image_path": "data/mineru_output/1901.10436/1901.10436/hybrid_auto/images/1901.10436_page0_fig4.jpg", "caption": "Figure 3: Process for extracting facial symmetry measures for coding scheme 4, starting with (a) rectified face showing face mid-line and reference points for inner canthus (C1 and C2) and philtrum (C3) and line segmented connecting them (point $a$ for C1-C2 and point $b$ connecting C3 to the midpoint of point $a$ ). Additionally, a Sobel filter is used to extract (b) edge magnitude and (c) orientation to derive the measure for edge orientation similarity.", "figure_type": "diagram", "visual_anchor": "Yellow points C1, C2, and C3 with connecting line segments", "text_evidence": "Facial symmetry has been found in psychology and anthropology studies to be", "query_type": "comparison_explanation"}
{"query_id": "l1_1901.10436_1901.10436_fig_5_702", "query": "Does the vertical dashed midline shown support measuring age progression features used for 0 to 18 age groups?", "answer": "The midline enables facial symmetry measurement, but the age progression features for 0-18 age groups specifically used craniofacial ratios from the third coding scheme, not symmetry.", "doc_id": "1901.10436", "figure_id": "1901.10436_fig_5", "figure_number": 3, "image_path": "data/mineru_output/1901.10436/1901.10436/hybrid_auto/images/1901.10436_page0_fig4.jpg", "caption": "Figure 3: Process for extracting facial symmetry measures for coding scheme 4, starting with (a) rectified face showing face mid-line and reference points for inner canthus (C1 and C2) and philtrum (C3) and line segmented connecting them (point $a$ for C1-C2 and point $b$ connecting C3 to the midpoint of point $a$ ). Additionally, a Sobel filter is used to extract (b) edge magnitude and (c) orientation to derive the measure for edge orientation similarity.", "figure_type": "diagram", "visual_anchor": "Vertical dashed midline dividing the face", "text_evidence": "The third coding scheme comprises measures corresponding to different ratios of the face. These features were used to estimate age progression from faces in the age groups of 0 to 18 in [4].", "query_type": "value_context"}
{"query_id": "l1_1901.10436_1901.10436_fig_8_703", "query": "Do the red outer and green inner regions around the mouth support the cross-cultural age perception findings?", "answer": "Yes, the contrast measurement between outer (red) and inner (green) mouth regions implements the facial contrast method validated across Chinese, Latin American, and black South African women aged 20-80, showing high-contrast faces appear younger.", "doc_id": "1901.10436", "figure_id": "1901.10436_fig_8", "figure_number": 4, "image_path": "data/mineru_output/1901.10436/1901.10436/hybrid_auto/images/1901.10436_page0_fig7.jpg", "caption": "Figure 4: Process for extracting facial regions contrast measures for coding scheme 5. The computation is based on the average pixel intensity differences between the outer and inner regions for the lips, eyes and eyebrows as depicted above.", "figure_type": "diagram", "visual_anchor": "red outer polygon and green inner arrow around the mouth region", "text_evidence": "An analysis of full face color photographs of Chinese, Latin American and black South African women aged 20–80 in [6] found similar changes in facial contrast with ageing across races and were comparable to changes with Caucasian faces. This study found that high-contrast faces were judged to be younger than low-contrast face", "query_type": "value_context"}
{"query_id": "l1_1901.10436_1901.10436_fig_8_704", "query": "Why are three separate red-green region pairs shown instead of a single full-face contrast measurement?", "answer": "The three pairs (eyes, eyebrows, lips) enable computation based on average pixel intensity differences between outer and inner regions for each facial feature separately, rather than a single aggregate measure.", "doc_id": "1901.10436", "figure_id": "1901.10436_fig_8", "figure_number": 4, "image_path": "data/mineru_output/1901.10436/1901.10436/hybrid_auto/images/1901.10436_page0_fig7.jpg", "caption": "Figure 4: Process for extracting facial regions contrast measures for coding scheme 5. The computation is based on the average pixel intensity differences between the outer and inner regions for the lips, eyes and eyebrows as depicted above.", "figure_type": "diagram", "visual_anchor": "three distinct red outer and green inner region pairs at left eye, right eye, and mouth", "text_evidence": "The computation is based on the average pixel intensity differences between the outer and inner regions for the lips, eyes and eyebrows as depicted above.", "query_type": "comparison_explanation"}
{"query_id": "l1_1901.10436_1901.10436_fig_8_705", "query": "How do the red and green contrast zones relate to the limitation that no universal skin measure exists?", "answer": "The contrast zones measure relative intensity differences between facial regions rather than absolute skin characteristics, addressing the challenge that there is no universal measure for skin despite its large fraction of facial area.", "doc_id": "1901.10436", "figure_id": "1901.10436_fig_8", "figure_number": 4, "image_path": "data/mineru_output/1901.10436/1901.10436/hybrid_auto/images/1901.10436_page0_fig7.jpg", "caption": "Figure 4: Process for extracting facial regions contrast measures for coding scheme 5. The computation is based on the average pixel intensity differences between the outer and inner regions for the lips, eyes and eyebrows as depicted above.", "figure_type": "diagram", "visual_anchor": "red outer boundaries and green inner regions defining contrast measurement zones", "text_evidence": "Skin occupies a large fraction of the face. As such, characteristics of the skin influence the appearance and perception of faces. Prior work has studied different methods of characterizing skin based on skin color [7, 69, 70], skin type [7, 38] and skin reflectance [71]. Early studies used Fitzpatrick skin type (FST) to classify sun-reactive skin types [38], which was also adopted recently in [36]. However, to-date, there is no universal measure for s", "query_type": "value_context"}
{"query_id": "l1_1901.10436_1901.10436_fig_14_706", "query": "How does the masked ITA map in step (g) enable extraction of Individual Typology Angle-based skin color?", "answer": "The masked ITA map (g) isolates skin regions from the full ITA map (f), allowing the histogram (h) to compute ITA values exclusively from facial skin pixels for coding scheme 6.", "doc_id": "1901.10436", "figure_id": "1901.10436_fig_14", "figure_number": 5, "image_path": "data/mineru_output/1901.10436/1901.10436/hybrid_auto/images/1901.10436_page0_fig13.jpg", "caption": "Figure 5: Process for extracting skin color for coding scheme 6 based on Individual Typology Angle-based (ITA). (a) Input face (b) skin map (c) $L$ channel (d) $a$ channel (e) $b$ channel (f) ITA map (g) masked ITA map (h) ITA histogram.", "figure_type": "diagram", "visual_anchor": "masked ITA map labeled (g)", "text_evidence": "Figure 5: Process for extracting skin color for coding scheme 6 based on Individual Typology Angle-based (ITA).", "query_type": "visual_definition"}
{"query_id": "l1_1901.10436_1901.10436_fig_14_707", "query": "Why are channels (d) and (e) necessary before generating the ITA map shown in step (f)?", "answer": "The a channel (d) and b channel (e) provide the red-green and blue-yellow color axes required to calculate the Individual Typology Angle, which is then visualized in the ITA map (f).", "doc_id": "1901.10436", "figure_id": "1901.10436_fig_14", "figure_number": 5, "image_path": "data/mineru_output/1901.10436/1901.10436/hybrid_auto/images/1901.10436_page0_fig13.jpg", "caption": "Figure 5: Process for extracting skin color for coding scheme 6 based on Individual Typology Angle-based (ITA). (a) Input face (b) skin map (c) $L$ channel (d) $a$ channel (e) $b$ channel (f) ITA map (g) masked ITA map (h) ITA histogram.", "figure_type": "diagram", "visual_anchor": "channels (d) and (e) preceding ITA map (f)", "text_evidence": "(a) Input face (b) skin map (c) $L$ channel (d) $a$ channel (e) $b$ channel (f) ITA map (g) masked ITA map (h) ITA histogram.", "query_type": "comparison_explanation"}
{"query_id": "l1_1901.10436_1901.10436_fig_14_708", "query": "Does the eight-step pipeline ending in histogram (h) contrast with the CNN approach for predicting age attributes?", "answer": "Yes, the ITA-based pipeline uses explicit color channel decomposition and geometric angle computation for skin color, while the CNN for age prediction learns feature representations directly from face images without manual feature design.", "doc_id": "1901.10436", "figure_id": "1901.10436_fig_14", "figure_number": 5, "image_path": "data/mineru_output/1901.10436/1901.10436/hybrid_auto/images/1901.10436_page0_fig13.jpg", "caption": "Figure 5: Process for extracting skin color for coding scheme 6 based on Individual Typology Angle-based (ITA). (a) Input face (b) skin map (c) $L$ channel (d) $a$ channel (e) $b$ channel (f) ITA map (g) masked ITA map (h) ITA histogram.", "figure_type": "diagram", "visual_anchor": "histogram (h) as final step", "text_evidence": "As an alternative to designing specific feature representations for predicting age, for coding scheme 7, we adopt a Convolutional Neural Network (CNN) that is trained from face images to predict age.", "query_type": "comparison_explanation"}
{"query_id": "l1_1901.10436_1901.10436_fig_18_709", "query": "Do the horizontal lines at 1.00 for both Simpson E and Shannon E confirm balanced evenness across all class counts?", "answer": "Yes, the perfect 1.00 evenness values across all class counts demonstrate a uniform distribution where all classes have equal representation, confirming balanced evenness.", "doc_id": "1901.10436", "figure_id": "1901.10436_fig_18", "figure_number": 6, "image_path": "data/mineru_output/1901.10436/1901.10436/hybrid_auto/images/1901.10436_page0_fig17.jpg", "caption": "Figure 6: Illustration of how (a) diversity and (b) evenness varies for a uniform distribution compared to how (c) diversity and (d) evenness varies for a random distribution.", "figure_type": "plot", "visual_anchor": "Horizontal red and blue lines both at y=1.00", "text_evidence": "Evenness is generally balanced with highest Simpson $E$ and Shannon $E$ of 0.981 and 0.995, respectively.", "query_type": "value_context"}
{"query_id": "l1_1901.10436_1901.10436_fig_18_710", "query": "Why does the uniform distribution maintain constant evenness at 1.00 while craniofacial distances achieve only 0.981 Simpson E maximum?", "answer": "The uniform distribution represents perfect theoretical balance with equal class representation, while real craniofacial distance measurements show slight natural variation, preventing perfect evenness.", "doc_id": "1901.10436", "figure_id": "1901.10436_fig_18", "figure_number": 6, "image_path": "data/mineru_output/1901.10436/1901.10436/hybrid_auto/images/1901.10436_page0_fig17.jpg", "caption": "Figure 6: Illustration of how (a) diversity and (b) evenness varies for a uniform distribution compared to how (c) diversity and (d) evenness varies for a random distribution.", "figure_type": "plot", "visual_anchor": "Red Simpson E line at constant 1.00 value", "text_evidence": "The highest Simpson $D$ value is 5.888 and the lowest is 5.832. The highest and lowest Shannon $H$ values are 1.782 and 1.777.", "query_type": "comparison_explanation"}
{"query_id": "l1_1901.10436_1901.10436_fig_18_711", "query": "How does the perfect overlap of red and blue lines at 1.00 relate to typical class mapping from Shannon H?", "answer": "The perfect 1.00 evenness overlap illustrates ideal uniform distribution, contrasting with the craniofacial scheme where Shannon H values of 1.777-1.782 typically map to 6 classes rather than perfect uniformity.", "doc_id": "1901.10436", "figure_id": "1901.10436_fig_18", "figure_number": 6, "image_path": "data/mineru_output/1901.10436/1901.10436/hybrid_auto/images/1901.10436_page0_fig17.jpg", "caption": "Figure 6: Illustration of how (a) diversity and (b) evenness varies for a uniform distribution compared to how (c) diversity and (d) evenness varies for a random distribution.", "figure_type": "plot", "visual_anchor": "Overlapping red Simpson E and blue Shannon E lines at y=1.00", "text_evidence": "Based on the Shannon $H$ values, this feature dimension would typically map to 6 classes.", "query_type": "comparison_explanation"}
{"query_id": "l1_1901.10436_1901.10436_fig_26_712", "query": "Does the bar height of approximately 2×10^5 at value 4 represent one of eight craniofacial distances?", "answer": "Yes, the bar represents the frequency distribution of one of the 8 craniofacial distances in coding scheme 1, as the figure summarizes feature distribution for all eight dimensions.", "doc_id": "1901.10436", "figure_id": "1901.10436_fig_26", "figure_number": 7, "image_path": "data/mineru_output/1901.10436/1901.10436/hybrid_auto/images/1901.10436_page0_fig25.jpg", "caption": "Figure 7: Feature distribution of craniofacial distances (coding scheme 1) for the $D i F$ data set.", "figure_type": "plot", "visual_anchor": "Highest bar at value 4 reaching approximately 2×10^5", "text_evidence": "Figure 7 summarizes the feature distribution for the 8 craniofacial distances in coding scheme 1.", "query_type": "value_context"}
{"query_id": "l1_1901.10436_1901.10436_fig_26_713", "query": "Why does the bar at value 8 reach only 1.3×10^5 while coding scheme 2 shows similar ranges?", "answer": "Both coding schemes map to similar value ranges (around 6 classes), suggesting comparable discretization despite different feature types. The bar height reflects frequency distribution within the DiF dataset.", "doc_id": "1901.10436", "figure_id": "1901.10436_fig_26", "figure_number": 7, "image_path": "data/mineru_output/1901.10436/1901.10436/hybrid_auto/images/1901.10436_page0_fig25.jpg", "caption": "Figure 7: Feature distribution of craniofacial distances (coding scheme 1) for the $D i F$ data set.", "figure_type": "plot", "visual_anchor": "Bar at value 8 reaching approximately 1.3×10^5", "text_evidence": "Compared to coding scheme 1, these values are in the similar range, mapping to 6 classes.", "query_type": "comparison_explanation"}
{"query_id": "l1_1901.10436_1901.10436_fig_26_714", "query": "How does the distribution shown by bars from value 4 to 8 relate to the twelve craniofacial area features?", "answer": "The distribution represents coding scheme 1 (craniofacial distances), not the twelve area features. The area features are summarized separately in coding scheme 2 with their own distinct dimensions.", "doc_id": "1901.10436", "figure_id": "1901.10436_fig_26", "figure_number": 7, "image_path": "data/mineru_output/1901.10436/1901.10436/hybrid_auto/images/1901.10436_page0_fig25.jpg", "caption": "Figure 7: Feature distribution of craniofacial distances (coding scheme 1) for the $D i F$ data set.", "figure_type": "plot", "visual_anchor": "Bars spanning values 4 to 8 on x-axis", "text_evidence": "Table 7 summarizes the twelve dimensions of the craniofacial area features.", "query_type": "visual_definition"}
{"query_id": "l1_1901.10436_1901.10436_fig_38_715", "query": "Does the bar at value 20 reaching 1.9×10^5 faces demonstrate the feature distribution for craniofacial areas?", "answer": "Yes, the bar at value 20 shows the highest frequency in the craniofacial area feature distribution for coding scheme 2 in the DiF dataset.", "doc_id": "1901.10436", "figure_id": "1901.10436_fig_38", "figure_number": 8, "image_path": "data/mineru_output/1901.10436/1901.10436/hybrid_auto/images/1901.10436_page0_fig37.jpg", "caption": "Figure 8: Feature distribution of craniofacial areas (coding scheme 2) for the $D i F$ data set.", "figure_type": "plot", "visual_anchor": "tallest bar at value 20 reaching approximately 1.9×10^5", "text_evidence": "Figure 8 summarizes the feature distribution for the 12 craniofacial areas in coding scheme 2.", "query_type": "value_context"}
{"query_id": "l1_1901.10436_1901.10436_fig_38_716", "query": "How does the variation between the tallest bar at 1.9×10^5 and shortest at 1.2×10^5 compare to age progression measures?", "answer": "The variation in bar heights reflects craniofacial ratios similar to those used for estimating age progression from faces in age groups 0 to 18.", "doc_id": "1901.10436", "figure_id": "1901.10436_fig_38", "figure_number": 8, "image_path": "data/mineru_output/1901.10436/1901.10436/hybrid_auto/images/1901.10436_page0_fig37.jpg", "caption": "Figure 8: Feature distribution of craniofacial areas (coding scheme 2) for the $D i F$ data set.", "figure_type": "plot", "visual_anchor": "difference between tallest bar (~1.9×10^5 at value 20) and shortest bar (~1.2×10^5 at value 22)", "text_evidence": "These features were used to estimate age progression from faces in the age groups of 0 to 18 in [4]. Similar to the above features, the craniofacial ratios used the mapped DLIB key-points as facial landmarks.", "query_type": "comparison_explanation"}
{"query_id": "l1_1901.10436_1901.10436_fig_38_717", "query": "Why do all bars remain between 1.2 and 1.9×10^5 if coding scheme 3 uses eight craniofacial measures?", "answer": "This figure shows coding scheme 2 with 12 craniofacial areas, not coding scheme 3. Scheme 3 uses different ratio-based measures across the face.", "doc_id": "1901.10436", "figure_id": "1901.10436_fig_38", "figure_number": 8, "image_path": "data/mineru_output/1901.10436/1901.10436/hybrid_auto/images/1901.10436_page0_fig37.jpg", "caption": "Figure 8: Feature distribution of craniofacial areas (coding scheme 2) for the $D i F$ data set.", "figure_type": "plot", "visual_anchor": "all six bars constrained between 1.2×10^5 and 1.9×10^5", "text_evidence": "Table 8: Coding scheme 3 is made up of eight craniofacial measures that correspond to different ratios of the face [3].", "query_type": "anomaly_cause"}
{"query_id": "l1_1901.10436_1901.10436_fig_47_718", "query": "Does the bar at value 0.5 reaching approximately 190,000 faces support the Simpson D value of 5.902 being the largest?", "answer": "Yes, the high frequency at 0.5 indicates strong class separation, which aligns with the largest Simpson D value of 5.902 mentioned for craniofacial ratios in coding scheme 3.", "doc_id": "1901.10436", "figure_id": "1901.10436_fig_47", "figure_number": 9, "image_path": "data/mineru_output/1901.10436/1901.10436/hybrid_auto/images/1901.10436_page0_fig46.jpg", "caption": "Figure 9: Feature distribution of craniofacial ratios (coding scheme 3) for the $D i F$ data set.   ", "figure_type": "plot", "visual_anchor": "leftmost bar at value 0.5 with height approximately 1.9 × 10^5", "text_evidence": "Figure 9 summarizes the feature distribution for the 8 craniofacial ratios in coding scheme 3. The largest Simpson $D$ value is 5.902 and smallest is 5.870.", "query_type": "value_context"}
{"query_id": "l1_1901.10436_1901.10436_fig_47_719", "query": "Why does the bar at value 0.8 show the lowest frequency despite Shannon H values mapping to approximately 6 classes?", "answer": "The low frequency at 0.8 reflects natural variation in craniofacial ratio distributions across the 6 classes, while Shannon H values of approximately 1.783 to 1.781 indicate overall entropy across all ratio bins.", "doc_id": "1901.10436", "figure_id": "1901.10436_fig_47", "figure_number": 9, "image_path": "data/mineru_output/1901.10436/1901.10436/hybrid_auto/images/1901.10436_page0_fig46.jpg", "caption": "Figure 9: Feature distribution of craniofacial ratios (coding scheme 3) for the $D i F$ data set.   ", "figure_type": "plot", "visual_anchor": "bar at value 0.8 with height approximately 1.3 × 10^5, the shortest among all bars", "text_evidence": "Similarly, the largest Shannon $H$ value is 1.783 and smallest is 1.781. This would map to approximately to 6 classes.", "query_type": "anomaly_cause"}
{"query_id": "l1_1901.10436_1901.10436_fig_47_720", "query": "How does the distribution pattern with peaks at 0.5 and 0.9 relate to computing facial symmetry measures using density difference?", "answer": "The bimodal distribution at 0.5 and 0.9 likely reflects the craniofacial ratio (ls-sto)/(sto-li) computed from facial symmetry measures, where extreme values indicate distinct facial structure categories in the left and right 128x64 parts.", "doc_id": "1901.10436", "figure_id": "1901.10436_fig_47", "figure_number": 9, "image_path": "data/mineru_output/1901.10436/1901.10436/hybrid_auto/images/1901.10436_page0_fig46.jpg", "caption": "Figure 9: Feature distribution of craniofacial ratios (coding scheme 3) for the $D i F$ data set.   ", "figure_type": "plot", "visual_anchor": "two tallest bars at values 0.5 and 0.9, both reaching approximately 1.9 × 10^5", "text_evidence": "Finally, we compute two facial symmetry measures based on density difference $D D ( x , y )$ and edge orientation similarity $E O S ( x , y )$ as follows: for each pixel $( x , y )$ in the left 128x64 part ( $I$ and $I _ { e }$ ) and the corresponding 128x64 right part", "query_type": "comparison_explanation"}
{"query_id": "l1_1901.10436_1901.10436_fig_50_721", "query": "Why does the tallest bar at 0.005 reach 2.35×10^5 faces despite facial symmetry having only middle-range diversity?", "answer": "The middle-range diversity (Simpson D of 5.510 and Shannon H of 1.748) indicates moderate variation in symmetry features, but the peak at 0.005 shows most faces cluster around this particular edge orientation similarity value, creating a concentrated distribution within the moderate diversity range.", "doc_id": "1901.10436", "figure_id": "1901.10436_fig_50", "figure_number": 10, "image_path": "data/mineru_output/1901.10436/1901.10436/hybrid_auto/images/1901.10436_page0_fig49.jpg", "caption": "Figure 10: Feature distribution of facial symmetry (coding scheme 4): (a) density difference and (b) edge orientation similarity for the $D i F$ data set.", "figure_type": "plot", "visual_anchor": "Tallest bar at x=0.005 reaching approximately 2.35×10^5 on y-axis", "text_evidence": "Figure 10 summarizes the feature distribution for facial symmetry in coding scheme 4. The diversity value is in a middle range compared to the previous coding schemes. For example, the highest Simpson $D$ is 5.510 and the largest Shannon $H$ is 1.748.", "query_type": "comparison_explanation"}
{"query_id": "l1_1901.10436_1901.10436_fig_50_722", "query": "How does the bar height at 0.01 compare to the peak, given that evenness values are lower for this coding scheme?", "answer": "The bar at 0.01 reaches approximately 2.15×10^5, only slightly lower than the peak at 2.35×10^5. The lower evenness values explain this near-peak concentration, indicating the distribution is not flat but clustered around specific similarity values.", "doc_id": "1901.10436", "figure_id": "1901.10436_fig_50", "figure_number": 10, "image_path": "data/mineru_output/1901.10436/1901.10436/hybrid_auto/images/1901.10436_page0_fig49.jpg", "caption": "Figure 10: Feature distribution of facial symmetry (coding scheme 4): (a) density difference and (b) edge orientation similarity for the $D i F$ data set.", "figure_type": "plot", "visual_anchor": "Second tallest bar at x=0.01 reaching approximately 2.15×10^5", "text_evidence": "The diversity value is in a middle range compared to the previous coding schemes. For example, the highest Simpson $D$ is 5.510 and the largest Shannon $H$ is 1.748. The evenness values are lower as well with highe", "query_type": "comparison_explanation"}
{"query_id": "l1_1901.10436_1901.10436_fig_50_723", "query": "What facial feature computation produces the edge orientation similarity distribution shown with peak at 0.005?", "answer": "The edge orientation similarity is computed using pixel intensity at (x,y) for CIE-Lab channels, comparing outer and inner regions around each facial part to measure symmetry. This computation generates the distribution with its characteristic peak at 0.005.", "doc_id": "1901.10436", "figure_id": "1901.10436_fig_50", "figure_number": 10, "image_path": "data/mineru_output/1901.10436/1901.10436/hybrid_auto/images/1901.10436_page0_fig49.jpg", "caption": "Figure 10: Feature distribution of facial symmetry (coding scheme 4): (a) density difference and (b) edge orientation similarity for the $D i F$ data set.", "figure_type": "plot", "visual_anchor": "Distribution peak at x=0.005 in edge orientation similarity axis", "text_evidence": "The computation is summarized in Table 10, where $I _ { k } ( x , y )$ is the pixel intensity at $( x , y )$ for CIE-Lab channel $k$ and $p t _ { o u t e r } , p t _ { i n n e r }$ correspond to the outer and inner regions around each facial part $p t$ .", "query_type": "visual_definition"}
{"query_id": "l1_1901.10436_1901.10436_fig_58_724", "query": "Does the tallest bar at value 0.01 reaching 200,000 faces support the Shannon E evenness factor of 0.979?", "answer": "No, the prominent peak at 0.01 contradicts perfect evenness. A Shannon E of 0.979 indicates measures are close to evenly distributed, but the histogram shows clear imbalance with one dominant bar.", "doc_id": "1901.10436", "figure_id": "1901.10436_fig_58", "figure_number": 11, "image_path": "data/mineru_output/1901.10436/1901.10436/hybrid_auto/images/1901.10436_page0_fig57.jpg", "caption": "Figure 11: Feature distribution of facial regions contrast (coding scheme 5) for the $D i F$ data set.   ", "figure_type": "plot", "visual_anchor": "Tallest blue bar at x=0.01 reaching approximately 200,000 on y-axis", "text_evidence": "The evenness factor Shannon $E$ is very close to 0.979 indicating that the measures are close to ev", "query_type": "anomaly_cause"}
{"query_id": "l1_1901.10436_1901.10436_fig_58_725", "query": "How does the bar at value -0.01 reaching approximately 100,000 faces relate to the highest Simpson D value of 5.872?", "answer": "The Simpson D value of 5.872 represents diversity across all bins. The bar at -0.01 is the shortest, contributing to overall diversity by showing lower frequency compared to other contrast values.", "doc_id": "1901.10436", "figure_id": "1901.10436_fig_58", "figure_number": 11, "image_path": "data/mineru_output/1901.10436/1901.10436/hybrid_auto/images/1901.10436_page0_fig57.jpg", "caption": "Figure 11: Feature distribution of facial regions contrast (coding scheme 5) for the $D i F$ data set.   ", "figure_type": "plot", "visual_anchor": "Shortest blue bar at x=-0.01 reaching approximately 100,000 faces", "text_evidence": "The highest Simpson $D$ value is 5.872 and highest Shannon $H$ value is 1.781, which is equivalent to 5.9 classes.", "query_type": "value_context"}
{"query_id": "l1_1901.10436_1901.10436_fig_58_726", "query": "Why do the seven bars span from -0.02 to 0.01 when contrast is computed from a single ITA score?", "answer": "The contrast values represent differences between facial regions, not the ITA score itself. Averaging ITA values per face produces the baseline, while regional contrasts create the distribution shown spanning negative to positive values.", "doc_id": "1901.10436", "figure_id": "1901.10436_fig_58", "figure_number": 11, "image_path": "data/mineru_output/1901.10436/1901.10436/hybrid_auto/images/1901.10436_page0_fig57.jpg", "caption": "Figure 11: Feature distribution of facial regions contrast (coding scheme 5) for the $D i F$ data set.   ", "figure_type": "plot", "visual_anchor": "Seven blue bars distributed across x-axis from -0.02 to 0.01", "text_evidence": "Average the values to give a single ITA score for each face", "query_type": "comparison_explanation"}
{"query_id": "l1_1901.10436_1901.10436_fig_61_727", "query": "Does the tallest bar at 20-30 reaching 340,000 faces explain why Simpson D equals 4.368 for this distribution?", "answer": "Yes, the extreme peak at age 20-30 with 340,000 faces creates an uneven distribution, directly resulting in the calculated Simpson D value of 4.368 and Shannon H value of 1.601.", "doc_id": "1901.10436", "figure_id": "1901.10436_fig_61", "figure_number": 13, "image_path": "data/mineru_output/1901.10436/1901.10436/hybrid_auto/images/1901.10436_page0_fig60.jpg", "caption": "Figure 13: Feature distribution of (a) age prediction (coding scheme 7) and (b) gender prediction (coding scheme 8) for the $D i F$ data set.   ", "figure_type": "plot", "visual_anchor": "tallest dark blue bar at 20-30 age group reaching approximately 3.4×10^5", "text_evidence": "The Simpson $D$ and Shannon $H$ values are 4.368 and 1.601. Because of the data distribution not being even", "query_type": "value_context"}
{"query_id": "l1_1901.10436_1901.10436_fig_61_728", "query": "Why does the 20-30 bar exceed the 31-45 bar by over 120,000 faces in coding scheme 7?", "answer": "The DiF dataset has an uneven age distribution concentrated in the 20-30 range, which is reflected in the seven-group binning structure of coding scheme 7 ([0-3],[4-12],[13-19],[20-30],[31-45],[46-60],[61-]).", "doc_id": "1901.10436", "figure_id": "1901.10436_fig_61", "figure_number": 13, "image_path": "data/mineru_output/1901.10436/1901.10436/hybrid_auto/images/1901.10436_page0_fig60.jpg", "caption": "Figure 13: Feature distribution of (a) age prediction (coding scheme 7) and (b) gender prediction (coding scheme 8) for the $D i F$ data set.   ", "figure_type": "plot", "visual_anchor": "20-30 bar at ~3.4×10^5 compared to 31-45 bar at ~2.1×10^5", "text_evidence": "summarizes the feature distribution for age prediction in coding scheme 7, where we bin the age values into seven groups: [0-3],[4-12],[13-19],[20-30],[31-45],[46-60],[61-]", "query_type": "comparison_explanation"}
{"query_id": "l1_1901.10436_1901.10436_fig_61_729", "query": "How does the minimal height of the >60 bar relate to the feature distribution summary for gender prediction?", "answer": "The >60 bar's minimal height (under 0.3×10^5) shows age imbalance in the DiF dataset, which contrasts with the separate gender prediction analysis also performed on the same DiF dataset using coding scheme 8.", "doc_id": "1901.10436", "figure_id": "1901.10436_fig_61", "figure_number": 13, "image_path": "data/mineru_output/1901.10436/1901.10436/hybrid_auto/images/1901.10436_page0_fig60.jpg", "caption": "Figure 13: Feature distribution of (a) age prediction (coding scheme 7) and (b) gender prediction (coding scheme 8) for the $D i F$ data set.   ", "figure_type": "plot", "visual_anchor": "shortest bar at >60 group with height approximately 0.2×10^5", "text_evidence": "Figure 13 also summarizes the feature distribution for gender prediction in coding scheme 8", "query_type": "value_context"}
{"query_id": "l1_1901.10436_1901.10436_fig_64_730", "query": "Do the nearly equal bar heights at 4.6×10^5 for Female and Male support higher evenness scores in gender annotations?", "answer": "Yes, the equal distribution between Female and Male categories in the bar chart reflects balanced representation, which contributes to evenness scores closer to one as mentioned for the coding scheme measurements.", "doc_id": "1901.10436", "figure_id": "1901.10436_fig_64", "figure_number": 14, "image_path": "data/mineru_output/1901.10436/1901.10436/hybrid_auto/images/1901.10436_page0_fig63.jpg", "caption": "Figure 14: Feature distributions of subjective annotations (coding scheme 9) for (a) age and (b) gender for the $D i F$ data set.", "figure_type": "plot", "visual_anchor": "Two bars at approximately 4.6×10^5 height for Female and Male", "text_evidence": "Generally, they are higher than measures used for age and gender, whether using a predictive model or subjective annotation. Similarly, their evenness scores are also closer to one.", "query_type": "comparison_explanation"}
{"query_id": "l1_1901.10436_1901.10436_fig_64_731", "query": "Why do both bars reach approximately 460,000 when this represents subjective annotations for the DiF data set?", "answer": "The equal heights around 460,000 for both Female and Male indicate that the DiF data set contains roughly balanced gender representation in its subjective annotations, showing approximately equal numbers of images annotated for each gender category.", "doc_id": "1901.10436", "figure_id": "1901.10436_fig_64", "figure_number": 14, "image_path": "data/mineru_output/1901.10436/1901.10436/hybrid_auto/images/1901.10436_page0_fig63.jpg", "caption": "Figure 14: Feature distributions of subjective annotations (coding scheme 9) for (a) age and (b) gender for the $D i F$ data set.", "figure_type": "plot", "visual_anchor": "Both bars at 4.6×10^5 level", "text_evidence": "Figure 14 summarizes the feature distribution for the subjective annotations of age and gender for coding scheme 9.", "query_type": "value_context"}
{"query_id": "l1_1901.10436_1901.10436_fig_64_732", "query": "How does the uniform height of both gender bars contrast with the diversity scores observed in craniofacial schemes?", "answer": "The uniform bar heights show limited diversity with only two gender categories, whereas craniofacial schemes demonstrate higher diversity scores relative to gender measures, indicating more varied dimensional distributions across their feature spaces.", "doc_id": "1901.10436", "figure_id": "1901.10436_fig_64", "figure_number": 14, "image_path": "data/mineru_output/1901.10436/1901.10436/hybrid_auto/images/1901.10436_page0_fig63.jpg", "caption": "Figure 14: Feature distributions of subjective annotations (coding scheme 9) for (a) age and (b) gender for the $D i F$ data set.", "figure_type": "plot", "visual_anchor": "Equal height bars for Female and Male categories", "text_evidence": "One is that the many of the dimensions of the craniofacial schemes have high scores in diversity relative to the other coding schemes.", "query_type": "comparison_explanation"}
{"query_id": "l1_1901.10436_1901.10436_fig_65_733", "query": "Why does the Frontal bar reach 9×10⁵ while craniofacial schemes show higher diversity than pose?", "answer": "The Frontal pose dominates the dataset with approximately 900,000 images, creating low diversity (Shannon H=0.39). Craniofacial schemes achieve higher diversity scores across their multiple dimensions despite this pose imbalance.", "doc_id": "1901.10436", "figure_id": "1901.10436_fig_65", "figure_number": 15, "image_path": "data/mineru_output/1901.10436/1901.10436/hybrid_auto/images/1901.10436_page0_fig64.jpg", "caption": "Figure 15: Feature distribution of pose and resolution (coding scheme 10) for the $D i F$ data set, including (a) pose, (b) face region bounding box size, (c) intra-ocular distance (IOD).", "figure_type": "plot", "visual_anchor": "Middle bar labeled 'Frontal' at approximately 9×10⁵ on y-axis", "text_evidence": "One is that the many of the dimensions of the craniofacial schemes have high scores in diversity relative to the other coding schemes. Generally, they are higher than measures used for age and gender", "query_type": "comparison_explanation"}
{"query_id": "l1_1901.10436_1901.10436_fig_65_734", "query": "Does the overwhelming height of the Frontal bar compared to tilted categories support the Shannon H of 0.39?", "answer": "Yes, the Frontal category contains roughly 18 times more images than either tilted category, creating severe imbalance. This extreme skew directly produces the low Shannon H value of 0.39 for the three-class pose distribution.", "doc_id": "1901.10436", "figure_id": "1901.10436_fig_65", "figure_number": 15, "image_path": "data/mineru_output/1901.10436/1901.10436/hybrid_auto/images/1901.10436_page0_fig64.jpg", "caption": "Figure 15: Feature distribution of pose and resolution (coding scheme 10) for the $D i F$ data set, including (a) pose, (b) face region bounding box size, (c) intra-ocular distance (IOD).", "figure_type": "plot", "visual_anchor": "Frontal bar at 9×10⁵ versus Frontal Tilted Left and Right bars at approximately 0.5×10⁵", "text_evidence": "The three class pose distribution has a Shannon $H$ value of 0.39", "query_type": "value_context"}
{"query_id": "l1_1901.10436_1901.10436_fig_65_735", "query": "How do the three pose categories shown relate to the DLIB face detection output dimensions mentioned?", "answer": "The three bars represent binned pose values derived from DLIB face detection output. The values used are Frontal Tilted Left, Frontal, and Frontal Tilted Right, computed from DLIB's facial landmark detection for the DiF dataset.", "doc_id": "1901.10436", "figure_id": "1901.10436_fig_65", "figure_number": 15, "image_path": "data/mineru_output/1901.10436/1901.10436/hybrid_auto/images/1901.10436_page0_fig64.jpg", "caption": "Figure 15: Feature distribution of pose and resolution (coding scheme 10) for the $D i F$ data set, including (a) pose, (b) face region bounding box size, (c) intra-ocular distance (IOD).", "figure_type": "plot", "visual_anchor": "Three x-axis categories: Frontal Tilted Left, Frontal, and Frontal Tilted Right", "text_evidence": "Pose uses three dimensions from the output of DLIB face detection and the distribution is shown in 15 (a). When computing mean and variance for pose in Table 12, we used the following values: Frontal Tilted L", "query_type": "visual_definition"}
{"query_id": "l1_1902.03519_1902.03519_fig_4_736", "query": "How does the top node containing 3 blue and 1 red point demonstrate the embedding into γ-HST?", "answer": "The top node represents the root of the γ-HST embedding of input points, which is the first step in the fairlet decomposition algorithm after embedding the point set into a hierarchical tree structure.", "doc_id": "1902.03519", "figure_id": "1902.03519_fig_4", "figure_number": 1, "image_path": "data/mineru_output/1902.03519/1902.03519/hybrid_auto/images/1902.03519_page0_fig3.jpg", "caption": "Figure 1: A run of our algorithm for (1,3)-fairlet decomposition on 8 blue points and 4 red points in $\\mathbb { R } ^ { 2 }$ . Steps (c)-(e) show the three stages of step 1 in FairletDecomposition.", "figure_type": "diagram", "visual_anchor": "top node with 3 blue circles and 1 red circle", "text_evidence": "The first step in our algorithm is to embed the input point set into a γ-HST (for a value of γ to be determined later in this section). Once we build a γ-HST embedding T of the input points", "query_type": "value_context"}
{"query_id": "l1_1902.03519_1902.03519_fig_4_737", "query": "Why do all three bottom child nodes show balanced 1:2 red-to-blue ratios when the algorithm processes 8 blue and 4 red points?", "answer": "The three child nodes demonstrate the fairlet decomposition stages where the algorithm creates balanced (1,3)-fairlets, ensuring each fairlet maintains the required r:b ratio of 1 red to 3 blue points as specified by the decomposition constraint.", "doc_id": "1902.03519", "figure_id": "1902.03519_fig_4", "figure_number": 1, "image_path": "data/mineru_output/1902.03519/1902.03519/hybrid_auto/images/1902.03519_page0_fig3.jpg", "caption": "Figure 1: A run of our algorithm for (1,3)-fairlet decomposition on 8 blue points and 4 red points in $\\mathbb { R } ^ { 2 }$ . Steps (c)-(e) show the three stages of step 1 in FairletDecomposition.", "figure_type": "diagram", "visual_anchor": "three bottom nodes each containing 1 red and 2 blue circles", "text_evidence": "A run of our algorithm for (1,3)-fairlet decomposition on 8 blue points and 4 red points in $\\mathbb { R } ^ { 2 }$ . Steps (c)-(e) show the three stages of step 1 in FairletDecomposition.", "query_type": "comparison_explanation"}
{"query_id": "l1_1902.03519_1902.03519_fig_4_738", "query": "Does the hierarchical decomposition shown improve cost compared to the baseline, given each node can have γ^d children?", "answer": "Yes, the empirical results show cost improvements on all instances compared to the baseline algorithm, despite the potential for each node to have up to γ^d children in the tree structure.", "doc_id": "1902.03519", "figure_id": "1902.03519_fig_4", "figure_number": 1, "image_path": "data/mineru_output/1902.03519/1902.03519/hybrid_auto/images/1902.03519_page0_fig3.jpg", "caption": "Figure 1: A run of our algorithm for (1,3)-fairlet decomposition on 8 blue points and 4 red points in $\\mathbb { R } ^ { 2 }$ . Steps (c)-(e) show the three stages of step 1 in FairletDecomposition.", "figure_type": "diagram", "visual_anchor": "tree structure with parent node branching to three child nodes", "text_evidence": "Although the number of children of $T ( v )$ can be as large as $\\gamma ^ { d }$ , for each node $v$ in $T$ , MinHeavyPoints performs... Comparing the cost of the solution returned by our fairlet decomposition algorithm with the result of (Chierichetti et al., 2017) (as in Table 1) shows that we achieve empirical improvements on all instances.", "query_type": "comparison_explanation"}
{"query_id": "l1_1902.03519_1902.03519_fig_8_739", "query": "Does the runtime increase from 0.01 to 0.23 seconds across 5000 points confirm the claimed almost linear scaling?", "answer": "Yes, the smooth, nearly linear growth from 0.01 to 0.23 seconds as points increase from 0 to 5000 demonstrates the algorithm's almost linear scalability in the number of points.", "doc_id": "1902.03519", "figure_id": "1902.03519_fig_8", "figure_number": 2, "image_path": "data/mineru_output/1902.03519/1902.03519/hybrid_auto/images/1902.03519_page0_fig7.jpg", "caption": "Figure 2: Each figure captures the running time of our fairlet decomposition algorithms with the specified balance parameter on different number of sample points from one of the four datasets: Diabetes, Bank, Census and Census II.", "figure_type": "plot", "visual_anchor": "Runtime curve rising from 0.01 seconds at 0 points to 0.23 seconds at 5000 points", "text_evidence": "At the same time, the empirical runtime of our algorithm scales almost linearly in the number of points, making it applicable to massive data sets", "query_type": "comparison_explanation"}
{"query_id": "l1_1902.03519_1902.03519_fig_8_740", "query": "Why does the runtime reach only 0.23 seconds at 5000 points for the Bank dataset's fairlet decomposition?", "answer": "The fairlet decomposition algorithm for the Bank dataset with (1,2)-fairlet balance achieves efficient performance, with runtime staying below 0.25 seconds even at 5000 points, demonstrating its applicability to massive datasets.", "doc_id": "1902.03519", "figure_id": "1902.03519_fig_8", "figure_number": 2, "image_path": "data/mineru_output/1902.03519/1902.03519/hybrid_auto/images/1902.03519_page0_fig7.jpg", "caption": "Figure 2: Each figure captures the running time of our fairlet decomposition algorithms with the specified balance parameter on different number of sample points from one of the four datasets: Diabetes, Bank, Census and Census II.", "figure_type": "plot", "visual_anchor": "Maximum runtime value of approximately 0.23 seconds at 5000 sub-sampled points", "text_evidence": "Each figure captures the running time of our fairlet decomposition algorithms with the specified balance parameter on different number of sample points from one of the four datasets: Diabetes, Bank, Census and Census II.", "query_type": "value_context"}
{"query_id": "l1_1902.03519_1902.03519_fig_8_741", "query": "Does the consistent upward slope without plateaus between 1000 and 4000 points support applicability to massive datasets?", "answer": "Yes, the steady linear increase without performance degradation between 1000 and 4000 points indicates the algorithm maintains consistent efficiency as data size grows, validating its suitability for massive datasets.", "doc_id": "1902.03519", "figure_id": "1902.03519_fig_8", "figure_number": 2, "image_path": "data/mineru_output/1902.03519/1902.03519/hybrid_auto/images/1902.03519_page0_fig7.jpg", "caption": "Figure 2: Each figure captures the running time of our fairlet decomposition algorithms with the specified balance parameter on different number of sample points from one of the four datasets: Diabetes, Bank, Census and Census II.", "figure_type": "plot", "visual_anchor": "Smooth, linear upward slope between 1000 points (0.07 seconds) and 4000 points (0.20 seconds)", "text_evidence": "the empirical runtime of our algorithm scales almost linearly in the number of points, making it applicable to massive data sets", "query_type": "comparison_explanation"}
{"query_id": "l1_1902.07823_1902.07823_fig_2_742", "query": "Why does the orange KAAS-St curve drop sharply to 40 between λ=0 and λ=0.01 for race attribute?", "answer": "KAAS is formulated as Program (RegFair) and selected as a representative algorithm ensuring statistical parity. The sharp stability drop reflects how the regularization parameter λ impacts the algorithm's convergence behavior when enforcing fairness constraints.", "doc_id": "1902.07823", "figure_id": "1902.07823_fig_2", "figure_number": 1, "image_path": "data/mineru_output/1902.07823/1902.07823/hybrid_auto/images/1902.07823_page0_fig1.jpg", "caption": "Figure 1: stab vs. $\\lambda$ for race attribute.", "figure_type": "plot", "visual_anchor": "orange dashed line dropping from ~100 to ~40 between λ=0 and λ=0.01", "text_evidence": "We also select KAAS [43] and GYF [34] as representatives of algorithms that are formulated as Program (RegFair).", "query_type": "anomaly_cause"}
{"query_id": "l1_1902.07823_1902.07823_fig_2_743", "query": "Does ZVRG-St's stability of approximately 35 at λ=0.05 support its reported better fairness than other algorithms?", "answer": "Yes, ZVRG-St (solid blue line) maintains relatively low and stable values (~35) at λ=0.05, consistent with its formulation in the convex optimization framework. This stability pattern aligns with reports that ZVRG achieves better fairness and comparable accuracy.", "doc_id": "1902.07823", "figure_id": "1902.07823_fig_2", "figure_number": 1, "image_path": "data/mineru_output/1902.07823/1902.07823/hybrid_auto/images/1902.07823_page0_fig1.jpg", "caption": "Figure 1: stab vs. $\\lambda$ for race attribute.", "figure_type": "plot", "visual_anchor": "solid blue line with diamond markers ending at ~35 when λ=0.05", "text_evidence": "We choose ZVRG [77] since it is reported to achieve the better fairness than, and comparable accuracy to, other algorithms [32].", "query_type": "value_context"}
{"query_id": "l1_1902.07823_1902.07823_fig_2_744", "query": "How do the three statistical parity algorithms compare in stability at λ=0.03 on the race attribute?", "answer": "At λ=0.03, ZVRG-St (blue) and GYR-St (green dotted) both show stability around 40, while KAAS-St (orange dashed) peaks near 60. All three algorithms are designed to ensure statistical parity through convex optimization, but exhibit different stability responses to the regularization parameter.", "doc_id": "1902.07823", "figure_id": "1902.07823_fig_2", "figure_number": 1, "image_path": "data/mineru_output/1902.07823/1902.07823/hybrid_auto/images/1902.07823_page0_fig1.jpg", "caption": "Figure 1: stab vs. $\\lambda$ for race attribute.", "figure_type": "plot", "visual_anchor": "three curves at λ=0.03: blue at ~40, green dotted at ~40, orange dashed at ~60", "text_evidence": "We select three fair classification algorithms designed to ensure statistical parity that can be formulated in the convex optimization framework of Program (ConFair).", "query_type": "comparison_explanation"}
{"query_id": "l1_1902.07823_1902.07823_fig_3_745", "query": "Why does KAAS-St's orange dashed line peak at λ=0.03 when the expected number of different predictions should decrease monotonically?", "answer": "The peak at λ=0.03 (around 65) contradicts typical monotonic behavior. The stab metric measures expected differences between predictions on training sets S and S', which may not decrease uniformly across all λ values for KAAS-St.", "doc_id": "1902.07823", "figure_id": "1902.07823_fig_3", "figure_number": 2, "image_path": "data/mineru_output/1902.07823/1902.07823/hybrid_auto/images/1902.07823_page0_fig2.jpg", "caption": "Figure 2: stab vs. $\\lambda$ for sex attribute.", "figure_type": "plot", "visual_anchor": "orange dashed line (KAAS-St) peaking at approximately 65 when λ=0.03", "text_evidence": "$\\mathrm { s t a b } _ { T } ( \\mathcal { A } )$ indicates the expected number of different predictions of $A _ { S }$ and $\\mathbf { \\boldsymbol { A } } _ { S ^ { \\prime } }$ over the testing set $T$", "query_type": "anomaly_cause"}
{"query_id": "l1_1902.07823_1902.07823_fig_3_746", "query": "Does the solid blue ZVRG-St line reaching its minimum around 25 at λ=0.03 demonstrate better prediction stability than GYR-St's dotted line?", "answer": "Yes, ZVRG-St achieves the lowest stab value (approximately 25) at λ=0.03, indicating fewer different predictions between training sets compared to GYR-St which stabilizes around 35-40.", "doc_id": "1902.07823", "figure_id": "1902.07823_fig_3", "figure_number": 2, "image_path": "data/mineru_output/1902.07823/1902.07823/hybrid_auto/images/1902.07823_page0_fig2.jpg", "caption": "Figure 2: stab vs. $\\lambda$ for sex attribute.", "figure_type": "plot", "visual_anchor": "solid blue line (ZVRG-St) minimum at approximately 25 when λ=0.03", "text_evidence": "Note that this metric is considered in [32], but is slightly different from prediction stability since $S$ and $S ^ { \\prime }$ may differ by more than one training sample", "query_type": "comparison_explanation"}
{"query_id": "l1_1902.07823_1902.07823_fig_3_747", "query": "How does the convergence pattern of all three methods after λ=0.03 relate to stability measurement for the sex attribute?", "answer": "After λ=0.03, all three methods show convergence toward similar stab values (30-45 range), suggesting that higher λ values reduce differences in prediction stability across algorithms for the sex attribute.", "doc_id": "1902.07823", "figure_id": "1902.07823_fig_3", "figure_number": 2, "image_path": "data/mineru_output/1902.07823/1902.07823/hybrid_auto/images/1902.07823_page0_fig2.jpg", "caption": "Figure 2: stab vs. $\\lambda$ for sex attribute.", "figure_type": "plot", "visual_anchor": "convergence of all three lines (blue solid, orange dashed, purple dotted) between λ=0.03 and λ=0.05", "text_evidence": "Adult dataset, sex attribute   \nFigure 2: stab vs. $\\lambda$ for sex attribute", "query_type": "comparison_explanation"}
{"query_id": "l1_1903.08136_1903.08136_fig_1_748", "query": "Why does the gray sector contain only triangles and circles without showing how low-degree users get excluded?", "answer": "The gray sector demonstrates a community where both opinion types (shapes) and variations (textures) coexist, but methods like CESNA and Louvain fail to assign low-degree nodes or assign them to trivially small communities that prevent inclusion in analysis.", "doc_id": "1903.08136", "figure_id": "1903.08136_fig_1", "figure_number": 1, "image_path": "data/mineru_output/1903.08136/1903.08136/hybrid_auto/images/1903.08136_page0_fig0.jpg", "caption": "Figure 1: Demonstration of versatility of relevant low-degree people in social networks who get ignored by biased community detection methods. Shapes (circle, triangle) represent opinion, and texture represents variations of the opinion. We provide real-world examples of how this manifests in subsequent sections.", "figure_type": "diagram", "visual_anchor": "gray quarter-circle sector with mixed blue triangles and red circles", "text_evidence": "many community detection approaches either fail to assign low degree (or lowly-connected) users to communities, or assign them to trivially small communities that prevent them from being included in analysis", "query_type": "comparison_explanation"}
{"query_id": "l1_1903.08136_1903.08136_fig_1_749", "query": "Do the scattered shapes in the light gray region represent the versatile relevant people ignored by biased methods?", "answer": "Yes, the scattered low-degree nodes in the light gray region represent versatile relevant people who hold diverse opinions (circle vs triangle) and opinion variations (different textures) but get omitted when excluding lowly-connected users.", "doc_id": "1903.08136", "figure_id": "1903.08136_fig_1", "figure_number": 1, "image_path": "data/mineru_output/1903.08136/1903.08136/hybrid_auto/images/1903.08136_page0_fig0.jpg", "caption": "Figure 1: Demonstration of versatility of relevant low-degree people in social networks who get ignored by biased community detection methods. Shapes (circle, triangle) represent opinion, and texture represents variations of the opinion. We provide real-world examples of how this manifests in subsequent sections.", "figure_type": "diagram", "visual_anchor": "scattered blue triangles and red circles throughout light gray three-quarter region", "text_evidence": "Methods that tend to exclude low-degree nodes are at greater risk of losing information in their detected significant communities. We demonstrate the existence of this bias by showing what is omitted by existing community detection approaches.", "query_type": "visual_definition"}
{"query_id": "l1_1903.08136_1903.08136_fig_1_750", "query": "How do the different textures on red circles and blue triangles relate to analysis bias risks?", "answer": "The different textures represent variations within opinions that provide important information. Analysis focusing only on largest communities risks losing this nuanced information from low-degree users, as shown by what Louvain and CESNA methods omit.", "doc_id": "1903.08136", "figure_id": "1903.08136_fig_1", "figure_number": 1, "image_path": "data/mineru_output/1903.08136/1903.08136/hybrid_auto/images/1903.08136_page0_fig0.jpg", "caption": "Figure 1: Demonstration of versatility of relevant low-degree people in social networks who get ignored by biased community detection methods. Shapes (circle, triangle) represent opinion, and texture represents variations of the opinion. We provide real-world examples of how this manifests in subsequent sections.", "figure_type": "diagram", "visual_anchor": "red circles and blue triangles with striped, solid, and horizontal line textures", "text_evidence": "Analysis on community detection tends to focus on the largest communities. Methods that tend to exclude low-degree nodes are at greater risk of losing information in their detected significant communities.", "query_type": "value_context"}
{"query_id": "l1_1903.08136_1903.08136_fig_2_751", "query": "Why does the green cluster in the right visualization appear despite containing 10,074 seed users from a million-user dataset?", "answer": "The dataset was reduced to only seed users to ensure pure ground truth labels obtained independently from network structure and label propagation, bringing the size down from over a million to 10,074 users.", "doc_id": "1903.08136", "figure_id": "1903.08136_fig_2", "figure_number": 2, "image_path": "data/mineru_output/1903.08136/1903.08136/hybrid_auto/images/1903.08136_page0_fig1.jpg", "caption": "Figure 2: The 2016 U.S. presidential election seed-user retweet network colored by the political party from modularity. The callout emphasizes the low-degree users.", "figure_type": "diagram", "visual_anchor": "green cluster in right network visualization", "text_evidence": "we only utilized the seed users from the whole dataset which brought our dataset size down from more than million users to 10,074 users since we required pure ground truth labels that were obtained away from the network structure and label pr", "query_type": "value_context"}
{"query_id": "l1_1903.08136_1903.08136_fig_2_752", "query": "Does the difference between the left modularity-colored and right ground-truth-colored networks confirm disagreement between structural and actual political labels?", "answer": "Yes, the visual disagreement between the left network colored by network structure and the right network colored by ground truth labels confirms that modularity-based communities do not align with actual political party affiliations.", "doc_id": "1903.08136", "figure_id": "1903.08136_fig_2", "figure_number": 2, "image_path": "data/mineru_output/1903.08136/1903.08136/hybrid_auto/images/1903.08136_page0_fig1.jpg", "caption": "Figure 2: The 2016 U.S. presidential election seed-user retweet network colored by the political party from modularity. The callout emphasizes the low-degree users.", "figure_type": "diagram", "visual_anchor": "contrasting color patterns between left and right network visualizations", "text_evidence": "Figure 2 confirms this fact by showing the disagreement between the left hand side picture, which is colored based on the network structure, and the picture on the right colored by the ground truth labels.", "query_type": "comparison_explanation"}
{"query_id": "l1_1903.08136_1903.08136_fig_2_753", "query": "What do the scattered colored dots emphasized in the callout reveal about the 2016 presidential election retweet network structure?", "answer": "The callout emphasizes low-degree users who are sparsely connected in the network, representing users with fewer retweet connections in the political discussion dataset.", "doc_id": "1903.08136", "figure_id": "1903.08136_fig_2", "figure_number": 2, "image_path": "data/mineru_output/1903.08136/1903.08136/hybrid_auto/images/1903.08136_page0_fig1.jpg", "caption": "Figure 2: The 2016 U.S. presidential election seed-user retweet network colored by the political party from modularity. The callout emphasizes the low-degree users.", "figure_type": "diagram", "visual_anchor": "callout box highlighting scattered colored dots (low-degree users)", "text_evidence": "This dataset contains 10,074 users who discuss the U.S. presidential election in 2016. This dataset consists of two major groups which indicate the political party of each user.", "query_type": "visual_definition"}
{"query_id": "l1_1903.08136_1903.08136_fig_3_754", "query": "Why does the callout box show disagreement between purple and green node distributions despite using 8,128 labeled users?", "answer": "The network structure-based communities (left) show very low agreement with the ground truth labels from Mechanical Turk (right), revealing that network attributes alone do not capture the manually-labeled user affiliations.", "doc_id": "1903.08136", "figure_id": "1903.08136_fig_3", "figure_number": 3, "image_path": "data/mineru_output/1903.08136/1903.08136/hybrid_auto/images/1903.08136_page0_fig2.jpg", "caption": "Figure 3: The Gamergate retweet network colored based on the network structure is shown on the left hand side, and the network colored by the ground truth labels is shown on the right hand side. The callout zooms one of the components, showing the disagreement between the two labeling approaches. Purple nodes represent Gamergate opposers and green nodes represent Gamergate supporters.", "figure_type": "diagram", "visual_anchor": "Callout box showing mixed purple and green nodes in zoomed component", "text_evidence": "After obtaining the ground truth labels and having three ground truth groups of users, Gamergate supporters, Gamergate opposers, and unaffiliated, the communities obtained using network attributes were then compared with the groups obtained by the labels from the Mechanical Turk experiment. Surprisingly these results had a very low agreement", "query_type": "anomaly_cause"}
{"query_id": "l1_1903.08136_1903.08136_fig_3_755", "query": "What explains the purple cluster positions in the left network given that users with single tweets were excluded?", "answer": "The left network shows communities detected purely by network structure (retweet patterns), while excluding single-tweet users from 21,441 total ensured sufficient data for structural analysis. The purple clusters emerge from connectivity patterns rather than content-based labels.", "doc_id": "1903.08136", "figure_id": "1903.08136_fig_3", "figure_number": 3, "image_path": "data/mineru_output/1903.08136/1903.08136/hybrid_auto/images/1903.08136_page0_fig2.jpg", "caption": "Figure 3: The Gamergate retweet network colored based on the network structure is shown on the left hand side, and the network colored by the ground truth labels is shown on the right hand side. The callout zooms one of the components, showing the disagreement between the two labeling approaches. Purple nodes represent Gamergate opposers and green nodes represent Gamergate supporters.", "figure_type": "diagram", "visual_anchor": "Purple clusters in left network visualization", "text_evidence": "Out of 21,441 total users, we excluded users who had only single tweets and duplicated users with same tweets. We then asked the turkers on Amazon Mechanical Turk to label each of the 8,128 users left", "query_type": "comparison_explanation"}
{"query_id": "l1_1903.08136_1903.08136_fig_3_756", "query": "How do the green node concentrations differ between left and right networks given the Gamergate supporter label definition?", "answer": "The left network shows green nodes clustered by retweet connectivity patterns, while the right network shows green nodes positioned based on users fighting for ethics in journalism. The spatial disagreement reveals that structural communities do not align with ideological affiliations.", "doc_id": "1903.08136", "figure_id": "1903.08136_fig_3", "figure_number": 3, "image_path": "data/mineru_output/1903.08136/1903.08136/hybrid_auto/images/1903.08136_page0_fig2.jpg", "caption": "Figure 3: The Gamergate retweet network colored based on the network structure is shown on the left hand side, and the network colored by the ground truth labels is shown on the right hand side. The callout zooms one of the components, showing the disagreement between the two labeling approaches. Purple nodes represent Gamergate opposers and green nodes represent Gamergate supporters.", "figure_type": "diagram", "visual_anchor": "Green node clusters in left versus right network", "text_evidence": "Gamergate Supporter: Users fighting for ethics in journalis", "query_type": "comparison_explanation"}
{"query_id": "l1_1903.08136_1903.08136_fig_6_757", "query": "Do the two distinct red-green clusters in the network support CESNA's ability to label the anti-Gamergate tweet correctly?", "answer": "The table shows CESNA assigned N/A to the anti-Gamergate tweet, while CLAN correctly labeled it, indicating the visual clusters may represent CLAN's superior performance rather than CESNA's capabilities.", "doc_id": "1903.08136", "figure_id": "1903.08136_fig_6", "figure_number": 4, "image_path": "data/mineru_output/1903.08136/1903.08136/hybrid_auto/images/1903.08136_page0_fig5.jpg", "caption": "Figure 4: Networks colored by agreement with the ground truth labels for three methods for the Gamergate dataset.", "figure_type": "plot", "visual_anchor": "Two separated red-green network clusters", "text_evidence": "\"#gamerge is like a particularly lame version of pakistani twitter political flame wars like ridiculously lame\" CESNA Label: N/A, CLAN Label: Anti Gamerge, Ground Truth Label: Anti Gamerge", "query_type": "comparison_explanation"}
{"query_id": "l1_1903.08136_1903.08136_fig_6_758", "query": "Why do the upper and lower clusters show similar red-green mixing patterns despite Modularity assigning N/A labels?", "answer": "Modularity's N/A assignments for both example tweets suggest it failed to classify these cases, so the visual mixing likely represents other methods' performance rather than Modularity's agreement with ground truth.", "doc_id": "1903.08136", "figure_id": "1903.08136_fig_6", "figure_number": 4, "image_path": "data/mineru_output/1903.08136/1903.08136/hybrid_auto/images/1903.08136_page0_fig5.jpg", "caption": "Figure 4: Networks colored by agreement with the ground truth labels for three methods for the Gamergate dataset.", "figure_type": "plot", "visual_anchor": "Red and green mixed patterns in upper and lower clusters", "text_evidence": "\"#gamerge is like a particularly lame version of pakistani twitter political flame wars like ridiculously lame\" CESNA Label: N/A, Modularity Label: N/A", "query_type": "anomaly_cause"}
{"query_id": "l1_1903.08136_1903.08136_fig_6_759", "query": "Does the green coloring in the network clusters indicate where CLAN disagreed with the anti-Gamergate ground truth?", "answer": "No, CLAN correctly labeled the anti-Gamergate tweet, so green coloring represents disagreement by other methods. The visualization shows agreement patterns across all three methods, not just CLAN.", "doc_id": "1903.08136", "figure_id": "1903.08136_fig_6", "figure_number": 4, "image_path": "data/mineru_output/1903.08136/1903.08136/hybrid_auto/images/1903.08136_page0_fig5.jpg", "caption": "Figure 4: Networks colored by agreement with the ground truth labels for three methods for the Gamergate dataset.", "figure_type": "plot", "visual_anchor": "Green colored nodes distributed throughout both clusters", "text_evidence": "Networks colored by agreement with the ground truth labels for three methods for the Gamergate dataset. CLAN Label: Anti Gamerge, Ground Truth Label: Anti Gamerge", "query_type": "visual_definition"}
{"query_id": "l1_1903.08136_1903.08136_fig_9_760", "query": "How do the green clusters in the top-right network relate to testing CLAN against overlooked introverts?", "answer": "The top-right shows the original Gamergate distribution used to test whether CLAN's Step 1 community detection would ignore communities with members having low degree, like introverts who form few links.", "doc_id": "1903.08136", "figure_id": "1903.08136_fig_9", "figure_number": 5, "image_path": "data/mineru_output/1903.08136/1903.08136/hybrid_auto/images/1903.08136_page0_fig8.jpg", "caption": "Figure 5: Synthetic distributions with their corresponding network and obtained results for the Gamergate dataset.", "figure_type": "diagram", "visual_anchor": "green clusters in top-right oval network", "text_evidence": "One potential issue with community detection is that it could completely ignore entire communities if membership in that community is correlated with degree. For example, a community of introverts is likely to be overlooked because they do not form many links. This presents a challenge to our approach as well, because Step 1 consists of identifying the major communities in the dataset.", "query_type": "value_context"}
{"query_id": "l1_1903.08136_1903.08136_fig_9_761", "query": "Why does the bottom-left network with cyan cluster represent a synthetic distribution with particular slope?", "answer": "The bottom-left network is generated from a synthetic distribution designed with a specific slope parameter to test CLAN's resilience to skewed data where community membership correlates with degree.", "doc_id": "1903.08136", "figure_id": "1903.08136_fig_9", "figure_number": 5, "image_path": "data/mineru_output/1903.08136/1903.08136/hybrid_auto/images/1903.08136_page0_fig8.jpg", "caption": "Figure 5: Synthetic distributions with their corresponding network and obtained results for the Gamergate dataset.", "figure_type": "diagram", "visual_anchor": "bottom-left oval network with cyan cluster and label referencing slope", "text_evidence": "In Figure 5, the original distribution of the Gamergate dataset is shown on the top right corner of the figure with its corresponding network colored with the modularity value, and one of the synthetic distributions with a particular slope is shown in the bottom left with its corresponding network", "query_type": "visual_definition"}
{"query_id": "l1_1903.08136_1903.08136_fig_9_762", "query": "Do the four network ovals with varying cluster colors demonstrate results from both Gamergate and presidential election datasets?", "answer": "No, all four network visualizations show results exclusively from the Gamergate dataset, comparing the original distribution to synthetic variations. The presidential election dataset results appear separately in Table 5.", "doc_id": "1903.08136", "figure_id": "1903.08136_fig_9", "figure_number": 5, "image_path": "data/mineru_output/1903.08136/1903.08136/hybrid_auto/images/1903.08136_page0_fig8.jpg", "caption": "Figure 5: Synthetic distributions with their corresponding network and obtained results for the Gamergate dataset.", "figure_type": "diagram", "visual_anchor": "four oval networks with green, magenta, and cyan clusters", "text_evidence": "Table 4 contains the results from the Gamergate dataset, while Table 5 contains the results for the 2016 presidential election dataset.", "query_type": "comparison_explanation"}
{"query_id": "l1_1903.08136_1903.08136_fig_14_763", "query": "How do the green particle clusters in each oval relate to lowly-connected nodes that CLAN aims to properly include?", "answer": "The green clusters represent sparsely-connected nodes that CLAN minimizes bias for by including them into their true communities, unlike existing detection methods that fail to properly account for them.", "doc_id": "1903.08136", "figure_id": "1903.08136_fig_14", "figure_number": 6, "image_path": "data/mineru_output/1903.08136/1903.08136/hybrid_auto/images/1903.08136_page0_fig13.jpg", "caption": "Figure 6: Synthetic distributions with their corresponding network and obtained results for the 2016 U.S. Presidential Election dataset.", "figure_type": "plot", "visual_anchor": "green particle clusters visible in all four oval plots", "text_evidence": "CLAN minimizes such biases by including the lowly-connected nodes into their true communities.", "query_type": "visual_definition"}
{"query_id": "l1_1903.08136_1903.08136_fig_14_764", "query": "Why does the rightmost oval show a horizontal distribution pattern compared to the vertical patterns in the middle ovals?", "answer": "The different distribution patterns correspond to synthetic distributions with particular slope values generated for the 2016 U.S. Presidential Election dataset, where varying slopes produce different spatial arrangements of communities.", "doc_id": "1903.08136", "figure_id": "1903.08136_fig_14", "figure_number": 6, "image_path": "data/mineru_output/1903.08136/1903.08136/hybrid_auto/images/1903.08136_page0_fig13.jpg", "caption": "Figure 6: Synthetic distributions with their corresponding network and obtained results for the 2016 U.S. Presidential Election dataset.", "figure_type": "plot", "visual_anchor": "horizontal green-to-cyan distribution in rightmost oval versus vertical distributions in middle ovals", "text_evidence": "one of the synthetic distributions with a particular slope is shown in the bottom left with its corresponding network", "query_type": "comparison_explanation"}
{"query_id": "l1_1903.08136_1903.08136_fig_14_765", "query": "What relationship exists between the magenta-cyan clusters in the top portions and the modularity value mentioned for network coloring?", "answer": "The magenta and cyan colored clusters represent the network colored by modularity value, showing how communities are distinguished in the 2016 U.S. Presidential Election dataset's synthetic distributions.", "doc_id": "1903.08136", "figure_id": "1903.08136_fig_14", "figure_number": 6, "image_path": "data/mineru_output/1903.08136/1903.08136/hybrid_auto/images/1903.08136_page0_fig13.jpg", "caption": "Figure 6: Synthetic distributions with their corresponding network and obtained results for the 2016 U.S. Presidential Election dataset.", "figure_type": "plot", "visual_anchor": "magenta and cyan colored particles in upper portions of the ovals", "text_evidence": "the original distribution of the Gamergate dataset is shown on the top right corner of the figure with its corresponding network colored with the modularity value", "query_type": "value_context"}
{"query_id": "l1_1903.10561_1903.10561_fig_1_766", "query": "Why do InferSent and GenSen show dark blue cells for sent-weat tests despite using BiLSTM architectures trained on MultiNLI and SNLI?", "answer": "InferSent is a 4096-dimensional BiLSTM trained on MultiNLI and SNLI, while GenSen is a 2048-dimensional BiLSTM jointly trained on MultiNLI, SNLI, and other tasks. The dark blue cells indicate these encoders significantly reproduce biases at the 0.01 level after correction.", "doc_id": "1903.10561", "figure_id": "1903.10561_fig_1", "figure_number": 1, "image_path": "data/mineru_output/1903.10561/1903.10561/hybrid_auto/images/1903.10561_page0_fig0.jpg", "caption": "Figure 1: Significance of results for all models and tests.", "figure_type": "plot", "visual_anchor": "Dark blue cells in sent-weat rows (middle section) for InferSent and GenSen columns", "text_evidence": "InferSent: A 4096-dimensional BiLSTM trained on both MultiNLI (Williams et al., 2018) and SNLI (Bowman et al., 2015) with max pooling over the hidden states of the sequence", "query_type": "comparison_explanation"}
{"query_id": "l1_1903.10561_1903.10561_fig_1_767", "query": "Does the light blue coloring for CBoW in angry_black_woman_stereotype tests match its use of 300-dimensional GloVe Common Crawl vectors?", "answer": "Yes, the light blue indicates significance at the 0.01 level. CBoW uses 300-dimensional GloVe vectors trained on Common Crawl as a simple baseline, encoding sentences as word embedding averages, which still captures stereotypical associations.", "doc_id": "1903.10561", "figure_id": "1903.10561_fig_1", "figure_number": 1, "image_path": "data/mineru_output/1903.10561/1903.10561/hybrid_auto/images/1903.10561_page0_fig0.jpg", "caption": "Figure 1: Significance of results for all models and tests.", "figure_type": "plot", "visual_anchor": "Light blue cell in angry_black_woman_stereotype row at bow (CBoW) column", "text_evidence": "CBoW: As a simple baseline, we encode sentences as an average of the word embeddings. We use 300-dimensional GloVe vectors trained on the Common Crawl (Pennington et al., 2014).", "query_type": "value_context"}
{"query_id": "l1_1903.10561_1903.10561_fig_1_768", "query": "What explains the predominantly white cells in heilman_double_bind tests compared to dark blue sent-weat rows for the same encoder models?", "answer": "The heilman tests measure gender stereotypes using female given names from Sweeney (2013) as targets with specific stereotype-related adjectives as attributes. The white cells suggest these particular stereotype dimensions are less consistently reproduced across encoders than the broader biases captured in sent-weat tests.", "doc_id": "1903.10561", "figure_id": "1903.10561_fig_1", "figure_number": 1, "image_path": "data/mineru_output/1903.10561/1903.10561/hybrid_auto/images/1903.10561_page0_fig0.jpg", "caption": "Figure 1: Significance of results for all models and tests.", "figure_type": "plot", "visual_anchor": "White and very light blue cells in bottom heilman_double_bind rows versus dark blue cells in middle sent-weat rows", "text_evidence": "To measure sentence encoders' reproduction of the angry black woman stereotype, we create a test whose target concepts are black-identifying and white-identifying female given names from Sweeney (2013, Table 1) and whose attributes are adjectives used in the discussion of the stereotype", "query_type": "anomaly_cause"}
{"query_id": "l1_1903.10598_1903.10598_fig_1_769", "query": "Do the gray squares for CART at discrimination 0.6-1.2 support using leaf node notation for categorical feature levels?", "answer": "Yes, CART's gray squares show a concentrated cluster at 83-85% accuracy across discrimination 0.6-1.2, consistent with decision tree structures that partition data using leaf nodes as described in the formulation.", "doc_id": "1903.10598", "figure_id": "1903.10598_fig_1", "figure_number": null, "image_path": "data/mineru_output/1903.10598/1903.10598/hybrid_auto/images/1903.10598_page0_fig0.jpg", "caption": "", "figure_type": "plot", "visual_anchor": "gray squares (CART) clustered at 83-85% accuracy in discrimination range 0.6-1.2", "text_evidence": "For $\\nu \\in \\mathcal V$ , let $\\mathcal { L } ^ { \\mathrm { r } } ( \\nu )$ (resp. $\\mathcal { L } ^ { 1 } ( \\nu ) )$ denote all the leaf nodes that lie to the right (resp. left) of node $\\nu$", "query_type": "value_context"}
{"query_id": "l1_1903.10598_1903.10598_fig_1_770", "query": "What explains the steep accuracy drop from 78% to 72% in gray triangles as discrimination decreases from 0.4 to 0.8?", "answer": "The DADT method shows this steep decline because it enforces fairness constraints more aggressively at higher discrimination values, trading off accuracy. This contrasts with the feature-based formulation where the jth feature values are constrained by possible levels in χ_j.", "doc_id": "1903.10598", "figure_id": "1903.10598_fig_1", "figure_number": null, "image_path": "data/mineru_output/1903.10598/1903.10598/hybrid_auto/images/1903.10598_page0_fig0.jpg", "caption": "", "figure_type": "plot", "visual_anchor": "gray triangles (DADT) dropping from 78% at discrimination 0.4 to 72% at discrimination 0.8", "text_evidence": "Denote with $\\mathbf { x } _ { i , j }$ the value attained by the $j$ th feature of the ith data point and for $j \\in \\mathcal { F } _ { \\mathrm { c } }$ , let $\\chi _ { j }$ collect the possible levels attainable by feature $j$", "query_type": "anomaly_cause"}
{"query_id": "l1_1903.10598_1903.10598_fig_4_771", "query": "Why do triangular markers span discrimination values 0.010 to 0.015 when representing results from 5-fold cross-validation?", "answer": "Each triangular marker (CART) represents accuracy and discrimination evaluated on a different test set fold. The spread from 0.010 to 0.015 reflects variability across the 5 cross-validation splits.", "doc_id": "1903.10598", "figure_id": "1903.10598_fig_4", "figure_number": 1, "image_path": "data/mineru_output/1903.10598/1903.10598/hybrid_auto/images/1903.10598_page0_fig3.jpg", "caption": "Figure 1: Accuracy-discrimination trade-off of 4 families of approaches on 3 classification datasets: (a) Default, (b) Adult, and (c) COMPAS. Each dot represents a different sample from 5-fold cross-validation and each shaded area corresponds to the convex hull of the results associated with each approach in accuracy-discrimination space. Same trade-off of 3 families of approaches on the regression dataset Crime is shown in (d).", "figure_type": "plot", "visual_anchor": "Triangular markers (CART) positioned between 0.010 and 0.015 on discrimination axis", "text_evidence": "Each dot represents a different sample from 5-fold cross-validation and each shaded area corresponds to the convex hull of the results associated with each approach in accuracy-discrimination space.", "query_type": "visual_definition"}
{"query_id": "l1_1903.10598_1903.10598_fig_4_772", "query": "Do the square markers near discrimination 0.013 achieve lower MAE than triangular markers at discrimination 0.010?", "answer": "Yes, the square markers (Regression) near 0.013 show MAE around 0.14-0.15, which is lower than the triangular markers (CART) at 0.010 showing MAE around 0.14 or higher, supporting different regression versus classification trade-offs.", "doc_id": "1903.10598", "figure_id": "1903.10598_fig_4", "figure_number": 1, "image_path": "data/mineru_output/1903.10598/1903.10598/hybrid_auto/images/1903.10598_page0_fig3.jpg", "caption": "Figure 1: Accuracy-discrimination trade-off of 4 families of approaches on 3 classification datasets: (a) Default, (b) Adult, and (c) COMPAS. Each dot represents a different sample from 5-fold cross-validation and each shaded area corresponds to the convex hull of the results associated with each approach in accuracy-discrimination space. Same trade-off of 3 families of approaches on the regression dataset Crime is shown in (d).", "figure_type": "plot", "visual_anchor": "Square markers at discrimination ~0.013 versus triangular markers at discrimination ~0.010", "text_evidence": "Same trade-off of 3 families of approaches on the regression dataset Crime is shown in (d).", "query_type": "comparison_explanation"}
{"query_id": "l1_1903.10598_1903.10598_fig_4_773", "query": "Which approach shows the smallest discrimination value among the circular markers clustered near MAE 0.18?", "answer": "The circular markers (MIP) clustered near MAE 0.18 show discrimination values very close to 0.000, representing the approach with minimal discrimination at that accuracy level on the Crime regression dataset.", "doc_id": "1903.10598", "figure_id": "1903.10598_fig_4", "figure_number": 1, "image_path": "data/mineru_output/1903.10598/1903.10598/hybrid_auto/images/1903.10598_page0_fig3.jpg", "caption": "Figure 1: Accuracy-discrimination trade-off of 4 families of approaches on 3 classification datasets: (a) Default, (b) Adult, and (c) COMPAS. Each dot represents a different sample from 5-fold cross-validation and each shaded area corresponds to the convex hull of the results associated with each approach in accuracy-discrimination space. Same trade-off of 3 families of approaches on the regression dataset Crime is shown in (d).", "figure_type": "plot", "visual_anchor": "Circular markers (MIP) clustered at MAE ~0.18 and discrimination near 0.000", "text_evidence": "we then evaluate accuracy (misclassification rate/MAE) and discrimination of the classification/regression tree associated with λ⋆ on the test set and add this as a point in the corresponding graph in Figure 1.", "query_type": "value_context"}
{"query_id": "l1_1903.10598_1903.10598_fig_6_774", "query": "Why does MIP's DTDI triangle marker stay near zero across all tree depths while CART's rises to 40%?", "answer": "MIP maintains fairness by ensuring individuals differing only in protected characteristics are twice as likely to receive the same treatment compared to CART, keeping DTDI near zero.", "doc_id": "1903.10598", "figure_id": "1903.10598_fig_6", "figure_number": 2, "image_path": "data/mineru_output/1903.10598/1903.10598/hybrid_auto/images/1903.10598_page0_fig5.jpg", "caption": "Figure 2: From left to right: (a) MIP objective value and (b) Accuracy and fairness in dependence of tree depth; (c) Comparison of upper and lower bound evolution while solving MILP problem; and (d) Empirical distribution of $\\gamma ( \\mathbf { x } ) : = \\mathsf { P } ( y | \\mathbf { x } _ { \\mathrm { { \\overline { { p } } } } } , \\mathbf { x } _ { \\mathrm { { p } } } ) - \\mathsf { P } ( y | \\mathbf { x } _ { \\mathrm { { \\overline { { p } } } } } )$ (see Definition 2.5) when $\\mathbf { X }$ is valued in the test set in both CART $\\lambda = 0$ ) and MIP.", "figure_type": "plot", "visual_anchor": "Triangle markers showing DTDI: MIP near 0%, CART rising to ~40%", "text_evidence": "the likelihood for individuals (that only differ in their protected characteristics, being otherwise similar) to be treated in the same way is twice as high in MIP than in CART", "query_type": "comparison_explanation"}
{"query_id": "l1_1903.10598_1903.10598_fig_6_775", "query": "Does MIP's accuracy circle marker at approximately 80% across depths 2-6 validate the fairness metric improvement?", "answer": "Yes, MIP maintains high accuracy around 80% while achieving DTDI of 0, demonstrating that fairness improvements do not compromise predictive performance.", "doc_id": "1903.10598", "figure_id": "1903.10598_fig_6", "figure_number": 2, "image_path": "data/mineru_output/1903.10598/1903.10598/hybrid_auto/images/1903.10598_page0_fig5.jpg", "caption": "Figure 2: From left to right: (a) MIP objective value and (b) Accuracy and fairness in dependence of tree depth; (c) Comparison of upper and lower bound evolution while solving MILP problem; and (d) Empirical distribution of $\\gamma ( \\mathbf { x } ) : = \\mathsf { P } ( y | \\mathbf { x } _ { \\mathrm { { \\overline { { p } } } } } , \\mathbf { x } _ { \\mathrm { { p } } } ) - \\mathsf { P } ( y | \\mathbf { x } _ { \\mathrm { { \\overline { { p } } } } } )$ (see Definition 2.5) when $\\mathbf { X }$ is valued in the test set in both CART $\\lambda = 0$ ) and MIP.", "figure_type": "plot", "visual_anchor": "Circle markers for MIP Accuracy at ~80% from tree depth 2 to 6", "text_evidence": "this is in line with our metric – in this experiment, DTDI was $0", "query_type": "value_context"}
{"query_id": "l1_1903.10598_1903.10598_fig_9_776", "query": "Why does the white bar in the Default group show approximately 45% accuracy with large error bars?", "answer": "The white bar represents the maximally non-discriminative model (N) in the Default approach, showing lower accuracy with high variance because it achieves zero discrimination at the cost of predictive performance.", "doc_id": "1903.10598", "figure_id": "1903.10598_fig_9", "figure_number": 3, "image_path": "data/mineru_output/1903.10598/1903.10598/hybrid_auto/images/1903.10598_page0_fig8.jpg", "caption": "Figure 3: Accuracy of maximally non-discriminative models in each approach for (a) classification and (b) regression.", "figure_type": "plot", "visual_anchor": "white bar with large error bars at approximately 45% in the Default group", "text_evidence": "Accuracy results for the most accurate models with zero discrimination (when available) are shown in Figure 3.", "query_type": "value_context"}
{"query_id": "l1_1903.10598_1903.10598_fig_9_777", "query": "Do the COMPAS dark bars reaching approximately 90% accuracy support claims about maximally non-discriminative model performance?", "answer": "Yes, the COMPAS dark bars (MIP-BT approaches) show that maximally non-discriminative models can achieve high accuracy around 90%, demonstrating that zero discrimination does not always require sacrificing predictive performance substantially.", "doc_id": "1903.10598", "figure_id": "1903.10598_fig_9", "figure_number": 3, "image_path": "data/mineru_output/1903.10598/1903.10598/hybrid_auto/images/1903.10598_page0_fig8.jpg", "caption": "Figure 3: Accuracy of maximally non-discriminative models in each approach for (a) classification and (b) regression.", "figure_type": "plot", "visual_anchor": "dark bars in COMPAS group reaching approximately 90% accuracy", "text_evidence": "The authors gratefully acknowledge support from Schmidt Futures and from the James H. Zumberge Faculty Research and Innovation Fund at the University of Southern California.", "query_type": "comparison_explanation"}
{"query_id": "l1_1903.10598_1903.10598_fig_9_778", "query": "Why do all Adult Dataset bars cluster near 75% accuracy despite representing different fairness approaches?", "answer": "All Adult Dataset approaches achieve similar accuracy around 75% because they represent maximally non-discriminative models that maintain competitive performance while enforcing zero discrimination constraints across different methodological frameworks.", "doc_id": "1903.10598", "figure_id": "1903.10598_fig_9", "figure_number": 3, "image_path": "data/mineru_output/1903.10598/1903.10598/hybrid_auto/images/1903.10598_page0_fig8.jpg", "caption": "Figure 3: Accuracy of maximally non-discriminative models in each approach for (a) classification and (b) regression.", "figure_type": "plot", "visual_anchor": "all bars in Adult Dataset group clustered between approximately 70-80% accuracy", "text_evidence": "They thank the 6 anonymous referees whose reviews helped substantially improve the quality of the paper.", "query_type": "anomaly_cause"}
{"query_id": "l1_1904.03035_1904.03035_fig_1_779", "query": "Why does the bottom path include λ(N,B) term while the top Cross Entropy Loss box lacks regularization parameters?", "answer": "The bottom path applies a regularization procedure that encourages embeddings to depend minimally on gender, controlled by λ, while the top path uses standard cross entropy loss without debiasing.", "doc_id": "1904.03035", "figure_id": "1904.03035_fig_1", "figure_number": 1, "image_path": "data/mineru_output/1904.03035/1904.03035/hybrid_auto/images/1904.03035_page0_fig0.jpg", "caption": "Figure 1: Word level language model is a three layer LSTM model. $\\lambda$ controls the importance of minimizing bias in the embedding matrix.", "figure_type": "diagram", "visual_anchor": "Cross Entropy Loss + λ(N,B) box in bottom path versus plain Cross Entropy Loss box in top path", "text_evidence": "we then apply a regularization procedure that encourages the embeddings learned by the model to depend minimally on gender", "query_type": "comparison_explanation"}
{"query_id": "l1_1904.03035_1904.03035_fig_1_780", "query": "How does the bias curve shape change from Generated Text to Bias after Regularization in the diagram's right side?", "answer": "The bias curve becomes lower and flatter after regularization, indicating reduced discriminatory bias in the generated output compared to the unregularized model's generated text.", "doc_id": "1904.03035", "figure_id": "1904.03035_fig_1", "figure_number": 1, "image_path": "data/mineru_output/1904.03035/1904.03035/hybrid_auto/images/1904.03035_page0_fig0.jpg", "caption": "Figure 1: Word level language model is a three layer LSTM model. $\\lambda$ controls the importance of minimizing bias in the embedding matrix.", "figure_type": "diagram", "visual_anchor": "Bias in Generated Text curve (top right) compared to Bias after Regularization curve (bottom right)", "text_evidence": "Existing biases in data can be amplified by models and the resulting output consumed by the public can influence them, encourage and reinforce harmful stereotypes", "query_type": "comparison_explanation"}
{"query_id": "l1_1904.03035_1904.03035_fig_1_781", "query": "What role does the vertical Training component play between the corpus and the two distinct loss function pathways?", "answer": "The Training component represents a three-layer LSTM word-level language model that processes the corpus and feeds into both the standard cross entropy loss and the regularized loss with λ(N,B) pathways.", "doc_id": "1904.03035", "figure_id": "1904.03035_fig_1", "figure_number": 1, "image_path": "data/mineru_output/1904.03035/1904.03035/hybrid_auto/images/1904.03035_page0_fig0.jpg", "caption": "Figure 1: Word level language model is a three layer LSTM model. $\\lambda$ controls the importance of minimizing bias in the embedding matrix.", "figure_type": "diagram", "visual_anchor": "Training (Word Level Language Model) vertical bar positioned between Training Corpus on left and both loss boxes on right", "text_evidence": "We then train an LSTM word-level language model on a dataset and measure the bias of the generated outputs", "query_type": "visual_definition"}
{"query_id": "l1_1904.03310_1904.03310_fig_1_782", "query": "Why do the first two bars at positions 0 and 1 reach 0.27 and 0.22 instead of one dominant component?", "answer": "ELMo has two principal components for gender, unlike GloVe which has only one. This explains why the first two bars show comparable variance levels rather than a single dominant component.", "doc_id": "1904.03310", "figure_id": "1904.03310_fig_1", "figure_number": 1, "image_path": "data/mineru_output/1904.03310/1904.03310/hybrid_auto/images/1904.03310_page0_fig0.jpg", "caption": "Figure 1: Left: Percentage of explained variance in PCA in the embedding differences. Right: Selected words projecting to the first two principle components where the blue dots are the sentences with male context and the orange dots are from the sentences with female context.", "figure_type": "plot", "visual_anchor": "First bar at position 0 (0.27) and second bar at position 1 (0.22)", "text_evidence": "Figure 1 shows there are two principal components for gender in ELMo, in contrast to GloVe which only has one (Bolukbasi et al., 2016).", "query_type": "comparison_explanation"}
{"query_id": "l1_1904.03310_1904.03310_fig_1_783", "query": "Does the rapid decline from bar 2 to bar 3 dropping below 0.05 support the claim about male-female pronoun co-occurrence patterns?", "answer": "Yes, the sharp drop after the first two components suggests that gender variance is captured in just two dimensions. This aligns with the corpus analysis showing systematic co-occurrence patterns between gendered pronouns and occupation words.", "doc_id": "1904.03310", "figure_id": "1904.03310_fig_1", "figure_number": 1, "image_path": "data/mineru_output/1904.03310/1904.03310/hybrid_auto/images/1904.03310_page0_fig0.jpg", "caption": "Figure 1: Left: Percentage of explained variance in PCA in the embedding differences. Right: Selected words projecting to the first two principle components where the blue dots are the sentences with male context and the orange dots are from the sentences with female context.", "figure_type": "plot", "visual_anchor": "Bar at position 2 (0.09) dropping to bar at position 3 (below 0.05)", "text_evidence": "We show counts for the number of occurrences of male pronouns (he, his and him) and female pronouns (she and her) in the corpus as well as the co-occurrence of occupation words with those pronouns.", "query_type": "comparison_explanation"}
{"query_id": "l1_1904.03310_1904.03310_fig_1_784", "query": "What explains why bars 3 through 8 all remain below 0.05 when testing ELMo's gender embedding behavior?", "answer": "The classifier testing ELMo's gender prediction from occupation words reveals that gender information is concentrated in the first two components. The remaining components contribute negligible variance, indicating gender is encoded in a low-dimensional subspace.", "doc_id": "1904.03310", "figure_id": "1904.03310_fig_1", "figure_number": 1, "image_path": "data/mineru_output/1904.03310/1904.03310/hybrid_auto/images/1904.03310_page0_fig0.jpg", "caption": "Figure 1: Left: Percentage of explained variance in PCA in the embedding differences. Right: Selected words projecting to the first two principle components where the blue dots are the sentences with male context and the orange dots are from the sentences with female context.", "figure_type": "plot", "visual_anchor": "Bars at positions 3-8 all below 0.05", "text_evidence": "To test how ELMo embeds gender information in contextualized word embeddings, we train a classifier to predict the gender of entities from occupation words in the same sentence.", "query_type": "value_context"}
{"query_id": "l1_1905.03674_1905.03674_fig_1_785", "query": "Why does the vertical column contain both the blue and green centers when proportionality requires equal distribution?", "answer": "The vertical column represents group M, which is entitled to one of the two centers proportionally. The blue center serves M while the green center is positioned at M's boundary, demonstrating the proportional allocation where each group receives k centers proportional to its size.", "doc_id": "1905.03674", "figure_id": "1905.03674_fig_1", "figure_number": 1, "image_path": "data/mineru_output/1905.03674/1905.03674/hybrid_auto/images/1905.03674_page0_fig0.jpg", "caption": "Figure 1: Proportionality Example", "figure_type": "example", "visual_anchor": "blue dot and green dot positioned vertically in the middle column", "text_evidence": "In Figure 1, $\\mathcal N = \\mathcal M$, $k = 2$, and there are 12 individuals, represented by the embedded points.", "query_type": "comparison_explanation"}
{"query_id": "l1_1905.03674_1905.03674_fig_1_786", "query": "Do the two red centers support the claim that groups are entitled to proportional shares of centers?", "answer": "Yes, the two red centers are positioned to serve individuals outside the middle column, demonstrating that when groups N and M are equal in size, each receives a proportional share of the k=2 centers available.", "doc_id": "1905.03674", "figure_id": "1905.03674_fig_1", "figure_number": 1, "image_path": "data/mineru_output/1905.03674/1905.03674/hybrid_auto/images/1905.03674_page0_fig0.jpg", "caption": "Figure 1: Proportionality Example", "figure_type": "example", "visual_anchor": "two red dots positioned symmetrically on left and right sides", "text_evidence": "Proportionality has many advantages as a notion of fairness in clustering, beyond the intuitive appeal of groups being entitled to a proportional share of centers.", "query_type": "value_context"}
{"query_id": "l1_1905.03674_1905.03674_fig_1_787", "query": "How does the green dot's position relate to the stated impossibility of always achieving proportional solutions?", "answer": "The green dot sits at the boundary between groups, illustrating the tension in achieving exact proportionality. This positioning demonstrates why proportional solutions may not always exist and why the algorithm can only guarantee approximate proportionality in the worst case.", "doc_id": "1905.03674", "figure_id": "1905.03674_fig_1", "figure_number": 1, "image_path": "data/mineru_output/1905.03674/1905.03674/hybrid_auto/images/1905.03674_page0_fig0.jpg", "caption": "Figure 1: Proportionality Example", "figure_type": "example", "visual_anchor": "green dot at bottom of vertical column near boundary", "text_evidence": "In Section 2 we show that proportional solutions may not always exist. In fact, one cannot get better than a 2-proportional solution in the worst case.", "query_type": "anomaly_cause"}
{"query_id": "l1_1905.03674_1905.03674_fig_2_788", "query": "Why does the distance r_y from i* to y appear in the diagram when the algorithm smoothly increases δ?", "answer": "The diagram illustrates the proof scenario where r_y represents the radius at which point y is added to set X during the greedy algorithm's execution as δ increases smoothly from 0.", "doc_id": "1905.03674", "figure_id": "1905.03674_fig_2", "figure_number": 2, "image_path": "data/mineru_output/1905.03674/1905.03674/hybrid_auto/images/1905.03674_page0_fig1.jpg", "caption": "Figure 2: Diagram for Proof of Theorem 1", "figure_type": "diagram", "visual_anchor": "vertical line segment labeled r_y connecting i* to y", "text_evidence": "Smoothly increase $\\delta$ ... while $\\exists x \\in X$ s.t. $| B ( x , \\delta ) \\cap N | \\geq 1$ do", "query_type": "value_context"}
{"query_id": "l1_1905.03674_1905.03674_fig_2_789", "query": "How does point i's position between the two dashed circles relate to the triangle inequality condition d(x,y) ≤ d(i,x) + d(i,y)?", "answer": "Point i lies in the intersection region B(x, r_y) ∩ S, allowing the triangle inequality to bound the distance between x and y through their respective distances to i, as shown by the overlapping circles.", "doc_id": "1905.03674", "figure_id": "1905.03674_fig_2", "figure_number": 2, "image_path": "data/mineru_output/1905.03674/1905.03674/hybrid_auto/images/1905.03674_page0_fig1.jpg", "caption": "Figure 2: Diagram for Proof of Theorem 1", "figure_type": "diagram", "visual_anchor": "point i positioned in the overlapping region between the two dashed circles", "text_evidence": "$\\exists x \\in X$ and $\\exists i \\in N$ such that $i \\in B ( x , r _ { y } ) \\cap S$ ... By the triangle inequality, $d ( x , y ) \\leq d ( i , x ) + d ( i , y )$", "query_type": "comparison_explanation"}
{"query_id": "l1_1905.03674_1905.03674_fig_2_790", "query": "What constraint is violated if point x captures fewer than ⌈n/k⌉ agents from N at the radius shown?", "answer": "Equation 1 would be violated because the algorithm only adds x to X when its ball B(x, δ) captures at least ⌈n/k⌉ agents from N, ensuring proper coverage constraints are maintained.", "doc_id": "1905.03674", "figure_id": "1905.03674_fig_2", "figure_number": 2, "image_path": "data/mineru_output/1905.03674/1905.03674/hybrid_auto/images/1905.03674_page0_fig1.jpg", "caption": "Figure 2: Diagram for Proof of Theorem 1", "figure_type": "diagram", "visual_anchor": "right dashed circle centered at point x", "text_evidence": "while $\\exists x \\in ( { \\mathcal { M } } \\backslash X )$ s.t. $\\begin{array} { r } { | B ( x , \\delta ) \\cap N | \\ge \\lceil \\frac { n } { k } \\rceil } \\end{array}$ do ... which violates equation 1", "query_type": "anomaly_cause"}
{"query_id": "l1_1905.03674_1905.03674_fig_4_791", "query": "Why do the blue circles and red squares both remain near ρ=1.0 across all k values?", "answer": "Both Local Capture (blue circles) and k-means++ (red squares) consistently achieve near-optimal ρ-proportional solutions close to 1.0, indicating they produce solutions very close to proportional across different values of k.", "doc_id": "1905.03674", "figure_id": "1905.03674_fig_4", "figure_number": 4, "image_path": "data/mineru_output/1905.03674/1905.03674/hybrid_auto/images/1905.03674_page0_fig3.jpg", "caption": "Figure 4: Minimum $\\rho$ such that the solution is $\\rho$ -proportional   ", "figure_type": "plot", "visual_anchor": "Blue circles and red squares both clustered near ρ=1.0 on the y-axis", "text_evidence": "Minimum $\\rho$ such that the solution is $\\rho$ -proportional", "query_type": "comparison_explanation"}
{"query_id": "l1_1905.03674_1905.03674_fig_4_792", "query": "What explains the green triangles fluctuating between ρ=1.27 and ρ=1.67 while remaining consistently above 1.2?", "answer": "Greedy Capture produces solutions that require significantly higher ρ values (1.27 to 1.67) to be ρ-proportional compared to the other methods, indicating it achieves less proportional fairness, particularly at k=2, k=6, and k=8 where peaks occur.", "doc_id": "1905.03674", "figure_id": "1905.03674_fig_4", "figure_number": 4, "image_path": "data/mineru_output/1905.03674/1905.03674/hybrid_auto/images/1905.03674_page0_fig3.jpg", "caption": "Figure 4: Minimum $\\rho$ such that the solution is $\\rho$ -proportional   ", "figure_type": "plot", "visual_anchor": "Green triangles showing variable heights between 1.27 and 1.67 on the ρ axis", "text_evidence": "Minimum $\\rho$ such that the solution is $\\rho$ -proportional", "query_type": "anomaly_cause"}
{"query_id": "l1_1905.03674_1905.03674_fig_4_793", "query": "Does the consistent gap between green triangles and blue circles demonstrate algorithm performance differences on Iris data?", "answer": "Yes, the substantial gap shows that Greedy Capture requires ρ values 20-67% higher than Local Capture to achieve proportionality, demonstrating significant performance differences between the algorithms on the Iris dataset.", "doc_id": "1905.03674", "figure_id": "1905.03674_fig_4", "figure_number": 4, "image_path": "data/mineru_output/1905.03674/1905.03674/hybrid_auto/images/1905.03674_page0_fig3.jpg", "caption": "Figure 4: Minimum $\\rho$ such that the solution is $\\rho$ -proportional   ", "figure_type": "plot", "visual_anchor": "Vertical separation between green triangles (1.2-1.7) and blue circles (near 1.0)", "text_evidence": "Minimum $\\rho$ such that the solution is $\\rho$ -proportional (a) Iris", "query_type": "comparison_explanation"}
{"query_id": "l1_1905.03674_1905.03674_fig_7_794", "query": "Why does Local Capture's blue circle objective at k=2 (~12 million) exceed k-means++ despite achieving proportionality?", "answer": "Local Capture prioritizes proportionality over k-means objective optimization. Taking the union of both solutions with 2k centers would trivially achieve both proportionality and k-means++ objective, but Local Capture uses only k centers.", "doc_id": "1905.03674", "figure_id": "1905.03674_fig_7", "figure_number": 5, "image_path": "data/mineru_output/1905.03674/1905.03674/hybrid_auto/images/1905.03674_page0_fig6.jpg", "caption": "Figure 5: $k$ -means objective", "figure_type": "plot", "visual_anchor": "Blue circle at k=2 showing ~12 million objective compared to orange square at ~3 million", "text_evidence": "one can trivially achieve the proportionality of Local Capture and the $k$ -means objective of the $k$ -means $^ { + + }$ algorithm by taking the union of the two solutions", "query_type": "comparison_explanation"}
{"query_id": "l1_1905.03674_1905.03674_fig_7_795", "query": "Does the green triangle's peak at k=2 (~16 million) illustrate the tradeoff when prioritizing proportionality over objective minimization?", "answer": "Yes, Greedy Capture shows the highest k-means objective at k=2, demonstrating the cost of approximately proportional solutions. The figure quantifies how many extra centers would be needed to match k-means++ objective while maintaining proportionality.", "doc_id": "1905.03674", "figure_id": "1905.03674_fig_7", "figure_number": 5, "image_path": "data/mineru_output/1905.03674/1905.03674/hybrid_auto/images/1905.03674_page0_fig6.jpg", "caption": "Figure 5: $k$ -means objective", "figure_type": "plot", "visual_anchor": "Green triangle at k=2 showing highest objective value of ~16 million", "text_evidence": "Given an approximately proportional solution, how many extra", "query_type": "value_context"}
{"query_id": "l1_1905.03674_1905.03674_fig_7_796", "query": "Why do all three methods converge to similar objectives around 4-5 million at k=10 despite different proportionality guarantees?", "answer": "As k increases, the tradeoff between proportionality and k-means objective diminishes. With more centers available, even proportionality-focused algorithms can achieve objectives comparable to k-means++, reducing the need for extra centers mentioned in the tradeoff quantification.", "doc_id": "1905.03674", "figure_id": "1905.03674_fig_7", "figure_number": 5, "image_path": "data/mineru_output/1905.03674/1905.03674/hybrid_auto/images/1905.03674_page0_fig6.jpg", "caption": "Figure 5: $k$ -means objective", "figure_type": "plot", "visual_anchor": "Convergence of blue circles, orange squares, and green triangles near 4-5 million at k=10", "text_evidence": "Thinking in this way leads to a different way of quantifying the tradeoff between proportionality and the $k$ -means objective", "query_type": "anomaly_cause"}
{"query_id": "l1_1905.10674_1905.10674_fig_1_797", "query": "How do the three green attribute boxes relate to the compositional approach described for enforcing fairness constraints?", "answer": "The three green boxes (Gender, Occupation, Age) represent sensitive attributes that the compositional adversarial framework can filter in different combinations, allowing flexible generation of embeddings invariant to any subset of these attributes.", "doc_id": "1905.10674", "figure_id": "1905.10674_fig_1", "figure_number": 1, "image_path": "data/mineru_output/1905.10674/1905.10674/hybrid_auto/images/1905.10674_page0_fig0.jpg", "caption": "Figure 1. Overview of our approach: Our goal is to generate graph embeddings that are invariant to particular sensitive attributes (e.g., age or gender). We train a set of “filters” to prevent adversarial discriminators from classifying the sensitive information from the filtered embeddings. After training, these filters can be composed together in different combinations, allowing the flexible generation of embeddings that are invariant w.r.t. any subset of the sensitive attributes.", "figure_type": "diagram", "visual_anchor": "three green boxes labeled Gender, Occupation, and Age positioned between Filters and Filtered Embedding", "text_evidence": "Our approach is compositi[onal] ... allowing the flexible generation of embeddings that are invariant w.r.t. any subset of the sensitive attributes.", "query_type": "visual_definition"}
{"query_id": "l1_1905.10674_1905.10674_fig_1_798", "query": "Why do the blue filter layers connect to pink discriminator boxes if the goal is preventing classification of sensitive information?", "answer": "The filters are trained adversarially against the discriminators (D_Gender, D_Occupation, D_Age) to prevent them from classifying sensitive information, with the prohibition symbol indicating this adversarial relationship that enforces invariance.", "doc_id": "1905.10674", "figure_id": "1905.10674_fig_1", "figure_number": 1, "image_path": "data/mineru_output/1905.10674/1905.10674/hybrid_auto/images/1905.10674_page0_fig0.jpg", "caption": "Figure 1. Overview of our approach: Our goal is to generate graph embeddings that are invariant to particular sensitive attributes (e.g., age or gender). We train a set of “filters” to prevent adversarial discriminators from classifying the sensitive information from the filtered embeddings. After training, these filters can be composed together in different combinations, allowing the flexible generation of embeddings that are invariant w.r.t. any subset of the sensitive attributes.", "figure_type": "diagram", "visual_anchor": "blue stacked layers labeled Filters connecting through prohibition symbol to pink boxes labeled D_Gender, D_Occupation, D_Age", "text_evidence": "We train a set of 'filters' to prevent adversarial discriminators from classifying the sensitive information from the filtered embeddings.", "query_type": "comparison_explanation"}
{"query_id": "l1_1905.10674_1905.10674_fig_1_799", "query": "Does the input graph's blue and red node structure support embedding generation that operates on social networks?", "answer": "Yes, the bipartite-style input graph with blue and red nodes represents the type of graph data structure used in social networks and recommender systems that the embedding technique is designed to handle.", "doc_id": "1905.10674", "figure_id": "1905.10674_fig_1", "figure_number": 1, "image_path": "data/mineru_output/1905.10674/1905.10674/hybrid_auto/images/1905.10674_page0_fig0.jpg", "caption": "Figure 1. Overview of our approach: Our goal is to generate graph embeddings that are invariant to particular sensitive attributes (e.g., age or gender). We train a set of “filters” to prevent adversarial discriminators from classifying the sensitive information from the filtered embeddings. After training, these filters can be composed together in different combinations, allowing the flexible generation of embeddings that are invariant w.r.t. any subset of the sensitive attributes.", "figure_type": "diagram", "visual_anchor": "Input Graph box showing blue and red nodes connected in a network structure on the left side", "text_evidence": "Learning high-quality node embeddings is an important building block for machine learning models that operate on graph data, such as social networks and recommender systems.", "query_type": "value_context"}
{"query_id": "l1_1905.10674_1905.10674_fig_2_800", "query": "Why does the orange compositional adversary curve stabilize near RMSE 1.01 while baseline stays at 0.865?", "answer": "The compositional adversary filters embeddings to make sensitive attributes nearly impossible to predict, causing RMSE to degrade from 0.865 to 1.01 as the cost of achieving on-par majority-vote classification accuracy for sensitive attributes.", "doc_id": "1905.10674", "figure_id": "1905.10674_fig_2", "figure_number": 2, "image_path": "data/mineru_output/1905.10674/1905.10674/hybrid_auto/images/1905.10674_page0_fig1.jpg", "caption": "Figure 2. Performance on the edge prediction (i.e., recommendation) task on MovieLens, using RMSE as in Berg et al. (2017).", "figure_type": "plot", "visual_anchor": "Orange line labeled 'Compositional Adversary' stabilizing at RMSE ~1.01 versus pink baseline at ~0.865", "text_evidence": "Table 2 and Figure 2 summarize these results for the MovieLens data, where we can see that the accuracy of classifying the sensitive attributes is on-par with a majorityvote classifier (Table 2) while the RMSE degrades from 0.865 to 1.01 with the compositional adversary.", "query_type": "comparison_explanation"}
{"query_id": "l1_1905.10674_1905.10674_fig_2_801", "query": "Do the blue, green, and teal lines converging near RMSE 1.0 demonstrate the invariance-accuracy tradeoff?", "answer": "Yes, the convergence of all adversarial training curves (gender, age, occupation) near RMSE 1.0 demonstrates the tradeoff between achieving invariance to sensitive attributes and maintaining edge prediction accuracy, which is roughly 10% worse than baseline.", "doc_id": "1905.10674", "figure_id": "1905.10674_fig_2", "figure_number": 2, "image_path": "data/mineru_output/1905.10674/1905.10674/hybrid_auto/images/1905.10674_page0_fig1.jpg", "caption": "Figure 2. Performance on the edge prediction (i.e., recommendation) task on MovieLens, using RMSE as in Berg et al. (2017).", "figure_type": "plot", "visual_anchor": "Blue (Gender), green (Age), and teal (Occupation) adversary lines all converging around RMSE 1.0", "text_evidence": "In other words, on these two datasets the sensitive attributes were nearly impossible to predict from the filtered embeddings, while the accuracy on the main edge prediction task was roughly $10 \\%$ worse than", "query_type": "value_context"}
{"query_id": "l1_1905.10674_1905.10674_fig_2_802", "query": "What causes all adversarial curves to drop sharply from epoch 0 to 50 before flattening?", "answer": "The sharp initial drop reflects the rapid learning phase where the adversarial training mechanism quickly filters embeddings to achieve near-majority-vote classification accuracy for sensitive attributes, after which performance stabilizes as the model balances invariance with edge prediction task performance.", "doc_id": "1905.10674", "figure_id": "1905.10674_fig_2", "figure_number": 2, "image_path": "data/mineru_output/1905.10674/1905.10674/hybrid_auto/images/1905.10674_page0_fig1.jpg", "caption": "Figure 2. Performance on the edge prediction (i.e., recommendation) task on MovieLens, using RMSE as in Berg et al. (2017).", "figure_type": "plot", "visual_anchor": "All colored adversarial lines (blue, green, teal, orange) dropping steeply from RMSE ~1.8-1.6 at epoch 0 to ~1.0 by epoch 50", "text_evidence": "where we can see that the accuracy of classifying the sensitive attributes is on-par with a majorityvote classifier (Table 2) while the RMSE degrades from 0.865 to 1.01 with the compositional adversary.", "query_type": "anomaly_cause"}
{"query_id": "l1_1905.10674_1905.10674_fig_3_803", "query": "Why does the green Held Out Compositional line decline to ~0.74 AUC while addressing sensitive attribute invariance?", "answer": "The compositional approach was trained with adversarial filtering to remove sensitive attribute information, which creates a tradeoff where invariance to sensitive attributes comes at the cost of edge prediction performance.", "doc_id": "1905.10674", "figure_id": "1905.10674_fig_3", "figure_number": 3, "image_path": "data/mineru_output/1905.10674/1905.10674/hybrid_auto/images/1905.10674_page0_fig2.jpg", "caption": "Figure 3. Performance on the edge prediction (i.e., recommendation) task on the Reddit data. Evaluation is using the AUC score, since there is only one edge/relation type.", "figure_type": "plot", "visual_anchor": "Green line (Held Out Compositional) declining from ~0.77 to ~0.74 AUC", "text_evidence": "In order to quantify the extent to which the learned embeddings are invariant to the sensitive attributes (e.g., after adversarial training), we freeze the trained compositional encoder C-ENC and train an new MLP classifier to predict each sensitive attribute from the filtered embeddings", "query_type": "comparison_explanation"}
{"query_id": "l1_1905.10674_1905.10674_fig_3_804", "query": "Does the compositional approach's worse performance at ~0.74 AUC align with its reduced ability to remove sensitive information?", "answer": "Yes, on Reddit data the compositional approach both performed worse at removing sensitive attribute information and showed a small drop in edge prediction task performance, confirming the observed tradeoff.", "doc_id": "1905.10674", "figure_id": "1905.10674_fig_3", "figure_number": 3, "image_path": "data/mineru_output/1905.10674/1905.10674/hybrid_auto/images/1905.10674_page0_fig2.jpg", "caption": "Figure 3. Performance on the edge prediction (i.e., recommendation) task on the Reddit data. Evaluation is using the AUC score, since there is only one edge/relation type.", "figure_type": "plot", "visual_anchor": "Green and brown lines both at ~0.74 AUC by epoch 50", "text_evidence": "on the Reddit data we observed the opposite trend and found that the compositional approach performed worse in terms of its ability to remove information about the sensitive attributes (Figure 4) as well as a small drop on the performance of the main edge prediction task (Figure 3)", "query_type": "value_context"}
{"query_id": "l1_1905.10674_1905.10674_fig_3_805", "query": "Why is the Baseline's sharp rise to ~0.81 AUC evaluated using only AUC score on Reddit data?", "answer": "The evaluation uses AUC score because the Reddit edge prediction task involves only one edge or relation type, making AUC the appropriate single metric.", "doc_id": "1905.10674", "figure_id": "1905.10674_fig_3", "figure_number": 3, "image_path": "data/mineru_output/1905.10674/1905.10674/hybrid_auto/images/1905.10674_page0_fig2.jpg", "caption": "Figure 3. Performance on the edge prediction (i.e., recommendation) task on the Reddit data. Evaluation is using the AUC score, since there is only one edge/relation type.", "figure_type": "plot", "visual_anchor": "Blue Baseline line rising sharply to ~0.81 AUC", "text_evidence": "Performance on the edge prediction (i.e., recommendation) task on the Reddit data. Evaluation is using the AUC score, since there is only one edge/relation type.", "query_type": "value_context"}
{"query_id": "l1_1905.10674_1905.10674_fig_4_806", "query": "Does the green bar at 0.6 AUC contradict the claim that compositional approaches performed worse on Reddit data?", "answer": "No, the green bar shows the compositional approach achieved 0.6 AUC compared to baseline's 0.9, confirming worse performance in removing sensitive attribute information as stated.", "doc_id": "1905.10674", "figure_id": "1905.10674_fig_4", "figure_number": 4, "image_path": "data/mineru_output/1905.10674/1905.10674/hybrid_auto/images/1905.10674_page0_fig3.jpg", "caption": "Figure 4. Ability to predict sensitive attributes on the Reddit data when using various embedding approaches. Bar plots correspond to the average AUC across the 10 binary sensitive attributes.", "figure_type": "plot", "visual_anchor": "Green bar labeled 'No Held Out Compositional' at approximately 0.6 AUC", "text_evidence": "on the Reddit data we observed the opposite trend and found that the compositional approach performed worse in terms of its ability to remove information about the sensitive attributes", "query_type": "comparison_explanation"}
{"query_id": "l1_1905.10674_1905.10674_fig_4_807", "query": "How does the 0.025 difference between the two rightmost orange bars support the generalization capability claim?", "answer": "The small 0.025 AUC difference between 'No Held Out Compositional' (0.6) and 'Held Out Compositional' (0.6) bars demonstrates minimal performance drop, indicating effective generalization to unseen attribute combinations.", "doc_id": "1905.10674", "figure_id": "1905.10674_fig_4", "figure_number": 4, "image_path": "data/mineru_output/1905.10674/1905.10674/hybrid_auto/images/1905.10674_page0_fig3.jpg", "caption": "Figure 4. Ability to predict sensitive attributes on the Reddit data when using various embedding approaches. Bar plots correspond to the average AUC across the 10 binary sensitive attributes.", "figure_type": "plot", "visual_anchor": "Two rightmost bars: green 'No Held Out Compositional' and orange 'Held Out Compositional' both near 0.6 AUC", "text_evidence": "the performance drop for the held-out combinations is very small (0.025), indicating that our compositional approach is capable of effectively generalizing to unseen combinations", "query_type": "value_context"}
{"query_id": "l1_1905.10674_1905.10674_fig_4_808", "query": "Why does the blue baseline bar reach 0.9 while the orange non-compositional bar drops to 0.5?", "answer": "The baseline retains sensitive attribute information (high AUC of 0.9), while the non-compositional approach successfully removes much of this information, reducing AUC to 0.5, demonstrating effective attribute masking.", "doc_id": "1905.10674", "figure_id": "1905.10674_fig_4", "figure_number": 4, "image_path": "data/mineru_output/1905.10674/1905.10674/hybrid_auto/images/1905.10674_page0_fig3.jpg", "caption": "Figure 4. Ability to predict sensitive attributes on the Reddit data when using various embedding approaches. Bar plots correspond to the average AUC across the 10 binary sensitive attributes.", "figure_type": "plot", "visual_anchor": "Blue 'Baseline' bar at 0.9 AUC versus orange 'Non Compositional' bar at 0.5 AUC", "text_evidence": "average AUC across the 10 binary sensitive attributes", "query_type": "comparison_explanation"}
{"query_id": "l1_1905.10674_1905.10674_fig_5_809", "query": "Why does the solid blue Compositional Gender AUC curve drop sharply below 0.60 between λ=100 and λ=1000?", "answer": "The sharp drop represents the tradeoff where increasing the regularization parameter λ forces the compositional adversary on MovieLens1M to sacrifice gender prediction accuracy to achieve its compositional objectives.", "doc_id": "1905.10674", "figure_id": "1905.10674_fig_5", "figure_number": 5, "image_path": "data/mineru_output/1905.10674/1905.10674/hybrid_auto/images/1905.10674_page0_fig4.jpg", "caption": "Figure 5. Tradeoff of Gender AUC score on MovieLens1M for a compositional adversary versus different $\\lambda$", "figure_type": "plot", "visual_anchor": "solid blue curve dropping from approximately 0.69 to 0.51 between λ=100 and λ=1000", "text_evidence": "Tradeoff of Gender AUC score on MovieLens1M for a compositional adversary versus different λ", "query_type": "anomaly_cause"}
{"query_id": "l1_1905.10674_1905.10674_fig_5_810", "query": "At which λ value does the compositional adversary's gender prediction performance reach its minimum on MovieLens1M?", "answer": "The compositional adversary achieves its minimum Gender AUC of approximately 0.51 at λ=1000, representing the strongest tradeoff point before slight recovery at higher λ values.", "doc_id": "1905.10674", "figure_id": "1905.10674_fig_5", "figure_number": 5, "image_path": "data/mineru_output/1905.10674/1905.10674/hybrid_auto/images/1905.10674_page0_fig4.jpg", "caption": "Figure 5. Tradeoff of Gender AUC score on MovieLens1M for a compositional adversary versus different $\\lambda$", "figure_type": "plot", "visual_anchor": "lowest point of solid blue curve at λ=1000 with AUC ≈ 0.51", "text_evidence": "Tradeoff of Gender AUC score on MovieLens1M for a compositional adversary versus different λ", "query_type": "value_context"}
{"query_id": "l1_1905.10674_1905.10674_fig_6_811", "query": "Does the compositional adversary's RMSE exceeding 0.95 at λ=1000 demonstrate favorable performance compared to individual attribute enforcement?", "answer": "Yes, the compositional approach performed favorably, and on MovieLens-1M the compositionally trained adversary outperformed individually trained adversaries in removing sensitive attribute information.", "doc_id": "1905.10674", "figure_id": "1905.10674_fig_6", "figure_number": 6, "image_path": "data/mineru_output/1905.10674/1905.10674/hybrid_auto/images/1905.10674_page0_fig5.jpg", "caption": "Figure 6. RMSE on MoveLens1M with various $\\lambda$ .", "figure_type": "plot", "visual_anchor": "Solid blue line reaching above 0.95 RMSE at λ=1000", "text_evidence": "In all our experiments, we observed that our compositional approach performed favorably compared to an approach that individually enforced fairness on each individual attribute. In fact, on the MovieLens-1M data (and the synthetic Freebase15k-237 data), the compostionally trained adversary outperformed the individually trained adversaries in terms of removing information about the sensitive attributes", "query_type": "value_context"}
{"query_id": "l1_1905.10674_1905.10674_fig_6_812", "query": "Why does the solid blue compositional adversary curve remain below the dashed red baseline until approximately λ=100?", "answer": "At lower λ values, the adversarial regularization is too weak to effectively remove sensitive attribute information, so the model maintains lower RMSE closer to baseline performance before the adversary becomes strong enough to degrade prediction accuracy.", "doc_id": "1905.10674", "figure_id": "1905.10674_fig_6", "figure_number": 6, "image_path": "data/mineru_output/1905.10674/1905.10674/hybrid_auto/images/1905.10674_page0_fig5.jpg", "caption": "Figure 6. RMSE on MoveLens1M with various $\\lambda$ .", "figure_type": "plot", "visual_anchor": "Blue curve staying near 0.87-0.88 while red baseline is at 0.86 for λ < 100", "text_evidence": "training a model to jointly remove information about multiple sensitive attributes can be more effective than training separate models for each attribute", "query_type": "comparison_explanation"}
{"query_id": "l1_1905.10674_1905.10674_fig_6_813", "query": "What causes the steep rise in the blue RMSE curve between λ=100 and λ=1000 on MovieLens-1M data?", "answer": "The sharp increase occurs because stronger adversarial regularization at higher λ values forces the model to remove more sensitive attribute information, which degrades performance on the original edge prediction task as these attributes are relevant to the main task.", "doc_id": "1905.10674", "figure_id": "1905.10674_fig_6", "figure_number": 6, "image_path": "data/mineru_output/1905.10674/1905.10674/hybrid_auto/images/1905.10674_page0_fig5.jpg", "caption": "Figure 6. RMSE on MoveLens1M with various $\\lambda$ .", "figure_type": "plot", "visual_anchor": "Blue curve rising sharply from ~0.89 at λ=100 to ~1.00 at λ=1000", "text_evidence": "This result is not entirely surprising, since for this dataset the \"sensitive\" attributes were synthetically constructed from entity type annotations, which are presumably very relevant to the main edge", "query_type": "anomaly_cause"}
{"query_id": "l1_1905.10674_1905.10674_fig_7_814", "query": "Why does the blue Gender bar reach 0.05 while Age only reaches 0.012 despite both being sensitive attributes?", "answer": "The compositional encoder can flexibly generate embeddings invariant to any subset of the K sensitive attributes, but different attributes exhibit different baseline prediction biases. Gender shows the highest baseline bias at 0.05, requiring stronger regularization.", "doc_id": "1905.10674", "figure_id": "1905.10674_fig_7", "figure_number": 7, "image_path": "data/mineru_output/1905.10674/1905.10674/hybrid_auto/images/1905.10674_page0_fig6.jpg", "caption": "Figure 7. Prediction Bias for different Sensitive Attributes under three settings in MovieLens1M.", "figure_type": "plot", "visual_anchor": "blue bars at Gender (~0.05) and Age (~0.012)", "text_evidence": "One of the key benefits of the compositional encoder is that it can flexibly generate embeddings that are invariant to any subset of $S \\subseteq \\{ 1 , . . . , K \\}$ of the sensitive attributes in a domain.", "query_type": "comparison_explanation"}
{"query_id": "l1_1905.10674_1905.10674_fig_7_815", "query": "Does the green bar being lower than orange for Gender support that compositional adversaries reduce prediction bias better?", "answer": "Yes, the green Compositional Adversary bar at approximately 0.016 is lower than the orange Single Adversary bar at approximately 0.018 for Gender. This aligns with findings that compositional adversaries work better, hypothesized to be due to correlation handling.", "doc_id": "1905.10674", "figure_id": "1905.10674_fig_7", "figure_number": 7, "image_path": "data/mineru_output/1905.10674/1905.10674/hybrid_auto/images/1905.10674_page0_fig6.jpg", "caption": "Figure 7. Prediction Bias for different Sensitive Attributes under three settings in MovieLens1M.", "figure_type": "plot", "visual_anchor": "green bar (~0.016) vs orange bar (~0.018) at Gender", "text_evidence": "Interestingly, using a compositional adversary works better than a single adversary for a specific sensitive attribute which we hypothesize is due to correlatio", "query_type": "comparison_explanation"}
{"query_id": "l1_1905.10674_1905.10674_fig_7_816", "query": "How do the blue versus orange and green bars demonstrate that adversarial regularization drastically reduces prediction bias?", "answer": "The blue Baseline bars are substantially higher (Gender at 0.05, Occupation at 0.019) compared to orange Single Adversary and green Compositional Adversary bars (both below 0.02 for Gender). This dramatic reduction across all attributes demonstrates the effectiveness of adversarial regularization.", "doc_id": "1905.10674", "figure_id": "1905.10674_fig_7", "figure_number": 7, "image_path": "data/mineru_output/1905.10674/1905.10674/hybrid_auto/images/1905.10674_page0_fig6.jpg", "caption": "Figure 7. Prediction Bias for different Sensitive Attributes under three settings in MovieLens1M.", "figure_type": "plot", "visual_anchor": "blue bars at 0.05 (Gender) and 0.019 (Occupation) versus orange/green bars below 0.02", "text_evidence": "Figure 7 highlights these results, which show that adversarial regularization does in-\n\ndeed drastically reduce prediction bias.", "query_type": "value_context"}
{"query_id": "l1_1905.12843_1905.12843_fig_4_817", "query": "Why does the solid black line in bottom plots perform better despite the claim of dominance on all datasets except adult?", "answer": "The adult dataset is the exception where fair classification (solid black line) slightly outperforms the proposed method. On all other datasets, the proposed method dominates or matches baselines up to statistical uncertainty.", "doc_id": "1905.12843", "figure_id": "1905.12843_fig_4", "figure_number": 1, "image_path": "data/mineru_output/1905.12843/1905.12843/hybrid_auto/images/1905.12843_page0_fig3.jpg", "caption": "Figure 1. Relative test loss versus the worst constraint violation with respect to SP. Relative losses are computed by subtracting the smallest baseline loss from the actual loss. For our algorithm and fair classification we plot the convex envelope of the predictors obtained on training data at various accuracy–fairness tradeoffs. We show $9 5 \\%$ confidence bands for the relative loss of our method and fair classification, and also show $9 5 \\%$ confidence intervals for constraint violation (the same for all methods). Our method dominates or matches the baselines up to statistical uncertainty on all datasets except adult, where fair classification is slightly better.", "figure_type": "plot", "visual_anchor": "solid black line (fair class., oracle=LR, model=linear) in bottom plots", "text_evidence": "Our method dominates or matches the baselines up to statistical uncertainty on all datasets except adult, where fair classification is slightly better.", "query_type": "anomaly_cause"}
{"query_id": "l1_1905.12843_1905.12843_fig_4_818", "query": "Do the convex envelopes shown for fair classification and the proposed algorithm reflect different training data selection procedures?", "answer": "Yes, both methods plot convex envelopes of predictors obtained on training data at various accuracy-fairness tradeoffs, then evaluate these selected predictors on the test set to produce the Pareto fronts shown.", "doc_id": "1905.12843", "figure_id": "1905.12843_fig_4", "figure_number": 1, "image_path": "data/mineru_output/1905.12843/1905.12843/hybrid_auto/images/1905.12843_page0_fig3.jpg", "caption": "Figure 1. Relative test loss versus the worst constraint violation with respect to SP. Relative losses are computed by subtracting the smallest baseline loss from the actual loss. For our algorithm and fair classification we plot the convex envelope of the predictors obtained on training data at various accuracy–fairness tradeoffs. We show $9 5 \\%$ confidence bands for the relative loss of our method and fair classification, and also show $9 5 \\%$ confidence intervals for constraint violation (the same for all methods). Our method dominates or matches the baselines up to statistical uncertainty on all datasets except adult, where fair classification is slightly better.", "figure_type": "plot", "visual_anchor": "convex envelope curves (solid blue, solid black lines in bottom plots)", "text_evidence": "For our algorithm and fair classification we plot the convex envelope of the predictors obtained on training data at various accuracy–fairness tradeoffs. We then evaluated the selected predictors on the test set, and show the resulting Pareto front in Figure 1.", "query_type": "visual_definition"}
{"query_id": "l1_1905.12843_1905.12843_fig_4_819", "query": "How do the confidence bands around the proposed method's curve relate to the statistical comparison with baseline methods?", "answer": "The 95% confidence bands for relative loss and constraint violation intervals enable statistical comparison, showing that the proposed method's dominance or matching of baselines holds within these uncertainty bounds across datasets.", "doc_id": "1905.12843", "figure_id": "1905.12843_fig_4", "figure_number": 1, "image_path": "data/mineru_output/1905.12843/1905.12843/hybrid_auto/images/1905.12843_page0_fig3.jpg", "caption": "Figure 1. Relative test loss versus the worst constraint violation with respect to SP. Relative losses are computed by subtracting the smallest baseline loss from the actual loss. For our algorithm and fair classification we plot the convex envelope of the predictors obtained on training data at various accuracy–fairness tradeoffs. We show $9 5 \\%$ confidence bands for the relative loss of our method and fair classification, and also show $9 5 \\%$ confidence intervals for constraint violation (the same for all methods). Our method dominates or matches the baselines up to statistical uncertainty on all datasets except adult, where fair classification is slightly better.", "figure_type": "plot", "visual_anchor": "95% confidence bands around solid blue and solid black curves", "text_evidence": "We show $9 5 \\%$ confidence bands for the relative loss of our method and fair classification, and also show $9 5 \\%$ confidence intervals for constraint violation (the same for all methods).", "query_type": "comparison_explanation"}
{"query_id": "l1_1905.12843_1905.12843_fig_10_820", "query": "How does the solid blue curve's Pareto frontier demonstrate the algorithm's ability to vary fairness slackness?", "answer": "The solid blue curve shows a continuous tradeoff between training loss and constraint violation, achieved by varying the fairness slackness parameter to generate different predictors along the Pareto frontier.", "doc_id": "1905.12843", "figure_id": "1905.12843_fig_10", "figure_number": 2, "image_path": "data/mineru_output/1905.12843/1905.12843/hybrid_auto/images/1905.12843_page0_fig9.jpg", "caption": "Figure 2. Training loss versus constraint violation with respect to DP. For our algorithm, we varied the fairness slackness parameter and plot the Pareto frontiers of the sets of returned predictors. For the logistic regression experiments, we also plot the Pareto frontiers of the sets of returned predictors given by fair classification reduction methods.", "figure_type": "plot", "visual_anchor": "solid blue line labeled 'fair reg. (oracle=LR, model=linear)' showing Pareto frontier", "text_evidence": "For our algorithm, we varied the fairness slackness parameter and plot the Pareto frontiers of the sets of returned predictors.", "query_type": "comparison_explanation"}
{"query_id": "l1_1905.12843_1905.12843_fig_10_821", "query": "Why does the blue inverted triangle SEO marker appear isolated compared to the Pareto frontier curves?", "answer": "The SEO method returns a single predictor rather than a set of predictors with varying fairness-accuracy tradeoffs, unlike the algorithms that generate Pareto frontiers by varying parameters.", "doc_id": "1905.12843", "figure_id": "1905.12843_fig_10", "figure_number": 2, "image_path": "data/mineru_output/1905.12843/1905.12843/hybrid_auto/images/1905.12843_page0_fig9.jpg", "caption": "Figure 2. Training loss versus constraint violation with respect to DP. For our algorithm, we varied the fairness slackness parameter and plot the Pareto frontiers of the sets of returned predictors. For the logistic regression experiments, we also plot the Pareto frontiers of the sets of returned predictors given by fair classification reduction methods.", "figure_type": "plot", "visual_anchor": "blue inverted triangle marker labeled 'SEO (model=linear)' at single point", "text_evidence": "In Figure 2 we include the training performances of our algorithm and the baseline methods, including the SEO method and the unconstrained regressors.", "query_type": "anomaly_cause"}
{"query_id": "l1_1905.12843_1905.12843_fig_10_822", "query": "Do the black dashed Pareto frontiers support including fair classification reduction methods in the logistic regression experiments?", "answer": "Yes, the black dashed lines show the Pareto frontiers of fair classification reduction methods specifically for logistic regression experiments, demonstrating their comparative performance against other approaches.", "doc_id": "1905.12843", "figure_id": "1905.12843_fig_10", "figure_number": 2, "image_path": "data/mineru_output/1905.12843/1905.12843/hybrid_auto/images/1905.12843_page0_fig9.jpg", "caption": "Figure 2. Training loss versus constraint violation with respect to DP. For our algorithm, we varied the fairness slackness parameter and plot the Pareto frontiers of the sets of returned predictors. For the logistic regression experiments, we also plot the Pareto frontiers of the sets of returned predictors given by fair classification reduction methods.", "figure_type": "plot", "visual_anchor": "black dashed lines labeled 'fair class. (oracle=LR, model=linear)' and 'fair class. (oracle=LR, model=tree ensemble)'", "text_evidence": "For the logistic regression experiments, we also plot the Pareto frontiers of the sets of returned predictors given by fair classification reduction methods.", "query_type": "value_context"}
{"query_id": "l1_1905.12843_1905.12843_fig_13_823", "query": "Does the orange CS oracle requiring fewer calls than red LS at slackness 0.2 demonstrate oracle efficiency differences?", "answer": "Yes, at slackness 0.2, the CS oracle (orange, approximately 12 calls) is more efficient than the LS oracle (red, approximately 12 calls), though the difference is minimal at this point.", "doc_id": "1905.12843", "figure_id": "1905.12843_fig_13", "figure_number": 3, "image_path": "data/mineru_output/1905.12843/1905.12843/hybrid_auto/images/1905.12843_page0_fig12.jpg", "caption": "Figure 3. Number of oracle calls versus specified value of fairness slackness.", "figure_type": "plot", "visual_anchor": "Orange and red lines at x=0.2 showing approximately 12 oracle calls each", "text_evidence": "Fair reg. (oracle=LS, class=linear) and Fair reg. (oracle=CS, class=linear)", "query_type": "comparison_explanation"}
{"query_id": "l1_1905.12843_1905.12843_fig_13_824", "query": "What explains the sharp drop in both oracle curves between slackness 0.0 and 0.1 compared to gradual decline afterward?", "answer": "The sharp initial drop from approximately 26 to 15 oracle calls indicates that even small increases in fairness slackness dramatically reduce computational burden, while further relaxation yields diminishing returns in oracle efficiency.", "doc_id": "1905.12843", "figure_id": "1905.12843_fig_13", "figure_number": 3, "image_path": "data/mineru_output/1905.12843/1905.12843/hybrid_auto/images/1905.12843_page0_fig12.jpg", "caption": "Figure 3. Number of oracle calls versus specified value of fairness slackness.", "figure_type": "plot", "visual_anchor": "Steep decline in both red and orange lines between x=0.0 and x=0.1", "text_evidence": "Number of oracle calls versus specified value of fairness slackness", "query_type": "anomaly_cause"}
{"query_id": "l1_1906.02589_1906.02589_fig_1_825", "query": "Why do both z and b boxes point to the sensitive observations a box during test time?", "answer": "At test time, the model modifies the learned representations using both non-sensitive latents z and sensitive latents b to achieve subgroup demographic parity with respect to multiple sensitive attributes.", "doc_id": "1906.02589", "figure_id": "1906.02589_fig_1", "figure_number": 1, "image_path": "data/mineru_output/1906.02589/1906.02589/hybrid_auto/images/1906.02589_page0_fig0.jpg", "caption": "Figure 1. Data flow at train time (1a) and test time (1b) for our model, Flexibly Fair VAE (FFVAE).", "figure_type": "diagram", "visual_anchor": "arrows from z box (teal gradient) and b box (orange gradient) converging on a box (dark red gradient)", "text_evidence": "they can be easily modified at test time to achieve subgroup demographic parity with respect to multiple sensitive attributes", "query_type": "comparison_explanation"}
{"query_id": "l1_1906.02589_1906.02589_fig_1_826", "query": "How does the curved bidirectional arrow between z and x support learning compact representations for reconstruction?", "answer": "The bidirectional arrow between non-sensitive latents z (teal bar) and non-sensitive observations x (green bar) enables the model to encode x into z and decode z back to x, facilitating reconstruction while maintaining useful representations.", "doc_id": "1906.02589", "figure_id": "1906.02589_fig_1", "figure_number": 1, "image_path": "data/mineru_output/1906.02589/1906.02589/hybrid_auto/images/1906.02589_page0_fig0.jpg", "caption": "Figure 1. Data flow at train time (1a) and test time (1b) for our model, Flexibly Fair VAE (FFVAE).", "figure_type": "diagram", "visual_anchor": "curved bidirectional arrow connecting z box (teal gradient bar) and x box (green gradient bar)", "text_evidence": "learning compact representations of datasets that are useful for reconstruction and prediction", "query_type": "visual_definition"}
{"query_id": "l1_1906.02589_1906.02589_fig_1_827", "query": "Does the orange sensitive latents b box receive input from non-sensitive observations x at train time?", "answer": "No, the diagram shows no arrow from x to b at train time. The sensitive latents b and non-sensitive latents z operate on separate pathways, with z having a bidirectional connection to x only.", "doc_id": "1906.02589", "figure_id": "1906.02589_fig_1", "figure_number": 1, "image_path": "data/mineru_output/1906.02589/1906.02589/hybrid_auto/images/1906.02589_page0_fig0.jpg", "caption": "Figure 1. Data flow at train time (1a) and test time (1b) for our model, Flexibly Fair VAE (FFVAE).", "figure_type": "diagram", "visual_anchor": "absence of arrow from x box (green gradient) to b box (orange gradient)", "text_evidence": "we consider labeled examples $x , a , y \\sim p _ { \\mathrm { d a t a } }$ where $y \\in \\mathcal { V }$ are labels we wish to predict, $a \\in { \\mathcal { A } }$ are sensitive attributes, and $x \\in \\mathcal { X }$ are non-sensitive attributes", "query_type": "anomaly_cause"}
{"query_id": "l1_1906.02589_1906.02589_fig_5_828", "query": "Why does the solid blue FFVAE curve nearly overlap with dashed FactorVAE across all ΔDP values?", "answer": "Both models are sweeping hyperparameters to generate Pareto fronts for the fairness-accuracy tradeoff, with encoder outputs modified to remove information about the same sensitive attribute a.", "doc_id": "1906.02589", "figure_id": "1906.02589_fig_5", "figure_number": 2, "image_path": "data/mineru_output/1906.02589/1906.02589/hybrid_auto/images/1906.02589_page0_fig4.jpg", "caption": "Figure 2. Fairness-accuracy tradeoff curves, DSpritesUnfair dataset. We sweep a range of hyperparameters for each model and report Pareto fronts. Optimal point is the top left hand corner — this represents perfect accuracy and fairness. MLP is a baseline classifier trained directly on the input data. For each model, encoder outputs are modified to remove information about a. $y =$ XPosition for each plot.   ", "figure_type": "plot", "visual_anchor": "Solid blue FFVAE curve and dashed blue FactorVAE curve overlapping from ΔDP=0.00 to 0.07", "text_evidence": "We sweep a range of hyperparameters for each model and report Pareto fronts. For each model, encoder outputs are modified to remove information about a.", "query_type": "comparison_explanation"}
{"query_id": "l1_1906.02589_1906.02589_fig_5_829", "query": "Does the purple MLP baseline reaching approximately 0.99 accuracy at ΔDP=0.07 demonstrate advantage over encoder-based models?", "answer": "No, the MLP is a baseline classifier trained directly on input data without fairness constraints, so its high accuracy at higher ΔDP values reflects lack of fairness optimization rather than superiority.", "doc_id": "1906.02589", "figure_id": "1906.02589_fig_5", "figure_number": 2, "image_path": "data/mineru_output/1906.02589/1906.02589/hybrid_auto/images/1906.02589_page0_fig4.jpg", "caption": "Figure 2. Fairness-accuracy tradeoff curves, DSpritesUnfair dataset. We sweep a range of hyperparameters for each model and report Pareto fronts. Optimal point is the top left hand corner — this represents perfect accuracy and fairness. MLP is a baseline classifier trained directly on the input data. For each model, encoder outputs are modified to remove information about a. $y =$ XPosition for each plot.   ", "figure_type": "plot", "visual_anchor": "Purple dotted MLP curve at approximately 0.99 accuracy when ΔDP=0.07", "text_evidence": "MLP is a baseline classifier trained directly on the input data.", "query_type": "value_context"}
{"query_id": "l1_1906.02589_1906.02589_fig_5_830", "query": "What explains the optimal top-left corner location relative to all Pareto front curves shown in the plot?", "answer": "The top-left corner represents perfect accuracy and fairness simultaneously, which is the ideal but unattainable goal. All models' Pareto fronts fall below this point, showing the inherent tradeoff between these objectives.", "doc_id": "1906.02589", "figure_id": "1906.02589_fig_5", "figure_number": 2, "image_path": "data/mineru_output/1906.02589/1906.02589/hybrid_auto/images/1906.02589_page0_fig4.jpg", "caption": "Figure 2. Fairness-accuracy tradeoff curves, DSpritesUnfair dataset. We sweep a range of hyperparameters for each model and report Pareto fronts. Optimal point is the top left hand corner — this represents perfect accuracy and fairness. MLP is a baseline classifier trained directly on the input data. For each model, encoder outputs are modified to remove information about a. $y =$ XPosition for each plot.   ", "figure_type": "plot", "visual_anchor": "Top-left corner at coordinates (0.00, 1.00) with all curves positioned below and to the right", "text_evidence": "Optimal point is the top left hand corner — this represents perfect accuracy and fairness.", "query_type": "visual_definition"}
{"query_id": "l1_1906.02589_1906.02589_fig_14_831", "query": "Why does the solid blue FFVAE curve plateau near 0.85 accuracy while violentCrimesPerCapita is the prediction target?", "answer": "FFVAE achieves fairness-accuracy balance for predicting violentCrimesPerCapita using sensitive attributes racePctBlack, blackPerCapIncome, and pctNotSpeakEnglWell, reaching a stable maximum accuracy around 0.85.", "doc_id": "1906.02589", "figure_id": "1906.02589_fig_14", "figure_number": 4, "image_path": "data/mineru_output/1906.02589/1906.02589/hybrid_auto/images/1906.02589_page0_fig13.jpg", "caption": "Figure 4. Communities & Crime subgroup fairness-accuracy tradeoffs. Sensitive attributes: racePctBlack (R), blackPerCapIncome (B), and pctNotSpeakEnglWell (P). $y =$ violentCrimesPerCaptia.", "figure_type": "plot", "visual_anchor": "solid blue line labeled FFVAE plateauing at approximately 0.85 accuracy", "text_evidence": "Sensitive attributes: racePctBlack (R), blackPerCapIncome (B), and pctNotSpeakEnglWell (P). $y =$ violentCrimesPerCaptia.", "query_type": "value_context"}
{"query_id": "l1_1906.02589_1906.02589_fig_14_832", "query": "Does the dotted brown E-VAE line starting at 0.73 accuracy reflect the first exploration of fair representation learning?", "answer": "No, the E-VAE baseline shows standard performance. The first exploration of fair representation refers to CelebA experiments with Chubby, Eyeglasses, and Male attributes, not the Communities & Crime dataset shown here.", "doc_id": "1906.02589", "figure_id": "1906.02589_fig_14", "figure_number": 4, "image_path": "data/mineru_output/1906.02589/1906.02589/hybrid_auto/images/1906.02589_page0_fig13.jpg", "caption": "Figure 4. Communities & Crime subgroup fairness-accuracy tradeoffs. Sensitive attributes: racePctBlack (R), blackPerCapIncome (B), and pctNotSpeakEnglWell (P). $y =$ violentCrimesPerCaptia.", "figure_type": "plot", "visual_anchor": "dotted brown line labeled E-VAE starting at approximately 0.73 accuracy at λ_cox=0.00", "text_evidence": "We chose three attributes, Chubby, Eyeglasses, and Male as sensitive attributes7, and report fair classification results on 3 groups and 12 two-attribute-conjunction subgroups only (for brevity we omit three-attribute conjunctions). To our knowledge this is the first exploration of fair repre", "query_type": "comparison_explanation"}
{"query_id": "l1_1906.02589_1906.02589_fig_14_833", "query": "How does the dashed red CVAE curve reaching 0.82 accuracy relate to the CelebA dataset containing 200,000 images?", "answer": "The dashed red CVAE curve shows performance on Communities & Crime data, not CelebA. The CelebA dataset with 200,000 celebrity face images is a separate experiment discussed in the subsequent section.", "doc_id": "1906.02589", "figure_id": "1906.02589_fig_14", "figure_number": 4, "image_path": "data/mineru_output/1906.02589/1906.02589/hybrid_auto/images/1906.02589_page0_fig13.jpg", "caption": "Figure 4. Communities & Crime subgroup fairness-accuracy tradeoffs. Sensitive attributes: racePctBlack (R), blackPerCapIncome (B), and pctNotSpeakEnglWell (P). $y =$ violentCrimesPerCaptia.", "figure_type": "plot", "visual_anchor": "dashed red line labeled CVAE reaching approximately 0.82 accuracy at λ_cox=0.20", "text_evidence": "The CelebA6 dataset contains over 200, 000 images of celebrity faces. Each image is associated with 40 human-labeled binary attributes (OvalFace, HeavyMakeup, etc.).", "query_type": "comparison_explanation"}
{"query_id": "l1_1906.02589_1906.02589_fig_29_834", "query": "Does the solid blue TEXAS curve plateauing at 0.80 accuracy demonstrate flexible fairness with respect to Chubby, Eyeglasses, and Male attributes?", "answer": "Yes, FFVAE (TEXAS) provides flexibly fair representations that can be modified compositionally at test time to yield fair representations with respect to multiple sensitive attributes including Chubby, Eyeglasses, and Male.", "doc_id": "1906.02589", "figure_id": "1906.02589_fig_29", "figure_number": 5, "image_path": "data/mineru_output/1906.02589/1906.02589/hybrid_auto/images/1906.02589_page0_fig28.jpg", "caption": "Figure 5. Celeb-A subgroup fair classification results. Sensitive attributes: Chubby (C), Eyeglasses (E), and Male (M). $y =$ Heavy-Makeup.", "figure_type": "plot", "visual_anchor": "solid blue line labeled TEXAS plateauing at approximately 0.80 accuracy", "text_evidence": "The proposed model, FFVAE, provides flexibly fair representations, which can be modified simply and compositionally at test time to yield a fair representation with respect to multiple sensitive attributes and their conjunctions", "query_type": "value_context"}
{"query_id": "l1_1906.02589_1906.02589_fig_29_835", "query": "Why does the solid blue TEXAS line maintain higher accuracy than dashed orange X-VAE across all Δ_CP values when predicting Heavy-Makeup?", "answer": "FFVAE's structured latent code using multiple sensitive attributes (Chubby, Eyeglasses, Male) enables better disentanglement, aligning with subgroup fair machine learning goals more effectively than X-VAE's approach.", "doc_id": "1906.02589", "figure_id": "1906.02589_fig_29", "figure_number": 5, "image_path": "data/mineru_output/1906.02589/1906.02589/hybrid_auto/images/1906.02589_page0_fig28.jpg", "caption": "Figure 5. Celeb-A subgroup fair classification results. Sensitive attributes: Chubby (C), Eyeglasses (E), and Male (M). $y =$ Heavy-Makeup.", "figure_type": "plot", "visual_anchor": "solid blue TEXAS line consistently above dashed orange X-VAE line from Δ_CP 0.00 to 0.40", "text_evidence": "we discussed how disentangled representation learning aligns with the goals of subgroup fair machine learning, and presented a method for learning a structured latent code using multiple sensitive attributes", "query_type": "comparison_explanation"}
{"query_id": "l1_1906.02589_1906.02589_fig_29_836", "query": "Does the steep accuracy rise between Δ_CP 0.00 and 0.10 for both curves reflect test-time modification of sensitive attribute conjunctions?", "answer": "Yes, the steep initial rise reflects the flexible modification capability at test time, enabling fair representations even when test-time sensitive attributes involve conjunctions not seen during training.", "doc_id": "1906.02589", "figure_id": "1906.02589_fig_29", "figure_number": 5, "image_path": "data/mineru_output/1906.02589/1906.02589/hybrid_auto/images/1906.02589_page0_fig28.jpg", "caption": "Figure 5. Celeb-A subgroup fair classification results. Sensitive attributes: Chubby (C), Eyeglasses (E), and Male (M). $y =$ Heavy-Makeup.", "figure_type": "plot", "visual_anchor": "steep rise in both blue and orange curves between Δ_CP 0.00 (accuracy ~0.66) and 0.10 (accuracy ~0.77)", "text_evidence": "flexibly fair representations, which can be modified simply and compositionally at test time to yield a fair representation with respect to multiple sensitive attributes and their conjunctions, even when test-time s", "query_type": "anomaly_cause"}
{"query_id": "l1_1906.02589_1906.02589_fig_32_837", "query": "Why does the black dashed mean line remain near 0.65 across alpha values despite models being trained on DspritesUnfair?", "answer": "The mean MIG is calculated on Dsprites (not DspritesUnfair), evaluating how well the encoder learns ground truth factors of variation. The consistent mean suggests stable disentanglement quality across hyperparameters when evaluated on the clean dataset.", "doc_id": "1906.02589", "figure_id": "1906.02589_fig_32", "figure_number": 6, "image_path": "data/mineru_output/1906.02589/1906.02589/hybrid_auto/images/1906.02589_page0_fig31.jpg", "caption": "Figure 6. Mutual Information Gap (MIG) for various $( \\alpha , \\gamma )$ settings of the FFVAE. In Fig. 6a, each line is a different value of $\\gamma \\in [ 1 0 , 2 0 , 3 0 , 4 0 , 5 0 , 7 0 , 1 0 0 ]$ , with brighter colors indicating larger values of $\\gamma$ . In Fig. 6b, each line is a different value of $\\alpha \\in [ 3 0 0 , 4 0 0 , 1 0 0 0 ]$ , with brighter colors indicating larger values of $\\alpha$ . Models trained on DspritesUnfair, MIG calculated on Dsprites. Higher MIG is better. Black dashed line indicates mean (with outliers excluded). $\\alpha = 0$ is equivalent to the FactorVAE.", "figure_type": "plot", "visual_anchor": "Black dashed line at approximately y=0.65", "text_evidence": "Here we analyze the encoder mutual information in the synthetic setting of the DSpritesUnfair dataset, where we know the ground truth factors of variation. In Fig. 6, we calculate the Mutual Information Gap (MIG)", "query_type": "comparison_explanation"}
{"query_id": "l1_1906.02589_1906.02589_fig_32_838", "query": "Does the decline from peak MIG at alpha=400 to 0.55 at alpha=1000 contradict claims of representation invariance to interventions?", "answer": "No, the decline suggests that extreme alpha values reduce disentanglement quality, but the maintained MIG above 0.5 still indicates partial invariance to interventions on alpha, supporting the partial invariance claim.", "doc_id": "1906.02589", "figure_id": "1906.02589_fig_32", "figure_number": 6, "image_path": "data/mineru_output/1906.02589/1906.02589/hybrid_auto/images/1906.02589_page0_fig31.jpg", "caption": "Figure 6. Mutual Information Gap (MIG) for various $( \\alpha , \\gamma )$ settings of the FFVAE. In Fig. 6a, each line is a different value of $\\gamma \\in [ 1 0 , 2 0 , 3 0 , 4 0 , 5 0 , 7 0 , 1 0 0 ]$ , with brighter colors indicating larger values of $\\gamma$ . In Fig. 6b, each line is a different value of $\\alpha \\in [ 3 0 0 , 4 0 0 , 1 0 0 0 ]$ , with brighter colors indicating larger values of $\\alpha$ . Models trained on DspritesUnfair, MIG calculated on Dsprites. Higher MIG is better. Black dashed line indicates mean (with outliers excluded). $\\alpha = 0$ is equivalent to the FactorVAE.", "figure_type": "plot", "visual_anchor": "Curve peaks around alpha=300-400 then declining to approximately 0.55 at alpha=1000", "text_evidence": "We can interpret Figure 6 as evidence that our learned representations are (at least partially) invariant to interventions on a.", "query_type": "anomaly_cause"}
{"query_id": "l1_1906.02589_1906.02589_fig_32_839", "query": "How do the brighter colored curves at higher gamma values relate to the FactorVAE baseline at alpha equals zero?", "answer": "The brighter curves represent larger gamma values in the FFVAE model, while alpha=0 represents the FactorVAE baseline. The progression shows how FFVAE extends FactorVAE by introducing the alpha hyperparameter to control fairness.", "doc_id": "1906.02589", "figure_id": "1906.02589_fig_32", "figure_number": 6, "image_path": "data/mineru_output/1906.02589/1906.02589/hybrid_auto/images/1906.02589_page0_fig31.jpg", "caption": "Figure 6. Mutual Information Gap (MIG) for various $( \\alpha , \\gamma )$ settings of the FFVAE. In Fig. 6a, each line is a different value of $\\gamma \\in [ 1 0 , 2 0 , 3 0 , 4 0 , 5 0 , 7 0 , 1 0 0 ]$ , with brighter colors indicating larger values of $\\gamma$ . In Fig. 6b, each line is a different value of $\\alpha \\in [ 3 0 0 , 4 0 0 , 1 0 0 0 ]$ , with brighter colors indicating larger values of $\\alpha$ . Models trained on DspritesUnfair, MIG calculated on Dsprites. Higher MIG is better. Black dashed line indicates mean (with outliers excluded). $\\alpha = 0$ is equivalent to the FactorVAE.", "figure_type": "plot", "visual_anchor": "Brighter colored curves and leftmost point at alpha=0", "text_evidence": "In Fig. 6a, each line is a different value of γ ∈ [ 1 0 , 2 0 , 3 0 , 4 0 , 5 0 , 7 0 , 1 0 0 ] , with brighter colors indicating larger values of γ. α = 0 is equivalent to the FactorVAE.", "query_type": "visual_definition"}
{"query_id": "l1_1906.02589_1906.02589_fig_34_840", "query": "Why does the black dashed mean line decline from approximately 0.5 to 0.15 as γ increases to 100?", "answer": "The mean MIG (excluding outliers) shows that increasing γ generally degrades disentanglement performance across different α settings, indicating that higher γ values harm the model's ability to isolate interpretable factors.", "doc_id": "1906.02589", "figure_id": "1906.02589_fig_34", "figure_number": 7, "image_path": "data/mineru_output/1906.02589/1906.02589/hybrid_auto/images/1906.02589_page0_fig33.jpg", "caption": "Figure 7. Mutual Information Gap (MIG) for various $( \\alpha , \\gamma )$ settings of the FFVAE. In Fig. 7a, each line is a different value of $\\alpha \\in [ 0 , 5 0 , 1 0 0 , 1 5 0 , 2 0 0 ]$ , with brighter colours indicating larger values of $\\alpha$ . In Fig. 7b, all combinations with $\\alpha , \\gamma > 0$ are shown. Models trained on DspritesUnfair, MIG calculated on Dsprites. Higher MIG is better. Black dashed line indicates mean (outliers excluded). $\\alpha = 0$ is equivalent to the FactorVAE.", "figure_type": "plot", "visual_anchor": "Black dashed line declining from ~0.5 at γ=0 to ~0.15 at γ=100", "text_evidence": "Black dashed line indicates mean (outliers excluded). $\\alpha = 0$ is equivalent to the FactorVAE.", "query_type": "comparison_explanation"}
{"query_id": "l1_1906.02589_1906.02589_fig_34_841", "query": "Do the brighter colored lines representing larger alpha values from 0 to 200 consistently outperform darker lines?", "answer": "No, the brighter lines (larger α values) do not consistently outperform darker ones. Performance varies across γ values, suggesting that simply increasing α does not guarantee better disentanglement as measured by MIG.", "doc_id": "1906.02589", "figure_id": "1906.02589_fig_34", "figure_number": 7, "image_path": "data/mineru_output/1906.02589/1906.02589/hybrid_auto/images/1906.02589_page0_fig33.jpg", "caption": "Figure 7. Mutual Information Gap (MIG) for various $( \\alpha , \\gamma )$ settings of the FFVAE. In Fig. 7a, each line is a different value of $\\alpha \\in [ 0 , 5 0 , 1 0 0 , 1 5 0 , 2 0 0 ]$ , with brighter colours indicating larger values of $\\alpha$ . In Fig. 7b, all combinations with $\\alpha , \\gamma > 0$ are shown. Models trained on DspritesUnfair, MIG calculated on Dsprites. Higher MIG is better. Black dashed line indicates mean (outliers excluded). $\\alpha = 0$ is equivalent to the FactorVAE.", "figure_type": "plot", "visual_anchor": "Brighter colored lines (larger α ∈ [0,50,100,150,200]) compared to darker lines across γ range", "text_evidence": "each line is a different value of $\\alpha \\in [ 0 , 5 0 , 1 0 0 , 1 5 0 , 2 0 0 ]$ , with brighter colours indicating larger values of $\\alpha$", "query_type": "comparison_explanation"}
{"query_id": "l1_1906.02589_1906.02589_fig_34_842", "query": "How does training on DspritesUnfair while evaluating on Dsprites affect interpretation of the declining MIG after γ equals 60?", "answer": "The decline suggests that FFVAE's fairness regularization (via γ) trained on biased data increasingly harms disentanglement when evaluated on unbiased data, indicating a trade-off between fairness constraints and representational quality across distribution shifts.", "doc_id": "1906.02589", "figure_id": "1906.02589_fig_34", "figure_number": 7, "image_path": "data/mineru_output/1906.02589/1906.02589/hybrid_auto/images/1906.02589_page0_fig33.jpg", "caption": "Figure 7. Mutual Information Gap (MIG) for various $( \\alpha , \\gamma )$ settings of the FFVAE. In Fig. 7a, each line is a different value of $\\alpha \\in [ 0 , 5 0 , 1 0 0 , 1 5 0 , 2 0 0 ]$ , with brighter colours indicating larger values of $\\alpha$ . In Fig. 7b, all combinations with $\\alpha , \\gamma > 0$ are shown. Models trained on DspritesUnfair, MIG calculated on Dsprites. Higher MIG is better. Black dashed line indicates mean (outliers excluded). $\\alpha = 0$ is equivalent to the FactorVAE.", "figure_type": "plot", "visual_anchor": "All lines declining sharply after γ=60, dropping from ~0.4-0.7 to ~0.1-0.3", "text_evidence": "Models trained on DspritesUnfair, MIG calculated on Dsprites. Higher MIG is better.", "query_type": "anomaly_cause"}
{"query_id": "l1_1907.06430_1907.06430_fig_2_843", "query": "Does the approximately equal proportion of reoffenders within each risk category for both races demonstrate Y ⊥⊥ A | Ŷ?", "answer": "Yes, the visual similarity in the proportion of light blue (reoffended) to dark blue (did not reoffend) within Low and Medium/High categories for both Black and White panels illustrates that within each risk category, the proportion of defendants who reoffend is approximately the same regardless of race.", "doc_id": "1907.06430", "figure_id": "1907.06430_fig_2", "figure_number": 1, "image_path": "data/mineru_output/1907.06430/1907.06430/hybrid_auto/images/1907.06430_page0_fig1.jpg", "caption": "Fig. 1. Number of black and white defendants in each of two aggregate risk categories [15]. The overall recidivism rate for black defendants is higher than for white defendants ( $5 2 \\%$ vs. 39%), i.e. Y ✚⊥⊥A. Within each risk category, the proportion of defendants who reoffend is approximately the same regardless of race, i.e. $Y \\perp \\perp A | \\hat { Y }$ . Black defendants are more likely to be classified as medium or high risk (58% vs. 33%) i.e. $\\hat { Y } \\mathcal { M } A$ . Among individuals who did not reoffend, black defendants are more likely to be classified as medium or high risk than white defendants (44.9% to $2 3 . 5 \\%$ ). Among individuals who did reoffend, white defendant are more likely to be classified as low risk than black defendants (47.7% vs 28 $\\%$ ), i.e. ${ \\hat { Y } } { \\mathcal { H } } A | Y$ .", "figure_type": "plot", "visual_anchor": "Proportion of light blue to dark blue bars within Low and Medium/High categories across both panels", "text_evidence": "Within each risk category, the proportion of defendants who reoffend is approximately the same regardless of race, i.e. $Y \\perp \\perp A | \\hat { Y }$", "query_type": "visual_definition"}
{"query_id": "l1_1907.06430_1907.06430_fig_2_844", "query": "Why is the Medium/High bar taller than Low for Black defendants but shorter for White defendants?", "answer": "Black defendants are more likely to be classified as medium or high risk (58% vs. 33%), demonstrating that risk classification is not independent of race. This disparity shows that Ŷ ⊥⊥ A does not hold.", "doc_id": "1907.06430", "figure_id": "1907.06430_fig_2", "figure_number": 1, "image_path": "data/mineru_output/1907.06430/1907.06430/hybrid_auto/images/1907.06430_page0_fig1.jpg", "caption": "Fig. 1. Number of black and white defendants in each of two aggregate risk categories [15]. The overall recidivism rate for black defendants is higher than for white defendants ( $5 2 \\%$ vs. 39%), i.e. Y ✚⊥⊥A. Within each risk category, the proportion of defendants who reoffend is approximately the same regardless of race, i.e. $Y \\perp \\perp A | \\hat { Y }$ . Black defendants are more likely to be classified as medium or high risk (58% vs. 33%) i.e. $\\hat { Y } \\mathcal { M } A$ . Among individuals who did not reoffend, black defendants are more likely to be classified as medium or high risk than white defendants (44.9% to $2 3 . 5 \\%$ ). Among individuals who did reoffend, white defendant are more likely to be classified as low risk than black defendants (47.7% vs 28 $\\%$ ), i.e. ${ \\hat { Y } } { \\mathcal { H } } A | Y$ .", "figure_type": "plot", "visual_anchor": "Total height comparison between Low and Medium/High bars in Black panel (~1350 vs ~1800) versus White panel (~1400 vs ~700)", "text_evidence": "Black defendants are more likely to be classified as medium or high risk (58% vs. 33%) i.e. $\\hat { Y } \\mathcal { M } A$", "query_type": "comparison_explanation"}
{"query_id": "l1_1907.06430_1907.06430_fig_2_845", "query": "Does the dark blue segment being larger in Medium/High than Low for Black defendants support the 44.9% versus 23.5% claim?", "answer": "Yes, the dark blue (did not reoffend) segment is proportionally larger in the Medium/High category for Black defendants compared to White defendants, visually confirming that among individuals who did not reoffend, black defendants are more likely to be classified as medium or high risk.", "doc_id": "1907.06430", "figure_id": "1907.06430_fig_2", "figure_number": 1, "image_path": "data/mineru_output/1907.06430/1907.06430/hybrid_auto/images/1907.06430_page0_fig1.jpg", "caption": "Fig. 1. Number of black and white defendants in each of two aggregate risk categories [15]. The overall recidivism rate for black defendants is higher than for white defendants ( $5 2 \\%$ vs. 39%), i.e. Y ✚⊥⊥A. Within each risk category, the proportion of defendants who reoffend is approximately the same regardless of race, i.e. $Y \\perp \\perp A | \\hat { Y }$ . Black defendants are more likely to be classified as medium or high risk (58% vs. 33%) i.e. $\\hat { Y } \\mathcal { M } A$ . Among individuals who did not reoffend, black defendants are more likely to be classified as medium or high risk than white defendants (44.9% to $2 3 . 5 \\%$ ). Among individuals who did reoffend, white defendant are more likely to be classified as low risk than black defendants (47.7% vs 28 $\\%$ ), i.e. ${ \\hat { Y } } { \\mathcal { H } } A | Y$ .", "figure_type": "plot", "visual_anchor": "Dark blue segment height in Medium/High category: Black panel (~650) versus White panel (~300), relative to total non-reoffenders", "text_evidence": "Among individuals who did not reoffend, black defendants are more likely to be classified as medium or high risk than white defendants (44.9% to $2 3 . 5 \\%$)", "query_type": "value_context"}
{"query_id": "l1_1907.06430_1907.06430_fig_3_846", "query": "Why does the red arrow from A directly to Y represent a statistical dependency in this Bayesian network?", "answer": "In Bayesian networks, edges represent statistical dependencies between random variables. The red arrow from A to Y indicates that Y's conditional distribution depends on A as a parent.", "doc_id": "1907.06430", "figure_id": "1907.06430_fig_3", "figure_number": 2, "image_path": "data/mineru_output/1907.06430/1907.06430/hybrid_auto/images/1907.06430_page0_fig2.jpg", "caption": "Fig. 2. Possible CBN underlying the dataset used for COMPAS.", "figure_type": "diagram", "visual_anchor": "red arrow from node A to node Y", "text_evidence": "A Bayesian network is a directed acyclic graph where nodes and edges represent random variables and statistical dependencies.", "query_type": "visual_definition"}
{"query_id": "l1_1907.06430_1907.06430_fig_3_847", "query": "How does node Y's conditional distribution differ from node M's, given Y has three parent nodes while M has none?", "answer": "Node Y's conditional distribution is p(Y|pa(Y)) with three parents (A, F, M), while M has no parents so its distribution is simply p(M). Y depends on more variables than M.", "doc_id": "1907.06430", "figure_id": "1907.06430_fig_3", "figure_number": 2, "image_path": "data/mineru_output/1907.06430/1907.06430/hybrid_auto/images/1907.06430_page0_fig2.jpg", "caption": "Fig. 2. Possible CBN underlying the dataset used for COMPAS.", "figure_type": "diagram", "visual_anchor": "node Y with three incoming arrows versus node M with no incoming arrows", "text_evidence": "Each node $X _ { i }$ in the graph is associated with the conditional distribution $p ( X _ { i } | \\mathrm { p a } ( X _ { i } ) )$ , where $\\operatorname { p a } ( X _ { i } )$ is the set of parents of $X _ { i }$", "query_type": "comparison_explanation"}
{"query_id": "l1_1907.06430_1907.06430_fig_3_848", "query": "What role do the four nodes A, F, M, and Y play in computing the joint distribution for COMPAS dataset?", "answer": "These four nodes represent random variables whose joint distribution p(A,F,M,Y) is computed as the product of all conditional distributions based on the parent-child relationships shown in the network.", "doc_id": "1907.06430", "figure_id": "1907.06430_fig_3", "figure_number": 2, "image_path": "data/mineru_output/1907.06430/1907.06430/hybrid_auto/images/1907.06430_page0_fig2.jpg", "caption": "Fig. 2. Possible CBN underlying the dataset used for COMPAS.", "figure_type": "diagram", "visual_anchor": "four circular nodes labeled A, F, M, and Y", "text_evidence": "The joint distribution of all nodes, $p ( X _ { 1 } , \\ldots , X _ { I } )$ , is given by the product of all conditi", "query_type": "value_context"}
{"query_id": "l1_1907.06430_1907.06430_fig_4_849", "query": "Why does node C have only the marginal distribution p(C) while nodes A and Y have conditional distributions?", "answer": "Node C is the root node with no parents, so it has only a marginal distribution p(C). Each node Xi in a Bayesian network is associated with the conditional distribution p(Xi|pa(Xi)), and since C has no parents, it requires only its marginal distribution.", "doc_id": "1907.06430", "figure_id": "1907.06430_fig_4", "figure_number": 3, "image_path": "data/mineru_output/1907.06430/1907.06430/hybrid_auto/images/1907.06430_page0_fig3.jpg", "caption": "Fig. 3. (a): CBN with a confounder $C$ for the effect of $A$ on $Y$ . (b): Modified CBN resulting from intervening on $A$ .", "figure_type": "diagram", "visual_anchor": "Node C at top with label p(C), contrasted with node A labeled p(A|C) and node Y labeled p(Y|C,A)", "text_evidence": "Each node $X _ { i }$ in the graph is associated with the conditional distribution $p ( X _ { i } | \\mathrm { p a } ( X _ { i } ) )$ , where $\\operatorname { p a } ( X _ { i } )$ is the set of parents of $X _ { i }$", "query_type": "visual_definition"}
{"query_id": "l1_1907.06430_1907.06430_fig_4_850", "query": "How does the directed edge labeled g from C to A demonstrate that C is a confounder?", "answer": "The edge g from C to A, combined with C's edge to Y, shows that C influences both treatment A and outcome Y, creating a spurious association. This defines C as a confounder that creates non-causal correlation between A and Y.", "doc_id": "1907.06430", "figure_id": "1907.06430_fig_4", "figure_number": 3, "image_path": "data/mineru_output/1907.06430/1907.06430/hybrid_auto/images/1907.06430_page0_fig3.jpg", "caption": "Fig. 3. (a): CBN with a confounder $C$ for the effect of $A$ on $Y$ . (b): Modified CBN resulting from intervening on $A$ .", "figure_type": "diagram", "visual_anchor": "Arrow labeled g connecting node C to node A, and separate arrow from C to Y", "text_evidence": "CBN with a confounder $C$ for the effect of $A$ on $Y$", "query_type": "comparison_explanation"}
{"query_id": "l1_1907.06430_1907.06430_fig_4_851", "query": "Why does node Y's conditional distribution p(Y|C,A) depend on both parent nodes C and A?", "answer": "Node Y has two incoming arrows, one from C and one from A, making both C and A parents of Y. The joint distribution is given by the product of all conditional distributions, so Y's distribution must condition on all its parents.", "doc_id": "1907.06430", "figure_id": "1907.06430_fig_4", "figure_number": 3, "image_path": "data/mineru_output/1907.06430/1907.06430/hybrid_auto/images/1907.06430_page0_fig3.jpg", "caption": "Fig. 3. (a): CBN with a confounder $C$ for the effect of $A$ on $Y$ . (b): Modified CBN resulting from intervening on $A$ .", "figure_type": "diagram", "visual_anchor": "Node Y on bottom right with two incoming arrows from nodes C and A, labeled p(Y|C,A)", "text_evidence": "The joint distribution of all nodes, $p ( X _ { 1 } , \\ldots , X _ { I } )$ , is given by the product of all conditi", "query_type": "value_context"}
{"query_id": "l1_1907.06430_1907.06430_fig_6_852", "query": "Why does conditioning on node C open the path from E through C to Y?", "answer": "Conditioning on C closes the paths A→C→X→Y and A→C→Y but opens the path A←E→C←X→Y, creating a new association through the collider at C.", "doc_id": "1907.06430", "figure_id": "1907.06430_fig_6", "figure_number": 4, "image_path": "data/mineru_output/1907.06430/1907.06430/hybrid_auto/images/1907.06430_page0_fig5.jpg", "caption": "Fig. 4. (a): CBN in which conditioning on $C$ closes the paths $A  C  X  Y$ and $A \\left. C \\right. Y$ but opens the path $A \\left. E \\right.$ $C \\left. X \\right. Y$ . (b): CBN with one direct and one indirect causal path from $A$ to $Y$ .", "figure_type": "diagram", "visual_anchor": "Node C with incoming edges from E and A, and outgoing edge to Y", "text_evidence": "conditioning on $C$ closes the paths $A  C  X  Y$ and $A \\left. C \\right. Y$ but opens the path $A \\left. E \\right.$ $C \\left. X \\right. Y$", "query_type": "anomaly_cause"}
{"query_id": "l1_1907.06430_1907.06430_fig_6_853", "query": "How does the direct path A→Y differ from the indirect path through M in isolating causal contributions?", "answer": "The direct path A→Y represents immediate causal effect, while the indirect path through M allows isolation of mediated effects by performing different interventions along different causal paths.", "doc_id": "1907.06430", "figure_id": "1907.06430_fig_6", "figure_number": 4, "image_path": "data/mineru_output/1907.06430/1907.06430/hybrid_auto/images/1907.06430_page0_fig5.jpg", "caption": "Fig. 4. (a): CBN in which conditioning on $C$ closes the paths $A  C  X  Y$ and $A \\left. C \\right. Y$ but opens the path $A \\left. E \\right.$ $C \\left. X \\right. Y$ . (b): CBN with one direct and one indirect causal path from $A$ to $Y$ .", "figure_type": "diagram", "visual_anchor": "Direct edge from node A to node Y versus the two-edge path A→M→Y", "text_evidence": "by performing different interventions on $A$ along different causal paths, it is possible to isolate the contribution of the causal effect of $A$ on $Y$ along a group of paths", "query_type": "comparison_explanation"}
{"query_id": "l1_1907.06430_1907.06430_fig_8_854", "query": "Why are the paths A→M, M→L, and L→Y highlighted in red in the top diagram?", "answer": "These red paths represent the indirect causal paths from A to Y that pass through M, which are being isolated to estimate path-specific effects alongside the direct A→Y path.", "doc_id": "1907.06430", "figure_id": "1907.06430_fig_8", "figure_number": 5, "image_path": "data/mineru_output/1907.06430/1907.06430/hybrid_auto/images/1907.06430_page0_fig7.jpg", "caption": "Fig. 5. Top: CBN with the direct path from $A$ to $Y$ and the indirect paths passing through $M$ highlighted in red. Bottom: CBN corresponding to Eq. (1).", "figure_type": "diagram", "visual_anchor": "Red highlighted edges connecting A→M→L→Y in the top diagram", "text_evidence": "assume that we are interested in isolating the effect of $A$ on $Y$ along the direct path $A  Y$ and the pat", "query_type": "visual_definition"}
{"query_id": "l1_1907.06430_1907.06430_fig_8_855", "query": "How does the bottom diagram's theta coefficient notation relate to computing effects along the red paths shown above?", "answer": "The PSE is obtained by summing over the three causal paths of interest (A→Y, A→M→Y, and A→M→L→Y) the product of all theta coefficients in each path, as shown in the annotated bottom diagram.", "doc_id": "1907.06430", "figure_id": "1907.06430_fig_8", "figure_number": 5, "image_path": "data/mineru_output/1907.06430/1907.06430/hybrid_auto/images/1907.06430_page0_fig7.jpg", "caption": "Fig. 5. Top: CBN with the direct path from $A$ to $Y$ and the indirect paths passing through $M$ highlighted in red. Bottom: CBN corresponding to Eq. (1).", "figure_type": "diagram", "visual_anchor": "Theta coefficients (θ^m_a, θ^l_m, θ^y_l, θ^y_m, θ^y_a) labeling arrows in bottom diagram", "text_evidence": "The PSE is obtained by summing over the three causal paths of interest ( $A  Y$ , $A  M  Y$ , and $A \\to M \\to L \\to Y$ ) the product of all coefficients in each path.", "query_type": "comparison_explanation"}
{"query_id": "l1_1907.06430_1907.06430_fig_8_856", "query": "Does node C's position above M and L in both diagrams explain its absence from fairness reasoning?", "answer": "Yes, the hiring decision Y is only based on E (educational background), not on religious belief A or participation patterns influenced by C, as shown by the back-door path structure.", "doc_id": "1907.06430", "figure_id": "1907.06430_fig_8", "figure_number": 5, "image_path": "data/mineru_output/1907.06430/1907.06430/hybrid_auto/images/1907.06430_page0_fig7.jpg", "caption": "Fig. 5. Top: CBN with the direct path from $A$ to $Y$ and the indirect paths passing through $M$ highlighted in red. Bottom: CBN corresponding to Eq. (1).", "figure_type": "diagram", "visual_anchor": "Node C at the top with arrows pointing down to M and L in both diagrams", "text_evidence": "Whilst Y ✚⊥⊥A due to the open back-door path from $A$ to $Y$ , the hiring decision $Y$ is\n\nonly based on $E$", "query_type": "value_context"}
{"query_id": "l1_1907.06430_1907.06430_fig_10_857", "query": "Why does the red arrow labeled 'a' from A to X create an open back-door path making Y not independent of A?", "answer": "The path A→X influences Y through the educational background pathway, and since there is no conditioning, this back-door path remains open. This prevents independence between the hiring decision Y and religious belief A.", "doc_id": "1907.06430", "figure_id": "1907.06430_fig_10", "figure_number": 6, "image_path": "data/mineru_output/1907.06430/1907.06430/hybrid_auto/images/1907.06430_page0_fig9.jpg", "caption": "Fig. 6. CBN underlying a music degree scenario.", "figure_type": "diagram", "visual_anchor": "red arrow labeled 'a' from node A to node X", "text_evidence": "Whilst Y ✚⊥⊥A due to the open back-door path from A to Y, the hiring decision Y is only based on E", "query_type": "comparison_explanation"}
{"query_id": "l1_1907.06430_1907.06430_fig_10_858", "query": "Does the arrow from M to X labeled 'β' enable retrieval of M when A is used alongside X?", "answer": "Yes, because M is d-separated from A given X (M ⊥⊥ A | X), using A in addition to X can provide information about the music degree M through the causal structure.", "doc_id": "1907.06430", "figure_id": "1907.06430_fig_10", "figure_number": 6, "image_path": "data/mineru_output/1907.06430/1907.06430/hybrid_auto/images/1907.06430_page0_fig9.jpg", "caption": "Fig. 6. CBN underlying a music degree scenario.", "figure_type": "diagram", "visual_anchor": "black arrow labeled 'β' from node M to node X", "text_evidence": "Indeed, since M { \\mathrel { \\mathop { A C A } } } | X, using A in addition to X can give information about M", "query_type": "value_context"}
{"query_id": "l1_1907.06430_1907.06430_fig_10_859", "query": "How does the λ-labeled arrow from M to Y support fairness when the predictor is Ŷ = γM?", "answer": "Since the predictor depends only on M (music degree) through the λ pathway, and not directly on the sensitive attribute A, this ensures fairness. The explicit use of A helps retrieve M without creating direct discrimination.", "doc_id": "1907.06430", "figure_id": "1907.06430_fig_10", "figure_number": 6, "image_path": "data/mineru_output/1907.06430/1907.06430/hybrid_auto/images/1907.06430_page0_fig9.jpg", "caption": "Fig. 6. CBN underlying a music degree scenario.", "figure_type": "diagram", "visual_anchor": "black arrow labeled 'λ' from node M to node Y", "text_evidence": "this example shows how explicit use of the sensitive attribute in a model can ensure", "query_type": "comparison_explanation"}
{"query_id": "l1_1907.06430_1907.06430_fig_11_860", "query": "Does the red solid arrow from A to Y represent the direct path whose unfairness is quantified by path-specific effect?", "answer": "Yes, the red solid arrow from A to Y represents the direct path A → Y, whose unfairness is quantified by the path-specific effect PSE^aa when the path A → D → Y is considered fair.", "doc_id": "1907.06430", "figure_id": "1907.06430_fig_11", "figure_number": 7, "image_path": "data/mineru_output/1907.06430/1907.06430/hybrid_auto/images/1907.06430_page0_fig10.jpg", "caption": "Fig. 7. CBN underlying a college admission scenario.", "figure_type": "diagram", "visual_anchor": "red solid arrow from node A to node Y", "text_evidence": "in which the path $A $ $D  Y$ is considered fair, unfairness can instead be quantified with the path-specific effect along the direct path $A  Y$ , PSE $^ { a a }$", "query_type": "visual_definition"}
{"query_id": "l1_1907.06430_1907.06430_fig_11_861", "query": "Why does the dashed arrow from D to Y matter for retrieving M when A is explicitly used?", "answer": "The dashed arrow from D to Y indicates D influences Y, and since M is conditionally independent of A given X, using A in addition to X can help retrieve M information that flows through D to Y.", "doc_id": "1907.06430", "figure_id": "1907.06430_fig_11", "figure_number": 7, "image_path": "data/mineru_output/1907.06430/1907.06430/hybrid_auto/images/1907.06430_page0_fig10.jpg", "caption": "Fig. 7. CBN underlying a college admission scenario.", "figure_type": "diagram", "visual_anchor": "dashed arrow from node D to node Y", "text_evidence": "Indeed, since $M { \\mathrel { \\mathop { A C A } } } | X$ , using $A$ in addition to $X$ can give information about $M$", "query_type": "comparison_explanation"}
{"query_id": "l1_1907.06430_1907.06430_fig_11_862", "query": "How does the black arrow from Q to Y relate to the twin network containing counterfactual variable Q*?", "answer": "The black arrow from Q to Y in the original CBN corresponds to the relationship maintained in the twin Bayesian network, where Q* represents the counterfactual world with intervention A = ā along A → Y.", "doc_id": "1907.06430", "figure_id": "1907.06430_fig_11", "figure_number": 7, "image_path": "data/mineru_output/1907.06430/1907.06430/hybrid_auto/images/1907.06430_page0_fig10.jpg", "caption": "Fig. 7. CBN underlying a college admission scenario.", "figure_type": "diagram", "visual_anchor": "black solid arrow from node Q to node Y", "text_evidence": "the network contains the variables $Q ^ { * }$ , $D _ { a }$ and $Y _ { \\bar { a } } ( D _ { a } )$ corresponding to the counterfactual world in which $A = \\bar { a }$ along $A  Y$ , with $Q ^ { * } = \\t", "query_type": "value_context"}
{"query_id": "l1_1907.06430_1907.06430_fig_13_863", "query": "Does the directed path from X₁ through X₃ to X₄ illustrate what the definition describes as linked nodes?", "answer": "Yes, the sequence X₁ → X₃ → X₄ represents a directed path, which is defined as a sequence of linked nodes starting at one node and ending at another.", "doc_id": "1907.06430", "figure_id": "1907.06430_fig_13", "figure_number": 8, "image_path": "data/mineru_output/1907.06430/1907.06430/hybrid_auto/images/1907.06430_page0_fig12.jpg", "caption": "Fig. 8. Directed (a) acyclic and (b) cyclic graph.", "figure_type": "diagram", "visual_anchor": "directed arrows forming path X₁ → X₃ → X₄", "text_evidence": "A path from node $X _ { i }$ to node $X _ { j }$ is a sequence of linked nodes starting at $X _ { i }$ and ending at $X _ { j }$ . A directed path is", "query_type": "visual_definition"}
{"query_id": "l1_1907.06430_1907.06430_fig_13_864", "query": "Why does the structure with X₃ connecting to both X₂ and X₄ create a cyclic pattern?", "answer": "The structure creates a cycle because X₃ → X₂ → X₄ forms one path, while X₃ → X₄ forms another, and these paths loop back through the shared connections, creating a directed cycle.", "doc_id": "1907.06430", "figure_id": "1907.06430_fig_13", "figure_number": 8, "image_path": "data/mineru_output/1907.06430/1907.06430/hybrid_auto/images/1907.06430_page0_fig12.jpg", "caption": "Fig. 8. Directed (a) acyclic and (b) cyclic graph.", "figure_type": "diagram", "visual_anchor": "node X₃ with arrows to both X₂ and X₄", "text_evidence": "A graph is a collection of nodes and links connecting pairs of nodes. The links may be directed or undirected, giving rise to directed or undirected graphs respectively.", "query_type": "comparison_explanation"}
{"query_id": "l1_1907.06430_1907.06430_fig_13_865", "query": "How do the directed links from X₁ to X₃ and X₃ to X₂ relate to acknowledgments of discussions?", "answer": "The directed links represent dependency relationships in Bayesian Networks, which connects to the collaborative discussions mentioned, as such networks model how variables influence each other through directed connections.", "doc_id": "1907.06430", "figure_id": "1907.06430_fig_13", "figure_number": 8, "image_path": "data/mineru_output/1907.06430/1907.06430/hybrid_auto/images/1907.06430_page0_fig12.jpg", "caption": "Fig. 8. Directed (a) acyclic and (b) cyclic graph.", "figure_type": "diagram", "visual_anchor": "directed arrows X₁ → X₃ → X₂", "text_evidence": "The authors would like to thank Ray Jiang, Christina Heinze-Deml, Tom Stepleton, Tom Everitt, and Shira Mitchell for useful discussions.", "query_type": "value_context"}
{"query_id": "l1_1907.09013_1907.09013_fig_1_866", "query": "Why do bidirectional gray arrows connect Business Understanding and Data Understanding at the top of the cycle?", "answer": "These arrows represent the iterative relationship between understanding business requirements and data characteristics, which is fundamental to identifying discrimination causes during the initial CRISP-DM phases.", "doc_id": "1907.09013", "figure_id": "1907.09013_fig_1", "figure_number": 2, "image_path": "data/mineru_output/1907.09013/1907.09013/hybrid_auto/images/1907.09013_page0_fig0.jpg", "caption": "Figure 2: Commonly cited causes of classifier discrimination.   ", "figure_type": "diagram", "visual_anchor": "Bidirectional gray arrows between the two blue boxes at top (Business Understanding and Data Understanding)", "text_evidence": "For each procedural step of the building process, we categorize the discrimination by cause: either data issues or misspecification.", "query_type": "visual_definition"}
{"query_id": "l1_1907.09013_1907.09013_fig_1_867", "query": "How does the central gray database icon relate to discrimination emerging throughout the blue-boxed process stages?", "answer": "The central database represents the data that flows through all CRISP-DM stages, showing that unintended discrimination can emerge at any point in the data mining process where data issues or procedural failures occur.", "doc_id": "1907.09013", "figure_id": "1907.09013_fig_1", "figure_number": 2, "image_path": "data/mineru_output/1907.09013/1907.09013/hybrid_auto/images/1907.09013_page0_fig0.jpg", "caption": "Figure 2: Commonly cited causes of classifier discrimination.   ", "figure_type": "diagram", "visual_anchor": "Gray cylindrical database icon labeled 'Data' positioned in the center of the circular process", "text_evidence": "A classifier alone does not discriminate, but unintended discrimination can find its way into a classification system in various ways, and at all parts of the data mining process.", "query_type": "comparison_explanation"}
{"query_id": "l1_1907.09013_1907.09013_fig_1_868", "query": "What discrimination source does the Deployment stage on the left represent beyond the building phases?", "answer": "The Deployment stage represents procedural failures that occur when using the classifier, a distinct category from data issues and misspecification encountered during the building process.", "doc_id": "1907.09013", "figure_id": "1907.09013_fig_1", "figure_number": 2, "image_path": "data/mineru_output/1907.09013/1907.09013/hybrid_auto/images/1907.09013_page0_fig0.jpg", "caption": "Figure 2: Commonly cited causes of classifier discrimination.   ", "figure_type": "diagram", "visual_anchor": "Blue Deployment box positioned on the left side of the circular flow", "text_evidence": "When we consider how the classifier is used, we include in our taxonomy \"procedural failures\"", "query_type": "value_context"}
{"query_id": "l1_1907.09013_1907.09013_fig_2_869", "query": "Why does the first diamond decision node enable optional dashed-box pre-processing before reaching Training Data?", "answer": "The decision node checks for discrimination potential in raw data. If discrimination is detected (Yes path), it routes to discrimination-aware pre-processing to mitigate bias before training begins.", "doc_id": "1907.09013", "figure_id": "1907.09013_fig_2", "figure_number": 3, "image_path": "data/mineru_output/1907.09013/1907.09013/hybrid_auto/images/1907.09013_page0_fig1.jpg", "caption": "Figure 3. Zemel et al. and Kamishima et al. both introduce augmented cost functions, where they augment a standard log-likelihood loss function with a ‘fairness’ regularizer11,19. These regularizers take into account differences in how the learning algorithm classifies protected vs. non-protected classes (i.e. $S ^ { I }$ vs $S ^ { \\boldsymbol { 0 } }$ ) and penalizes the total loss based on the extent of the difference. Much like in standard L1/L2 regularization, the penalization is controlled by a tunable hyper-parameter. Zemel et al give examples on ways to tune hyper-parameters to achieve different outcomes. Other in-processing techniques involve discrimination specific modifications of traditional learning algorithms, such as Naïve Bayes and Decision Trees9,10.", "figure_type": "diagram", "visual_anchor": "First diamond-shaped 'Discrim Potential?' node with Yes arrow pointing to dashed 'Discrim Aware Pre-processing' box", "text_evidence": "The greatest error one can make during evaluation is to not test an algorithm for potential discriminatory behavior. This is an error that starts with the data scientist but should extend upwards to the executive team.", "query_type": "comparison_explanation"}
{"query_id": "l1_1907.09013_1907.09013_fig_2_870", "query": "How do the dashed boxes for training relate to augmented cost functions that penalize classification differences?", "answer": "The dashed 'Discrim Aware Training' box represents in-processing techniques like augmented cost functions. These add fairness regularizers that penalize differences between protected and non-protected class classifications during training.", "doc_id": "1907.09013", "figure_id": "1907.09013_fig_2", "figure_number": 3, "image_path": "data/mineru_output/1907.09013/1907.09013/hybrid_auto/images/1907.09013_page0_fig1.jpg", "caption": "Figure 3. Zemel et al. and Kamishima et al. both introduce augmented cost functions, where they augment a standard log-likelihood loss function with a ‘fairness’ regularizer11,19. These regularizers take into account differences in how the learning algorithm classifies protected vs. non-protected classes (i.e. $S ^ { I }$ vs $S ^ { \\boldsymbol { 0 } }$ ) and penalizes the total loss based on the extent of the difference. Much like in standard L1/L2 regularization, the penalization is controlled by a tunable hyper-parameter. Zemel et al give examples on ways to tune hyper-parameters to achieve different outcomes. Other in-processing techniques involve discrimination specific modifications of traditional learning algorithms, such as Naïve Bayes and Decision Trees9,10.", "figure_type": "diagram", "visual_anchor": "Dashed 'Discrim Aware Training' box positioned between Training Data cylinder and Classifier", "text_evidence": "Zemel et al. and Kamishima et al. both introduce augmented cost functions, where they augment a standard log-likelihood loss function with a 'fairness' regularizer11,19. These regularizers take into account differences in how the learning algorithm classifies protected vs. non-protected classes", "query_type": "visual_definition"}
{"query_id": "l1_1907.09013_1907.09013_fig_2_871", "query": "What triggers the Yes path from the second diamond to the dashed post-processing box?", "answer": "When Classifier Unit Tests detect discrimination potential, the Yes path activates discrimination-reducing post-processing. This modifies classifier outputs to reduce discriminatory behavior before potential reprocessing or deployment.", "doc_id": "1907.09013", "figure_id": "1907.09013_fig_2", "figure_number": 3, "image_path": "data/mineru_output/1907.09013/1907.09013/hybrid_auto/images/1907.09013_page0_fig1.jpg", "caption": "Figure 3. Zemel et al. and Kamishima et al. both introduce augmented cost functions, where they augment a standard log-likelihood loss function with a ‘fairness’ regularizer11,19. These regularizers take into account differences in how the learning algorithm classifies protected vs. non-protected classes (i.e. $S ^ { I }$ vs $S ^ { \\boldsymbol { 0 } }$ ) and penalizes the total loss based on the extent of the difference. Much like in standard L1/L2 regularization, the penalization is controlled by a tunable hyper-parameter. Zemel et al give examples on ways to tune hyper-parameters to achieve different outcomes. Other in-processing techniques involve discrimination specific modifications of traditional learning algorithms, such as Naïve Bayes and Decision Trees9,10.", "figure_type": "diagram", "visual_anchor": "Second 'Discrim Potential?' diamond with Yes arrow leading to dashed 'Discrim Reducing Post-Processing' box", "text_evidence": "Other in-processing techniques involve discrimination specific modifications of traditional learning algorithms, such as Naïve Bayes and Decision Trees9,10.", "query_type": "value_context"}
{"query_id": "l1_1907.12059_1907.12059_fig_8_872", "query": "Why do the bottom histograms cluster near 0.0 after training with beta equals 100?", "answer": "After 10,000 training steps with α=0 and β=100, each group histogram matches the Wasserstein-1 barycenter, which concentrates model beliefs near 0.0. This demonstrates that the Wasserstein-1 penalty forces all demographic groups to align their prediction distributions.", "doc_id": "1907.12059", "figure_id": "1907.12059_fig_8", "figure_number": 1, "image_path": "data/mineru_output/1907.12059/1907.12059/hybrid_auto/images/1907.12059_page0_fig7.jpg", "caption": "Figure 1: Histograms of model beliefs for groups of Black females, Black males, White females, and White males, and their barycenter on the Adult dataset using Wass-1 Penalty. Top: Initial state. Bottom: After 10,000 training steps with $\\alpha = 0 , \\beta = 1 0 0$ each group histogram matches the barycenter.   ", "figure_type": "plot", "visual_anchor": "Red vertical bars concentrated at left edge (near 0.0) in bottom histogram", "text_evidence": "After 10,000 training steps with $\\alpha = 0 , \\beta = 1 0 0$ each group histogram matches the barycenter", "query_type": "comparison_explanation"}
{"query_id": "l1_1907.12059_1907.12059_fig_8_873", "query": "Does the concentration near 0.0 in the bottom plot support minimal modification of model decisions?", "answer": "Yes, the barycenter concentration demonstrates that the Wasserstein-1 approach achieves independence by aligning all group distributions to a common target. This enables reaching independence with minimal modifications as stated.", "doc_id": "1907.12059", "figure_id": "1907.12059_fig_8", "figure_number": 1, "image_path": "data/mineru_output/1907.12059/1907.12059/hybrid_auto/images/1907.12059_page0_fig7.jpg", "caption": "Figure 1: Histograms of model beliefs for groups of Black females, Black males, White females, and White males, and their barycenter on the Adult dataset using Wass-1 Penalty. Top: Initial state. Bottom: After 10,000 training steps with $\\alpha = 0 , \\beta = 1 0 0$ each group histogram matches the barycenter.   ", "figure_type": "plot", "visual_anchor": "Bottom histogram with all groups aligned near 0.0 value", "text_evidence": "using the Wasserstein-1 barycenter enables us to reach independence with minimal modifications of the model decisions", "query_type": "value_context"}
{"query_id": "l1_1907.12059_1907.12059_fig_8_874", "query": "How does the top histogram's spread across 0.2 to 1.0 relate to sensitive information dependence?", "answer": "The dispersed distributions in the top histogram show that model outputs initially vary across demographic groups (Black females, Black males, White females, White males), indicating dependence on sensitive attributes. The Wasserstein-1 distance approach eliminates this dependence.", "doc_id": "1907.12059", "figure_id": "1907.12059_fig_8", "figure_number": 1, "image_path": "data/mineru_output/1907.12059/1907.12059/hybrid_auto/images/1907.12059_page0_fig7.jpg", "caption": "Figure 1: Histograms of model beliefs for groups of Black females, Black males, White females, and White males, and their barycenter on the Adult dataset using Wass-1 Penalty. Top: Initial state. Bottom: After 10,000 training steps with $\\alpha = 0 , \\beta = 1 0 0$ each group histogram matches the barycenter.   ", "figure_type": "plot", "visual_anchor": "Top histogram showing distributions spread across 0.2 to 1.0 range", "text_evidence": "ensure that the output of a classification system does not depend on sensitive information using the Wasserstein-1 distance", "query_type": "comparison_explanation"}
{"query_id": "l1_1907.12059_1907.12059_fig_11_875", "query": "Why do the horizontal hatching lines in the shaded region demonstrate the equality in Eq. (12)?", "answer": "The horizontal hatching represents integrating |f⁻¹ - g⁻¹| along the x-axis (left interpretation) and integrating |f - g| along the y-axis (right interpretation), both computing the same shaded area and thus proving the equality.", "doc_id": "1907.12059", "figure_id": "1907.12059_fig_11", "figure_number": 3, "image_path": "data/mineru_output/1907.12059/1907.12059/hybrid_auto/images/1907.12059_page0_fig10.jpg", "caption": "Figure 3: Integrating $| f ^ { - 1 } - g ^ { - 1 } |$ along the $x$ axis (left) and integrating $| f - g |$ along the $y$ axis (right) both compute the area of the same shaded region, thus the equality in Eq. (12).", "figure_type": "diagram", "visual_anchor": "horizontal hatching lines filling the shaded region between curves f and g", "text_evidence": "Integrating $| f ^ { - 1 } - g ^ { - 1 } |$ along the $x$ axis (left) and integrating $| f - g |$ along the $y$ axis (right) both compute the area of the same shaded region, thus the equality in Eq. (12)", "query_type": "visual_definition"}
{"query_id": "l1_1907.12059_1907.12059_fig_11_876", "query": "How does the shaded region between curves f and g support the claim about two ways of computing?", "answer": "The shaded region visually demonstrates that integrating the difference of inverse CDFs horizontally yields the same area as integrating the difference of CDFs vertically, providing geometric proof of the analytical equality.", "doc_id": "1907.12059", "figure_id": "1907.12059_fig_11", "figure_number": 3, "image_path": "data/mineru_output/1907.12059/1907.12059/hybrid_auto/images/1907.12059_page0_fig10.jpg", "caption": "Figure 3: Integrating $| f ^ { - 1 } - g ^ { - 1 } |$ along the $x$ axis (left) and integrating $| f - g |$ along the $y$ axis (right) both compute the area of the same shaded region, thus the equality in Eq. (12).", "figure_type": "diagram", "visual_anchor": "shaded region bounded by curves f and g from origin to point (1,1)", "text_evidence": "Intuitively, we see that the left and right side of Eq. (12) correspond to two ways of computing the same shaded area in Figure 3", "query_type": "comparison_explanation"}
{"query_id": "l1_1907.12059_1907.12059_fig_11_877", "query": "Why must curves f and g both reach the point at coordinates (1,1) given they represent invertible CDFs?", "answer": "Invertible CDFs are strictly increasing bijective functions mapping [0,1] to [0,1], so they must start at (0,0) and end at (1,1) to satisfy the probability space constraint Ω = [0,1].", "doc_id": "1907.12059", "figure_id": "1907.12059_fig_11", "figure_number": 3, "image_path": "data/mineru_output/1907.12059/1907.12059/hybrid_auto/images/1907.12059_page0_fig10.jpg", "caption": "Figure 3: Integrating $| f ^ { - 1 } - g ^ { - 1 } |$ along the $x$ axis (left) and integrating $| f - g |$ along the $y$ axis (right) both compute the area of the same shaded region, thus the equality in Eq. (12).", "figure_type": "diagram", "visual_anchor": "convergence point of curves f and g at coordinates (1,1) with vertical dashed line", "text_evidence": "Given two differentiable and invertible cumulative distribution functions $f , g$ over the probability space $\\Omega = [ 0 , 1 ]$ , thus $f , g : [ 0 , 1 ] \\to [ 0 , 1 ]$", "query_type": "value_context"}
{"query_id": "l1_1909.05088_1909.05088_fig_1_878", "query": "Why does the 20-30 age group show 65% female representation despite representing a very small percentage of total sentences?", "answer": "The 20-30 age group represents only about 0% of the total amount of sentences in the dataset, making this demographic pattern statistically insignificant despite its visual appearance in the chart.", "doc_id": "1909.05088", "figure_id": "1909.05088_fig_1", "figure_number": 1, "image_path": "data/mineru_output/1909.05088/1909.05088/hybrid_auto/images/1909.05088_page0_fig0.jpg", "caption": "Figure 1: Percentage of female and male speakers per age group", "figure_type": "plot", "visual_anchor": "Blue bar at approximately 65% in the 20-30 age group", "text_evidence": "with the exception of the youngest age group (20–30), which represents only a very small percentage of the total amount of sentences $( 0 .", "query_type": "anomaly_cause"}
{"query_id": "l1_1909.05088_1909.05088_fig_1_879", "query": "How does the declining blue bar trend from 65% to near 0% across age groups reflect gender unbalance in translations?", "answer": "The decreasing female representation across age groups creates a gender imbalance in the Europarl dataset, which will be reflected in the translations that MT systems trained on this data produce.", "doc_id": "1909.05088", "figure_id": "1909.05088_fig_1", "figure_number": 1, "image_path": "data/mineru_output/1909.05088/1909.05088/hybrid_auto/images/1909.05088_page0_fig0.jpg", "caption": "Figure 1: Percentage of female and male speakers per age group", "figure_type": "plot", "visual_anchor": "Decreasing blue (female) bars from 65% at 20-30 to nearly 0% at 80-90", "text_evidence": "there is a gender unbalance in the Europarl dataset, which will be reflected in the translations that MT systems trained on this data produce.", "query_type": "comparison_explanation"}
{"query_id": "l1_1909.05088_1909.05088_fig_1_880", "query": "Does the predominantly orange composition after age 40 justify augmenting sentences with gender tags for speaker identification?", "answer": "Yes, the gender imbalance shown across age groups necessitates augmenting every sentence with a tag on the English source side, identifying the gender of the speaker, to address personalization challenges.", "doc_id": "1909.05088", "figure_id": "1909.05088_fig_1", "figure_number": 1, "image_path": "data/mineru_output/1909.05088/1909.05088/hybrid_auto/images/1909.05088_page0_fig0.jpg", "caption": "Figure 1: Percentage of female and male speakers per age group", "figure_type": "plot", "visual_anchor": "Orange (male) bars dominating above 60% from age 40-50 onwards", "text_evidence": "We augmented every sentence with a tag on the English source side, identifying the gender of the speaker, as illustrated in (1).", "query_type": "value_context"}
{"query_id": "l1_1910.10872_1910.10872_fig_1_881", "query": "Why is Charlotte in example 1 tagged as CITY when CoreNLP should recognize it as a person name?", "answer": "CoreNLP wrongfully tags person entities as non-PERSON or NULL entities. The figure demonstrates this systematic bias by showing Charlotte incorrectly classified as CITY despite appearing in a human-like context (\"is a person\").", "doc_id": "1910.10872", "figure_id": "1910.10872_fig_1", "figure_number": 1, "image_path": "data/mineru_output/1910.10872/1910.10872/hybrid_auto/images/1910.10872_page0_fig0.jpg", "caption": "Figure 1: Examples of PERSON entities that are wrongfully tagged as non-PERSON or NULL entities by CoreNLP.", "figure_type": "example", "visual_anchor": "Example 1 with CITY label tag above 'Charlotte is a person'", "text_evidence": "Figure 1: Examples of PERSON entities that are wrongfully tagged as non-PERSON or NULL entities by CoreNLP.", "query_type": "anomaly_cause"}
{"query_id": "l1_1910.10872_1910.10872_fig_1_882", "query": "Do the gray-background rows showing Sofia and Rose illustrate the templated sentences starting with census names followed by human activities?", "answer": "Yes, these examples follow the template structure where census names (Sofia, Rose) are followed by human-like activities (eating cupcake, playing with dolls). However, Sofia is misclassified as CITY and Rose has no entity tag.", "doc_id": "1910.10872", "figure_id": "1910.10872_fig_1", "figure_number": 1, "image_path": "data/mineru_output/1910.10872/1910.10872/hybrid_auto/images/1910.10872_page0_fig0.jpg", "caption": "Figure 1: Examples of PERSON entities that are wrongfully tagged as non-PERSON or NULL entities by CoreNLP.", "figure_type": "example", "visual_anchor": "Gray-background rows 2 and 4 showing 'Sofia eats her favorite cupcake' and 'Rose plays with her dolls'", "text_evidence": "Our benchmark dataset consists of nine templates listed in Table 1 which are templated sentences that start with the existing names in the census data followed by a sentence that represents a human-like activity.", "query_type": "value_context"}
{"query_id": "l1_1910.10872_1910.10872_fig_1_883", "query": "How does the LOCATION tag on Gracie in example 5 relate to measuring bias in NER systems?", "answer": "The incorrect LOCATION tag on Gracie demonstrates the systematic bias being measured across five named entity models. This misclassification exemplifies how NER systems fail to recognize person names in human-activity contexts, which the study evaluates using 139 years of census data.", "doc_id": "1910.10872", "figure_id": "1910.10872_fig_1", "figure_number": 1, "image_path": "data/mineru_output/1910.10872/1910.10872/hybrid_auto/images/1910.10872_page0_fig0.jpg", "caption": "Figure 1: Examples of PERSON entities that are wrongfully tagged as non-PERSON or NULL entities by CoreNLP.", "figure_type": "example", "visual_anchor": "Example 5 with blue LOCATION label above 'Gracie is going to school'", "text_evidence": "To measure the existence of bias in NER systems, we evaluated five named entity models used in research and industry. We used Flair (Akbik, Blythe, and Vollgraf 2018; Akbik et al. 2019), CoreNLP version 3.9 (Manning et al. 2014; Finkel, Grenager, and Manning 2005), and Spacy version 2.1 with small, medium, and large models.", "query_type": "comparison_explanation"}
{"query_id": "l1_1910.10872_1910.10872_fig_14_884", "query": "Why does the dashed magenta line peak at 0.33 in 2015 when template #4 was used with weighted cases?", "answer": "The plot shows results from template #4 on weighted census data spanning 139 years, where female names consistently exhibit higher error rates. The 2015 peak reflects this persistent gender disparity in name tagging performance.", "doc_id": "1910.10872", "figure_id": "1910.10872_fig_14", "figure_number": 2, "image_path": "data/mineru_output/1910.10872/1910.10872/hybrid_auto/images/1910.10872_page0_fig13.jpg", "caption": "Figure 2: Results from different models that spanned the 139-year history of baby names from the census data on different error 1875 1895 1915 1935 1955 1975 1995 2015types for the weighted cases using template #4. Female names have higher error rates for all the cases. The y axis shows the 0.09 Flair 0.14 calculated error rates for each of the error types as described in their corresponding formulas, and the x axis represents the year 0.070.08 female malein which the baby name was given.", "figure_type": "plot", "visual_anchor": "dashed magenta line peaking at approximately 0.33 in year 2015", "text_evidence": "Results from different models that spanned the 139-year history of baby names from the census data on different error types for the weighted cases using template #4. Female names have higher error rates for all the cases.", "query_type": "value_context"}
{"query_id": "l1_1910.10872_1910.10872_fig_14_885", "query": "Does the Flair model's behavior explain why the magenta line rises sharply after 1995 compared to the cyan line?", "answer": "Yes, the Flair model frequently mistagged female names like Charlotte and Sofia as LOC (location) entities. This systematic error in the Flair model contributes to the steeper upward trajectory of female error rates after 1995.", "doc_id": "1910.10872", "figure_id": "1910.10872_fig_14", "figure_number": 2, "image_path": "data/mineru_output/1910.10872/1910.10872/hybrid_auto/images/1910.10872_page0_fig13.jpg", "caption": "Figure 2: Results from different models that spanned the 139-year history of baby names from the census data on different error 1875 1895 1915 1935 1955 1975 1995 2015types for the weighted cases using template #4. Female names have higher error rates for all the cases. The y axis shows the 0.09 Flair 0.14 calculated error rates for each of the error types as described in their corresponding formulas, and the x axis represents the year 0.070.08 female malein which the baby name was given.", "figure_type": "plot", "visual_anchor": "magenta dashed line showing sharp upward trend from 1995 to 2015, diverging from cyan line", "text_evidence": "We list the top six most frequent male and female names which were tagged erroneously by the Flair model in year 2018 from our benchmark evaluated on template #4 in Table 2. Charlotte 12,940 Tagged as LOC, Sofia 7,621 Tagged as LOC", "query_type": "comparison_explanation"}
{"query_id": "l1_1910.10872_1910.10872_fig_14_886", "query": "What causes the cyan male line to remain below 0.06 until 1955 despite the 139-year span?", "answer": "Male names maintained relatively stable and low error rates in early decades because the models performed more consistently on male names across the historical census data. The error formula calculations remained low for male names during this period.", "doc_id": "1910.10872", "figure_id": "1910.10872_fig_14", "figure_number": 2, "image_path": "data/mineru_output/1910.10872/1910.10872/hybrid_auto/images/1910.10872_page0_fig13.jpg", "caption": "Figure 2: Results from different models that spanned the 139-year history of baby names from the census data on different error 1875 1895 1915 1935 1955 1975 1995 2015types for the weighted cases using template #4. Female names have higher error rates for all the cases. The y axis shows the 0.09 Flair 0.14 calculated error rates for each of the error types as described in their corresponding formulas, and the x axis represents the year 0.070.08 female malein which the baby name was given.", "figure_type": "plot", "visual_anchor": "solid cyan line staying flat between 0.04 and 0.06 from 1875 to 1955", "text_evidence": "The y axis shows the calculated error rates for each of the error types as described in their corresponding formulas, and the x axis represents the year in which the baby name was given.", "query_type": "anomaly_cause"}
{"query_id": "l1_1910.10872_1910.10872_fig_31_887", "query": "Why does the magenta female line reach 0.55 in 2015 while male stays at 0.4?", "answer": "Female names have consistently higher error rates than male names across nearly all time periods. The analysis uses template #4 for unweighted cases, which reveals this persistent gender disparity in naming error rates.", "doc_id": "1910.10872", "figure_id": "1910.10872_fig_31", "figure_number": 3, "image_path": "data/mineru_output/1910.10872/1910.10872/hybrid_auto/images/1910.10872_page0_fig30.jpg", "caption": "Figure 3: Results from different models over the 139-year history of baby names from the census data on different error types 0.020.031875 1895 1915 1935 1955 1975 1995 2015 1875 1895 1915 1935 1955 1975 1995 2015for the unweighted cases using template #4. Female names have higher error rates for all cases except four marginal cases. The 0.010.1 y axis shows the calculated error rates for each of the error types as described in their corresponding formulas, and the x axis 1875 1895 10.080.09represents the year in which the baby name was given.", "figure_type": "plot", "visual_anchor": "magenta dashed line at 0.55 and cyan solid line at 0.4 in year 2015", "text_evidence": "Female names have higher error rates for all cases except four marginal cases.", "query_type": "comparison_explanation"}
{"query_id": "l1_1910.10872_1910.10872_fig_31_888", "query": "Does the cyan line's rise from 0.15 to 0.4 between 1975 and 2015 reflect CoreNLP version changes?", "answer": "The upward trend in male error rates from 1975 to 2015 could reflect model evolution. CoreNLP underwent version updates (e.g., 3.8 to 3.9) that changed tagging behavior, potentially affecting error patterns over time.", "doc_id": "1910.10872", "figure_id": "1910.10872_fig_31", "figure_number": 3, "image_path": "data/mineru_output/1910.10872/1910.10872/hybrid_auto/images/1910.10872_page0_fig30.jpg", "caption": "Figure 3: Results from different models over the 139-year history of baby names from the census data on different error types 0.020.031875 1895 1915 1935 1955 1975 1995 2015 1875 1895 1915 1935 1955 1975 1995 2015for the unweighted cases using template #4. Female names have higher error rates for all cases except four marginal cases. The 0.010.1 y axis shows the calculated error rates for each of the error types as described in their corresponding formulas, and the x axis 1875 1895 10.080.09represents the year in which the baby name was given.", "figure_type": "plot", "visual_anchor": "cyan solid line increasing from approximately 0.15 at 1975 to 0.4 at 2015", "text_evidence": "As examples, some of the changes from version 3.8 to version 3.9 of the CoreNLP model are shown in Table 3.", "query_type": "comparison_explanation"}
{"query_id": "l1_1910.10872_1910.10872_fig_31_889", "query": "How do the calculated error rates on the y-axis relate to machine translation performance issues?", "answer": "The y-axis displays calculated error rates from specific formulas measuring naming biases. These error metrics are relevant to broader NLP applications including machine translation, where similar bias patterns can affect system performance.", "doc_id": "1910.10872", "figure_id": "1910.10872_fig_31", "figure_number": 3, "image_path": "data/mineru_output/1910.10872/1910.10872/hybrid_auto/images/1910.10872_page0_fig30.jpg", "caption": "Figure 3: Results from different models over the 139-year history of baby names from the census data on different error types 0.020.031875 1895 1915 1935 1955 1975 1995 2015 1875 1895 1915 1935 1955 1975 1995 2015for the unweighted cases using template #4. Female names have higher error rates for all cases except four marginal cases. The 0.010.1 y axis shows the calculated error rates for each of the error types as described in their corresponding formulas, and the x axis 1875 1895 10.080.09represents the year in which the baby name was given.", "figure_type": "plot", "visual_anchor": "y-axis ranging from 0 to 0.6 showing calculated error rates", "text_evidence": "The y axis shows the calculated error rates for each of the error types as described in their corresponding formulas, and the x axis 1875 1895 10.080.09represents the year in which the baby name was given.", "query_type": "value_context"}
{"query_id": "l1_1910.10872_1910.10872_fig_35_890", "query": "Does the blue dashed TMPL1 line's unique upward spike to 0.08 after 1995 demonstrate how context helps some models?", "answer": "Yes, the blue dashed TMPL1 line shows a dramatic increase to 0.08 after 1995, while other templates remain below 0.05, illustrating that context in some templates helped certain models but showed negative effects on others.", "doc_id": "1910.10872", "figure_id": "1910.10872_fig_35", "figure_number": 4, "image_path": "data/mineru_output/1910.10872/1910.10872/hybrid_auto/images/1910.10872_page0_fig34.jpg", "caption": "Figure 4: Performance of different models on different templates from our benchmark for female and male names collected for 0.25 Male CoreNLPover 139 years. Notice how context in some of the templates helped some models, but showed negative effects on other models.", "figure_type": "plot", "visual_anchor": "Blue dashed line (TMPL1) reaching approximately 0.08 by 2015", "text_evidence": "Notice how context in some of the templates helped some models, but showed negative effects on other models.", "query_type": "comparison_explanation"}
{"query_id": "l1_1910.10872_1910.10872_fig_35_891", "query": "Why do all nine template lines cluster between 0.01-0.03 before 1995 despite representing different models?", "answer": "The clustering suggests relatively uniform low performance across all templates and models during the 1875-1995 period. This pattern changes after 1995, showing an interesting trend in how different templates perform over the 139-year span for male names.", "doc_id": "1910.10872", "figure_id": "1910.10872_fig_35", "figure_number": 4, "image_path": "data/mineru_output/1910.10872/1910.10872/hybrid_auto/images/1910.10872_page0_fig34.jpg", "caption": "Figure 4: Performance of different models on different templates from our benchmark for female and male names collected for 0.25 Male CoreNLPover 139 years. Notice how context in some of the templates helped some models, but showed negative effects on other models.", "figure_type": "plot", "visual_anchor": "All nine lines (TMPL1-TMPL9) clustered between 0.01 and 0.03 from 1875 to 1995", "text_evidence": "That being said, in Figure 4 we showed how all the models perform on all the tem-0plates, for Error Type-1 Weighted (the super-set error) case, 1875 1895 1915 1935 1955 1975 1995 2015which on its own is showing an interesting trend.", "query_type": "anomaly_cause"}
{"query_id": "l1_1910.10872_1910.10872_fig_35_892", "query": "Do the diverging performance lines after 1995 reflect dataset biases rather than real-world gender name distributions?", "answer": "Yes, the diverging performance patterns after 1995, with TMPL1 reaching 0.08 while others stay lower, suggest the datasets used do not reflect the real world but rather the opposite, indicating bias toward specific groups.", "doc_id": "1910.10872", "figure_id": "1910.10872_fig_35", "figure_number": 4, "image_path": "data/mineru_output/1910.10872/1910.10872/hybrid_auto/images/1910.10872_page0_fig34.jpg", "caption": "Figure 4: Performance of different models on different templates from our benchmark for female and male names collected for 0.25 Male CoreNLPover 139 years. Notice how context in some of the templates helped some models, but showed negative effects on other models.", "figure_type": "plot", "visual_anchor": "Diverging lines after 1995 with TMPL1 reaching 0.08 and others at 0.03-0.05", "text_evidence": "Our results shown in Table 4 indicate that the datasets 0.06used do not reflect the real world, but rather exactly the op-0.020.04posite of that.", "query_type": "comparison_explanation"}
{"query_id": "l1_1910.10872_1910.10872_fig_41_893", "query": "Why do the green dashed and yellow solid lines nearly overlap throughout 1875-2015 despite Version 3.9's entity tagging improvements?", "answer": "Despite Version 3.9 trying to tag more entities and achieving slight improvement in Error Type-3, degraded performance in Error Type-2 offset the gains, resulting in overall no change in Error Type-1, which is the super-set shown in this plot.", "doc_id": "1910.10872", "figure_id": "1910.10872_fig_41", "figure_number": 5, "image_path": "data/mineru_output/1910.10872/1910.10872/hybrid_auto/images/1910.10872_page0_fig40.jpg", "caption": "Figure 5: Version update in the CoreNLP model tried to tag more entities and thus a subtle boost in performance with regard to 1875 1895 1915 1935 1955 1975 1995 2015  1875 1895 1915 1935 1955 1975 1995 2015Error Type-3. However, this resulted in more PERSON entities being tagged as non-PERSON entities. This then degraded per-Male CoreNLP 0.2 Male CoreNLPformance with regard to Error Type-2 in the newer version, and overall no change in the Error Type-1 case which is considered 0.12 Version 3.8Version 3.9 0.18 Version 3.8Version 3.9to be the super-set. We observe how the degrade in Error Type-2 affected more female names than males on average.0.8 Female Medium Spacy", "figure_type": "plot", "visual_anchor": "overlapping green dashed (Version 3.8) and yellow solid (Version 3.9) lines across entire timeline", "text_evidence": "This then degraded performance with regard to Error Type-2 in the newer version, and overall no change in the Error Type-1 case which is considered to be the super-set.", "query_type": "anomaly_cause"}
{"query_id": "l1_1910.10872_1910.10872_fig_41_894", "query": "Does the minimal separation between Version 3.8 and 3.9 lines reflect the subtle boost mentioned for Error Type-3?", "answer": "No, this figure shows Error Type-1 (the super-set) where no change occurred. The subtle boost in Error Type-3 performance was offset by degradation in Error Type-2, resulting in the overlapping lines observed here.", "doc_id": "1910.10872", "figure_id": "1910.10872_fig_41", "figure_number": 5, "image_path": "data/mineru_output/1910.10872/1910.10872/hybrid_auto/images/1910.10872_page0_fig40.jpg", "caption": "Figure 5: Version update in the CoreNLP model tried to tag more entities and thus a subtle boost in performance with regard to 1875 1895 1915 1935 1955 1975 1995 2015  1875 1895 1915 1935 1955 1975 1995 2015Error Type-3. However, this resulted in more PERSON entities being tagged as non-PERSON entities. This then degraded per-Male CoreNLP 0.2 Male CoreNLPformance with regard to Error Type-2 in the newer version, and overall no change in the Error Type-1 case which is considered 0.12 Version 3.8Version 3.9 0.18 Version 3.8Version 3.9to be the super-set. We observe how the degrade in Error Type-2 affected more female names than males on average.0.8 Female Medium Spacy", "figure_type": "plot", "visual_anchor": "near-identical trajectories of green dashed and yellow solid lines from 0.02 to 0.12", "text_evidence": "Version update in the CoreNLP model tried to tag more entities and thus a subtle boost in performance with regard to Error Type-3. However, this resulted in more PERSON entities being tagged as non-PERSON entities.", "query_type": "comparison_explanation"}
{"query_id": "l1_1910.10872_1910.10872_fig_41_895", "query": "How does the U-shaped pattern with minimum near 1955-1975 relate to CoreNLP's increased entity tagging attempts?", "answer": "The U-shaped pattern reflects temporal variation in name recognition errors across different birth year cohorts. CoreNLP's attempt to tag more entities in Version 3.9 did not change this overall pattern, as improvements in one error type were offset by degradation in another.", "doc_id": "1910.10872", "figure_id": "1910.10872_fig_41", "figure_number": 5, "image_path": "data/mineru_output/1910.10872/1910.10872/hybrid_auto/images/1910.10872_page0_fig40.jpg", "caption": "Figure 5: Version update in the CoreNLP model tried to tag more entities and thus a subtle boost in performance with regard to 1875 1895 1915 1935 1955 1975 1995 2015  1875 1895 1915 1935 1955 1975 1995 2015Error Type-3. However, this resulted in more PERSON entities being tagged as non-PERSON entities. This then degraded per-Male CoreNLP 0.2 Male CoreNLPformance with regard to Error Type-2 in the newer version, and overall no change in the Error Type-1 case which is considered 0.12 Version 3.8Version 3.9 0.18 Version 3.8Version 3.9to be the super-set. We observe how the degrade in Error Type-2 affected more female names than males on average.0.8 Female Medium Spacy", "figure_type": "plot", "visual_anchor": "both lines reaching minimum values around 0.02 near year 1955-1975, then rising to 0.12 by 2015", "text_evidence": "Similarly, for CoreNLP more entities tried to be tagged—which resulted in a slight improvement in Error Type-3, as shown in Figure 5.", "query_type": "value_context"}
{"query_id": "l1_1910.10872_1910.10872_fig_44_896", "query": "Does the V2.1 orange line reaching 0.70 for males versus 0.55 for females by 2015 demonstrate bias twice as large?", "answer": "Yes, the V2.1 version shows substantially higher error rates for female names (~0.65-0.70) compared to male names (~0.70), supporting the claim that bias against female names was on average twice that of male names.", "doc_id": "1910.10872", "figure_id": "1910.10872_fig_44", "figure_number": 6, "image_path": "data/mineru_output/1910.10872/1910.10872/hybrid_auto/images/1910.10872_page0_fig43.jpg", "caption": "Figure 6: Biased performance of version 2.1 over 2.0 in 0 1875 1895 1915 1935 1955 1975 1995 2015Spacy medium. The bias against female names was on average twice that of male names.", "figure_type": "plot", "visual_anchor": "V2.1 orange solid line at 2015: ~0.70 in Female panel versus ~0.70 in Male panel", "text_evidence": "The bias against female names was on average twice that of male names.", "query_type": "comparison_explanation"}
{"query_id": "l1_1910.10872_1910.10872_fig_44_897", "query": "Why does the V2.1 orange line remain higher than V2.0 green across all years if newer versions aimed to tag more entities?", "answer": "The version update tried to tag more entities, resulting in more PERSON entities being incorrectly tagged as non-PERSON entities. This degraded performance with regard to Error Type-2 in the newer version, explaining the consistently higher error rates shown by the orange line.", "doc_id": "1910.10872", "figure_id": "1910.10872_fig_44", "figure_number": 6, "image_path": "data/mineru_output/1910.10872/1910.10872/hybrid_auto/images/1910.10872_page0_fig43.jpg", "caption": "Figure 6: Biased performance of version 2.1 over 2.0 in 0 1875 1895 1915 1935 1955 1975 1995 2015Spacy medium. The bias against female names was on average twice that of male names.", "figure_type": "plot", "visual_anchor": "V2.1 orange solid line consistently above V2.0 green dashed line from 1875 to 2015", "text_evidence": "Version update in the CoreNLP model tried to tag more entities and thus a subtle boost in performance with regard to Error Type-3. However, this resulted in more PERSON entities being tagged as non-PERSON entities. This then degraded performance with regard to Error Type-2 in the newer version", "query_type": "anomaly_cause"}
{"query_id": "l1_1910.10872_1910.10872_fig_44_898", "query": "How does the V2.0 green dashed decline from 0.45 to 0.32 during 1875-1955 relate to Error Type-1 Weighted performance?", "answer": "The newer version (V2.1) is more erroneous when it comes to Error Type-1 Weighted case, which is a superset of all error types. The decline in V2.0 reflects baseline performance, while V2.1's consistently higher values indicate worse overall weighted error rates.", "doc_id": "1910.10872", "figure_id": "1910.10872_fig_44", "figure_number": 6, "image_path": "data/mineru_output/1910.10872/1910.10872/hybrid_auto/images/1910.10872_page0_fig43.jpg", "caption": "Figure 6: Biased performance of version 2.1 over 2.0 in 0 1875 1895 1915 1935 1955 1975 1995 2015Spacy medium. The bias against female names was on average twice that of male names.", "figure_type": "plot", "visual_anchor": "V2.0 green dashed line declining from ~0.45 to ~0.32 between 1875 and 1955 in Female panel", "text_evidence": "For instance, as shown in Figure 6 for the Spacy medium model, the newer version is more erroneous when it comes to the Error Type-1 Weighted case which is a superset of all the error types discussed in this paper.", "query_type": "value_context"}
{"query_id": "l1_1910.10872_1910.10872_fig_60_899", "query": "Why does the magenta dashed line remain consistently above the cyan line across all years from 1875 to 2015?", "answer": "Female names have higher error rates for all cases when using template #5 over the 139-year history, as shown by the magenta line staying above the cyan line throughout the entire time period.", "doc_id": "1910.10872", "figure_id": "1910.10872_fig_60", "figure_number": 8, "image_path": "data/mineru_output/1910.10872/1910.10872/hybrid_auto/images/1910.10872_page0_fig59.jpg", "caption": "Figure 8: Results from different models ran over 139-year history of baby names from the census data on different error types 1875 1895 1915 1935 1955 1975 1995 2015for the weighted case using template # 5. Female names have higher error rates for all the cases. The y axis shows the calculated 0.08  0.12 error rates for each of the error types as described in their corresponding formulas, and the x axis represents the year in which 0.07 the baby name was given.", "figure_type": "plot", "visual_anchor": "magenta dashed line (female) positioned above cyan solid line (male) across entire x-axis", "text_evidence": "Female names have higher error rates for all the cases. The y axis shows the calculated error rates for each of the error types as described in their corresponding formulas", "query_type": "comparison_explanation"}
{"query_id": "l1_1910.10872_1910.10872_fig_60_900", "query": "Does the sharp upward trend in the magenta line after 1975 align with weighted template 5 calculations?", "answer": "Yes, the magenta line's rise from approximately 0.08 in 1975 to 0.22 in 2015 reflects the weighted case error rates calculated using template #5 for female baby names from the census data.", "doc_id": "1910.10872", "figure_id": "1910.10872_fig_60", "figure_number": 8, "image_path": "data/mineru_output/1910.10872/1910.10872/hybrid_auto/images/1910.10872_page0_fig59.jpg", "caption": "Figure 8: Results from different models ran over 139-year history of baby names from the census data on different error types 1875 1895 1915 1935 1955 1975 1995 2015for the weighted case using template # 5. Female names have higher error rates for all the cases. The y axis shows the calculated 0.08  0.12 error rates for each of the error types as described in their corresponding formulas, and the x axis represents the year in which 0.07 the baby name was given.", "figure_type": "plot", "visual_anchor": "magenta dashed line increasing steeply from 1975 to 2015, rising from ~0.08 to ~0.22", "text_evidence": "Results from different models ran over 139-year history of baby names from the census data on different error types for the weighted case using template # 5", "query_type": "value_context"}
{"query_id": "l1_1910.10872_1910.10872_fig_60_901", "query": "What causes both the cyan and magenta lines to reach their minimum values around 1955?", "answer": "Both gender error rates drop to their lowest points around 1955, with the cyan line near 0.02 and magenta near 0.04. This temporal pattern corresponds to when the baby names were given in the census data.", "doc_id": "1910.10872", "figure_id": "1910.10872_fig_60", "figure_number": 8, "image_path": "data/mineru_output/1910.10872/1910.10872/hybrid_auto/images/1910.10872_page0_fig59.jpg", "caption": "Figure 8: Results from different models ran over 139-year history of baby names from the census data on different error types 1875 1895 1915 1935 1955 1975 1995 2015for the weighted case using template # 5. Female names have higher error rates for all the cases. The y axis shows the calculated 0.08  0.12 error rates for each of the error types as described in their corresponding formulas, and the x axis represents the year in which 0.07 the baby name was given.", "figure_type": "plot", "visual_anchor": "both cyan and magenta lines reaching minimum values around 1955 on x-axis", "text_evidence": "the x axis represents the year in which the baby name was given. Results from different models ran over 139-year history of baby names from the census data", "query_type": "anomaly_cause"}
{"query_id": "l1_1910.10872_1910.10872_fig_79_902", "query": "Why does the dashed magenta line remain consistently above the solid cyan line across the 139-year census history?", "answer": "Female names have higher error rates for all cases except three marginal cases, indicating overall performance is more biased towards female names throughout the 139-year baby name history.", "doc_id": "1910.10872", "figure_id": "1910.10872_fig_79", "figure_number": 9, "image_path": "data/mineru_output/1910.10872/1910.10872/hybrid_auto/images/1910.10872_page0_fig78.jpg", "caption": "Figure 9: Results from different models ran over 139-year history of baby names from the census data on different error typesfor the unweighted case using template #5. Female names have higher error rates for all the cases except three marginal cases. 0.08 Overall performance is more biased towards female names. The y axis shows the calculated error rates for each of the error 1875 1895 1915 1935 1955 1975 1995 20150.07 types as described in their corresponding formulas, and the x axis represents the year in which the baby name was given.", "figure_type": "plot", "visual_anchor": "dashed magenta line (female) positioned above solid cyan line (male) from 1875 to 2015", "text_evidence": "Female names have higher error rates for all the cases except three marginal cases. Overall performance is more biased towards female names.", "query_type": "comparison_explanation"}
{"query_id": "l1_1910.10872_1910.10872_fig_79_903", "query": "Does the magenta line's peak near 0.45 around 2005 reflect calculated error rates from template #5 formulas?", "answer": "Yes, the y-axis shows the calculated error rates for each error type as described in their corresponding formulas, and the peak represents the highest bias against female names in recent decades using template #5.", "doc_id": "1910.10872", "figure_id": "1910.10872_fig_79", "figure_number": 9, "image_path": "data/mineru_output/1910.10872/1910.10872/hybrid_auto/images/1910.10872_page0_fig78.jpg", "caption": "Figure 9: Results from different models ran over 139-year history of baby names from the census data on different error typesfor the unweighted case using template #5. Female names have higher error rates for all the cases except three marginal cases. 0.08 Overall performance is more biased towards female names. The y axis shows the calculated error rates for each of the error 1875 1895 1915 1935 1955 1975 1995 20150.07 types as described in their corresponding formulas, and the x axis represents the year in which the baby name was given.", "figure_type": "plot", "visual_anchor": "magenta line peak at approximately 0.45 between 1995 and 2015", "text_evidence": "The y axis shows the calculated error rates for each of the error types as described in their corresponding formulas, and the x axis represents the year in which the baby name was given.", "query_type": "value_context"}
{"query_id": "l1_1910.10872_1910.10872_fig_79_904", "query": "What explains the cyan line's sharp upward trend starting around 1975, given the unweighted case methodology?", "answer": "The increase in male error rates after 1975 reflects changes in baby naming patterns over time, captured by running models over the 139-year census data using the unweighted case with template #5.", "doc_id": "1910.10872", "figure_id": "1910.10872_fig_79", "figure_number": 9, "image_path": "data/mineru_output/1910.10872/1910.10872/hybrid_auto/images/1910.10872_page0_fig78.jpg", "caption": "Figure 9: Results from different models ran over 139-year history of baby names from the census data on different error typesfor the unweighted case using template #5. Female names have higher error rates for all the cases except three marginal cases. 0.08 Overall performance is more biased towards female names. The y axis shows the calculated error rates for each of the error 1875 1895 1915 1935 1955 1975 1995 20150.07 types as described in their corresponding formulas, and the x axis represents the year in which the baby name was given.", "figure_type": "plot", "visual_anchor": "solid cyan line showing steep increase from approximately 0.15 to 0.35 between 1975 and 2015", "text_evidence": "Results from different models ran over 139-year history of baby names from the census data on different error typesfor the unweighted case using template #5.", "query_type": "anomaly_cause"}
{"query_id": "l1_2005.07293_2005.07293_fig_1_905", "query": "How does the different-height box arrangement under the yellow figure in the right panel operationalize compensation for historically disadvantaged groups?", "answer": "The taller box under the yellow figure represents resource distribution to overcome obstacles and raise opportunities for access, compensating historically disadvantaged groups so they can reach the apples like others.", "doc_id": "2005.07293", "figure_id": "2005.07293_fig_1", "figure_number": 1, "image_path": "data/mineru_output/2005.07293/2005.07293/hybrid_auto/images/2005.07293_page0_fig0.jpg", "caption": "Figure 1: Notion of equality in fairness is depicted and formalized along with our newly formalized notion of equity.", "figure_type": "diagram", "visual_anchor": "Right panel 'Equity' showing yellow figure standing on tallest brown box", "text_evidence": "Equity is the distribution of resources among groups to overcome obstacles and to raise their opportunities for access [Schement, 2001]. Thus, historically disadvantaged groups are compensated, and others get their fair share to reach their goals.", "query_type": "visual_definition"}
{"query_id": "l1_2005.07293_2005.07293_fig_1_906", "query": "Why does the right panel's formalization include both p(Ŷ|A=blue) and p(Y|A=blue) terms unlike the left panel's single term?", "answer": "The right panel formalizes equity by learning existing biases from historical data p(Y|A) and compensating for them in model predictions p(Ŷ|A), whereas equality only considers predictions without accounting for pre-existing biases.", "doc_id": "2005.07293", "figure_id": "2005.07293_fig_1", "figure_number": 1, "image_path": "data/mineru_output/2005.07293/2005.07293/hybrid_auto/images/2005.07293_page0_fig0.jpg", "caption": "Figure 1: Notion of equality in fairness is depicted and formalized along with our newly formalized notion of equity.", "figure_type": "diagram", "visual_anchor": "Mathematical notation showing p(Ŷ|A=blue) + p(Y|A=blue) under 'Equity' versus p(Ŷ|A=blue) under 'Equality'", "text_evidence": "To operationalize equity, we learn the existing biases from the historical data and compensate for them in the predictions generated by the model.", "query_type": "comparison_explanation"}
{"query_id": "l1_2005.07293_2005.07293_fig_1_907", "query": "Does the equal-height box arrangement in the left panel address the pre-existing biases that the proposed definition aims to account for?", "answer": "No, the equal-height boxes represent equality that does not consider existing biases in the data, whereas the proposed equity definition attempts to make equitable decisions accounting for these pre-existing biases.", "doc_id": "2005.07293", "figure_id": "2005.07293_fig_1", "figure_number": 1, "image_path": "data/mineru_output/2005.07293/2005.07293/hybrid_auto/images/2005.07293_page0_fig0.jpg", "caption": "Figure 1: Notion of equality in fairness is depicted and formalized along with our newly formalized notion of equity.", "figure_type": "diagram", "visual_anchor": "Left panel 'Equality' showing all three figures standing on identical-height brown boxes", "text_evidence": "We propose a new fairness definition, motivated by the principle of equity, that considers existing biases in the data and attempts to make equitable decisions that account for these pre", "query_type": "value_context"}
{"query_id": "l1_2005.07293_2005.07293_fig_3_908", "query": "Why does the blue Equity line drop from 85% to 79% as beta increases, while Parity and Classifier remain stable?", "answer": "The Equity method trades off accuracy for fairness as beta increases, while Parity and Classifier maintain accuracy by not effectively reducing disparities among demographics, as measured by fairness gain comparing historical and future predictive outcomes.", "doc_id": "2005.07293", "figure_id": "2005.07293_fig_3", "figure_number": 2, "image_path": "data/mineru_output/2005.07293/2005.07293/hybrid_auto/images/2005.07293_page0_fig2.jpg", "caption": "Figure 2: Accuracy and fairness gain results for the COMPAS and Adult datasets over different $\\beta$ values. Top plots report the accuracy results, while bottom plots report the fairness gain results. Each point on the plots is the average value of 10 experiments performed on the 10 random splits. Notice that the 10 random split sets are the same across different $\\beta$ values. For details of these values along with standard deviation numbers refer to Tables 7 and 8 in the Appendixes section.", "figure_type": "plot", "visual_anchor": "Blue line with square markers declining from approximately 85% at beta=0.1 to 79% at beta=0.9", "text_evidence": "we measure how effective a method was in reducing disparities among demographics compared to a classifier with no fairness constraint. Note how this measure is similar to the statistical parity except considering both the history and future predictive outcome.", "query_type": "comparison_explanation"}
{"query_id": "l1_2005.07293_2005.07293_fig_3_909", "query": "Do the near-flat red and green lines at 84% across all beta values support consistent performance across random splits?", "answer": "Yes, the flat Parity and Classifier lines demonstrate consistent performance. Each point represents the average value of 10 experiments performed on the same 10 random splits across different beta values, ensuring robustness.", "doc_id": "2005.07293", "figure_id": "2005.07293_fig_3", "figure_number": 2, "image_path": "data/mineru_output/2005.07293/2005.07293/hybrid_auto/images/2005.07293_page0_fig2.jpg", "caption": "Figure 2: Accuracy and fairness gain results for the COMPAS and Adult datasets over different $\\beta$ values. Top plots report the accuracy results, while bottom plots report the fairness gain results. Each point on the plots is the average value of 10 experiments performed on the 10 random splits. Notice that the 10 random split sets are the same across different $\\beta$ values. For details of these values along with standard deviation numbers refer to Tables 7 and 8 in the Appendixes section.", "figure_type": "plot", "visual_anchor": "Red line with X markers and green line with triangle markers both maintaining approximately 84-85% accuracy from beta=0 to beta=0.9", "text_evidence": "Each point on the plots is the average value of 10 experiments performed on the 10 random splits. Notice that the 10 random split sets are the same across different beta values.", "query_type": "value_context"}
{"query_id": "l1_2005.07293_2005.07293_fig_3_910", "query": "What explains the blue Equity curve's steeper decline after beta=0.4 compared to the gradual drop before that threshold?", "answer": "The steeper decline after beta=0.4 indicates increasing fairness constraint weight, causing more aggressive accuracy sacrifice. Robustness across 10 random train, validation, and test splits ensures the significance of this observed trend.", "doc_id": "2005.07293", "figure_id": "2005.07293_fig_3", "figure_number": 2, "image_path": "data/mineru_output/2005.07293/2005.07293/hybrid_auto/images/2005.07293_page0_fig2.jpg", "caption": "Figure 2: Accuracy and fairness gain results for the COMPAS and Adult datasets over different $\\beta$ values. Top plots report the accuracy results, while bottom plots report the fairness gain results. Each point on the plots is the average value of 10 experiments performed on the 10 random splits. Notice that the 10 random split sets are the same across different $\\beta$ values. For details of these values along with standard deviation numbers refer to Tables 7 and 8 in the Appendixes section.", "figure_type": "plot", "visual_anchor": "Blue line showing gradual decline from beta=0 to 0.4 (85% to 82%), then steeper descent from beta=0.4 to 0.9 (82% to 79%)", "text_evidence": "For robustness purposes, each of our experiments were performed on 10 random train, validation, and test splits for each of the datasets and the significance of our hypotheses were reported accordingly.", "query_type": "anomaly_cause"}
{"query_id": "l1_2005.07293_2005.07293_fig_9_911", "query": "Why does the blue Equity line in Adult dataset decline more steeply than the red Parity line across iterations?", "answer": "The steeper decline of Equity demonstrates its superior effectiveness in reducing bias over iterations, as the feedback loop simulation reveals Equity outperforms Parity in bias reduction.", "doc_id": "2005.07293", "figure_id": "2005.07293_fig_9", "figure_number": 3, "image_path": "data/mineru_output/2005.07293/2005.07293/hybrid_auto/images/2005.07293_page0_fig8.jpg", "caption": "Figure 3: Simulation of the feedback loop phenomenon and results obtained in reduction of bias via different methods in COMPAS and Adult datasets. As expected higher $\\beta$ values result in reduction of more bias in the two fairness based objectives (Equity and Parity). It also shows how Equity is more effective in reducing the bias over iterations. Each point on the plots is the average value of 10 experiments performed on the 10 random splits. Notice that the 10 random split sets are the same across different $\\beta$ values.", "figure_type": "plot", "visual_anchor": "Blue Equity line showing steeper decline compared to red Parity line from iteration 0 to 9", "text_evidence": "As expected higher $\\beta$ values result in reduction of more bias in the two fairness based objectives (Equity and Parity). It also shows how Equity is more effective in reducing the bias over iterations.", "query_type": "comparison_explanation"}
{"query_id": "l1_2005.07293_2005.07293_fig_9_912", "query": "Do the averaged points on the Adult plot confirm consistent results across the ten random split experiments conducted?", "answer": "Yes, each point represents the average of 10 experiments performed on 10 random splits, ensuring consistency. The same random split sets were used across different beta values for fair comparison.", "doc_id": "2005.07293", "figure_id": "2005.07293_fig_9", "figure_number": 3, "image_path": "data/mineru_output/2005.07293/2005.07293/hybrid_auto/images/2005.07293_page0_fig8.jpg", "caption": "Figure 3: Simulation of the feedback loop phenomenon and results obtained in reduction of bias via different methods in COMPAS and Adult datasets. As expected higher $\\beta$ values result in reduction of more bias in the two fairness based objectives (Equity and Parity). It also shows how Equity is more effective in reducing the bias over iterations. Each point on the plots is the average value of 10 experiments performed on the 10 random splits. Notice that the 10 random split sets are the same across different $\\beta$ values.", "figure_type": "plot", "visual_anchor": "Data points on Adult plot showing smooth declining trends from iteration 0 to 9", "text_evidence": "Each point on the plots is the average value of 10 experiments performed on the 10 random splits. Notice that the 10 random split sets are the same across different $\\beta$ values.", "query_type": "value_context"}
{"query_id": "l1_2005.07293_2005.07293_fig_9_913", "query": "Does the green Classifier line starting near 20% bias validate the feedback loop phenomenon simulation in Adult dataset?", "answer": "Yes, the Classifier line starting at approximately 20% bias and declining through iterations demonstrates the simulated feedback loop phenomenon, showing how bias reduces over time even without explicit fairness objectives.", "doc_id": "2005.07293", "figure_id": "2005.07293_fig_9", "figure_number": 3, "image_path": "data/mineru_output/2005.07293/2005.07293/hybrid_auto/images/2005.07293_page0_fig8.jpg", "caption": "Figure 3: Simulation of the feedback loop phenomenon and results obtained in reduction of bias via different methods in COMPAS and Adult datasets. As expected higher $\\beta$ values result in reduction of more bias in the two fairness based objectives (Equity and Parity). It also shows how Equity is more effective in reducing the bias over iterations. Each point on the plots is the average value of 10 experiments performed on the 10 random splits. Notice that the 10 random split sets are the same across different $\\beta$ values.", "figure_type": "plot", "visual_anchor": "Green Classifier line beginning at approximately 20% bias at iteration 0 in Adult plot", "text_evidence": "Simulation of the feedback loop phenomenon and results obtained in reduction of bias via different methods in COMPAS and Adult datasets.", "query_type": "visual_definition"}
{"query_id": "l1_2005.07293_2005.07293_fig_13_914", "query": "Why does the orange Equity distribution show higher median ratings than the blue Parity distribution in Scenario 3?", "answer": "The Mann-Whitney U test demonstrates that Equity is significantly more effective at reducing bias in the feedback loop compared to Parity across different iterations, which explains the higher human fairness ratings for Equity.", "doc_id": "2005.07293", "figure_id": "2005.07293_fig_13", "figure_number": 4, "image_path": "data/mineru_output/2005.07293/2005.07293/hybrid_auto/images/2005.07293_page0_fig12.jpg", "caption": "Figure 4: Human ratings of equity and parity notions of fairness in different scenarios.", "figure_type": "plot", "visual_anchor": "Orange violin plot (Equity) with higher median line compared to blue violin plot (Parity)", "text_evidence": "Table 4 shows the significance of these results for COM-PAS and Adult datasets for β value of 0.5 for different iterations supporting our hypothesis.", "query_type": "comparison_explanation"}
{"query_id": "l1_2005.07293_2005.07293_fig_13_915", "query": "Does the Equity violin's concentration near rating 4 align with its performance superiority over Parity and Classifier losses?", "answer": "Yes, the orange Equity violin shows higher ratings clustered near 4 (Completely Fair), which is consistent with Table 4's Mann-Whitney U test results demonstrating Equity's effectiveness in reducing bias compared to both Parity and Classifier losses.", "doc_id": "2005.07293", "figure_id": "2005.07293_fig_13", "figure_number": 4, "image_path": "data/mineru_output/2005.07293/2005.07293/hybrid_auto/images/2005.07293_page0_fig12.jpg", "caption": "Figure 4: Human ratings of equity and parity notions of fairness in different scenarios.", "figure_type": "plot", "visual_anchor": "Orange violin plot (Equity) showing distribution concentrated at upper end near rating 4", "text_evidence": "Table 4: Performance of Mann-Whitney U test for showing the effectiveness of Equity in reducing bias in the feedback loop compared to Parity and Classifier losses over different iterations for COMPAS and Adult datasets.", "query_type": "value_context"}
{"query_id": "l1_2005.07293_2005.07293_fig_13_916", "query": "How do the blue Parity ratings near 2 relate to the earlier finding about beta value zero?", "answer": "The lower Parity ratings (blue violin centered around 2-3) reflect weaker fairness perception. This is consistent with findings that β = 0 (representing no equity consideration) produces different fairness outcomes than incorporating equity measures.", "doc_id": "2005.07293", "figure_id": "2005.07293_fig_13", "figure_number": 4, "image_path": "data/mineru_output/2005.07293/2005.07293/hybrid_auto/images/2005.07293_page0_fig12.jpg", "caption": "Figure 4: Human ratings of equity and parity notions of fairness in different scenarios.", "figure_type": "plot", "visual_anchor": "Blue violin plot (Parity) with median and distribution centered around rating 2-3", "text_evidence": "This is consistent with our earlier finding that β = 0 .", "query_type": "value_context"}
{"query_id": "l1_2103.11320_2103.11320_fig_2_917", "query": "Does the Profession category's blue box spanning roughly 2-17% demonstrate the large variation in negative triples for groups?", "answer": "Yes, the Profession category shows the largest span among blue boxes, illustrating substantial disparity in negative triple counts. This visual evidence directly supports the observation about large variation within the same category.", "doc_id": "2103.11320", "figure_id": "2103.11320_fig_2", "figure_number": 1, "image_path": "data/mineru_output/2103.11320/2103.11320/hybrid_auto/images/2103.11320_page0_fig1.jpg", "caption": "Figure 1: Negative and positive regard and sentiment results from ConceptNet and GenericsKB. We find outlier target groups with high regard and sentiment percentages that show the severity of overgeneralization issues. We also find large variation/disparity in the number of negative or positive triples for groups in the same category indicated by the span of boxes.", "figure_type": "plot", "visual_anchor": "Blue box for Profession category with span from approximately 2% to 17%", "text_evidence": "We also find large variation/disparity in the number of negative or positive triples for groups in the same category indicated by the span of boxes.", "query_type": "comparison_explanation"}
{"query_id": "l1_2103.11320_2103.11320_fig_2_918", "query": "Why do the red outlier points above 20% in Origin and Profession categories illustrate overgeneralization severity?", "answer": "These outlier points represent target groups with exceptionally high positive sentiment percentages, demonstrating extreme cases of biased generalizations. The outliers directly show how certain groups receive disproportionately positive characterizations in the knowledge bases.", "doc_id": "2103.11320", "figure_id": "2103.11320_fig_2", "figure_number": 1, "image_path": "data/mineru_output/2103.11320/2103.11320/hybrid_auto/images/2103.11320_page0_fig1.jpg", "caption": "Figure 1: Negative and positive regard and sentiment results from ConceptNet and GenericsKB. We find outlier target groups with high regard and sentiment percentages that show the severity of overgeneralization issues. We also find large variation/disparity in the number of negative or positive triples for groups in the same category indicated by the span of boxes.", "figure_type": "plot", "visual_anchor": "Red outlier asterisk points above 20% in Origin and Profession categories", "text_evidence": "We find outlier target groups with high regard and sentiment percentages that show the severity of overgeneralization issues.", "query_type": "anomaly_cause"}
{"query_id": "l1_2103.11320_2103.11320_fig_2_919", "query": "How does the median positive sentiment around 5-7% for Religion relate to examining whether targets are perceived positively?", "answer": "The Religion category's red box shows a median around 5-7%, indicating that religious groups receive relatively low positive sentiment in statements. This quantifies how targets like religious identities are perceived in the overgeneralization analysis covering stereotyping and favoritism.", "doc_id": "2103.11320", "figure_id": "2103.11320_fig_2", "figure_number": 1, "image_path": "data/mineru_output/2103.11320/2103.11320/hybrid_auto/images/2103.11320_page0_fig1.jpg", "caption": "Figure 1: Negative and positive regard and sentiment results from ConceptNet and GenericsKB. We find outlier target groups with high regard and sentiment percentages that show the severity of overgeneralization issues. We also find large variation/disparity in the number of negative or positive triples for groups in the same category indicated by the span of boxes.", "figure_type": "plot", "visual_anchor": "Red box for Religion category with median approximately 5-7%", "text_evidence": "We consider overgeneralization that directly examines whether targets such as \"lawyer\" or \"lady\" are perceived positively or negatively in the statements (examples in Table 1), covering categories including stereotyping, denigration, and favoritism.", "query_type": "value_context"}
{"query_id": "l1_2103.11320_2103.11320_fig_5_920", "query": "Does the purple prejudice region placement of electrician and hairdresser align with polarized perceptions found in GenericsKB?", "answer": "Yes, the figure shows electrician and hairdresser in the prejudice region (low positive, low negative regard), consistent with polarized perceptions observed across both ConceptNet and GenericsKB for certain groups.", "doc_id": "2103.11320", "figure_id": "2103.11320_fig_5", "figure_number": 2, "image_path": "data/mineru_output/2103.11320/2103.11320/hybrid_auto/images/2103.11320_page0_fig4.jpg", "caption": "Figure 2: Examples of targets from the “Profession” and “Religion” categories from Nadeem et al. (2020) labeled by the regard measure. Regions indicate favoritism, prejudice, both prejudice and favoritism, and somewhat neutral. Higher negative regard percentages indicate prejudice-leaning and higher positive regard percentages indicate favoritism-leaning. We also compare ConceptNet (Speer et al., 2017) and GenericsKB (Bhakthavatsalam et al., 2020) on the “Religion” category and find similar polarized perceptions of certain groups, despite a much larger percentage range for GenericsKB.", "figure_type": "plot", "visual_anchor": "Purple prejudice region in bottom-left containing electrician and hairdresser labels", "text_evidence": "We also compare ConceptNet (Speer et al., 2017) and GenericsKB (Bhakthavatsalam et al., 2020) on the 'Religion' category and find similar polarized perceptions of certain groups", "query_type": "comparison_explanation"}
{"query_id": "l1_2103.11320_2103.11320_fig_5_921", "query": "Why does eco appear in the cyan favoritism region with approximately 12% positive regard despite low negative regard?", "answer": "Eco shows high positive regard with minimal negative regard, placing it in the favoritism-leaning region. Higher positive regard percentages indicate favoritism-leaning targets according to the regard measure labeling system.", "doc_id": "2103.11320", "figure_id": "2103.11320_fig_5", "figure_number": 2, "image_path": "data/mineru_output/2103.11320/2103.11320/hybrid_auto/images/2103.11320_page0_fig4.jpg", "caption": "Figure 2: Examples of targets from the “Profession” and “Religion” categories from Nadeem et al. (2020) labeled by the regard measure. Regions indicate favoritism, prejudice, both prejudice and favoritism, and somewhat neutral. Higher negative regard percentages indicate prejudice-leaning and higher positive regard percentages indicate favoritism-leaning. We also compare ConceptNet (Speer et al., 2017) and GenericsKB (Bhakthavatsalam et al., 2020) on the “Religion” category and find similar polarized perceptions of certain groups, despite a much larger percentage range for GenericsKB.", "figure_type": "plot", "visual_anchor": "Cyan favoritism region containing eco label at approximately 12% positive regard and 2% negative regard", "text_evidence": "Higher negative regard percentages indicate prejudice-leaning and higher positive regard percentages indicate favoritism-leaning", "query_type": "value_context"}
{"query_id": "l1_2103.11320_2103.11320_fig_5_922", "query": "How does the magenta Both region placement of politician relate to targets showing simultaneous favoritism and prejudice?", "answer": "Politician appears in the magenta Both region at high negative and positive regard percentages, indicating it receives both prejudice and favoritism. This region specifically captures targets from the Profession and Religion categories that exhibit dual biased perceptions.", "doc_id": "2103.11320", "figure_id": "2103.11320_fig_5", "figure_number": 2, "image_path": "data/mineru_output/2103.11320/2103.11320/hybrid_auto/images/2103.11320_page0_fig4.jpg", "caption": "Figure 2: Examples of targets from the “Profession” and “Religion” categories from Nadeem et al. (2020) labeled by the regard measure. Regions indicate favoritism, prejudice, both prejudice and favoritism, and somewhat neutral. Higher negative regard percentages indicate prejudice-leaning and higher positive regard percentages indicate favoritism-leaning. We also compare ConceptNet (Speer et al., 2017) and GenericsKB (Bhakthavatsalam et al., 2020) on the “Religion” category and find similar polarized perceptions of certain groups, despite a much larger percentage range for GenericsKB.", "figure_type": "plot", "visual_anchor": "Magenta Both region in bottom-right corner containing politician label at approximately 18% negative and 1% positive regard", "text_evidence": "Examples of targets from the 'Profession' and 'Religion' categories from Nadeem et al. (2020) labeled by the regard measure. Regions indicate favoritism, prejudice, both prejudice and favoritism, and somewhat neutral", "query_type": "visual_definition"}
{"query_id": "l1_2103.11320_2103.11320_fig_9_923", "query": "Why do both purple and blue sentiment lines in the Gender panel peak above 5%, demonstrating overgeneralization toward both target groups?", "answer": "The Gender category demonstrates extreme overgeneralization where both target groups (woman and man) show elevated sentiment associations, as evidenced by the overlapping distributions and accompanying biased concept associations like 'Slut is related to woman' and 'Man is related to integrity.'", "doc_id": "2103.11320", "figure_id": "2103.11320_fig_9", "figure_number": 3, "image_path": "data/mineru_output/2103.11320/2103.11320/hybrid_auto/images/2103.11320_page0_fig8.jpg", "caption": "Figure 3: Four different representations from four categories each demonstrating a certain aspect of bias. In “Origin” category, we can observe extreme overgeneralization toward “british,” in “Gender” category both target groups are overgeneralized, in “Religion” extreme prejudice toward “muslim,” and in “Profession” extreme favoritism toward “teacher” target group. Each case is accompanied with an example of negative and positive associations detected by sentiment.", "figure_type": "plot", "visual_anchor": "Purple (negative) and blue (positive) sentiment lines both peaking above 5% in Gender subplot", "text_evidence": "In \"Gender\" category both target groups are overgeneralized, in \"Religion\" extreme prejudice toward \"muslim,\" and in \"Profession\" extreme favoritism toward \"teacher\" target group.", "query_type": "comparison_explanation"}
{"query_id": "l1_2103.11320_2103.11320_fig_9_924", "query": "Does the ConceptNet Sentiment bar chart showing approximately 2.0 for woman and 2.5 for man support the favoritism toward teacher observed?", "answer": "No, the ConceptNet Sentiment bar chart compares gender bias (woman vs. man), not profession bias. The favoritism toward 'teacher' is demonstrated separately in the Profession category panel, showing how sentiment analysis can detect different types of bias across categories.", "doc_id": "2103.11320", "figure_id": "2103.11320_fig_9", "figure_number": 3, "image_path": "data/mineru_output/2103.11320/2103.11320/hybrid_auto/images/2103.11320_page0_fig8.jpg", "caption": "Figure 3: Four different representations from four categories each demonstrating a certain aspect of bias. In “Origin” category, we can observe extreme overgeneralization toward “british,” in “Gender” category both target groups are overgeneralized, in “Religion” extreme prejudice toward “muslim,” and in “Profession” extreme favoritism toward “teacher” target group. Each case is accompanied with an example of negative and positive associations detected by sentiment.", "figure_type": "plot", "visual_anchor": "ConceptNet Sentiment bar chart with red and blue bars at approximately 2.0-2.5 height", "text_evidence": "Each case is accompanied with an example of negative and positive associations detected by sentiment. Saffron terror is related to hindu. Teacher causes the desire to study.", "query_type": "value_context"}
{"query_id": "l1_2103.11320_2103.11320_fig_9_925", "query": "How does the extreme prejudice toward muslim relate to the overgeneralization severity shown in the Gender panel's overlapping distributions?", "answer": "Both demonstrate severe bias manifestations: extreme prejudice shows concentrated negative sentiment toward one group (muslim), while overgeneralization in the Gender panel shows both target groups receiving elevated sentiment associations, illustrating different aspects of how bias appears in word embeddings.", "doc_id": "2103.11320", "figure_id": "2103.11320_fig_9", "figure_number": 3, "image_path": "data/mineru_output/2103.11320/2103.11320/hybrid_auto/images/2103.11320_page0_fig8.jpg", "caption": "Figure 3: Four different representations from four categories each demonstrating a certain aspect of bias. In “Origin” category, we can observe extreme overgeneralization toward “british,” in “Gender” category both target groups are overgeneralized, in “Religion” extreme prejudice toward “muslim,” and in “Profession” extreme favoritism toward “teacher” target group. Each case is accompanied with an example of negative and positive associations detected by sentiment.", "figure_type": "plot", "visual_anchor": "Gender panel showing overlapping purple and blue sentiment distributions with peaks above 5%", "text_evidence": "Severity of Overgeneralization Figure 3 further demonstrates how severe the problem of overgeneralization can be, along with some concrete examples.", "query_type": "comparison_explanation"}
{"query_id": "l1_2103.11320_2103.11320_fig_13_926", "query": "Why does the Gender box plot show outliers near 6000 when GenericsKB has much higher variance than ConceptNet?", "answer": "The outliers near 6000 in the Gender category reflect the much higher variance in GenericsKB compared to ConceptNet, demonstrating severe representation disparities between the two knowledge bases.", "doc_id": "2103.11320", "figure_id": "2103.11320_fig_13", "figure_number": 4, "image_path": "data/mineru_output/2103.11320/2103.11320/hybrid_auto/images/2103.11320_page0_fig12.jpg", "caption": "Figure 4: Box plots demonstrating the representation disparity in terms of number of triples/sentences for “Gender” and “Profession” categories from Concept-Net and GenericsKB. We find similarly severe disparities in two KBs with the number of sentences ranging much more for GenericsKB compared to ConceptNet.", "figure_type": "plot", "visual_anchor": "outlier points near 6000 on the Gender box plot", "text_evidence": "We find that GenericsKB has much higher variance compared to ConceptNet.", "query_type": "anomaly_cause"}
{"query_id": "l1_2103.11320_2103.11320_fig_13_927", "query": "Do the narrower ConceptNet boxes compared to wider GenericsKB boxes support the claim about sentence count range differences?", "answer": "Yes, the narrower ConceptNet boxes versus wider GenericsKB boxes visually confirm that the number of sentences ranges much more for GenericsKB compared to ConceptNet, as stated in the disparity analysis.", "doc_id": "2103.11320", "figure_id": "2103.11320_fig_13", "figure_number": 4, "image_path": "data/mineru_output/2103.11320/2103.11320/hybrid_auto/images/2103.11320_page0_fig12.jpg", "caption": "Figure 4: Box plots demonstrating the representation disparity in terms of number of triples/sentences for “Gender” and “Profession” categories from Concept-Net and GenericsKB. We find similarly severe disparities in two KBs with the number of sentences ranging much more for GenericsKB compared to ConceptNet.", "figure_type": "plot", "visual_anchor": "narrower boxes for ConceptNet versus wider boxes for GenericsKB", "text_evidence": "We find similarly severe disparities in two KBs with the number of sentences ranging much more for GenericsKB compared to ConceptNet.", "query_type": "comparison_explanation"}
{"query_id": "l1_2103.11320_2103.11320_fig_13_928", "query": "How do the Profession category boxes illustrate the overgeneralization disparity measured using Equation 4 across different targets?", "answer": "The Profession category boxes show varying spreads between ConceptNet and GenericsKB, visually demonstrating the disparities amongst targets in terms of overgeneralization, with GenericsKB exhibiting greater variation in representation.", "doc_id": "2103.11320", "figure_id": "2103.11320_fig_13", "figure_number": 4, "image_path": "data/mineru_output/2103.11320/2103.11320/hybrid_auto/images/2103.11320_page0_fig12.jpg", "caption": "Figure 4: Box plots demonstrating the representation disparity in terms of number of triples/sentences for “Gender” and “Profession” categories from Concept-Net and GenericsKB. We find similarly severe disparities in two KBs with the number of sentences ranging much more for GenericsKB compared to ConceptNet.", "figure_type": "plot", "visual_anchor": "Profession category box plots showing different spreads", "text_evidence": "We further analyze the disparities amongst targets in terms of overgeneralization (favoritism and prejudice perceptions measured by sentiment and regard) using Eq. (4), shown in Table 4.", "query_type": "value_context"}
{"query_id": "l1_2103.11320_2103.11320_fig_17_929", "query": "Do the red and blue box plot variances across Origin, Gender, Religion, and Profession demonstrate disparity in overgeneralization?", "answer": "Yes, the variance in both regard measures shown by the different box heights and whisker lengths across categories indicates disparity in overgeneralization, as COMeT produces inconsistent sentiment patterns across demographic groups.", "doc_id": "2103.11320", "figure_id": "2103.11320_fig_17", "figure_number": 5, "image_path": "data/mineru_output/2103.11320/2103.11320/hybrid_auto/images/2103.11320_page0_fig16.jpg", "caption": "Figure 5: Negative and positive sentiment and regard results from COMeT and CSG.", "figure_type": "plot", "visual_anchor": "Box plot variance shown by different heights and whisker lengths in red (positive) and blue (negative) boxes across all four categories", "text_evidence": "For instance, the results from COMeT shown in Figure 5 demonstrate the fact that variances exist in both regard and sentiment measures which is an indication of disparity in overgeneralization.", "query_type": "comparison_explanation"}
{"query_id": "l1_2103.11320_2103.11320_fig_17_930", "query": "Why does the filtering technique target the variance patterns visible in the red and blue boxes for mitigation?", "answer": "The filtering technique aims to reduce representational harms by addressing the disparate variance patterns in sentiment and regard across demographic categories, which reflect biased overgeneralization from ConceptNet propagating through COMeT.", "doc_id": "2103.11320", "figure_id": "2103.11320_fig_17", "figure_number": 5, "image_path": "data/mineru_output/2103.11320/2103.11320/hybrid_auto/images/2103.11320_page0_fig16.jpg", "caption": "Figure 5: Negative and positive sentiment and regard results from COMeT and CSG.", "figure_type": "plot", "visual_anchor": "Variance patterns in red positive and blue negative boxes across Origin, Gender, Religion, and Profession categories", "text_evidence": "To mitigate the observed representational harms in ConceptNet and their effects on downstream tasks, we propose a pre-processing data filtering technique that reduces the effect of existing representational harms in ConceptNet.", "query_type": "comparison_explanation"}
{"query_id": "l1_2103.11320_2103.11320_fig_17_931", "query": "Does the higher median in red boxes compared to blue across all categories confirm COMeT's effectiveness for the case study?", "answer": "The higher positive regard medians demonstrate COMeT's behavior patterns, making it an appropriate case study for testing the filtering mitigation technique on CSKB completion tasks with observable bias effects.", "doc_id": "2103.11320", "figure_id": "2103.11320_fig_17", "figure_number": 5, "image_path": "data/mineru_output/2103.11320/2103.11320/hybrid_auto/images/2103.11320_page0_fig16.jpg", "caption": "Figure 5: Negative and positive sentiment and regard results from COMeT and CSG.", "figure_type": "plot", "visual_anchor": "Red boxes showing consistently higher median values (around 15-25%) compared to blue boxes (around 3-7%) across all four demographic categories", "text_evidence": "We apply our mitigation technique on COMeT as a case study. Table 7: Mitigation results of the filtering technique (COMeT-Filtered) compared to standard COMeT. COMeT-Filtered is effective at reducing overgeneralization", "query_type": "value_context"}
{"query_id": "l1_2103.11320_2103.11320_fig_20_932", "query": "Why does the CapableOf relation linking priests to sexual fun exemplify allocative versus representational harms?", "answer": "The relation demonstrates representational harm by encoding harmful stereotypes about religious figures in machine learning systems. Barocas et al. distinguish this from allocative harms that affect resource distribution.", "doc_id": "2103.11320", "figure_id": "2103.11320_fig_20", "figure_number": 6, "image_path": "data/mineru_output/2103.11320/2103.11320/hybrid_auto/images/2103.11320_page0_fig19.jpg", "caption": "Figure 6: Examples from ConceptNet.   ", "figure_type": "example", "visual_anchor": "CapableOf relation between 'A priest' and 'guess that sex is fun' with weight 1.0", "text_evidence": "Solon Barocas, Kate Crawford, Aaron Shapiro, and Hanna Wallach. 2017. The problem with bias: Allocative versus representational harms in machine learning.", "query_type": "value_context"}
{"query_id": "l1_2103.11320_2103.11320_fig_20_933", "query": "How does the AtLocation relation placing child molesters in churches contrast with the generic knowledge approach?", "answer": "The relation encodes harmful stereotypical associations rather than generic statements. GenericKB aims to capture broad, non-biased knowledge, unlike this specific harmful ConceptNet example.", "doc_id": "2103.11320", "figure_id": "2103.11320_fig_20", "figure_number": 6, "image_path": "data/mineru_output/2103.11320/2103.11320/hybrid_auto/images/2103.11320_page0_fig19.jpg", "caption": "Figure 6: Examples from ConceptNet.   ", "figure_type": "example", "visual_anchor": "AtLocation relation between 'child molesters' and 'church' with weight 1.0", "text_evidence": "Sumithra Bhakthavatsalam, Chloe Anastasiades, and Peter Clark. 2020. Genericskb: A knowledge base of generic statements. arXiv preprint arXiv:2005.00660.", "query_type": "comparison_explanation"}
{"query_id": "l1_2103.11320_2103.11320_fig_20_934", "query": "What causes these five ConceptNet examples to require the mitigation framework addressing language technology bias?", "answer": "These examples encode harmful stereotypes about priests, religious institutions, nationalities, elderly people, and politicians. Blodgett et al. address how language technology perpetuates such biases, necessitating filtering frameworks.", "doc_id": "2103.11320", "figure_id": "2103.11320_fig_20", "figure_number": 6, "image_path": "data/mineru_output/2103.11320/2103.11320/hybrid_auto/images/2103.11320_page0_fig19.jpg", "caption": "Figure 6: Examples from ConceptNet.   ", "figure_type": "example", "visual_anchor": "All five relations with weight 1.0 showing stereotypical associations", "text_evidence": "Su Lin Blodgett, Solon Barocas, Hal Daumé III, and Hanna Wallach. 2020. Language (technolo", "query_type": "anomaly_cause"}
{"query_id": "l1_2103.11320_2103.11320_fig_23_935", "query": "Why does girlfriend at 20% negative regard fall in negligible bias rather than prejudice or favoritism regions?", "answer": "The negligible bias region (purple background) encompasses moderate levels of both positive and negative regard. Girlfriend's position at 20% negative and 20% positive regard places it within this balanced zone, indicating neither strong prejudice nor favoritism.", "doc_id": "2103.11320", "figure_id": "2103.11320_fig_23", "figure_number": 8, "image_path": "data/mineru_output/2103.11320/2103.11320/hybrid_auto/images/2103.11320_page0_fig22.jpg", "caption": "Figure 8: Examples of targets and the regions they fall under within each category considering the regard measure. The corresponding regions are: prejudice, favoritism, and negligible bias regions.", "figure_type": "plot", "visual_anchor": "girlfriend data point at approximately 20% negative regard and 20% positive regard within purple shaded region", "text_evidence": "Examples of targets and the regions they fall under within each category considering the regard measure. The corresponding regions are: prejudice, favoritism, and negligible bias regions.", "query_type": "value_context"}
{"query_id": "l1_2103.11320_2103.11320_fig_23_936", "query": "How does woman's position near 50% negative regard demonstrate the distinction between regard and sentiment label agreement?", "answer": "Woman's placement at approximately 50% negative regard and 7% positive regard shows high negative bias. This spatial positioning illustrates how regard measures capture bias patterns that can be compared against human labels for agreement assessment.", "doc_id": "2103.11320", "figure_id": "2103.11320_fig_23", "figure_number": 8, "image_path": "data/mineru_output/2103.11320/2103.11320/hybrid_auto/images/2103.11320_page0_fig22.jpg", "caption": "Figure 8: Examples of targets and the regions they fall under within each category considering the regard measure. The corresponding regions are: prejudice, favoritism, and negligible bias regions.", "figure_type": "plot", "visual_anchor": "woman data point at approximately 50% negative regard and 7% positive regard", "text_evidence": "Percentages represent how much regard and sentiment labels ran on COMeT and COMeT-Filtered triples agree with labels coming from humans.", "query_type": "comparison_explanation"}
{"query_id": "l1_2103.11320_2103.11320_fig_23_937", "query": "Does male's lowest positive regard at 2% compared to man's 7% reflect filtering effects on generated triples?", "answer": "Male shows approximately 2% positive regard while man shows 7%, both at high negative regard around 50%. This difference in regard distributions between similar gender terms could reflect how COMeT-Filtered versus unfiltered triples capture different bias patterns.", "doc_id": "2103.11320", "figure_id": "2103.11320_fig_23", "figure_number": 8, "image_path": "data/mineru_output/2103.11320/2103.11320/hybrid_auto/images/2103.11320_page0_fig22.jpg", "caption": "Figure 8: Examples of targets and the regions they fall under within each category considering the regard measure. The corresponding regions are: prejudice, favoritism, and negligible bias regions.", "figure_type": "plot", "visual_anchor": "male at ~50% negative/2% positive versus man at ~50% negative/7% positive", "text_evidence": "Percentages represent how much regard and sentiment labels ran on COMeT and COMeT-Filtered triples agree with labels coming from humans.", "query_type": "comparison_explanation"}
{"query_id": "l1_2103.11320_2103.11320_fig_26_938", "query": "Why does grandmother fall in the orange Prejudice region despite ConceptNet containing destructive triples for many targets?", "answer": "Grandmother's position at approximately 15% negative and 4% positive sentiment places it in the Prejudice region. The destructive triples in ConceptNet likely contribute to this negative bias pattern shown in the qualitative results.", "doc_id": "2103.11320", "figure_id": "2103.11320_fig_26", "figure_number": 9, "image_path": "data/mineru_output/2103.11320/2103.11320/hybrid_auto/images/2103.11320_page0_fig25.jpg", "caption": "Figure 9: Examples of targets and the regions they fall under within each category considering sentiment as a measure. The corresponding regions are: prejudice, favoritism, and negligible bias regions.   ", "figure_type": "plot", "visual_anchor": "grandmother at ~15% negative, ~4% positive in orange Prejudice region", "text_evidence": "For instance, in Table 9 we include more of qualitative results and demonstrate some destructive triples existing in ConceptNet.", "query_type": "anomaly_cause"}
{"query_id": "l1_2103.11320_2103.11320_fig_26_939", "query": "Do the purple Negligible Bias region targets like girlfriend and father demonstrate the sentiment-based categorization method described?", "answer": "Yes, girlfriend (~1% negative, ~60% positive) and father (~20% negative, ~22% positive) both fall within the purple Negligible Bias region, exemplifying how sentiment measures determine category placement across the three defined regions.", "doc_id": "2103.11320", "figure_id": "2103.11320_fig_26", "figure_number": 9, "image_path": "data/mineru_output/2103.11320/2103.11320/hybrid_auto/images/2103.11320_page0_fig25.jpg", "caption": "Figure 9: Examples of targets and the regions they fall under within each category considering sentiment as a measure. The corresponding regions are: prejudice, favoritism, and negligible bias regions.   ", "figure_type": "plot", "visual_anchor": "girlfriend and father in purple Negligible Bias region", "text_evidence": "Figure 9: Examples of targets and the regions they fall under within each category considering sentiment as a measure. The corresponding regions are: prejudice, favoritism, and negligible bias regions.", "query_type": "visual_definition"}
{"query_id": "l1_2103.11320_2103.11320_fig_26_940", "query": "How does woman's placement in orange Prejudice region at 35% negative sentiment compare to brother's green Favoritism region position?", "answer": "Woman shows significantly higher negative sentiment (~35%) with lower positive sentiment (~18%) placing it in the Prejudice region, while brother has minimal negative sentiment (~2%) with higher positive sentiment (~22%) in the Favoritism region, demonstrating contrasting bias patterns.", "doc_id": "2103.11320", "figure_id": "2103.11320_fig_26", "figure_number": 9, "image_path": "data/mineru_output/2103.11320/2103.11320/hybrid_auto/images/2103.11320_page0_fig25.jpg", "caption": "Figure 9: Examples of targets and the regions they fall under within each category considering sentiment as a measure. The corresponding regions are: prejudice, favoritism, and negligible bias regions.   ", "figure_type": "plot", "visual_anchor": "woman at ~35% negative in orange vs brother at ~2% negative in green", "text_evidence": "The corresponding regions are: prejudice, favoritism, and negligible bias regions.", "query_type": "comparison_explanation"}
{"query_id": "l1_2103.11320_2103.11320_fig_30_941", "query": "Does the Religion box plot's median near 200 triples support the claim about representation disparity across Origin and Religion categories?", "answer": "Yes, the box plot demonstrates the representation disparity in terms of number of triples for Religion categories from ConceptNet and GenericsKB, with the median around 200 but significant outliers above 1000.", "doc_id": "2103.11320", "figure_id": "2103.11320_fig_30", "figure_number": 11, "image_path": "data/mineru_output/2103.11320/2103.11320/hybrid_auto/images/2103.11320_page0_fig29.jpg", "caption": "Figure 11: Box plots demonstrating the representation disparity in terms of number of triples/sentences for Origin and Religion categories from ConceptNet and GenericsKB.", "figure_type": "plot", "visual_anchor": "Religion box plot median around 200 with outliers above 1000", "text_evidence": "Box plots demonstrating the representation disparity in terms of number of triples/sentences for Origin and Religion categories from ConceptNet and GenericsKB.", "query_type": "value_context"}
{"query_id": "l1_2103.11320_2103.11320_fig_30_942", "query": "Why does the inset chart show bible at approximately 800 triples versus quran at roughly 50, given both are religious categories?", "answer": "The disparity reflects unequal representation in the knowledge bases. The mitigation framework shown in Figure 7 and filtering technique results in Table 11 address such imbalances through COMet_Filtered comparisons.", "doc_id": "2103.11320", "figure_id": "2103.11320_fig_30", "figure_number": 11, "image_path": "data/mineru_output/2103.11320/2103.11320/hybrid_auto/images/2103.11320_page0_fig29.jpg", "caption": "Figure 11: Box plots demonstrating the representation disparity in terms of number of triples/sentences for Origin and Religion categories from ConceptNet and GenericsKB.", "figure_type": "plot", "visual_anchor": "Inset bar chart with blue bible bar ~800 and small quran bar ~50", "text_evidence": "In addition, we provide a visual for our mitigation framework in Figure 7 and detailed results of COMeT vs COMet_Filtered comparisons over different categories.", "query_type": "comparison_explanation"}
{"query_id": "l1_2103.11320_2103.11320_fig_30_943", "query": "What causes the Religion box plot outliers exceeding 1000 triples when the median remains below 200?", "answer": "Certain religious categories have dramatically more triples in ConceptNet and GenericsKB than others. The detailed mitigation results in Table 11 measure sentiment and regard across all categories to address these disparities.", "doc_id": "2103.11320", "figure_id": "2103.11320_fig_30", "figure_number": 11, "image_path": "data/mineru_output/2103.11320/2103.11320/hybrid_auto/images/2103.11320_page0_fig29.jpg", "caption": "Figure 11: Box plots demonstrating the representation disparity in terms of number of triples/sentences for Origin and Religion categories from ConceptNet and GenericsKB.", "figure_type": "plot", "visual_anchor": "Outlier points above 1000 on Religion box plot while median is near 200", "text_evidence": "Table 11 contains detailed results for the sentiment and regard measures over all the categories", "query_type": "anomaly_cause"}
{"query_id": "l1_2103.11320_2103.11320_fig_36_944", "query": "Why do words like offensive and bitch appear prominently when the targets are british and female?", "answer": "These phrases appear in triples with negative regard and sentiment labels for \"british\" and \"female\" targets, reflecting biased associations in the dataset.", "doc_id": "2103.11320", "figure_id": "2103.11320_fig_36", "figure_number": 12, "image_path": "data/mineru_output/2103.11320/2103.11320/hybrid_auto/images/2103.11320_page0_fig35.jpg", "caption": "Figure 12: Wordcloud of phrases that appear in triples with negative regard and sentiment labels for “british” and “female” targets.", "figure_type": "wordcloud", "visual_anchor": "offensive (large, top-left) and bitch (large, center)", "text_evidence": "Wordcloud of phrases that appear in triples with negative regard and sentiment labels for \"british\" and \"female\" targets.", "query_type": "value_context"}
{"query_id": "l1_2103.11320_2103.11320_fig_36_945", "query": "How does the prominence of femalefist and accuseress relate to gender-specific bias in the profession category?", "answer": "These gendered negative terms appear in triples with female targets, indicating bias. The profession category includes targets like barber, coach, and businessperson where such bias manifests.", "doc_id": "2103.11320", "figure_id": "2103.11320_fig_36", "figure_number": 12, "image_path": "data/mineru_output/2103.11320/2103.11320/hybrid_auto/images/2103.11320_page0_fig35.jpg", "caption": "Figure 12: Wordcloud of phrases that appear in triples with negative regard and sentiment labels for “british” and “female” targets.", "figure_type": "wordcloud", "visual_anchor": "femalefist (left-center) and accuseress (left)", "text_evidence": "Table 14: Targets from the profession category in our dataset (most of them borrowed from Nadeem et al 2020). We considered triples that these words appeared in them.", "query_type": "comparison_explanation"}
{"query_id": "l1_2103.11320_2103.11320_fig_36_946", "query": "Does the appearance of sex and impersonator support filtering efforts to improve neutral sentiment for gender category?", "answer": "Yes, these negative phrases justify the COMeT-Filtered model's improved neutral sentiment mean from 59.169 to higher values by removing such biased triples from the gender category.", "doc_id": "2103.11320", "figure_id": "2103.11320_fig_36", "figure_number": 12, "image_path": "data/mineru_output/2103.11320/2103.11320/hybrid_auto/images/2103.11320_page0_fig35.jpg", "caption": "Figure 12: Wordcloud of phrases that appear in triples with negative regard and sentiment labels for “british” and “female” targets.", "figure_type": "wordcloud", "visual_anchor": "sex (large, center-top) and impersonator (center-left)", "text_evidence": "COMeT 64.527 58.578 59.169 61.610 COMeT-Filtered 65.257 59.485", "query_type": "anomaly_cause"}
{"query_id": "l1_2109.03952_2109.03952_fig_1_947", "query": "Why do the red boxes e1 through em have dimension d^e before entering the orange Attention layer?", "answer": "Each feature value is treated as an individual entity and learns a fixed-size embedding vector in R^(d^e), similar to how words are embedded in text classification tasks.", "doc_id": "2109.03952", "figure_id": "2109.03952_fig_1", "figure_number": 1, "image_path": "data/mineru_output/2109.03952/2109.03952/hybrid_auto/images/2109.03952_page0_fig0.jpg", "caption": "Figure 1: (a) In general classification model, for each feature $f _ { k }$ a vector representation $e _ { k }$ of length $d ^ { e }$ is learned. This is passed to the attention layer which produces a $d ^ { e }$ - dimensional vector representation for the sample instance $i$ which is passed to two dense layers to get the final classification output. (b) The Attribution framework has the same architecture as the general model. One outcome is obtained through the original model and another through the model that has some attention weights zeroed. The observed difference in accuracy and fairness measures will indicate the effect of the zeroed out features on accuracy and fairness.", "figure_type": "architecture", "visual_anchor": "red boxes labeled e1, e2, ..., em positioned between blue circles and orange Attention layer", "text_evidence": "We consider each feature value as an individual entity (like the words are considered in text-classification) and learn a fixed-size embedding {e_k}_{k=1}^m, e_k ∈ R^{d^e} for each feature, {f_k}_{k=1}^m", "query_type": "visual_definition"}
{"query_id": "l1_2109.03952_2109.03952_fig_1_948", "query": "How does the orange Attention layer produce its output representation before the yellow Dense Layers?", "answer": "The attention layer computes weights and takes a weighted combination of the input feature embeddings to produce a d^e-dimensional vector representation for the sample instance.", "doc_id": "2109.03952", "figure_id": "2109.03952_fig_1", "figure_number": 1, "image_path": "data/mineru_output/2109.03952/2109.03952/hybrid_auto/images/2109.03952_page0_fig0.jpg", "caption": "Figure 1: (a) In general classification model, for each feature $f _ { k }$ a vector representation $e _ { k }$ of length $d ^ { e }$ is learned. This is passed to the attention layer which produces a $d ^ { e }$ - dimensional vector representation for the sample instance $i$ which is passed to two dense layers to get the final classification output. (b) The Attribution framework has the same architecture as the general model. One outcome is obtained through the original model and another through the model that has some attention weights zeroed. The observed difference in accuracy and fairness measures will indicate the effect of the zeroed out features on accuracy and fairness.", "figure_type": "architecture", "visual_anchor": "orange Attention layer box positioned between red embedding boxes and yellow Dense Layer boxes", "text_evidence": "These vectors are passed to the attention layer. The Computation of attention weights and the final representation for a sample is", "query_type": "comparison_explanation"}
{"query_id": "l1_2109.03952_2109.03952_fig_1_949", "query": "What purpose does zeroing attention weights serve in the Attribution framework variant of this architecture?", "answer": "Zeroing or reducing attention weights allows observation of specific feature effects on fairness outcomes. The resulting differences in accuracy and fairness measures indicate the effect of the zeroed-out features.", "doc_id": "2109.03952", "figure_id": "2109.03952_fig_1", "figure_number": 1, "image_path": "data/mineru_output/2109.03952/2109.03952/hybrid_auto/images/2109.03952_page0_fig0.jpg", "caption": "Figure 1: (a) In general classification model, for each feature $f _ { k }$ a vector representation $e _ { k }$ of length $d ^ { e }$ is learned. This is passed to the attention layer which produces a $d ^ { e }$ - dimensional vector representation for the sample instance $i$ which is passed to two dense layers to get the final classification output. (b) The Attribution framework has the same architecture as the general model. One outcome is obtained through the original model and another through the model that has some attention weights zeroed. The observed difference in accuracy and fairness measures will indicate the effect of the zeroed out features on accuracy and fairness.", "figure_type": "architecture", "visual_anchor": "orange Attention layer where weights can be manipulated", "text_evidence": "To this end, we observe the effect of each attribute on the fairness of outcomes by zeroing out or reducing its attention weights and recording the change.", "query_type": "value_context"}
{"query_id": "l1_2109.03952_2109.03952_fig_3_950", "query": "Why does removing f_2 move the model from accuracy 0.87 to 0.53 as shown by ŷ_z^2?", "answer": "Feature f_2 is correctly attributed to being responsible for the accuracy in Scenario 1, so removing it causes accuracy to drop drastically from the original model outcome to approximately 0.53.", "doc_id": "2109.03952", "figure_id": "2109.03952_fig_3", "figure_number": 2, "image_path": "data/mineru_output/2109.03952/2109.03952/hybrid_auto/images/2109.03952_page0_fig2.jpg", "caption": "Figure 2: Results from the synthetic datasets. Following the $\\hat { y } _ { o }$ and $\\hat { y } _ { z }$ notations, $\\hat { y } _ { o }$ represents the original model outcome with all the attention weights intact, while $\\hat { y } _ { z } ^ { k }$ represents the outcome of the model in which the attention weights corresponding to $k ^ { t h }$ feature are zeroed out (e.g. $\\hat { y } _ { z } ^ { 1 }$ represents when attention weights of feature $f _ { 1 }$ are zeroed out). The results show how the accuracy and fairness of the model (in terms of statistical parity difference) change by exclusion of each feature.", "figure_type": "plot", "visual_anchor": "ŷ_z^2 blue diamond at (0.53, 1.0) compared to ŷ_o red circle at (0.87, 0.5)", "text_evidence": "In Scenario 1, as expected, $f _ { 2 }$ is correctly attributed to being responsible for the accuracy and removing it hurts the accuracy drastically.", "query_type": "comparison_explanation"}
{"query_id": "l1_2109.03952_2109.03952_fig_3_951", "query": "Does the position of ŷ_o at SPD 0.5 validate that zeroing attention weights changes model fairness?", "answer": "Yes, the original model ŷ_o has SPD around 0.5, while zeroing different features' attention weights produces outcomes with varying SPD values (from 0.05 to 1.0), demonstrating how attention weight exclusion affects statistical parity difference.", "doc_id": "2109.03952", "figure_id": "2109.03952_fig_3", "figure_number": 2, "image_path": "data/mineru_output/2109.03952/2109.03952/hybrid_auto/images/2109.03952_page0_fig2.jpg", "caption": "Figure 2: Results from the synthetic datasets. Following the $\\hat { y } _ { o }$ and $\\hat { y } _ { z }$ notations, $\\hat { y } _ { o }$ represents the original model outcome with all the attention weights intact, while $\\hat { y } _ { z } ^ { k }$ represents the outcome of the model in which the attention weights corresponding to $k ^ { t h }$ feature are zeroed out (e.g. $\\hat { y } _ { z } ^ { 1 }$ represents when attention weights of feature $f _ { 1 }$ are zeroed out). The results show how the accuracy and fairness of the model (in terms of statistical parity difference) change by exclusion of each feature.", "figure_type": "plot", "visual_anchor": "ŷ_o red circle at SPD=0.5 compared to other blue diamonds at different SPD levels", "text_evidence": "The results show how the accuracy and fairness of the model (in terms of statistical parity difference) change by exclusion of each feature.", "query_type": "value_context"}
{"query_id": "l1_2109.03952_2109.03952_fig_3_952", "query": "What causes ŷ_z^1 to maintain high accuracy at 0.92 while other zeroed features drop below 0.6?", "answer": "The synthetic experiment tests the method's ability to capture correct attributions in controlled conditions. Feature f_1 is likely not responsible for accuracy in Scenario 1, so zeroing its attention weights minimally impacts performance compared to features like f_2.", "doc_id": "2109.03952", "figure_id": "2109.03952_fig_3", "figure_number": 2, "image_path": "data/mineru_output/2109.03952/2109.03952/hybrid_auto/images/2109.03952_page0_fig2.jpg", "caption": "Figure 2: Results from the synthetic datasets. Following the $\\hat { y } _ { o }$ and $\\hat { y } _ { z }$ notations, $\\hat { y } _ { o }$ represents the original model outcome with all the attention weights intact, while $\\hat { y } _ { z } ^ { k }$ represents the outcome of the model in which the attention weights corresponding to $k ^ { t h }$ feature are zeroed out (e.g. $\\hat { y } _ { z } ^ { 1 }$ represents when attention weights of feature $f _ { 1 }$ are zeroed out). The results show how the accuracy and fairness of the model (in terms of statistical parity difference) change by exclusion of each feature.", "figure_type": "plot", "visual_anchor": "ŷ_z^1 blue circle at (0.92, 0.05) versus ŷ_z^2 and ŷ_z^{1,2} both near 0.53 accuracy", "text_evidence": "First, we test our method's ability to capture correct attributions in controlled experiments with synthetic data (described in Sec. 3.2).", "query_type": "anomaly_cause"}
{"query_id": "l1_2109.03952_2109.03952_fig_5_953", "query": "Why does FCRL with red circles remain below 0.04 parity difference until accuracy exceeds 0.81 on UCI Adult?", "answer": "FCRL demonstrates effective fairness control at lower accuracy levels, maintaining low statistical parity difference while gradually trading off accuracy. This behavior reflects the accuracy-fairness tradeoff characteristic of the UCI Adult dataset experiments.", "doc_id": "2109.03952", "figure_id": "2109.03952_fig_5", "figure_number": 3, "image_path": "data/mineru_output/2109.03952/2109.03952/hybrid_auto/images/2109.03952_page0_fig4.jpg", "caption": "Figure 3: Accuracy vs parity curves for UCI Adult and Heritage Health datasets.", "figure_type": "plot", "visual_anchor": "FCRL red circle markers clustered below 0.04 Statistical Parity Difference in the 0.80-0.82 accuracy range", "text_evidence": "Accuracy vs parity curves for UCI Adult and Heritage Health datasets", "query_type": "comparison_explanation"}
{"query_id": "l1_2109.03952_2109.03952_fig_5_954", "query": "Does the purple Attention method's horizontal error bars spanning 0.82 to 0.85 accuracy support flexible fairness mitigation across modalities?", "answer": "Yes, the wide horizontal spread of Attention (Ours) demonstrates its ability to achieve various accuracy-fairness tradeoffs, which aligns with the claim that the approach is flexible and useful for controlling fairness beyond tabular datasets.", "doc_id": "2109.03952", "figure_id": "2109.03952_fig_5", "figure_number": 3, "image_path": "data/mineru_output/2109.03952/2109.03952/hybrid_auto/images/2109.03952_page0_fig4.jpg", "caption": "Figure 3: Accuracy vs parity curves for UCI Adult and Heritage Health datasets.", "figure_type": "plot", "visual_anchor": "Purple plus markers with horizontal error bars extending from approximately 0.82 to 0.85 accuracy", "text_evidence": "In addition to providing interpretability, our approach is flexible and useful for controlling fairness in modalities other than tabular datasets", "query_type": "value_context"}
{"query_id": "l1_2109.03952_2109.03952_fig_5_955", "query": "How does MIFR's green curve reaching 0.18 parity at 0.83 accuracy compare to the biosbias dataset bias mitigation goal?", "answer": "MIFR shows relatively high parity difference at high accuracy on UCI Adult, suggesting it may struggle with fairness-accuracy tradeoffs. This contrasts with the goal of reducing observed biases in classification tasks like biosbias, where effective mitigation is needed.", "doc_id": "2109.03952", "figure_id": "2109.03952_fig_5", "figure_number": 3, "image_path": "data/mineru_output/2109.03952/2109.03952/hybrid_auto/images/2109.03952_page0_fig4.jpg", "caption": "Figure 3: Accuracy vs parity curves for UCI Adult and Heritage Health datasets.", "figure_type": "plot", "visual_anchor": "Green circle markers (MIFR) reaching approximately 0.18 Statistical Parity Difference at 0.83 accuracy", "text_evidence": "We consider the biosbias dataset (De-Arteaga et al. 2019), and use our mitigation technique to reduce observed biases in the classification task performed on this dataset", "query_type": "comparison_explanation"}
{"query_id": "l1_2109.03952_2109.03952_fig_7_956", "query": "Why does the Post-Processing model highlight R.N. instead of gendered words like the Not Debiased Model does?", "answer": "The Post-Processing model uses attention weights to focus on profession-based words like R.N. (Registered Nurse) to correctly predict \"nurse,\" while the Not Debiased Model mostly focuses on gendered words for its prediction.", "doc_id": "2109.03952", "figure_id": "2109.03952_fig_7", "figure_number": 4, "image_path": "data/mineru_output/2109.03952/2109.03952/hybrid_auto/images/2109.03952_page0_fig6.jpg", "caption": "Figure 4: Qualitative results from the non-tabular data experiment on the job classification task based on bio texts. Green regions are the top three words used by the model for its prediction based on the attention weights. While the Not Debiased Model mostly focuses on gendered words, our method focused on profession-based words, such as R.N. (Registered Nurse), to correctly predict “nurse.”", "figure_type": "example", "visual_anchor": "Green highlight on 'R.N.' in Post-Processing box versus 'She', 'Rebekah Bushey', 'she' in Not Debiased Model box", "text_evidence": "While the Not Debiased Model mostly focuses on gendered words, our method focused on profession-based words, such as R.N. (Registered Nurse), to correctly predict \"nurse.\"", "query_type": "comparison_explanation"}
{"query_id": "l1_2109.03952_2109.03952_fig_7_957", "query": "How does highlighting Nursing in all three models demonstrate the proposed mitigation technique reduces observed biases?", "answer": "All three models highlight \"Nursing\" as relevant, but the Post-Processing model also emphasizes profession-specific terms while avoiding gendered words. This demonstrates the mitigation technique's effectiveness in reducing bias on the biosbias dataset classification task.", "doc_id": "2109.03952", "figure_id": "2109.03952_fig_7", "figure_number": 4, "image_path": "data/mineru_output/2109.03952/2109.03952/hybrid_auto/images/2109.03952_page0_fig6.jpg", "caption": "Figure 4: Qualitative results from the non-tabular data experiment on the job classification task based on bio texts. Green regions are the top three words used by the model for its prediction based on the attention weights. While the Not Debiased Model mostly focuses on gendered words, our method focused on profession-based words, such as R.N. (Registered Nurse), to correctly predict “nurse.”", "figure_type": "example", "visual_anchor": "Green highlight on 'Nursing' appearing in all three bottom text boxes", "text_evidence": "We consider the biosbias dataset (De-Arteaga et al. 2019), and use our mitigation technique to reduce observed biases in the classification task performed on this dataset.", "query_type": "value_context"}
{"query_id": "l1_2109.03952_2109.03952_fig_7_958", "query": "Does the attention weight manipulation shown in green highlights validate the post-processing bias mitigation strategy on text data?", "answer": "Yes, the green highlights show that attention weight manipulation successfully shifts focus from gendered terms to profession-based words. This validates the proposed framework by demonstrating effectiveness on both tabular and text data through the attention mechanism.", "doc_id": "2109.03952", "figure_id": "2109.03952_fig_7", "figure_number": 4, "image_path": "data/mineru_output/2109.03952/2109.03952/hybrid_auto/images/2109.03952_page0_fig6.jpg", "caption": "Figure 4: Qualitative results from the non-tabular data experiment on the job classification task based on bio texts. Green regions are the top three words used by the model for its prediction based on the attention weights. While the Not Debiased Model mostly focuses on gendered words, our method focused on profession-based words, such as R.N. (Registered Nurse), to correctly predict “nurse.”", "figure_type": "example", "visual_anchor": "Green highlighted words distributed across all three columns showing different attention patterns", "text_evidence": "Using this interpretable attribution framework we then introduced a post-processing bias mitigation strategy based on attention weight manipulation. We validated the proposed framework by conducting experimen", "query_type": "value_context"}
{"query_id": "l1_2109.03952_2109.03952_fig_8_959", "query": "Why does MIFR show the widest accuracy spread from 0.795 to 0.835 despite the job classification task using bio texts?", "answer": "MIFR's wide accuracy spread likely reflects varying performance when attention weights focus on different top three words in bio texts for job prediction, as shown in the green regions of qualitative results.", "doc_id": "2109.03952", "figure_id": "2109.03952_fig_8", "figure_number": 5, "image_path": "data/mineru_output/2109.03952/2109.03952/hybrid_auto/images/2109.03952_page0_fig7.jpg", "caption": "Figure 5: Additional qualitative results from the non-tabular data experiment on the job classification task based on the bio texts. Green regions represent top three words that the model used for its prediction based on the attention weights.   ", "figure_type": "plot", "visual_anchor": "Green points (MIFR) spanning horizontally from accuracy 0.795 to 0.835", "text_evidence": "Green regions represent top three words that the model used for its prediction based on the attention weights.", "query_type": "anomaly_cause"}
{"query_id": "l1_2109.03952_2109.03952_fig_8_960", "query": "Does the purple cluster's concentration at accuracy 0.84-0.845 align with the cardiac telemetry internship context in the bio sample?", "answer": "The purple cluster represents the Attention method's performance on job classification, not the specific bio content about cardiac telemetry internship, which is merely example input text the model processes.", "doc_id": "2109.03952", "figure_id": "2109.03952_fig_8", "figure_number": 5, "image_path": "data/mineru_output/2109.03952/2109.03952/hybrid_auto/images/2109.03952_page0_fig7.jpg", "caption": "Figure 5: Additional qualitative results from the non-tabular data experiment on the job classification task based on the bio texts. Green regions represent top three words that the model used for its prediction based on the attention weights.   ", "figure_type": "plot", "visual_anchor": "Purple points (Attention) clustered tightly at accuracy range 0.84-0.845", "text_evidence": "Following graduation she completed a cardiac telemetry internship at Texas Health Resources Fort Worth. She continued to work there for two years taking care of postsurgical cardiac patients", "query_type": "value_context"}
{"query_id": "l1_2109.03952_2109.03952_fig_8_961", "query": "How does LAFTR's near-zero equality opportunity difference at 0.84 accuracy relate to the Baylor University nursing degree mentioned?", "answer": "The LAFTR yellow point at near-zero fairness represents algorithmic performance metrics, independent of the specific biographical content about Baylor University nursing education that serves as input data.", "doc_id": "2109.03952", "figure_id": "2109.03952_fig_8", "figure_number": 5, "image_path": "data/mineru_output/2109.03952/2109.03952/hybrid_auto/images/2109.03952_page0_fig7.jpg", "caption": "Figure 5: Additional qualitative results from the non-tabular data experiment on the job classification task based on the bio texts. Green regions represent top three words that the model used for its prediction based on the attention weights.   ", "figure_type": "plot", "visual_anchor": "Yellow point (LAFTR) at accuracy ~0.84 and equality opportunity difference ~0.0", "text_evidence": "She graduated from Baylor University Louise Herrington School of Nursing in with her Bachelors of Science in Nursing", "query_type": "value_context"}
{"query_id": "l1_2109.03952_2109.03952_fig_9_962", "query": "Why does the purple Attention curve achieve 0.77 accuracy at lower EQOP than MIFR's 0.69 accuracy peak?", "answer": "The Attention method maintains higher accuracy (0.77) while achieving better equality of opportunity (lower EQOP around 0.65) compared to MIFR, demonstrating superior fairness-accuracy trade-offs on the Heritage Health dataset.", "doc_id": "2109.03952", "figure_id": "2109.03952_fig_9", "figure_number": 6, "image_path": "data/mineru_output/2109.03952/2109.03952/hybrid_auto/images/2109.03952_page0_fig8.jpg", "caption": "Figure 6: Accuracy vs equality of opportunity curves for UCI Adult and Heritage Health datasets.", "figure_type": "plot", "visual_anchor": "Purple 'Attention (Ours)' markers reaching 0.77 accuracy at approximately 0.65 EQOP versus green 'MIFR' markers at 0.69 accuracy", "text_evidence": "Attention (Ours) 0.77 (0.006) ... 0.81 (0.013) ... when all problematic features are zeroed out", "query_type": "comparison_explanation"}
{"query_id": "l1_2109.03952_2109.03952_fig_9_963", "query": "Does the green MIFR curve's steep climb from 0.05 to 0.69 EQOP support post-processing approaches outperforming attention?", "answer": "No, the MIFR curve shows worse performance. Despite the steep climb, MIFR achieves lower maximum accuracy and higher EQOP values compared to the Attention method, contradicting any claim of superiority.", "doc_id": "2109.03952", "figure_id": "2109.03952_fig_9", "figure_number": 6, "image_path": "data/mineru_output/2109.03952/2109.03952/hybrid_auto/images/2109.03952_page0_fig8.jpg", "caption": "Figure 6: Accuracy vs equality of opportunity curves for UCI Adult and Heritage Health datasets.", "figure_type": "plot", "visual_anchor": "Green MIFR curve showing steep increase from approximately 0.05 to 0.69 on the EQOP axis", "text_evidence": "Adult results on post-processing approach from Hardt et al. vs our attention method", "query_type": "comparison_explanation"}
{"query_id": "l1_2109.03952_2109.03952_fig_9_964", "query": "What explains the purple curve's horizontal error bars widening significantly between 0.68 and 0.76 accuracy regions?", "answer": "The widening horizontal error bars indicate increasing variance in accuracy measurements as the Attention method explores different fairness-accuracy trade-off points, reflecting uncertainty in model performance across different configurations on the Heritage Health dataset.", "doc_id": "2109.03952", "figure_id": "2109.03952_fig_9", "figure_number": 6, "image_path": "data/mineru_output/2109.03952/2109.03952/hybrid_auto/images/2109.03952_page0_fig8.jpg", "caption": "Figure 6: Accuracy vs equality of opportunity curves for UCI Adult and Heritage Health datasets.", "figure_type": "plot", "visual_anchor": "Purple markers showing progressively wider horizontal error bars in the 0.68 to 0.76 accuracy range", "text_evidence": "Accuracy vs equality of opportunity curves for UCI Adult and Heritage Health datasets", "query_type": "anomaly_cause"}
{"query_id": "l1_2109.03952_2109.03952_fig_11_965", "query": "Why do the purple Attention points cluster between 0.68-0.76 accuracy when Heritage Health dataset results are presented?", "answer": "The plot shows accuracy versus equalized odds curves specifically for the Heritage Health dataset, which explains why the Attention method's performance spans this accuracy range with varying fairness trade-offs.", "doc_id": "2109.03952", "figure_id": "2109.03952_fig_11", "figure_number": 7, "image_path": "data/mineru_output/2109.03952/2109.03952/hybrid_auto/images/2109.03952_page0_fig10.jpg", "caption": "Figure 7: Accuracy vs equalized odds curves for UCI Adult and Heritage Health datasets.", "figure_type": "plot", "visual_anchor": "Purple points labeled 'Attention (Ours)' spanning accuracy range 0.68-0.76", "text_evidence": "Accuracy vs EQOD (Heritage Health)   \nFigure 7: Accuracy vs equalized odds curves for UCI Adult and Heritage Health datasets.", "query_type": "value_context"}
{"query_id": "l1_2109.03952_2109.03952_fig_11_966", "query": "Do the green MIFR points at accuracy 0.66-0.68 support the UCI Adult dataset's inclusion in this comparison?", "answer": "Yes, the leftmost cluster of green MIFR points represents results from the UCI Adult dataset, which is explicitly stated as one of the two datasets being compared in this accuracy-fairness trade-off analysis.", "doc_id": "2109.03952", "figure_id": "2109.03952_fig_11", "figure_number": 7, "image_path": "data/mineru_output/2109.03952/2109.03952/hybrid_auto/images/2109.03952_page0_fig10.jpg", "caption": "Figure 7: Accuracy vs equalized odds curves for UCI Adult and Heritage Health datasets.", "figure_type": "plot", "visual_anchor": "Green MIFR points clustered at lower accuracy values 0.66-0.68", "text_evidence": "Accuracy vs EQOD (UCI Adult)\nText after: Accuracy vs EQOD (Heritage Health)   \nFigure 7: Accuracy vs equalized odds curves for UCI Adult and Heritage Health datasets.", "query_type": "value_context"}
{"query_id": "l1_2109.03952_2109.03952_fig_11_967", "query": "How does the purple Attention method's maximum accuracy of approximately 0.76 compare to MIFR's 0.77 on Heritage Health?", "answer": "The green MIFR points reach slightly higher accuracy (approximately 0.77) compared to the purple Attention method's maximum (approximately 0.76), though both methods show similar equalized odds differences around 0.65-0.70 at their highest accuracy levels.", "doc_id": "2109.03952", "figure_id": "2109.03952_fig_11", "figure_number": 7, "image_path": "data/mineru_output/2109.03952/2109.03952/hybrid_auto/images/2109.03952_page0_fig10.jpg", "caption": "Figure 7: Accuracy vs equalized odds curves for UCI Adult and Heritage Health datasets.", "figure_type": "plot", "visual_anchor": "Rightmost purple and green points showing maximum accuracy values around 0.76-0.77", "text_evidence": "Figure 7: Accuracy vs equalized odds curves for UCI Adult and Heritage Health datasets.\nReferences: - Accuracy vs EQOD (Heritage Health)", "query_type": "comparison_explanation"}
{"query_id": "l1_2109.03952_2109.03952_fig_13_968", "query": "Why does removing dds_max yield approximately 4% improvement across all three fairness definitions shown in red bars?", "answer": "The dds_max feature is identified as the top problematic feature for adult and heritage health datasets, causing the most benefit when removed across Statistical Parity, Equality of Opportunity, and Equalized Odds.", "doc_id": "2109.03952", "figure_id": "2109.03952_fig_13", "figure_number": 8, "image_path": "data/mineru_output/2109.03952/2109.03952/hybrid_auto/images/2109.03952_page0_fig12.jpg", "caption": "Figure 8: Top three features for each fairness definition removing which caused the most benefit in improving the corresponding fairness definition. The percentage of improvement upon removal is marked on the $y$ -axis for adult and heritage health datasets.", "figure_type": "plot", "visual_anchor": "Red bars at approximately 4% height in all three grouped bar charts", "text_evidence": "Top three features for each fairness definition removing which caused the most benefit in improving the corresponding fairness definition.", "query_type": "comparison_explanation"}
{"query_id": "l1_2109.03952_2109.03952_fig_13_969", "query": "Does the yellow bar showing dds_range at roughly 3% improvement align with it being a top problematic feature?", "answer": "Yes, dds_range consistently ranks as the second most problematic feature across all fairness definitions, with its removal providing approximately 3% improvement as marked on the y-axis.", "doc_id": "2109.03952", "figure_id": "2109.03952_fig_13", "figure_number": 8, "image_path": "data/mineru_output/2109.03952/2109.03952/hybrid_auto/images/2109.03952_page0_fig12.jpg", "caption": "Figure 8: Top three features for each fairness definition removing which caused the most benefit in improving the corresponding fairness definition. The percentage of improvement upon removal is marked on the $y$ -axis for adult and heritage health datasets.", "figure_type": "plot", "visual_anchor": "Yellow bars at approximately 3% height across all three fairness definition charts", "text_evidence": "The percentage of improvement upon removal is marked on the y-axis for adult and heritage health datasets.", "query_type": "value_context"}
{"query_id": "l1_2109.03952_2109.03952_fig_13_970", "query": "How do the blue bars for LOS_TOT_UNKNOWN and drugsdont_maths at 2.8% compare as third-ranked problematic features?", "answer": "Both features show nearly identical improvement percentages (~2.8%) when removed, indicating they have similar negative impacts on fairness despite being from different datasets (adult vs. health).", "doc_id": "2109.03952", "figure_id": "2109.03952_fig_13", "figure_number": 8, "image_path": "data/mineru_output/2109.03952/2109.03952/hybrid_auto/images/2109.03952_page0_fig12.jpg", "caption": "Figure 8: Top three features for each fairness definition removing which caused the most benefit in improving the corresponding fairness definition. The percentage of improvement upon removal is marked on the $y$ -axis for adult and heritage health datasets.", "figure_type": "plot", "visual_anchor": "Blue bars at approximately 2.8% in Statistical Parity (LOS_TOT_UNKNOWN) and other charts (drugsdont_maths)", "text_evidence": "Top Problematic Features from Adult ... Top Problematic Features from Health", "query_type": "comparison_explanation"}
{"query_id": "l1_2109.03952_2109.03952_fig_14_971", "query": "Why do capital loss and capital gain points cluster near 0.78 accuracy, given removal improves fairness definitions?", "answer": "Removing capital loss and capital gain features substantially reduces accuracy but improves statistical parity difference (SPD) to around 0.30-0.33, showing these features contribute to both model performance and unfairness.", "doc_id": "2109.03952", "figure_id": "2109.03952_fig_14", "figure_number": 9, "image_path": "data/mineru_output/2109.03952/2109.03952/hybrid_auto/images/2109.03952_page0_fig13.jpg", "caption": "Figure 9: Results from the real-world datasets. Note that in our $\\hat { y } _ { z }$ notation we replaced indexes with actual feature names for clarity in these results on real-world datasets as there is not one universal indexing schema, but the feature names are more universal and discriptive for this case. Labels on the points represent the feature name that was removed (zeroed out) according to our $\\hat { y } _ { z }$ notation. The results show how the accuracy and fairness of the model (in terms of statistical parity difference) change by exclusion of each feature.", "figure_type": "plot", "visual_anchor": "Blue points labeled 'capital loss' and 'capital gain' at approximately (0.78, 0.33) and (0.79, 0.31)", "text_evidence": "Top three features for each fairness definition removing which caused the most benefit in improving the corresponding fairness definition. The percentage of improvement upon removal is marked on the y-axis", "query_type": "comparison_explanation"}
{"query_id": "l1_2109.03952_2109.03952_fig_14_972", "query": "Does the original red point at 0.85 accuracy confirm that feature exclusion changes fairness in statistical parity difference?", "answer": "Yes, the original model at (0.85, 0.21) SPD serves as the baseline, and all blue points show how zeroing out individual features shifts both accuracy and SPD, demonstrating feature-specific fairness impacts.", "doc_id": "2109.03952", "figure_id": "2109.03952_fig_14", "figure_number": 9, "image_path": "data/mineru_output/2109.03952/2109.03952/hybrid_auto/images/2109.03952_page0_fig13.jpg", "caption": "Figure 9: Results from the real-world datasets. Note that in our $\\hat { y } _ { z }$ notation we replaced indexes with actual feature names for clarity in these results on real-world datasets as there is not one universal indexing schema, but the feature names are more universal and discriptive for this case. Labels on the points represent the feature name that was removed (zeroed out) according to our $\\hat { y } _ { z }$ notation. The results show how the accuracy and fairness of the model (in terms of statistical parity difference) change by exclusion of each feature.", "figure_type": "plot", "visual_anchor": "Red point labeled 'original' at approximately (0.85, 0.21)", "text_evidence": "The results show how the accuracy and fairness of the model (in terms of statistical parity difference) change by exclusion of each feature.", "query_type": "value_context"}
{"query_id": "l1_2109.03952_2109.03952_fig_14_973", "query": "Why does removing sex reduce SPD to approximately 0.17 while maintaining 0.85 accuracy compared to other features?", "answer": "The sex feature appears to encode substantial bias with minimal contribution to accuracy. Its removal improves fairness significantly while preserving model performance, unlike capital features which trade accuracy for fairness.", "doc_id": "2109.03952", "figure_id": "2109.03952_fig_14", "figure_number": 9, "image_path": "data/mineru_output/2109.03952/2109.03952/hybrid_auto/images/2109.03952_page0_fig13.jpg", "caption": "Figure 9: Results from the real-world datasets. Note that in our $\\hat { y } _ { z }$ notation we replaced indexes with actual feature names for clarity in these results on real-world datasets as there is not one universal indexing schema, but the feature names are more universal and discriptive for this case. Labels on the points represent the feature name that was removed (zeroed out) according to our $\\hat { y } _ { z }$ notation. The results show how the accuracy and fairness of the model (in terms of statistical parity difference) change by exclusion of each feature.", "figure_type": "plot", "visual_anchor": "Blue point labeled 'sex' at approximately (0.85, 0.17)", "text_evidence": "In our ŷ_z notation we replaced indexes with actual feature names for clarity in these results on real-world datasets as there is not one universal indexing schema", "query_type": "anomaly_cause"}
