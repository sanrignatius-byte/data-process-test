{
  "metadata": {
    "source": "data/multimodal_elements.json",
    "existing_l1": "data/l1_cross_modal_queries_v3.jsonl",
    "max_pairs": 150,
    "max_per_doc": 5,
    "max_hops": 2,
    "min_quality": 0.6
  },
  "summary": {
    "total_selected": 150,
    "by_type": {
      "figure+table": 90,
      "figure+formula": 45,
      "formula+table": 15
    },
    "by_hop": {
      "1": 149,
      "2": 1
    },
    "overlap_with_existing_l1": 0,
    "docs_covered": 43
  },
  "pairs": [
    {
      "pair_id": "1306.5204_pair_1",
      "doc_id": "1306.5204",
      "element_a_id": "1306.5204_figure_1",
      "element_b_id": "1306.5204_table_1",
      "element_a_type": "figure",
      "element_b_type": "table",
      "pair_type": "figure+table",
      "hop_distance": 1,
      "path": [
        "1306.5204_figure_1",
        "1306.5204_table_1"
      ],
      "quality_score": 1.0,
      "overlap_with_existing_l1": false,
      "element_a": {
        "element_id": "1306.5204_figure_1",
        "element_type": "figure",
        "caption": "(b) Streaming API Figure 1: Tag cloud of top terms from each dataset.",
        "content": "(b) Streaming API Figure 1: Tag cloud of top terms from each dataset.",
        "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1306.5204/1306.5204/hybrid_auto/images/1306.5204_page0_fig1.jpg",
        "context_before": "",
        "context_after": "Twitter’s Streaming API has been used throughout the domain of social media and network analysis to generate understanding of how users behave on these platforms. It has been used to collect data for topic modeling (Hong and Davison 2010; Pozdnoukhov and Kaiser 2011), network analysis (Sofean and Smith 2012), and statistical analysis of content (Mathioudakis and Koudas 2010), among others. Researchers’ reliance upon this data source is significant, and these examples only provide a cursory glanc"
      },
      "element_b": {
        "element_id": "1306.5204_table_1",
        "element_type": "table",
        "caption": "Table 1: Parameters used to collect data from Syria. Coordinates below the boundary box indicate the Southwest and Northeast corner, respectively.",
        "content": "Table 1: Parameters used to collect data from Syria. Coordinates below the boundary box indicate the Southwest and Northeast corner, respectively.",
        "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1306.5204/1306.5204/hybrid_auto/images/5f5fb5eb0ac2d9647748e7eadf75690c81f0da1c4023ae31d2f2cdeec9ba1c6a.jpg",
        "context_before": "The raw counts of tweets we received each day from both sources are shown in Figure 2.\n\nTable 2 shows the results for the average of 28 daily networks, the min-max range, as well as the aggregated network including all 28 days.\n\nFrom December 14th, 2011 - January 10th, 2012 we collected tweets from the Twitter Firehose matching any of the keywords, geographical bounding boxes, and users in Table 1. During the same time period, we collected tweets from the Streaming API using TweetTracker (Kumar ",
        "context_after": "From December 14th, 2011 - January 10th, 2012 we collected tweets from the Twitter Firehose matching any of the keywords, geographical bounding boxes, and users in Table 1. During the same time period, we collected tweets from the Streaming API using TweetTracker (Kumar et al. 2011) with exactly the same parameters. During the time we collected 528,592 tweets from the Streaming API and 1,280,344 tweets from the Firehose. The raw counts of tweets we received each day from both sources are shown i"
      },
      "edge_contexts": [
        {
          "source": "1306.5204_table_1",
          "target": "1306.5204_figure_1",
          "ref_text": "Figure 1",
          "context_snippet": "de tag clouds for the top words in the Streaming API and the Firehose, shown in Figure 1."
        }
      ]
    },
    {
      "pair_id": "1306.5204_pair_2",
      "doc_id": "1306.5204",
      "element_a_id": "1306.5204_figure_2",
      "element_b_id": "1306.5204_table_1",
      "element_a_type": "figure",
      "element_b_type": "table",
      "pair_type": "figure+table",
      "hop_distance": 1,
      "path": [
        "1306.5204_figure_2",
        "1306.5204_table_1"
      ],
      "quality_score": 1.0,
      "overlap_with_existing_l1": false,
      "element_a": {
        "element_id": "1306.5204_figure_2",
        "element_type": "figure",
        "caption": "Figure 2: Raw tweet counts for each day from both the Streaming API and the Firehose.",
        "content": "Figure 2: Raw tweet counts for each day from both the Streaming API and the Firehose.",
        "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1306.5204/1306.5204/hybrid_auto/images/1306.5204_page0_fig2.jpg",
        "context_before": "To give the reader a sense for the top words in both datasets, we include tag clouds for the top words in the Streaming API and the Firehose, shown in Figure 1.\n\nWe start by grouping the locations of tweets by continent and can find a strong Asian bias due to the boundary box we used to collect the data from both sources, shown in Table 1.\n\nIn this work we compare the datasets by analyzing facets commonly used in the literature. We start by comparing the top hashtags found in the tweets, a featu",
        "context_after": "From December 14th, 2011 - January 10th, 2012 we collected tweets from the Twitter Firehose matching any of the keywords, geographical bounding boxes, and users in Table 1. During the same time period, we collected tweets from the Streaming API using TweetTracker (Kumar et al. 2011) with exactly the same parameters. During the time we collected 528,592 tweets from the Streaming API and 1,280,344 tweets from the Firehose. The raw counts of tweets we received each day from both sources are shown i"
      },
      "element_b": {
        "element_id": "1306.5204_table_1",
        "element_type": "table",
        "caption": "Table 1: Parameters used to collect data from Syria. Coordinates below the boundary box indicate the Southwest and Northeast corner, respectively.",
        "content": "Table 1: Parameters used to collect data from Syria. Coordinates below the boundary box indicate the Southwest and Northeast corner, respectively.",
        "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1306.5204/1306.5204/hybrid_auto/images/5f5fb5eb0ac2d9647748e7eadf75690c81f0da1c4023ae31d2f2cdeec9ba1c6a.jpg",
        "context_before": "The raw counts of tweets we received each day from both sources are shown in Figure 2.\n\nTable 2 shows the results for the average of 28 daily networks, the min-max range, as well as the aggregated network including all 28 days.\n\nFrom December 14th, 2011 - January 10th, 2012 we collected tweets from the Twitter Firehose matching any of the keywords, geographical bounding boxes, and users in Table 1. During the same time period, we collected tweets from the Streaming API using TweetTracker (Kumar ",
        "context_after": "From December 14th, 2011 - January 10th, 2012 we collected tweets from the Twitter Firehose matching any of the keywords, geographical bounding boxes, and users in Table 1. During the same time period, we collected tweets from the Streaming API using TweetTracker (Kumar et al. 2011) with exactly the same parameters. During the time we collected 528,592 tweets from the Streaming API and 1,280,344 tweets from the Firehose. The raw counts of tweets we received each day from both sources are shown i"
      },
      "edge_contexts": [
        {
          "source": "1306.5204_table_1",
          "target": "1306.5204_figure_2",
          "ref_text": "Figure 2",
          "context_snippet": "The raw counts of tweets we received each day from both sources are shown in Figure 2.\n\nTable 2 shows the results for the average of 28 daily networks, the min-max r"
        }
      ]
    },
    {
      "pair_id": "1306.5204_pair_3",
      "doc_id": "1306.5204",
      "element_a_id": "1306.5204_table_1",
      "element_b_id": "1306.5204_figure_3",
      "element_a_type": "table",
      "element_b_type": "figure",
      "pair_type": "figure+table",
      "hop_distance": 1,
      "path": [
        "1306.5204_table_1",
        "1306.5204_figure_3"
      ],
      "quality_score": 1.0,
      "overlap_with_existing_l1": false,
      "element_a": {
        "element_id": "1306.5204_table_1",
        "element_type": "table",
        "caption": "Table 1: Parameters used to collect data from Syria. Coordinates below the boundary box indicate the Southwest and Northeast corner, respectively.",
        "content": "Table 1: Parameters used to collect data from Syria. Coordinates below the boundary box indicate the Southwest and Northeast corner, respectively.",
        "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1306.5204/1306.5204/hybrid_auto/images/5f5fb5eb0ac2d9647748e7eadf75690c81f0da1c4023ae31d2f2cdeec9ba1c6a.jpg",
        "context_before": "The raw counts of tweets we received each day from both sources are shown in Figure 2.\n\nTable 2 shows the results for the average of 28 daily networks, the min-max range, as well as the aggregated network including all 28 days.\n\nFrom December 14th, 2011 - January 10th, 2012 we collected tweets from the Twitter Firehose matching any of the keywords, geographical bounding boxes, and users in Table 1. During the same time period, we collected tweets from the Streaming API using TweetTracker (Kumar ",
        "context_after": "From December 14th, 2011 - January 10th, 2012 we collected tweets from the Twitter Firehose matching any of the keywords, geographical bounding boxes, and users in Table 1. During the same time period, we collected tweets from the Streaming API using TweetTracker (Kumar et al. 2011) with exactly the same parameters. During the time we collected 528,592 tweets from the Streaming API and 1,280,344 tweets from the Firehose. The raw counts of tweets we received each day from both sources are shown i"
      },
      "element_b": {
        "element_id": "1306.5204_figure_3",
        "element_type": "figure",
        "caption": "Figure 3: Distribution of coverage for the Streaming data by day. Whiskers indicate extreme values.",
        "content": "Figure 3: Distribution of coverage for the Streaming data by day. Whiskers indicate extreme values.",
        "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1306.5204/1306.5204/hybrid_auto/images/1306.5204_page0_fig3.jpg",
        "context_before": "The number of geotagged tweets is low, with only 16,739 geotagged tweets in the Streaming data $( 3 . 1 7 \\% )$ and 18,579 in the Firehose data $( 1 . 4 5 \\% )$ . We notice that despite the difference in tweets collected on the whole we get $9 0 . 1 0 \\%$ coverage of geotagged tweets. We start by grouping the locations of tweets by continent and can find a strong Asian bias due to the boundary box we used to collect the data from both sources, shown in Table 1. To better understand the distribut",
        "context_after": "Statistical Measures\n\nWe investigate the statistical properties of the two datasets with the intent of understanding how well the characteristics of the sampled data match those of the Firehose. We begin first by comparing the top hashtags in the tweets for different levels of coverage using a rank correlation statistic. We continue to extract topics from the text, matching topical content and comparing topical distribution to better understand how sampling affects the results of this common pro"
      },
      "edge_contexts": [
        {
          "source": "1306.5204_figure_3",
          "target": "1306.5204_table_1",
          "ref_text": "Table 1",
          "context_snippet": "due to the boundary box we used to collect the data from both sources, shown in Table 1. To better understand the distribut\n\nTo give the reader a sense for the top wor"
        }
      ]
    },
    {
      "pair_id": "1306.5204_pair_4",
      "doc_id": "1306.5204",
      "element_a_id": "1306.5204_figure_4",
      "element_b_id": "1306.5204_table_3",
      "element_a_type": "figure",
      "element_b_type": "table",
      "pair_type": "figure+table",
      "hop_distance": 1,
      "path": [
        "1306.5204_figure_4",
        "1306.5204_table_3"
      ],
      "quality_score": 1.0,
      "overlap_with_existing_l1": false,
      "element_a": {
        "element_id": "1306.5204_figure_4",
        "element_type": "figure",
        "caption": "Figure 4: Relationship between $n$ - number of top hashtags, and the correlation coefficient, $\\tau _ { \\beta }$ .",
        "content": "Figure 4: Relationship between $n$ - number of top hashtags, and the correlation coefficient, $\\tau _ { \\beta }$ .",
        "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1306.5204/1306.5204/hybrid_auto/images/1306.5204_page0_fig4.jpg",
        "context_before": "These metrics are reported in Table 3 and are calculated as follows.\n\nAfter removing these tweets,\n\nTable 3: Comparison of Network-Level Social Network Analysis Metrics.\n\nTop Hashtag Analysis\n\nHashtags are an important communication device on Twitter. Users employ them to annotate the content they produce, allowing for other users to find their tweets and to facilitate interaction on the platform. Also, adding a hashtag to a tweet is equivalent to joining a community of users discussing the same",
        "context_after": "the two datasets using Kendall’s $\\tau$ rank correlation coefficient (Agresti 2010).\n\nKendall’s $\\tau$ of Top Hashtags Kendall’s $\\tau$ is a statistic which measures the correlation of two ordered lists by analyzing the number of concordant pairs between them. Consider two hashtags, #A and #B. If both lists rank #A higher than #B, then this is considered a concordant pair, otherwise it is counted as a discordant pair. Ties are handled using the $\\tau _ { \\beta }$ statistic as follows:\n\n$$ \\tau_ "
      },
      "element_b": {
        "element_id": "1306.5204_table_3",
        "element_type": "table",
        "caption": "Table 3: Comparison of Network-Level Social Network Analysis Metrics.",
        "content": "Table 3: Comparison of Network-Level Social Network Analysis Metrics.",
        "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1306.5204/1306.5204/hybrid_auto/images/82bef0f01c6a139b10e38358a274c9f8a694895e9eaea8c2f061702ccc2e5472.jpg",
        "context_before": "The raw counts of tweets we received each day from both sources are shown in Figure 2.\n\nTable 2 shows the results for the average of 28 daily networks, the min-max range, as well as the aggregated network including all 28 days.\n\nWe do not discuss all details of the individual results but focus on the differences between the two data sources. First, the coverage of nodes and links is similar to the coverage of tweets. This is a good indicator that the sub-sample is not biased to the specific Twit",
        "context_after": "One of the key questions we ask in this work is how the amount of coverage affects measures commonly performed on Twitter data. Here we define coverage as the ratio of data from the Streaming API to data from the Firehose. To better understand the coverage of the Streaming API for each day, we construct a box-and-whisker plot to visualize the distribution of daily coverage, shown in Figure 3. In this period of time the Streaming API receives, on average, $4 3 . 5 \\%$ of the data available on the"
      },
      "edge_contexts": [
        {
          "source": "1306.5204_figure_4",
          "target": "1306.5204_table_3",
          "ref_text": "Table 3",
          "context_snippet": "These metrics are reported in Table 3 and are calculated as follows.\n\nAfter removing these tweets,\n\nTable 3: Comparis"
        }
      ]
    },
    {
      "pair_id": "1306.5204_pair_5",
      "doc_id": "1306.5204",
      "element_a_id": "1306.5204_figure_4",
      "element_b_id": "1306.5204_table_4",
      "element_a_type": "figure",
      "element_b_type": "table",
      "pair_type": "figure+table",
      "hop_distance": 1,
      "path": [
        "1306.5204_figure_4",
        "1306.5204_table_4"
      ],
      "quality_score": 1.0,
      "overlap_with_existing_l1": false,
      "element_a": {
        "element_id": "1306.5204_figure_4",
        "element_type": "figure",
        "caption": "Figure 4: Relationship between $n$ - number of top hashtags, and the correlation coefficient, $\\tau _ { \\beta }$ .",
        "content": "Figure 4: Relationship between $n$ - number of top hashtags, and the correlation coefficient, $\\tau _ { \\beta }$ .",
        "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1306.5204/1306.5204/hybrid_auto/images/1306.5204_page0_fig4.jpg",
        "context_before": "These metrics are reported in Table 3 and are calculated as follows.\n\nAfter removing these tweets,\n\nTable 3: Comparison of Network-Level Social Network Analysis Metrics.\n\nTop Hashtag Analysis\n\nHashtags are an important communication device on Twitter. Users employ them to annotate the content they produce, allowing for other users to find their tweets and to facilitate interaction on the platform. Also, adding a hashtag to a tweet is equivalent to joining a community of users discussing the same",
        "context_after": "the two datasets using Kendall’s $\\tau$ rank correlation coefficient (Agresti 2010).\n\nKendall’s $\\tau$ of Top Hashtags Kendall’s $\\tau$ is a statistic which measures the correlation of two ordered lists by analyzing the number of concordant pairs between them. Consider two hashtags, #A and #B. If both lists rank #A higher than #B, then this is considered a concordant pair, otherwise it is counted as a discordant pair. Ties are handled using the $\\tau _ { \\beta }$ statistic as follows:\n\n$$ \\tau_ "
      },
      "element_b": {
        "element_id": "1306.5204_table_4",
        "element_type": "table",
        "caption": "Table 4: Geotagged Tweet Location by Continent. Excluding boundary box from parameters.",
        "content": "Table 4: Geotagged Tweet Location by Continent. Excluding boundary box from parameters.",
        "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1306.5204/1306.5204/hybrid_auto/images/df59d726dc6e60116c0e4a18a97581f7e62aca7cf10c0e5b313e5c03695300f0.jpg",
        "context_before": "To get an accurate representation of the differences in correlation at each level of Streaming coverage, we select five days with different levels of coverage as motivated by Figure 3: The minimum (December 27th), lower quartile (December 24th), median (December 29th), upper quartile (December 18th), and the maximum (December 19th).\n\nThese metrics are reported in Table 3 and are calculated as follows.\n\nAfter removing these tweets,\n\nTable 3: Comparison of Network-Level Social Network Analysis Met",
        "context_after": "more than $90 \\%$ of geotagged Tweets from both sources are excluded from the data and the Streaming coverage level is reduced to $3 9 . 1 9 \\%$ . The distribution of tweets by continent is shown in Table 4. Here we see a more even representation of the tweets’ locations in Asia and North America.\n\nConclusion and Future Work\n\nIn this work we ask whether data obtained through Twitter’s sampled Streaming API is a sufficient representation of activity on Twitter as a whole. To answer this question "
      },
      "edge_contexts": [
        {
          "source": "1306.5204_figure_4",
          "target": "1306.5204_table_4",
          "ref_text": "Table 4",
          "context_snippet": "reduced to $3 9 . 1 9 \\%$ . The distribution of tweets by continent is shown in Table 4. Here we see a more even representation of the tweets’ locations in Asia and No"
        }
      ]
    },
    {
      "pair_id": "1511.00830_pair_1",
      "doc_id": "1511.00830",
      "element_a_id": "1511.00830_figure_1",
      "element_b_id": "1511.00830_table_1",
      "element_a_type": "figure",
      "element_b_type": "table",
      "pair_type": "figure+table",
      "hop_distance": 1,
      "path": [
        "1511.00830_figure_1",
        "1511.00830_table_1"
      ],
      "quality_score": 1.0,
      "overlap_with_existing_l1": false,
      "element_a": {
        "element_id": "1511.00830_figure_1",
        "element_type": "figure",
        "caption": "Figure 1: Unsupervised model",
        "content": "Figure 1: Unsupervised model",
        "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1511.00830/1511.00830/hybrid_auto/images/1511.00830_page0_fig0.jpg",
        "context_before": "[Section: Published as a conference paper at ICLR 2016]\n\narXiv:1511.00830v6 [stat.ML] 10 Aug 2017\n\n2 LEARNING INVARIANT REPRESENTATIONS",
        "context_after": "As for the domain adaptation scenario and the Amazon reviews dataset, the results of our VFAE model can be seen in Table 1. Our model was successful in factoring out the domain information, since the accuracy, measured both linearly (LR) and non-linearly (RF), was towards random chance (which for this dataset is 0.5). We should also mention that, on this dataset at least, completely removing information about the domain does not guarantee a better performance on y. The same effect was also obser"
      },
      "element_b": {
        "element_id": "1511.00830_table_1",
        "element_type": "table",
        "caption": "Table 1: Results on the Amazon reviews dataset. The DANN column is taken directly from Ganin et al. (2015) (the column that uses the original representation as input).",
        "content": "Table 1: Results on the Amazon reviews dataset. The DANN column is taken directly from Ganin et al. (2015) (the column that uses the original representation as input).",
        "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1511.00830/1511.00830/hybrid_auto/images/fdf6c7c916149f26dac10569971359e954c5a0d7c91f2cd6d863483a273ca7f7.jpg",
        "context_before": "In order to further assess the nature of our new representations, we visualized two dimensional Barnes-Hut SNE (van der Maaten, 2013) embeddings of the $\\mathbf { z } _ { 1 }$ representations, obtained from the model trained on the Adult dataset, in Figure 4. As we can see, the nuisance/sensitive variables s can be identified both on the original representation x and on a latent representation $\\mathbf { z } _ { 1 }$ that does not have the MMD penalty and the independence properties between $\\ma",
        "context_after": "3.4 LEARNING INVARIANT REPRESENTATIONS\n\nRegarding the more general task of learning invariant representations; our results on the Extended Yale B dataset also demonstrate our model’s ability to learn such representations. As expected, on the original representation x the lighting conditions, s, are well identifiable with almost perfect accuracy from both RF and LR. This can also be seen in the two dimensional embeddings of the original space x in Figure 5a: the images are mostly clustered accord"
      },
      "edge_contexts": [
        {
          "source": "1511.00830_figure_1",
          "target": "1511.00830_table_1",
          "ref_text": "Table 1",
          "context_snippet": "io and the Amazon reviews dataset, the results of our VFAE model can be seen in Table 1. Our model was successful in factoring out the domain information, since the ac"
        }
      ]
    },
    {
      "pair_id": "1511.00830_pair_2",
      "doc_id": "1511.00830",
      "element_a_id": "1511.00830_figure_2",
      "element_b_id": "1511.00830_table_1",
      "element_a_type": "figure",
      "element_b_type": "table",
      "pair_type": "figure+table",
      "hop_distance": 1,
      "path": [
        "1511.00830_figure_2",
        "1511.00830_table_1"
      ],
      "quality_score": 1.0,
      "overlap_with_existing_l1": false,
      "element_a": {
        "element_id": "1511.00830_figure_2",
        "element_type": "figure",
        "caption": "Figure 2: Semi-supervised model",
        "content": "Figure 2: Semi-supervised model",
        "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1511.00830/1511.00830/hybrid_auto/images/1511.00830_page0_fig1.jpg",
        "context_before": "As for the domain adaptation scenario and the Amazon reviews dataset, the results of our VFAE model can be seen in Table 1. Our model was successful in factoring out the domain information, since the accuracy, measured both linearly (LR) and non-linearly (RF), was towards random chance (which for this dataset is 0.5). We should also mention that, on this dataset at least, completely removing information about the domain does not guarantee a better performance on y. The same effect was also obser",
        "context_after": "2.1 UNSUPERVISED MODEL\n\nFactoring out undesired variations from the data can be easily formulated as a general probabilistic model which admits two distinct (independent) “sources”; an observed variable s, which denotes the variations that we want to remove, and a continuous latent variable z which models all the remaining information. This generative process can be formally defined as:\n\n$$ \\mathbf {z} \\sim p (\\mathbf {z}); \\qquad \\mathbf {x} \\sim p _ {\\theta} (\\mathbf {x} | \\mathbf {z}, \\mathbf"
      },
      "element_b": {
        "element_id": "1511.00830_table_1",
        "element_type": "table",
        "caption": "Table 1: Results on the Amazon reviews dataset. The DANN column is taken directly from Ganin et al. (2015) (the column that uses the original representation as input).",
        "content": "Table 1: Results on the Amazon reviews dataset. The DANN column is taken directly from Ganin et al. (2015) (the column that uses the original representation as input).",
        "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1511.00830/1511.00830/hybrid_auto/images/fdf6c7c916149f26dac10569971359e954c5a0d7c91f2cd6d863483a273ca7f7.jpg",
        "context_before": "In order to further assess the nature of our new representations, we visualized two dimensional Barnes-Hut SNE (van der Maaten, 2013) embeddings of the $\\mathbf { z } _ { 1 }$ representations, obtained from the model trained on the Adult dataset, in Figure 4. As we can see, the nuisance/sensitive variables s can be identified both on the original representation x and on a latent representation $\\mathbf { z } _ { 1 }$ that does not have the MMD penalty and the independence properties between $\\ma",
        "context_after": "3.4 LEARNING INVARIANT REPRESENTATIONS\n\nRegarding the more general task of learning invariant representations; our results on the Extended Yale B dataset also demonstrate our model’s ability to learn such representations. As expected, on the original representation x the lighting conditions, s, are well identifiable with almost perfect accuracy from both RF and LR. This can also be seen in the two dimensional embeddings of the original space x in Figure 5a: the images are mostly clustered accord"
      },
      "edge_contexts": [
        {
          "source": "1511.00830_figure_2",
          "target": "1511.00830_table_1",
          "ref_text": "Table 1",
          "context_snippet": "io and the Amazon reviews dataset, the results of our VFAE model can be seen in Table 1. Our model was successful in factoring out the domain information, since the ac"
        }
      ]
    },
    {
      "pair_id": "1511.00830_pair_4",
      "doc_id": "1511.00830",
      "element_a_id": "1511.00830_figure_4",
      "element_b_id": "1511.00830_table_1",
      "element_a_type": "figure",
      "element_b_type": "table",
      "pair_type": "figure+table",
      "hop_distance": 1,
      "path": [
        "1511.00830_figure_4",
        "1511.00830_table_1"
      ],
      "quality_score": 1.0,
      "overlap_with_existing_l1": false,
      "element_a": {
        "element_id": "1511.00830_figure_4",
        "element_type": "figure",
        "caption": "(d) Figure 4: t-SNE (van der Maaten, 2013) visualizations from the Adult dataset on: (a): original x , (b): latent $\\mathbf { z } _ { 1 }$ without s and MMD, (c): latent $\\mathbf { z } _ { 1 }$ with s and without MMD, (d): latent $\\mathbf { z } _ { 1 }$ with s and MMD. Blue colour corresponds to males whereas red colour corresponds to females.",
        "content": "(d) Figure 4: t-SNE (van der Maaten, 2013) visualizations from the Adult dataset on: (a): original x , (b): latent $\\mathbf { z } _ { 1 }$ without s and MMD, (c): latent $\\mathbf { z } _ { 1 }$ with s and without MMD, (d): latent $\\mathbf { z } _ { 1 }$ with s and MMD. Blue colour corresponds to males whereas red colour corresponds to females.",
        "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1511.00830/1511.00830/hybrid_auto/images/1511.00830_page0_fig14.jpg",
        "context_before": "",
        "context_after": "3.3.2 DOMAIN ADAPTATION\n\nAs for the domain adaptation scenario and the Amazon reviews dataset, the results of our VFAE model can be seen in Table 1. Our model was successful in factoring out the domain information, since the accuracy, measured both linearly (LR) and non-linearly (RF), was towards random chance (which for this dataset is 0.5). We should also mention that, on this dataset at least, completely removing information about the domain does not guarantee a better performance on y. The s"
      },
      "element_b": {
        "element_id": "1511.00830_table_1",
        "element_type": "table",
        "caption": "Table 1: Results on the Amazon reviews dataset. The DANN column is taken directly from Ganin et al. (2015) (the column that uses the original representation as input).",
        "content": "Table 1: Results on the Amazon reviews dataset. The DANN column is taken directly from Ganin et al. (2015) (the column that uses the original representation as input).",
        "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1511.00830/1511.00830/hybrid_auto/images/fdf6c7c916149f26dac10569971359e954c5a0d7c91f2cd6d863483a273ca7f7.jpg",
        "context_before": "In order to further assess the nature of our new representations, we visualized two dimensional Barnes-Hut SNE (van der Maaten, 2013) embeddings of the $\\mathbf { z } _ { 1 }$ representations, obtained from the model trained on the Adult dataset, in Figure 4. As we can see, the nuisance/sensitive variables s can be identified both on the original representation x and on a latent representation $\\mathbf { z } _ { 1 }$ that does not have the MMD penalty and the independence properties between $\\ma",
        "context_after": "3.4 LEARNING INVARIANT REPRESENTATIONS\n\nRegarding the more general task of learning invariant representations; our results on the Extended Yale B dataset also demonstrate our model’s ability to learn such representations. As expected, on the original representation x the lighting conditions, s, are well identifiable with almost perfect accuracy from both RF and LR. This can also be seen in the two dimensional embeddings of the original space x in Figure 5a: the images are mostly clustered accord"
      },
      "edge_contexts": [
        {
          "source": "1511.00830_table_1",
          "target": "1511.00830_figure_4",
          "ref_text": "Figure 4",
          "context_snippet": " 1 }$ representations, obtained from the model trained on the Adult dataset, in Figure 4. As we can see, the nuisance/sensitive variables s can be identified both on th"
        }
      ]
    },
    {
      "pair_id": "1511.00830_pair_5",
      "doc_id": "1511.00830",
      "element_a_id": "1511.00830_table_1",
      "element_b_id": "1511.00830_figure_5",
      "element_a_type": "table",
      "element_b_type": "figure",
      "pair_type": "figure+table",
      "hop_distance": 1,
      "path": [
        "1511.00830_table_1",
        "1511.00830_figure_5"
      ],
      "quality_score": 1.0,
      "overlap_with_existing_l1": false,
      "element_a": {
        "element_id": "1511.00830_table_1",
        "element_type": "table",
        "caption": "Table 1: Results on the Amazon reviews dataset. The DANN column is taken directly from Ganin et al. (2015) (the column that uses the original representation as input).",
        "content": "Table 1: Results on the Amazon reviews dataset. The DANN column is taken directly from Ganin et al. (2015) (the column that uses the original representation as input).",
        "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1511.00830/1511.00830/hybrid_auto/images/fdf6c7c916149f26dac10569971359e954c5a0d7c91f2cd6d863483a273ca7f7.jpg",
        "context_before": "In order to further assess the nature of our new representations, we visualized two dimensional Barnes-Hut SNE (van der Maaten, 2013) embeddings of the $\\mathbf { z } _ { 1 }$ representations, obtained from the model trained on the Adult dataset, in Figure 4. As we can see, the nuisance/sensitive variables s can be identified both on the original representation x and on a latent representation $\\mathbf { z } _ { 1 }$ that does not have the MMD penalty and the independence properties between $\\ma",
        "context_after": "3.4 LEARNING INVARIANT REPRESENTATIONS\n\nRegarding the more general task of learning invariant representations; our results on the Extended Yale B dataset also demonstrate our model’s ability to learn such representations. As expected, on the original representation x the lighting conditions, s, are well identifiable with almost perfect accuracy from both RF and LR. This can also be seen in the two dimensional embeddings of the original space x in Figure 5a: the images are mostly clustered accord"
      },
      "element_b": {
        "element_id": "1511.00830_figure_5",
        "element_type": "figure",
        "caption": "(b) Figure 5: t-SNE (van der Maaten, 2013) visualizations of the Extended Yale B training set. (a): original x , (b): latent $\\mathbf { z } _ { 1 }$ from VFAE. Each example is plotted with the person ID and the image. Zoom in to see details.",
        "content": "(b) Figure 5: t-SNE (van der Maaten, 2013) visualizations of the Extended Yale B training set. (a): original x , (b): latent $\\mathbf { z } _ { 1 }$ from VFAE. Each example is plotted with the person ID and the image. Zoom in to see details.",
        "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1511.00830/1511.00830/hybrid_auto/images/1511.00830_page0_fig16.jpg",
        "context_before": "",
        "context_after": "returns 1 or 0, while $p ( z _ { k } = 1 | x _ { i } , s = 1 )$ returns values between values 0 and 1, then the penalty could still be satisfied, but information could still leak through. We addressed both of these issues in this paper.\n\nDomain adaptation can also be cast as learning representations that are “invariant” with respect to a discrete variable s, the domain. Most similar to our work are neural network approaches which try to match the feature distributions between the domains. This w"
      },
      "edge_contexts": [
        {
          "source": "1511.00830_table_1",
          "target": "1511.00830_figure_5",
          "ref_text": "Figure 5a",
          "context_snippet": "s can also be seen in the two dimensional embeddings of the original space x in Figure 5a: the images are mostly clustered according to the lighting conditions. As soon "
        }
      ]
    },
    {
      "pair_id": "1511.00830_pair_6",
      "doc_id": "1511.00830",
      "element_a_id": "1511.00830_table_2",
      "element_b_id": "1511.00830_figure_5",
      "element_a_type": "table",
      "element_b_type": "figure",
      "pair_type": "figure+table",
      "hop_distance": 1,
      "path": [
        "1511.00830_table_2",
        "1511.00830_figure_5"
      ],
      "quality_score": 1.0,
      "overlap_with_existing_l1": false,
      "element_a": {
        "element_id": "1511.00830_table_2",
        "element_type": "table",
        "caption": "Table 2: Results on the Extended Yale B dataset. We also included the best result from Li et al. (2014) under the $\\mathrm { N N } + \\mathrm { M M D }$ row.",
        "content": "Table 2: Results on the Extended Yale B dataset. We also included the best result from Li et al. (2014) under the $\\mathrm { N N } + \\mathrm { M M D }$ row.",
        "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1511.00830/1511.00830/hybrid_auto/images/445f1437429b794f8f4956a8323d1965c0e90d46c90f124eb2948385f2b41d36.jpg",
        "context_before": "is concerned, we compared against a recent neural network based state of the art method for domain adaptation, Domain Adversarial Neural Network (DANN) (Ganin et al., 2015). As we can observe in table 1, our accuracy on the labels y is higher on 9 out of the 12 domain adaptation tasks whereas on the remaining 3 it is quite similar to the DANN architecture.\n\nAs for the domain adaptation scenario and the Amazon reviews dataset, the results of our VFAE model can be seen in Table 1.\n\n3.4 LEARNING IN",
        "context_after": "Most related to our “fair” representations view is the work from Zemel et al. (2013). They proposed a neural network based semi-supervised clustering model for learning fair representations. The idea is to learn a localised representation that maps each datapoint to a cluster in such a way that each cluster gets assigned roughly equal proportions of data from each group in s. Although their approach was successfully applied on several datasets, the restriction to clustering means that it cannot "
      },
      "element_b": {
        "element_id": "1511.00830_figure_5",
        "element_type": "figure",
        "caption": "(b) Figure 5: t-SNE (van der Maaten, 2013) visualizations of the Extended Yale B training set. (a): original x , (b): latent $\\mathbf { z } _ { 1 }$ from VFAE. Each example is plotted with the person ID and the image. Zoom in to see details.",
        "content": "(b) Figure 5: t-SNE (van der Maaten, 2013) visualizations of the Extended Yale B training set. (a): original x , (b): latent $\\mathbf { z } _ { 1 }$ from VFAE. Each example is plotted with the person ID and the image. Zoom in to see details.",
        "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1511.00830/1511.00830/hybrid_auto/images/1511.00830_page0_fig16.jpg",
        "context_before": "",
        "context_after": "returns 1 or 0, while $p ( z _ { k } = 1 | x _ { i } , s = 1 )$ returns values between values 0 and 1, then the penalty could still be satisfied, but information could still leak through. We addressed both of these issues in this paper.\n\nDomain adaptation can also be cast as learning representations that are “invariant” with respect to a discrete variable s, the domain. Most similar to our work are neural network approaches which try to match the feature distributions between the domains. This w"
      },
      "edge_contexts": [
        {
          "source": "1511.00830_table_2",
          "target": "1511.00830_figure_5",
          "ref_text": "Figure 5a",
          "context_snippet": "s can also be seen in the two dimensional embeddings of the original space x in Figure 5a: the images are mostly clustered according to the lighting conditions. As soon "
        }
      ]
    },
    {
      "pair_id": "1602.05352_pair_1",
      "doc_id": "1602.05352",
      "element_a_id": "1602.05352_figure_2",
      "element_b_id": "1602.05352_table_1",
      "element_a_type": "figure",
      "element_b_type": "table",
      "pair_type": "figure+table",
      "hop_distance": 1,
      "path": [
        "1602.05352_figure_2",
        "1602.05352_table_1"
      ],
      "quality_score": 1.0,
      "overlap_with_existing_l1": false,
      "element_a": {
        "element_id": "1602.05352_figure_2",
        "element_type": "figure",
        "caption": "Figure 2. RMSE of the estimators in the experimental setting as the observed ratings exhibit varying degrees of selection bias.",
        "content": "Figure 2. RMSE of the estimators in the experimental setting as the observed ratings exhibit varying degrees of selection bias.",
        "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1602.05352/1602.05352/hybrid_auto/images/1602.05352_page0_fig3.jpg",
        "context_before": "Table 1 shows the results for estimating rating prediction accuracy via MAE and recommendation quality via $\\operatorname { D C G } @ 5 0$ for the following five prediction matrices $\\hat { Y _ { i } }$ .\n\nTable 1, described in Section 3.4, shows the estimated MAE and DCG@50 when $\\alpha = 0 .\n\nML100K Dataset. The ML100K dataset4 provides 100K MNAR ratings for 1683 movies by 944 users. To allow ground-truth evaluation against a fully known rating matrix, we complete these partial ratings using s",
        "context_after": "unrealistically high ratings to almost all movies. We therefore adjust ratings for the final $Y$ to match a more realistic rating distribution $[ p _ { 1 } , p _ { 2 } , p _ { 3 } , p _ { 4 } , p _ { 5 } ]$ for ratings 1 to 5 as given in Marlin & Zemel (2009) as follows: we assign the bottom $p _ { 1 }$ fraction of the entries by value in the completed matrix a rating of 1, and the next $p _ { 2 }$ fraction of entries by value a rating of 2, and so on. Hyper-parameters (rank $d$ and L2 regulariz"
      },
      "element_b": {
        "element_id": "1602.05352_table_1",
        "element_type": "table",
        "caption": "Table 1. Mean and standard deviation of the Naive, IPS, and SNIPS estimators compared to true MAE and DCG@50 on ML100K.",
        "content": "Table 1. Mean and standard deviation of the Naive, IPS, and SNIPS estimators compared to true MAE and DCG@50 on ML100K.",
        "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1602.05352/1602.05352/hybrid_auto/images/6a23148d20bdf053655c55091ed67546c43bc3309a9ba0257852a4f6561e3706.jpg",
        "context_before": "Table 1 shows the results for estimating rating prediction accuracy via MAE and recommendation quality via $\\operatorname { D C G } @ 5 0$ for the following five prediction matrices $\\hat { Y _ { i } }$ .\n\nTable 1, described in Section 3.4, shows the estimated MAE and DCG@50 when $\\alpha = 0 .\n\nUnlike the naive estimator $\\hat { R } _ { n a i v e } ( \\hat { Y } )$ , the IPS estimator is unbiased for any probabilistic assignment mechanism. Note that the IPS estimator only requires the marginal pr",
        "context_after": "$$ \\begin{array}{l} \\mathbb {E} _ {O} \\Big [ \\hat {R} _ {I P S} (\\hat {Y} | P) \\Big ] = \\frac {1}{U \\cdot I} \\sum_ {u} \\sum_ {i} \\mathbb {E} _ {O _ {u, i}} \\bigg [ \\frac {\\delta_ {u , i} (Y , \\hat {Y})}{P _ {u , i}} O _ {u, i} \\bigg ] \\\\ = \\frac {1}{U \\cdot I} \\sum_ {u} \\sum_ {i} \\delta_ {u, i} (Y, \\hat {Y}) = R (\\hat {Y}). \\\\ \\end{array} $$\n\nTo characterize the variability of the IPS estimator, however, we assume that observations are independent given $P$ , which corresponds to a multivariate "
      },
      "edge_contexts": [
        {
          "source": "1602.05352_table_1",
          "target": "1602.05352_figure_2",
          "ref_text": "Figure 2 r",
          "context_snippet": " we vary the severity of the sampling bias by changing $\\alpha \\in ( 0 , 1 ]$ . Figure 2 reports how accurately (in terms of root mean squared estimation error (RMSE)) ea"
        }
      ]
    },
    {
      "pair_id": "1602.05352_pair_2",
      "doc_id": "1602.05352",
      "element_a_id": "1602.05352_figure_2",
      "element_b_id": "1602.05352_table_2",
      "element_a_type": "figure",
      "element_b_type": "table",
      "pair_type": "figure+table",
      "hop_distance": 1,
      "path": [
        "1602.05352_figure_2",
        "1602.05352_table_2"
      ],
      "quality_score": 1.0,
      "overlap_with_existing_l1": false,
      "element_a": {
        "element_id": "1602.05352_figure_2",
        "element_type": "figure",
        "caption": "Figure 2. RMSE of the estimators in the experimental setting as the observed ratings exhibit varying degrees of selection bias.",
        "content": "Figure 2. RMSE of the estimators in the experimental setting as the observed ratings exhibit varying degrees of selection bias.",
        "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1602.05352/1602.05352/hybrid_auto/images/1602.05352_page0_fig3.jpg",
        "context_before": "Table 1 shows the results for estimating rating prediction accuracy via MAE and recommendation quality via $\\operatorname { D C G } @ 5 0$ for the following five prediction matrices $\\hat { Y _ { i } }$ .\n\nTable 1, described in Section 3.4, shows the estimated MAE and DCG@50 when $\\alpha = 0 .\n\nML100K Dataset. The ML100K dataset4 provides 100K MNAR ratings for 1683 movies by 944 users. To allow ground-truth evaluation against a fully known rating matrix, we complete these partial ratings using s",
        "context_after": "unrealistically high ratings to almost all movies. We therefore adjust ratings for the final $Y$ to match a more realistic rating distribution $[ p _ { 1 } , p _ { 2 } , p _ { 3 } , p _ { 4 } , p _ { 5 } ]$ for ratings 1 to 5 as given in Marlin & Zemel (2009) as follows: we assign the bottom $p _ { 1 }$ fraction of the entries by value in the completed matrix a rating of 1, and the next $p _ { 2 }$ fraction of entries by value a rating of 2, and so on. Hyper-parameters (rank $d$ and L2 regulariz"
      },
      "element_b": {
        "element_id": "1602.05352_table_2",
        "element_type": "table",
        "caption": "Table 2. Test set MAE and MSE on the Yahoo and Coat datasets.",
        "content": "Table 2. Test set MAE and MSE on the Yahoo and Coat datasets.",
        "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1602.05352/1602.05352/hybrid_auto/images/1efd2efeef564cec02953da4278f2598f1363cd1c27830c6d717517d9b0322e9.jpg",
        "context_before": "Figure 4 shows how the quality of the propensity estimates impacts evaluation using the same setup as in Section 6.2. Under no condition do the IPS and SNIPS estimator perform worse than Naive. Interestingly, IPS-NB with estimated propensities can perform even better than IPS-KNOWN with known propensities, as can be seen for MSE. This is a known effect, partly because the estimated propensities can provide an effect akin to stratification (Hirano et al., 2003; Wooldridge, 2007).\n\nFigure 4 shows ",
        "context_after": "models from (Hernandez-Lobato et al. ´ , 2014), abbreviated as HL-MNAR and HL-MAR (paired t-test, $p < 0 . 0 0 1$ for all). This holds for both MAE and MSE. Furthermore, the performance of MF-IPS beats the best published results for Yahoo in terms of MSE (1.115) and is close in terms of MAE (0.770) (the CTP-v model of (Marlin & Zemel, 2009) as reported in the supplementary material of Hernandez- ´ Lobato et al. (2014)). For MF-IPS and MF-Naive all hyperparameters (i.e., $\\lambda \\in \\{ 1 0 ^ { -"
      },
      "edge_contexts": [
        {
          "source": "1602.05352_table_2",
          "target": "1602.05352_figure_2",
          "ref_text": "Figure 2 r",
          "context_snippet": " we vary the severity of the sampling bias by changing $\\alpha \\in ( 0 , 1 ]$ . Figure 2 reports how accurately (in terms of root mean squared estimation error (RMSE)) ea"
        }
      ]
    },
    {
      "pair_id": "1602.05352_pair_3",
      "doc_id": "1602.05352",
      "element_a_id": "1602.05352_table_1",
      "element_b_id": "1602.05352_figure_1",
      "element_a_type": "table",
      "element_b_type": "figure",
      "pair_type": "figure+table",
      "hop_distance": 1,
      "path": [
        "1602.05352_table_1",
        "1602.05352_figure_1"
      ],
      "quality_score": 1.0,
      "overlap_with_existing_l1": false,
      "element_a": {
        "element_id": "1602.05352_table_1",
        "element_type": "table",
        "caption": "Table 1. Mean and standard deviation of the Naive, IPS, and SNIPS estimators compared to true MAE and DCG@50 on ML100K.",
        "content": "Table 1. Mean and standard deviation of the Naive, IPS, and SNIPS estimators compared to true MAE and DCG@50 on ML100K.",
        "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1602.05352/1602.05352/hybrid_auto/images/6a23148d20bdf053655c55091ed67546c43bc3309a9ba0257852a4f6561e3706.jpg",
        "context_before": "Table 1 shows the results for estimating rating prediction accuracy via MAE and recommendation quality via $\\operatorname { D C G } @ 5 0$ for the following five prediction matrices $\\hat { Y _ { i } }$ .\n\nTable 1, described in Section 3.4, shows the estimated MAE and DCG@50 when $\\alpha = 0 .\n\nUnlike the naive estimator $\\hat { R } _ { n a i v e } ( \\hat { Y } )$ , the IPS estimator is unbiased for any probabilistic assignment mechanism. Note that the IPS estimator only requires the marginal pr",
        "context_after": "$$ \\begin{array}{l} \\mathbb {E} _ {O} \\Big [ \\hat {R} _ {I P S} (\\hat {Y} | P) \\Big ] = \\frac {1}{U \\cdot I} \\sum_ {u} \\sum_ {i} \\mathbb {E} _ {O _ {u, i}} \\bigg [ \\frac {\\delta_ {u , i} (Y , \\hat {Y})}{P _ {u , i}} O _ {u, i} \\bigg ] \\\\ = \\frac {1}{U \\cdot I} \\sum_ {u} \\sum_ {i} \\delta_ {u, i} (Y, \\hat {Y}) = R (\\hat {Y}). \\\\ \\end{array} $$\n\nTo characterize the variability of the IPS estimator, however, we assume that observations are independent given $P$ , which corresponds to a multivariate "
      },
      "element_b": {
        "element_id": "1602.05352_figure_1",
        "element_type": "figure",
        "caption": "Figure 1. Movie-Lovers toy example. Top row: true rating matrix $Y$ , propensity matrix $P$ , observation indicator matrix $O$ . Bottom row: two rating prediction matrices $\\hat { Y _ { 1 } }$ and $\\hat { Y } _ { 2 }$ , and intervention indicator matrix $\\hat { Y } _ { 3 }$ .",
        "content": "Figure 1. Movie-Lovers toy example. Top row: true rating matrix $Y$ , propensity matrix $P$ , observation indicator matrix $O$ . Bottom row: two rating prediction matrices $\\hat { Y _ { 1 } }$ and $\\hat { Y } _ { 2 }$ , and intervention indicator matrix $\\hat { Y } _ { 3 }$ .",
        "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1602.05352/1602.05352/hybrid_auto/images/1602.05352_page0_fig2.jpg",
        "context_before": "",
        "context_after": "set of users are “horror lovers” who rate all horror movies 5 and all romance movies 1. Similarly, there is a subset of “romance lovers” who rate just the opposite way. However, both groups rate dramas as 3. The binary matrix ${ \\cal O } \\in \\{ 0 , 1 \\} ^ { \\bar { U } \\times \\bar { I } }$ in Figure 1 shows for which movies the users provided their rating to the system, $\\left[ O _ { u , i } \\right. = 1 ] \\Leftrightarrow$ $[ Y _ { u , i }$ observed]. Our toy example shows a strong correlation bet"
      },
      "edge_contexts": [
        {
          "source": "1602.05352_table_1",
          "target": "1602.05352_figure_1",
          "ref_text": "Figure 1 s",
          "context_snippet": "\\{ 1 , . . . , U \\}$ the users and with $i \\in \\{ 1 , . . . , I \\}$ the movies. Figure 1 shows the matrix of true ratings $\\dot { Y } \\in \\mathfrak { R } ^ { U \\times I }"
        }
      ]
    },
    {
      "pair_id": "1602.05352_pair_7",
      "doc_id": "1602.05352",
      "element_a_id": "1602.05352_table_2",
      "element_b_id": "1602.05352_figure_4",
      "element_a_type": "table",
      "element_b_type": "figure",
      "pair_type": "figure+table",
      "hop_distance": 1,
      "path": [
        "1602.05352_table_2",
        "1602.05352_figure_4"
      ],
      "quality_score": 1.0,
      "overlap_with_existing_l1": false,
      "element_a": {
        "element_id": "1602.05352_table_2",
        "element_type": "table",
        "caption": "Table 2. Test set MAE and MSE on the Yahoo and Coat datasets.",
        "content": "Table 2. Test set MAE and MSE on the Yahoo and Coat datasets.",
        "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1602.05352/1602.05352/hybrid_auto/images/1efd2efeef564cec02953da4278f2598f1363cd1c27830c6d717517d9b0322e9.jpg",
        "context_before": "Figure 4 shows how the quality of the propensity estimates impacts evaluation using the same setup as in Section 6.2. Under no condition do the IPS and SNIPS estimator perform worse than Naive. Interestingly, IPS-NB with estimated propensities can perform even better than IPS-KNOWN with known propensities, as can be seen for MSE. This is a known effect, partly because the estimated propensities can provide an effect akin to stratification (Hirano et al., 2003; Wooldridge, 2007).\n\nFigure 4 shows ",
        "context_after": "models from (Hernandez-Lobato et al. ´ , 2014), abbreviated as HL-MNAR and HL-MAR (paired t-test, $p < 0 . 0 0 1$ for all). This holds for both MAE and MSE. Furthermore, the performance of MF-IPS beats the best published results for Yahoo in terms of MSE (1.115) and is close in terms of MAE (0.770) (the CTP-v model of (Marlin & Zemel, 2009) as reported in the supplementary material of Hernandez- ´ Lobato et al. (2014)). For MF-IPS and MF-Naive all hyperparameters (i.e., $\\lambda \\in \\{ 1 0 ^ { -"
      },
      "element_b": {
        "element_id": "1602.05352_figure_4",
        "element_type": "figure",
        "caption": "Figure 4. RMSE of IPS and SNIPS as propensity estimates degrade. IPS with true propensities and Naive are given as reference.",
        "content": "Figure 4. RMSE of IPS and SNIPS as propensity estimates degrade. IPS with true propensities and Naive are given as reference.",
        "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1602.05352/1602.05352/hybrid_auto/images/1602.05352_page0_fig7.jpg",
        "context_before": "",
        "context_after": "severely degraded propensity estimates, demonstrating the robustness of the approach.\n\n6.5. Performance on Real-World Data\n\nOur final experiment studies performance on real-world datasets. We use the following two datasets, which both have a separate test set where users were asked to rate a uniformly drawn sample of items.\n\nFigure 4 shows how the quality of the propensity estimates impacts evaluation using the same setup as in Section 6.2. Under no condition do the IPS and SNIPS estimator perfo"
      },
      "edge_contexts": [
        {
          "source": "1602.05352_table_2",
          "target": "1602.05352_figure_4",
          "ref_text": "Figure 4 s",
          "context_snippet": "Figure 4 shows how the quality of the propensity estimates impacts evaluation using the sa"
        }
      ]
    },
    {
      "pair_id": "1603.07025_pair_1",
      "doc_id": "1603.07025",
      "element_a_id": "1603.07025_figure_1",
      "element_b_id": "1603.07025_table_1",
      "element_a_type": "figure",
      "element_b_type": "table",
      "pair_type": "figure+table",
      "hop_distance": 1,
      "path": [
        "1603.07025_figure_1",
        "1603.07025_table_1"
      ],
      "quality_score": 1.0,
      "overlap_with_existing_l1": false,
      "element_a": {
        "element_id": "1603.07025_figure_1",
        "element_type": "figure",
        "caption": "Figure 1: Reddit interface when visualizing a submission. This is Patrick Stewart’s “AmA” (ask me anything) in “IAmA” (I am a), a submission where he answers users’ questions in the comments. We can see the most upvoted comment and Patrick’s answer right below.",
        "content": "Figure 1: Reddit interface when visualizing a submission. This is Patrick Stewart’s “AmA” (ask me anything) in “IAmA” (I am a), a submission where he answers users’ questions in the comments. We can see the most upvoted comment and Patrick’s answer right below.",
        "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1603.07025/1603.07025/hybrid_auto/images/1603.07025_page0_fig0.jpg",
        "context_before": "We choose Reddit as our target community for a number of reasons. It has existed since 2005, meaning that there has been ample time for the community to evolve and for differences in user cohorts to appear. Second, it is one of the most popular online communities, allowing different types of contributions—comments and original submissions—across many different subreddits. Third, a number of Reddit users believe that it is, in fact, getting worse over time [1, 14, 20, 46, 50, 67]. Finally, Reddit",
        "context_after": "Each submission can be imagined as the root of a threaded comment tree, in which Redditors can comment on submissions or each other’s comments. Redditors can also vote on both submissions and comments; these votes affect the order in which submissions and comments are displayed and also form the basis of “karma”, a reputation system that tracks how often people upvote a given Redditor’s comments and submissions. We can observe these elements in Figure 1.\n\nTable 1 provides some clues to what migh"
      },
      "element_b": {
        "element_id": "1603.07025_table_1",
        "element_type": "table",
        "caption": "Table 1: Evolution of the average throughout the years for each cohort. Each column here is one cohort and each line is one year in time. Cohorts start generating data in their cohort year, therefore the upper diagonal is blank. On the right column we see the overall average for all users.",
        "content": "Table 1: Evolution of the average throughout the years for each cohort. Each column here is one cohort and each line is one year in time. Cohorts start generating data in their cohort year, therefore the upper diagonal is blank. On the right column we see the overall average for all users.",
        "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1603.07025/1603.07025/hybrid_auto/images/d2650899dadd8b1b66ac80641fc94d692a8f74840e1f041e562ad455e4e411b2.jpg",
        "context_before": "5.1 Comment length drops over time\n\nFigure 6a shows the overall comment length in Reddit over time (the darker line) and the overall length per cohort. Based on the downwards tendency of the overall comment length in Figure 6a, one might hypothesize that users’ commitment to the network is decreasing over time (H3), or that there is some community-wide norm toward shorter commenting (H4).\n\nHowever, this might not be the best way to interpret this information. Figure 6b shows the comment length p",
        "context_after": "Table 1 provides some clues to what might be going on. When we move down the rows, we observe an increasing tendency in each cohort column. It means that the average comment length increases for these users. However, when we move right through the columns, people in later cohorts tend to write less per comment. If we were to average each row, we would still get an overall increasing comment length per year, but that is not what we see in the overall column. What happens here is that the latter c"
      },
      "edge_contexts": [
        {
          "source": "1603.07025_table_1",
          "target": "1603.07025_figure_1",
          "ref_text": "Figure 1",
          "context_snippet": "e a given Redditor’s comments and submissions. We can observe these elements in Figure 1.\n\nTable 1 provides some clues to what might be going on. When we move down the "
        }
      ]
    },
    {
      "pair_id": "1603.07025_pair_2",
      "doc_id": "1603.07025",
      "element_a_id": "1603.07025_table_1",
      "element_b_id": "1603.07025_figure_6",
      "element_a_type": "table",
      "element_b_type": "figure",
      "pair_type": "figure+table",
      "hop_distance": 1,
      "path": [
        "1603.07025_table_1",
        "1603.07025_figure_6"
      ],
      "quality_score": 1.0,
      "overlap_with_existing_l1": false,
      "element_a": {
        "element_id": "1603.07025_table_1",
        "element_type": "table",
        "caption": "Table 1: Evolution of the average throughout the years for each cohort. Each column here is one cohort and each line is one year in time. Cohorts start generating data in their cohort year, therefore the upper diagonal is blank. On the right column we see the overall average for all users.",
        "content": "Table 1: Evolution of the average throughout the years for each cohort. Each column here is one cohort and each line is one year in time. Cohorts start generating data in their cohort year, therefore the upper diagonal is blank. On the right column we see the overall average for all users.",
        "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1603.07025/1603.07025/hybrid_auto/images/d2650899dadd8b1b66ac80641fc94d692a8f74840e1f041e562ad455e4e411b2.jpg",
        "context_before": "5.1 Comment length drops over time\n\nFigure 6a shows the overall comment length in Reddit over time (the darker line) and the overall length per cohort. Based on the downwards tendency of the overall comment length in Figure 6a, one might hypothesize that users’ commitment to the network is decreasing over time (H3), or that there is some community-wide norm toward shorter commenting (H4).\n\nHowever, this might not be the best way to interpret this information. Figure 6b shows the comment length p",
        "context_after": "Table 1 provides some clues to what might be going on. When we move down the rows, we observe an increasing tendency in each cohort column. It means that the average comment length increases for these users. However, when we move right through the columns, people in later cohorts tend to write less per comment. If we were to average each row, we would still get an overall increasing comment length per year, but that is not what we see in the overall column. What happens here is that the latter c"
      },
      "element_b": {
        "element_id": "1603.07025_figure_6",
        "element_type": "figure",
        "caption": "(e) 2012 cohort Figure 6: Figure (a) shows the average comment length over clock time and Figure (b) from the user-referential time. Both figures show the cohorted trends. The overall average length per comment decreases over time, although for any individual cohort, it increases after a sharp initial drop. Figures (c), (d) and (e), similar to Figure 5, show the monthly average comment length for active users in the cohorts of 2010, 2011 and 2012, segmented by the number of years that the user survived in the network. Opposite the analysis for average posts, which showed that low-activity users were the first to leave Reddit, here, people who start out as longer commenters are more likely to leave.",
        "content": "(e) 2012 cohort Figure 6: Figure (a) shows the average comment length over clock time and Figure (b) from the user-referential time. Both figures show the cohorted trends. The overall average length per comment decreases over time, although for any individual cohort, it increases after a sharp initial drop. Figures (c), (d) and (e), similar to Figure 5, show the monthly average comment length for active users in the cohorts of 2010, 2011 and 2012, segmented by the number of years that the user survived in the network. Opposite the analysis for average posts, which showed that low-activity users were the first to leave Reddit, here, people who start out as longer commenters are more likely to leave.",
        "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1603.07025/1603.07025/hybrid_auto/images/1603.07025_page0_fig14.jpg",
        "context_before": "",
        "context_after": "5.1 Comment length drops over time\n\nFigure 6a shows the overall comment length in Reddit over time (the darker line) and the overall length per cohort. Based on the downwards tendency of the overall comment length in Figure 6a, one might hypothesize that users’ commitment to the network is decreasing over time (H3), or that there is some community-wide norm toward shorter commenting (H4).\n\nHowever, this might not be the best way to interpret this information. Figure 6b shows the comment length p"
      },
      "edge_contexts": [
        {
          "source": "1603.07025_figure_6",
          "target": "1603.07025_table_1",
          "ref_text": "Figure 6a ... Table 1",
          "context_snippet": "Table 1 provides some clues to what might be going on. When we move down the rows, we observe an increasing tendency in each cohort column. It means that the average comment length increases for these users. However, when we move right through the columns, people in later cohorts tend to write less "
        }
      ]
    },
    {
      "pair_id": "1607.06520_pair_1",
      "doc_id": "1607.06520",
      "element_a_id": "1607.06520_figure_4",
      "element_b_id": "1607.06520_table_3",
      "element_a_type": "figure",
      "element_b_type": "table",
      "pair_type": "figure+table",
      "hop_distance": 1,
      "path": [
        "1607.06520_figure_4",
        "1607.06520_table_3"
      ],
      "quality_score": 1.0,
      "overlap_with_existing_l1": false,
      "element_a": {
        "element_id": "1607.06520_figure_4",
        "element_type": "figure",
        "caption": "Figure 4: Comparing the bias of two different embeddings–the w2vNEWS and the GloVe web-crawl embedding. In each embedding, the occupation words are projected onto the she-he direction. Each dot corresponds to one occupation word; the gender bias of occupations is highly consistent across embeddings (Spearman $\\rho = 0 . 8 1$ ).",
        "content": "Figure 4: Comparing the bias of two different embeddings–the w2vNEWS and the GloVe web-crawl embedding. In each embedding, the occupation words are projected onto the she-he direction. Each dot corresponds to one occupation word; the gender bias of occupations is highly consistent across embeddings (Spearman $\\rho = 0 . 8 1$ ).",
        "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1607.06520/1607.06520/hybrid_auto/images/1607.06520_page0_fig0.jpg",
        "context_before": "However we do see promising qualitative improvements, as shown in Figure 3 in the softball, football example.\n\nEvalution based on the standard metrics shows that the debiasing does not degrade the utility of the embedding (Table 3).\n\n4 Gender stereotypes in word embeddings\n\nOur first task is to understand the biases present in the word-embedding (i.e. which words are closer to she than to he, etc.) and the extent to which these geometric biases agree with human notion of gender stereotypes. We u",
        "context_after": "stereotypes on occupation words and 2) evaluate whether the embedding produces analogies that are judged to reflect stereotypes by humans. The exploratory analysis of this section will motivate the more rigorous metrics used in the next two sections.\n\nOccupational stereotypes. Figure 1 lists the occupations that are closest to she and to he in the w2vNEWS embeddings. We asked the crowdworkers to evaluate whether an occupation is considered femalestereotypic, male-stereotypic, or neutral. Each oc"
      },
      "element_b": {
        "element_id": "1607.06520_table_3",
        "element_type": "table",
        "caption": "Table 3: The columns show the performance of the original, complete w2vNEWS embedding (“before”) and the debiased w2vNEWS on the standard evaluation metrics measuring coherence and analogy-solving abilities: RG [32], WS [12], MSR-analogy [26]. Higher is better. The results show that the performance does not degrade after debiasing.",
        "content": "Table 3: The columns show the performance of the original, complete w2vNEWS embedding (“before”) and the debiased w2vNEWS on the standard evaluation metrics measuring coherence and analogy-solving abilities: RG [32], WS [12], MSR-analogy [26]. Higher is better. The results show that the performance does not degrade after debiasing.",
        "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1607.06520/1607.06520/hybrid_auto/images/d4ce383fe73dd438d80f253d60f5d7eeecca8bf1c8e6c6d9cc54fe16c7988dd4.jpg",
        "context_before": "H Debiasing the full w2vNEWS embedding.\n\nIn the main text, we focused on the results from a cleaned version of w2vNEWS consisting of 26,377 lower-case words. We have also applied our hard debiasing algorithm to the full w2vNEWS dataset. Evalution based on the standard metrics shows that the debiasing does not degrade the utility of the embedding (Table 3).\n\nH Debiasing the full w2vNEWS embedding.\n\nIn the main text, we focused on the results from a cleaned version of w2vNEWS consisting of 26,377 ",
        "context_after": "Indirect gender bias. The direct bias analyzed above manifests in the relative similarities between genderspecific words and gender neutral words. Gender bias could also affect the relative geometry between gender neutral words themselves. To test this indirect gender bias, we take pairs of words that are gender-neutral, for example softball and football. We project all the occupation words onto the softball − football direction and looked at the extremes words, which are listed in Figure 3. For"
      },
      "edge_contexts": [
        {
          "source": "1607.06520_figure_4",
          "target": "1607.06520_table_3",
          "ref_text": "Table 3",
          "context_snippet": "metrics shows that the debiasing does not degrade the utility of the embedding (Table 3).\n\n4 Gender stereotypes in word embeddings\n\nOur first task is to understand the"
        }
      ]
    },
    {
      "pair_id": "1607.06520_pair_2",
      "doc_id": "1607.06520",
      "element_a_id": "1607.06520_table_11",
      "element_b_id": "1607.06520_figure_4",
      "element_a_type": "table",
      "element_b_type": "figure",
      "pair_type": "figure+table",
      "hop_distance": 1,
      "path": [
        "1607.06520_table_11",
        "1607.06520_figure_4"
      ],
      "quality_score": 1.0,
      "overlap_with_existing_l1": false,
      "element_a": {
        "element_id": "1607.06520_table_11",
        "element_type": "table",
        "caption": "Figure 5: Ten possible word pairs to define gender, ordered by word frequency, along with agreement with two sets of 100 words solicited from the crowd, one with definitional and and one with stereotypical gender associa",
        "content": "Figure 5: Ten possible word pairs to define gender, ordered by word frequency, along with agreement with two sets of 100 words solicited from the crowd, one with definitional and and one with stereotypical gender associa",
        "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1607.06520/1607.06520/hybrid_auto/images/5fffe11f54319b8a82e5fe3d6445383995804ada42456ee8c791e3d41e8d0b63.jpg",
        "context_before": "stereotypes on occupation words and 2) evaluate whether the embedding produces analogies that are judged to reflect stereotypes by humans. The exploratory analysis of this section will motivate the more rigorous metrics used in the next two sections.\n\nOccupational stereotypes. Figure 1 lists the occupations that are closest to she and to he in the w2vNEWS embeddings. We asked the crowdworkers to evaluate whether an occupation is considered femalestereotypic, male-stereotypic, or neutral. Each oc",
        "context_after": "In English as in many languages, there are numerous gender pair terms, and for each we can consider the difference between their embeddings. Before looking at the data, one might imagine that they all had roughly the same vector differences, as in the following caricature:\n\n$$ \\overrightarrow {\\mathrm {g r a n d m o t h e r}} = \\overrightarrow {\\mathrm {w i s e}} + \\overrightarrow {\\mathrm {g a l}} $$\n\n$$ \\overrightarrow {\\text {g r a n d f a t h e r}} = \\overrightarrow {\\text {w i s e}} + \\over"
      },
      "element_b": {
        "element_id": "1607.06520_figure_4",
        "element_type": "figure",
        "caption": "Figure 4: Comparing the bias of two different embeddings–the w2vNEWS and the GloVe web-crawl embedding. In each embedding, the occupation words are projected onto the she-he direction. Each dot corresponds to one occupation word; the gender bias of occupations is highly consistent across embeddings (Spearman $\\rho = 0 . 8 1$ ).",
        "content": "Figure 4: Comparing the bias of two different embeddings–the w2vNEWS and the GloVe web-crawl embedding. In each embedding, the occupation words are projected onto the she-he direction. Each dot corresponds to one occupation word; the gender bias of occupations is highly consistent across embeddings (Spearman $\\rho = 0 . 8 1$ ).",
        "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1607.06520/1607.06520/hybrid_auto/images/1607.06520_page0_fig0.jpg",
        "context_before": "However we do see promising qualitative improvements, as shown in Figure 3 in the softball, football example.\n\nEvalution based on the standard metrics shows that the debiasing does not degrade the utility of the embedding (Table 3).\n\n4 Gender stereotypes in word embeddings\n\nOur first task is to understand the biases present in the word-embedding (i.e. which words are closer to she than to he, etc.) and the extent to which these geometric biases agree with human notion of gender stereotypes. We u",
        "context_after": "stereotypes on occupation words and 2) evaluate whether the embedding produces analogies that are judged to reflect stereotypes by humans. The exploratory analysis of this section will motivate the more rigorous metrics used in the next two sections.\n\nOccupational stereotypes. Figure 1 lists the occupations that are closest to she and to he in the w2vNEWS embeddings. We asked the crowdworkers to evaluate whether an occupation is considered femalestereotypic, male-stereotypic, or neutral. Each oc"
      },
      "edge_contexts": [
        {
          "source": "1607.06520_table_11",
          "target": "1607.06520_figure_4",
          "ref_text": "Figure 4",
          "context_snippet": " GloVe algorithm on a web-crawl corpus [30]. The results are highly consistent (Figure 4), suggesting that gender stereotypes is prevalent across different embeddings a"
        }
      ]
    },
    {
      "pair_id": "1607.06520_pair_6",
      "doc_id": "1607.06520",
      "element_a_id": "1607.06520_table_13",
      "element_b_id": "1607.06520_figure_7",
      "element_a_type": "table",
      "element_b_type": "figure",
      "pair_type": "figure+table",
      "hop_distance": 1,
      "path": [
        "1607.06520_table_13",
        "1607.06520_figure_7"
      ],
      "quality_score": 1.0,
      "overlap_with_existing_l1": false,
      "element_a": {
        "element_id": "1607.06520_table_13",
        "element_type": "table",
        "caption": "Direct Bias. First we used the same analogy generation task as before: for both the hard-debiased and the soft-debiased embeddings, we automatically generated pairs of words that are analogous to she-he and asked crowd-w",
        "content": "Direct Bias. First we used the same analogy generation task as before: for both the hard-debiased and the soft-debiased embeddings, we automatically generated pairs of words that are analogous to she-he and asked crowd-w",
        "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1607.06520/1607.06520/hybrid_auto/images/1521c469efb769ac5c325c60d800e4bf1cc84bb6fe2720ca9c637a031503a94d.jpg",
        "context_before": "Figure 7 illustrates the results of the classifier for separating gender-specific words from gender-neutral words. To make the figure legible, we show a subset of the words. The $x$ -axis correspond to projection of words onto the $\\overrightarrow { \\mathrm { s h e } } - \\overrightarrow { \\mathrm { h e } }$ direction and the $y$ -axis corresponds to the distance from the decision boundary of the trained SVM.\n\nFigure 7 illustrates the results of the classifier for separating gender-specific words",
        "context_after": "Direct Bias. First we used the same analogy generation task as before: for both the hard-debiased and the soft-debiased embeddings, we automatically generated pairs of words that are analogous to she-he and asked crowd-workers to evaluate whether these pairs reflect gender stereotypes. Figure 8 shows the results. On the initial w2vNEWS embedding, 19% of the top 150 analogies were judged as showing gender stereotypes by a majority of the ten workers. After applying our hard debiasing algorithm, o"
      },
      "element_b": {
        "element_id": "1607.06520_figure_7",
        "element_type": "figure",
        "caption": "Figure 7: Selected words projected along two axes: $x$ is a projection onto the difference between the embeddings of the words he and she, and $y$ is a direction learned in the embedding that captures gender neutrality, with gender neutral words above the line and gender specific words below the line. Our hard debiasing algorithm removes the gender pair associations for gender neutral words. In this figure, the words above the horizontal line would all be collapsed to the vertical line.",
        "content": "Figure 7: Selected words projected along two axes: $x$ is a projection onto the difference between the embeddings of the words he and she, and $y$ is a direction learned in the embedding that captures gender neutrality, with gender neutral words above the line and gender specific words below the line. Our hard debiasing algorithm removes the gender pair associations for gender neutral words. In this figure, the words above the horizontal line would all be collapsed to the vertical line.",
        "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1607.06520/1607.06520/hybrid_auto/images/1607.06520_page0_fig3.jpg",
        "context_before": "To identify the gender subspace, we took the ten gender pair difference vectors and computed its principal components (PCs). As Figure 6 shows, there is a single direction that explains the majority of variance in these vectors. The first eigenvalue is significantly larger than the rest. Note that, from the randomness in a finite sample of ten noisy vectors, one expects a decrease in eigenvalues. However, as also illustrated in 6, the decrease one observes due to random sampling is much more gra",
        "context_after": "−−−−→ softball − −−−−−→ football) are shown in the table. Words such as receptionist, waitress and homemaker are closer to softball than football, and the $\\beta$ ’s between these words and softball is substantial (67%, 35%, 38%, respectively). This suggests that the apparent similarity in the embeddings of these words to softball can be largely explained by gender biases in the embedding. Similarly, businessman and maestro are closer to football and this can also be attributed largely to indire"
      },
      "edge_contexts": [
        {
          "source": "1607.06520_table_13",
          "target": "1607.06520_figure_7",
          "ref_text": "Figure 7 i",
          "context_snippet": "Figure 7 illustrates the results of the classifier for separating gender-specific words fr"
        }
      ]
    },
    {
      "pair_id": "1610.07524_pair_3",
      "doc_id": "1610.07524",
      "element_a_id": "1610.07524_table_1",
      "element_b_id": "1610.07524_figure_1",
      "element_a_type": "table",
      "element_b_type": "figure",
      "pair_type": "figure+table",
      "hop_distance": 1,
      "path": [
        "1610.07524_table_1",
        "1610.07524_figure_1"
      ],
      "quality_score": 1.0,
      "overlap_with_existing_l1": false,
      "element_a": {
        "element_id": "1610.07524_table_1",
        "element_type": "table",
        "caption": "It is easily verified that test fairness of $S$ implies that the positive predictive value of the coarsened score $S _ { c }$ does not depend on $R$ . More precisely, it implies that that the quantity",
        "content": "It is easily verified that test fairness of $S$ implies that the positive predictive value of the coarsened score $S _ { c }$ does not depend on $R$ . More precisely, it implies that that the quantity",
        "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1610.07524/1610.07524/hybrid_auto/images/09498987ca3b129ebf2c38faa64e216d91beb00ee6d9dfb69e971601979d1f6c.jpg",
        "context_before": "by thresholding $S$ at some cutoff $s H R$\n\n$$ S _ {c} (x) \\equiv \\left\\{ \\begin{array}{l l} \\mathrm {H R} & \\text {i f} S (x) > s _ {H R} \\\\ \\mathrm {L R} & \\text {i f} S (x) \\leq s _ {H R} \\end{array} \\right. \\tag {2.2} $$\n\nThe coarsened score simply assesses each defendant as being at high-risk or low-risk of recidivism. For the purpose of our discussion, we will think of $S _ { c }$ as a classifier used to predict the binary outcome $Y$ . This allows us to summarize $S _ { c }$ in terms of a",
        "context_after": "It is easily verified that test fairness of $S$ implies that the positive predictive value of the coarsened score $S _ { c }$ does not depend on $R$ . More precisely, it implies that that the quantity\n\n$$ \\operatorname {P P V} \\left(S _ {c} \\mid R = r\\right) \\equiv \\mathbb {P} (Y = 1 \\mid S _ {c} = \\mathrm {H R}, R = r) \\tag {2.3} $$\n\ndoes not depend on $r$ . Equation (2.3) thus forms a necessary condition for the test fairness of $S$ . We can think of this as a constraint on the values of the c"
      },
      "element_b": {
        "element_id": "1610.07524_figure_1",
        "element_type": "figure",
        "caption": "Figure 1: Plot shows $\\mathbb { P } ( Y = 1 \\mid S = s , R )$ for the COM-PAS decile score, with $R \\in \\{ \\mathrm { B l a c k } , \\mathrm { W h i t e } \\}$ . Error bars represent 95% confidence intervals.",
        "content": "Figure 1: Plot shows $\\mathbb { P } ( Y = 1 \\mid S = s , R )$ for the COM-PAS decile score, with $R \\in \\{ \\mathrm { B l a c k } , \\mathrm { W h i t e } \\}$ . Error bars represent 95% confidence intervals.",
        "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1610.07524/1610.07524/hybrid_auto/images/1610.07524_page0_fig0.jpg",
        "context_before": "Figure 1 shows a plot of the observed recidivism rates across all possible values of the COMPAS score. We can see that the COMPAS RPI appears to adhere well to the test fairness condition. In their response to the ProPublica investigation, Flores et al. [10] further verify this adherence using logistic regression.\n\n2.1 Implied constraints on the false positive and false negative rates\n\nTo facilitate a simpler discussion of error rates, we introduce the coarsened score $S _ { c }$ , which is obta",
        "context_after": "by thresholding $S$ at some cutoff $s H R$\n\n$$ S _ {c} (x) \\equiv \\left\\{ \\begin{array}{l l} \\mathrm {H R} & \\text {i f} S (x) > s _ {H R} \\\\ \\mathrm {L R} & \\text {i f} S (x) \\leq s _ {H R} \\end{array} \\right. \\tag {2.2} $$\n\nThe coarsened score simply assesses each defendant as being at high-risk or low-risk of recidivism. For the purpose of our discussion, we will think of $S _ { c }$ as a classifier used to predict the binary outcome $Y$ . This allows us to summarize $S _ { c }$ in terms of a"
      },
      "edge_contexts": [
        {
          "source": "1610.07524_table_1",
          "target": "1610.07524_figure_1",
          "ref_text": "Figure 1 s",
          "context_snippet": "ws us to summarize $S _ { c }$ in terms of a confusion matrix, as shown below.\n\nFigure 1 shows a plot of the observed recidivism rates across all possible values of the C"
        }
      ]
    },
    {
      "pair_id": "1610.08452_pair_3",
      "doc_id": "1610.08452",
      "element_a_id": "1610.08452_figure_2",
      "element_b_id": "1610.08452_table_2",
      "element_a_type": "figure",
      "element_b_type": "table",
      "pair_type": "figure+table",
      "hop_distance": 1,
      "path": [
        "1610.08452_figure_2",
        "1610.08452_table_2"
      ],
      "quality_score": 1.0,
      "overlap_with_existing_l1": false,
      "element_a": {
        "element_id": "1610.08452_figure_2",
        "element_type": "figure",
        "caption": "(c) Boundaries Figure 2: [Synthetic data] Panel (a) shows that decreasing the covariance threshold causes the false positive rates for both groups to become similar. Panel (b) shows that an increasing degree of fairness corresponds to a steady decrease in accuracy. Panel (c) shows the original decision boundary (solid line) and fair decision boundary (dashed line), along with corresponding accuracy and false positive rates for groups $z = 0$ (crosses) and $z = 1$ (circles). Fairness constraints cause the original decision boundary to rotate such that previously misclassified examples with $z = 0$ are moved into the negative class (decreasing false positives), while well-classified examples with $z = 1$ are moved into the positive class (increasing false positives), leading to equal false positive rates for both groups.",
        "content": "(c) Boundaries Figure 2: [Synthetic data] Panel (a) shows that decreasing the covariance threshold causes the false positive rates for both groups to become similar. Panel (b) shows that an increasing degree of fairness corresponds to a steady decrease in accuracy. Panel (c) shows the original decision boundary (solid line) and fair decision boundary (dashed line), along with corresponding accuracy and false positive rates for groups $z = 0$ (crosses) and $z = 1$ (circles). Fairness constraints cause the original decision boundary to rotate such that previously misclassified examples with $z = 0$ are moved into the negative class (decreasing false positives), while well-classified examples with $z = 1$ are moved into the positive class (increasing false positives), leading to equal false positive rates for both groups.",
        "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1610.08452/1610.08452/hybrid_auto/images/1610.08452_page0_fig3.jpg",
        "context_before": "",
        "context_after": "(dashed). In this figure, we observe that: i) as the fairness constraint value $c = m c ^ { * }$ goes to zero, the false positive rates for both groups $z = 0$ and $z = 1$ ) converge, and hence, the outcomes of the classifier become more fair, i.e., $D _ { F P R } 0$ , while $D _ { F N R }$ remains close to zero (the invariance of $D _ { F N R }$ may however change depending on the underlying distribution of the data); ii) ensuring lower values of disparate mistreatment leads to a larger drop in"
      },
      "element_b": {
        "element_id": "1610.08452_table_2",
        "element_type": "table",
        "caption": "Table 2: Performance of different methods while removing disparate mistreatment with respect to false positive rate, false negative rate and both.",
        "content": "Table 2: Performance of different methods while removing disparate mistreatment with respect to false positive rate, false negative rate and both.",
        "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1610.08452/1610.08452/hybrid_auto/images/8a1c20185e7441e1887d712ed58fd47c4e719d3a989512597c1c13d1952e0d7c.jpg",
        "context_before": "",
        "context_after": "Results. Figure 2 summarizes the results for this scenario by showing (a) the relation between decision-boundary covariance and the false positive rates for both sensitive attribute values; (b) the trade-off between accuracy and fairness; and (c) the decision boundaries for both the unconstrained classifier (solid) and the fair constrained classifier\n\nComparison results. Table 2 shows the performance comparison for all the methods on the three synthetic datasets described above. We can observe t"
      },
      "edge_contexts": [
        {
          "source": "1610.08452_table_2",
          "target": "1610.08452_figure_2",
          "ref_text": "Figure 2 s",
          "context_snippet": "Results. Figure 2 summarizes the results for this scenario by showing (a) the relation between deci"
        }
      ]
    },
    {
      "pair_id": "1610.08452_pair_4",
      "doc_id": "1610.08452",
      "element_a_id": "1610.08452_figure_3",
      "element_b_id": "1610.08452_table_3",
      "element_a_type": "figure",
      "element_b_type": "table",
      "pair_type": "figure+table",
      "hop_distance": 1,
      "path": [
        "1610.08452_figure_3",
        "1610.08452_table_3"
      ],
      "quality_score": 1.0,
      "overlap_with_existing_l1": false,
      "element_a": {
        "element_id": "1610.08452_figure_3",
        "element_type": "figure",
        "caption": "Figure 3: [Synthetic data] $D _ { F P R }$ and $D _ { F N R }$ have opposite signs. Removing disparate mistreatment on FPR can potentially help remove disparate mistreatment on FNR. Removing disparate mistreatment on both at the same time leads to very similar results. (a) FPR constraints",
        "content": "Figure 3: [Synthetic data] $D _ { F P R }$ and $D _ { F N R }$ have opposite signs. Removing disparate mistreatment on FPR can potentially help remove disparate mistreatment on FNR. Removing disparate mistreatment on both at the same time leads to very similar results. (a) FPR constraints",
        "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1610.08452/1610.08452/hybrid_auto/images/1610.08452_page0_fig7.jpg",
        "context_before": "",
        "context_after": "Results. Figure 3 summarizes the results for this scenario by showing the decision boundaries for the unconstrained classifier (solid) and the constrained fair classifiers. Here, we can observe several interesting patterns. First, removing disparate mistreatment on only false positive rate causes a rotation in the decision boundary to move previously misclassified examples with $z = 1$ into the negative class, decreasing their false positive rate. However, in the process, it also moves previousl"
      },
      "element_b": {
        "element_id": "1610.08452_table_3",
        "element_type": "table",
        "caption": "Table 3: Recidivism rates in ProPublica COMPAS data for both races.",
        "content": "Table 3: Recidivism rates in ProPublica COMPAS data for both races.",
        "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1610.08452/1610.08452/hybrid_auto/images/0f6b9336e99bb68f8d1513846a95d0aaabaed0b5a0261aca4fc29f6f8844e9f8.jpg",
        "context_before": "Figure 2 summarizes the results for this scenario by showing (a) the relation between decision-boundary covariance and the false positive rates for both sensitive attribute values; (b) the trade-off between accuracy and fairness; and (c) the decision boundaries for both the unconstrained classifier (solid) and the fair constrained classifier\n\nTable 2 shows the performance comparison for all the methods on the three synthetic datasets described above.\n\nIn scenarios with sufficiently large trainin",
        "context_after": "Results. Figure 3 summarizes the results for this scenario by showing the decision boundaries for the unconstrained classifier (solid) and the constrained fair classifiers. Here, we can observe several interesting patterns. First, removing disparate mistreatment on only false positive rate causes a rotation in the decision boundary to move previously misclassified examples with $z = 1$ into the negative class, decreasing their false positive rate. However, in the process, it also moves previousl"
      },
      "edge_contexts": [
        {
          "source": "1610.08452_table_3",
          "target": "1610.08452_figure_3",
          "ref_text": "Figure 3",
          "context_snippet": "/td><td>0.02</td></tr></table>\n\n<table><tr><td rowspan=\"4\">Synthetic setting 2\n(Figure 3)</td><td>Our method</td></tr><tr><td>Our method sen</td></tr><tr><td>Baseline</"
        }
      ]
    },
    {
      "pair_id": "1610.08452_pair_5",
      "doc_id": "1610.08452",
      "element_a_id": "1610.08452_table_3",
      "element_b_id": "1610.08452_figure_2",
      "element_a_type": "table",
      "element_b_type": "figure",
      "pair_type": "figure+table",
      "hop_distance": 1,
      "path": [
        "1610.08452_table_3",
        "1610.08452_figure_2"
      ],
      "quality_score": 1.0,
      "overlap_with_existing_l1": false,
      "element_a": {
        "element_id": "1610.08452_table_3",
        "element_type": "table",
        "caption": "Table 3: Recidivism rates in ProPublica COMPAS data for both races.",
        "content": "Table 3: Recidivism rates in ProPublica COMPAS data for both races.",
        "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1610.08452/1610.08452/hybrid_auto/images/0f6b9336e99bb68f8d1513846a95d0aaabaed0b5a0261aca4fc29f6f8844e9f8.jpg",
        "context_before": "Figure 2 summarizes the results for this scenario by showing (a) the relation between decision-boundary covariance and the false positive rates for both sensitive attribute values; (b) the trade-off between accuracy and fairness; and (c) the decision boundaries for both the unconstrained classifier (solid) and the fair constrained classifier\n\nTable 2 shows the performance comparison for all the methods on the three synthetic datasets described above.\n\nIn scenarios with sufficiently large trainin",
        "context_after": "Results. Figure 3 summarizes the results for this scenario by showing the decision boundaries for the unconstrained classifier (solid) and the constrained fair classifiers. Here, we can observe several interesting patterns. First, removing disparate mistreatment on only false positive rate causes a rotation in the decision boundary to move previously misclassified examples with $z = 1$ into the negative class, decreasing their false positive rate. However, in the process, it also moves previousl"
      },
      "element_b": {
        "element_id": "1610.08452_figure_2",
        "element_type": "figure",
        "caption": "(c) Boundaries Figure 2: [Synthetic data] Panel (a) shows that decreasing the covariance threshold causes the false positive rates for both groups to become similar. Panel (b) shows that an increasing degree of fairness corresponds to a steady decrease in accuracy. Panel (c) shows the original decision boundary (solid line) and fair decision boundary (dashed line), along with corresponding accuracy and false positive rates for groups $z = 0$ (crosses) and $z = 1$ (circles). Fairness constraints cause the original decision boundary to rotate such that previously misclassified examples with $z = 0$ are moved into the negative class (decreasing false positives), while well-classified examples with $z = 1$ are moved into the positive class (increasing false positives), leading to equal false positive rates for both groups.",
        "content": "(c) Boundaries Figure 2: [Synthetic data] Panel (a) shows that decreasing the covariance threshold causes the false positive rates for both groups to become similar. Panel (b) shows that an increasing degree of fairness corresponds to a steady decrease in accuracy. Panel (c) shows the original decision boundary (solid line) and fair decision boundary (dashed line), along with corresponding accuracy and false positive rates for groups $z = 0$ (crosses) and $z = 1$ (circles). Fairness constraints cause the original decision boundary to rotate such that previously misclassified examples with $z = 0$ are moved into the negative class (decreasing false positives), while well-classified examples with $z = 1$ are moved into the positive class (increasing false positives), leading to equal false positive rates for both groups.",
        "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1610.08452/1610.08452/hybrid_auto/images/1610.08452_page0_fig3.jpg",
        "context_before": "",
        "context_after": "(dashed). In this figure, we observe that: i) as the fairness constraint value $c = m c ^ { * }$ goes to zero, the false positive rates for both groups $z = 0$ and $z = 1$ ) converge, and hence, the outcomes of the classifier become more fair, i.e., $D _ { F P R } 0$ , while $D _ { F N R }$ remains close to zero (the invariance of $D _ { F N R }$ may however change depending on the underlying distribution of the data); ii) ensuring lower values of disparate mistreatment leads to a larger drop in"
      },
      "edge_contexts": [
        {
          "source": "1610.08452_table_3",
          "target": "1610.08452_figure_2",
          "ref_text": "Figure 2 s",
          "context_snippet": "Figure 2 summarizes the results for this scenario by showing (a) the relation between deci"
        }
      ]
    },
    {
      "pair_id": "1610.08452_pair_6",
      "doc_id": "1610.08452",
      "element_a_id": "1610.08452_table_3",
      "element_b_id": "1610.08452_figure_4",
      "element_a_type": "table",
      "element_b_type": "figure",
      "pair_type": "figure+table",
      "hop_distance": 1,
      "path": [
        "1610.08452_table_3",
        "1610.08452_figure_4"
      ],
      "quality_score": 1.0,
      "overlap_with_existing_l1": false,
      "element_a": {
        "element_id": "1610.08452_table_3",
        "element_type": "table",
        "caption": "Table 3: Recidivism rates in ProPublica COMPAS data for both races.",
        "content": "Table 3: Recidivism rates in ProPublica COMPAS data for both races.",
        "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1610.08452/1610.08452/hybrid_auto/images/0f6b9336e99bb68f8d1513846a95d0aaabaed0b5a0261aca4fc29f6f8844e9f8.jpg",
        "context_before": "Figure 2 summarizes the results for this scenario by showing (a) the relation between decision-boundary covariance and the false positive rates for both sensitive attribute values; (b) the trade-off between accuracy and fairness; and (c) the decision boundaries for both the unconstrained classifier (solid) and the fair constrained classifier\n\nTable 2 shows the performance comparison for all the methods on the three synthetic datasets described above.\n\nIn scenarios with sufficiently large trainin",
        "context_after": "Results. Figure 3 summarizes the results for this scenario by showing the decision boundaries for the unconstrained classifier (solid) and the constrained fair classifiers. Here, we can observe several interesting patterns. First, removing disparate mistreatment on only false positive rate causes a rotation in the decision boundary to move previously misclassified examples with $z = 1$ into the negative class, decreasing their false positive rate. However, in the process, it also moves previousl"
      },
      "element_b": {
        "element_id": "1610.08452_figure_4",
        "element_type": "figure",
        "caption": "(c) Both constraints Figure 4: [Synthetic data] $D _ { F P R }$ and $D _ { F N R }$ have the same sign. Removing disparate mistreatment on FPR can potentially increase disparate mistreatment on FNR. Removing disparate mistreatment on both at the same time causes a larger drop in accuracy.",
        "content": "(c) Both constraints Figure 4: [Synthetic data] $D _ { F P R }$ and $D _ { F N R }$ have the same sign. Removing disparate mistreatment on FPR can potentially increase disparate mistreatment on FNR. Removing disparate mistreatment on both at the same time causes a larger drop in accuracy.",
        "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1610.08452/1610.08452/hybrid_auto/images/1610.08452_page0_fig9.jpg",
        "context_before": "",
        "context_after": "observe several interesting patterns. First, controlling disparate mistreatment for only false positive rate (false negative rate), leads to a minor drop in accuracy, but can exacerbate the disparate mistreatment on false negative rate (false positive rate). For example, while the decision boundary is moved to control for disparate mistreatment on false negative rate, that is, to ensure that more examples with $z = 0$ are well-classified in the positive class (reducing false negative rate), it a"
      },
      "edge_contexts": [
        {
          "source": "1610.08452_table_3",
          "target": "1610.08452_figure_4",
          "ref_text": "Figure 4",
          "context_snippet": "td><td>-0.01</td></tr></table>\n\n<table><tr><td rowspan=\"4\">Synthetic setting 3\n(Figure 4)</td><td>Our method</td></tr><tr><td>Our method Sen</td></tr><tr><td>Baseline</"
        }
      ]
    },
    {
      "pair_id": "1610.08452_pair_7",
      "doc_id": "1610.08452",
      "element_a_id": "1610.08452_table_4",
      "element_b_id": "1610.08452_figure_3",
      "element_a_type": "table",
      "element_b_type": "figure",
      "pair_type": "figure+table",
      "hop_distance": 1,
      "path": [
        "1610.08452_table_4",
        "1610.08452_figure_3"
      ],
      "quality_score": 1.0,
      "overlap_with_existing_l1": false,
      "element_a": {
        "element_id": "1610.08452_table_4",
        "element_type": "table",
        "caption": "Table 4: Description of features used from ProPublica COMPAS data.",
        "content": "Table 4: Description of features used from ProPublica COMPAS data.",
        "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1610.08452/1610.08452/hybrid_auto/images/b552bcd7b68dcb83e63a36f7b08a042de108a11f990805b90d1e2346d6abb749.jpg",
        "context_before": "In this analysis, for simplicity, we only consider a subset of offenders whose race was either black or white. Recidivism rates for the two groups are shown in Table 3.\n\nFigure 3 summarizes the results for this scenario by showing the decision boundaries for the unconstrained classifier (solid) and the constrained fair classifiers.\n\nIn scenarios with sufficiently large training datasets, we expect more reliable estimates of covariance,\n\n<table><tr><td rowspan=\"4\">Synthetic setting 1\n(Figure 2)</",
        "context_after": "and hence, a better performance from our method. On the other hand, the method by Hardt et al. is able to achieve both zero $D _ { F P R }$ and $D _ { F N R }$ while controlling for disparate mistreatment on both false positive and false negative rates (Table 2)—albeit at a considerable drop in terms of accuracy. Since this method operates on a data of much smaller dimensionality (the final classifier probability estimates), it is not expected to suffer as much from the small size of the dataset"
      },
      "element_b": {
        "element_id": "1610.08452_figure_3",
        "element_type": "figure",
        "caption": "Figure 3: [Synthetic data] $D _ { F P R }$ and $D _ { F N R }$ have opposite signs. Removing disparate mistreatment on FPR can potentially help remove disparate mistreatment on FNR. Removing disparate mistreatment on both at the same time leads to very similar results. (a) FPR constraints",
        "content": "Figure 3: [Synthetic data] $D _ { F P R }$ and $D _ { F N R }$ have opposite signs. Removing disparate mistreatment on FPR can potentially help remove disparate mistreatment on FNR. Removing disparate mistreatment on both at the same time leads to very similar results. (a) FPR constraints",
        "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1610.08452/1610.08452/hybrid_auto/images/1610.08452_page0_fig7.jpg",
        "context_before": "",
        "context_after": "Results. Figure 3 summarizes the results for this scenario by showing the decision boundaries for the unconstrained classifier (solid) and the constrained fair classifiers. Here, we can observe several interesting patterns. First, removing disparate mistreatment on only false positive rate causes a rotation in the decision boundary to move previously misclassified examples with $z = 1$ into the negative class, decreasing their false positive rate. However, in the process, it also moves previousl"
      },
      "edge_contexts": [
        {
          "source": "1610.08452_table_4",
          "target": "1610.08452_figure_3",
          "ref_text": "Figure 3 s",
          "context_snippet": "ther black or white. Recidivism rates for the two groups are shown in Table 3.\n\nFigure 3 summarizes the results for this scenario by showing the decision boundaries for t"
        }
      ]
    },
    {
      "pair_id": "1611.07438_pair_1",
      "doc_id": "1611.07438",
      "element_a_id": "1611.07438_table_1",
      "element_b_id": "1611.07438_figure_1",
      "element_a_type": "table",
      "element_b_type": "figure",
      "pair_type": "figure+table",
      "hop_distance": 1,
      "path": [
        "1611.07438_table_1",
        "1611.07438_figure_1"
      ],
      "quality_score": 1.0,
      "overlap_with_existing_l1": false,
      "element_a": {
        "element_id": "1611.07438_table_1",
        "element_type": "table",
        "caption": "Table 1: Summary statistics of Example 1.",
        "content": "Table 1: Summary statistics of Example 1.",
        "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1611.07438/1611.07438/hybrid_auto/images/125d9c1213112fd86b7b711a5de0fbb87973b1b695431ae92b73629bd5b46374.jpg",
        "context_before": "Achieving Non-Discrimination in Data Release\n\nDiscrimination discovery and prevention/removal are increasingly important tasks in data mining. Discrimination discovery aims to unveil discriminatory practices on the protected attribute (e.g., gender) by analyzing the dataset of historical decision records, and discrimination prevention aims to remove discrimination by modifying the biased data before conducting predictive analysis. In this paper, we show that the key to discrimination discovery a",
        "context_after": "tected attribute, and admission is the decision. We assume there is no correlation between gender and test score. The summary statistics of the admission rate is shown in Table 1. It can be observed that the average admission rate is $37 \\%$ for females and $46 \\%$ for males. It is already known that the judgment of discrimination cannot be made simply based on the average admission rates in the whole population and further partitioning is needed. If we partition the data conditioning on test sc"
      },
      "element_b": {
        "element_id": "1611.07438_figure_1",
        "element_type": "figure",
        "caption": "Figure 1: Causal graph of an example university admission system.",
        "content": "Figure 1: Causal graph of an example university admission system.",
        "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1611.07438/1611.07438/hybrid_auto/images/1611.07438_page0_fig0.jpg",
        "context_before": "For the example shown in Table 2, although examining {major} shows no discriminatory effect, when examining {major,test score} we observe $| \\Delta P | _ { \\mathrm { \\{ m a t h , B \\} } } | = 0 .\n\nThe constructed causal graph is shown in Figure 2.\n\nWe make use of the above connection to identify the direct causal effect of $C$ on $E$ . We construct a new DAG $\\scriptstyle { G ^ { \\prime } }$ by deleting the arc $C E$ from $\\mathcal { G }$ and keeping everything else unchanged. Thus, the possible",
        "context_after": "give the following theorem. The proof follows the above analysis.\n\nTheorem 2.1. A node set B forms a meaningful partition for measuring discrimination $i f$ and only if B is a block set, i.e., B satisfies: (1) $( C \\texttt { \\ \" } \\bot { E } | \\texttt { B } ) _ { \\mathcal { G } ^ { \\prime } }$ holds; (2) B contains none of $E$ ’s decedents, where $\\scriptstyle { G ^ { \\prime } }$ is the graph constructed by deleting arc $C \\ \\ E$ from $\\mathcal { G }$ . Discriminatory effect is considered to pre"
      },
      "edge_contexts": [
        {
          "source": "1611.07438_figure_1",
          "target": "1611.07438_table_1",
          "ref_text": "Table 1",
          "context_snippet": "gender and test score. The summary statistics of the admission rate is shown in Table 1. It can be observed that the average admission rate is $37 \\%$ for females and "
        }
      ]
    },
    {
      "pair_id": "1611.07438_pair_2",
      "doc_id": "1611.07438",
      "element_a_id": "1611.07438_table_2",
      "element_b_id": "1611.07438_figure_1",
      "element_a_type": "table",
      "element_b_type": "figure",
      "pair_type": "figure+table",
      "hop_distance": 1,
      "path": [
        "1611.07438_table_2",
        "1611.07438_figure_1"
      ],
      "quality_score": 1.0,
      "overlap_with_existing_l1": false,
      "element_a": {
        "element_id": "1611.07438_table_2",
        "element_type": "table",
        "caption": "Table 2: Summary statistics of Example 2.",
        "content": "Table 2: Summary statistics of Example 2.",
        "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1611.07438/1611.07438/hybrid_auto/images/7f7397e73e6c5668f821f78d526c1b92de2e41da8da2858fa7326406964ecc6a.jpg",
        "context_before": "tected attribute, and admission is the decision. We assume there is no correlation between gender and test score. The summary statistics of the admission rate is shown in Table 1. It can be observed that the average admission rate is $37 \\%$ for females and $46 \\%$ for males. It is already known that the judgment of discrimination cannot be made simply based on the average admission rates in the whole population and further partitioning is needed. If we partition the data conditioning on test sc",
        "context_after": "For a quantitative measurement of discrimination, a general legal principle is to measure the difference in the proportion of positive decisions between the protected group and non-protected group [24]. Discovering and preventing discrimination is not trivial. As shown by Zliobait ˇ e et ˙ al. [28], simply considering the difference measured in the whole population fails to take into account the part of differences that are explainable by other attributes, and removing all the differences will r"
      },
      "element_b": {
        "element_id": "1611.07438_figure_1",
        "element_type": "figure",
        "caption": "Figure 1: Causal graph of an example university admission system.",
        "content": "Figure 1: Causal graph of an example university admission system.",
        "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1611.07438/1611.07438/hybrid_auto/images/1611.07438_page0_fig0.jpg",
        "context_before": "For the example shown in Table 2, although examining {major} shows no discriminatory effect, when examining {major,test score} we observe $| \\Delta P | _ { \\mathrm { \\{ m a t h , B \\} } } | = 0 .\n\nThe constructed causal graph is shown in Figure 2.\n\nWe make use of the above connection to identify the direct causal effect of $C$ on $E$ . We construct a new DAG $\\scriptstyle { G ^ { \\prime } }$ by deleting the arc $C E$ from $\\mathcal { G }$ and keeping everything else unchanged. Thus, the possible",
        "context_after": "give the following theorem. The proof follows the above analysis.\n\nTheorem 2.1. A node set B forms a meaningful partition for measuring discrimination $i f$ and only if B is a block set, i.e., B satisfies: (1) $( C \\texttt { \\ \" } \\bot { E } | \\texttt { B } ) _ { \\mathcal { G } ^ { \\prime } }$ holds; (2) B contains none of $E$ ’s decedents, where $\\scriptstyle { G ^ { \\prime } }$ is the graph constructed by deleting arc $C \\ \\ E$ from $\\mathcal { G }$ . Discriminatory effect is considered to pre"
      },
      "edge_contexts": [
        {
          "source": "1611.07438_figure_1",
          "target": "1611.07438_table_2",
          "ref_text": "Table 2",
          "context_snippet": "For the example shown in Table 2, although examining {major} shows no discriminatory effect, when examining {maj"
        }
      ]
    },
    {
      "pair_id": "1611.07438_pair_3",
      "doc_id": "1611.07438",
      "element_a_id": "1611.07438_table_3",
      "element_b_id": "1611.07438_figure_1",
      "element_a_type": "table",
      "element_b_type": "figure",
      "pair_type": "figure+table",
      "hop_distance": 1,
      "path": [
        "1611.07438_table_3",
        "1611.07438_figure_1"
      ],
      "quality_score": 1.0,
      "overlap_with_existing_l1": false,
      "element_a": {
        "element_id": "1611.07438_table_3",
        "element_type": "table",
        "caption": "Table 3: Contingency table within subpopulation q.",
        "content": "Table 3: Contingency table within subpopulation q.",
        "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1611.07438/1611.07438/hybrid_auto/images/70ad0c655da5ed8499636b5de3224e6deb769b0861f3a77939cc7f55a89a4053.jpg",
        "context_before": "We use the illustrative examples in Section 1 to show how the criterion works. The causal graph of the examples is shown in Figure 1. There are two block sets in this graph: {major}, and {major,test score}. Note that test score alone is not a block set. That is why conditioning on it will produce misleading results. For the example shown in Table 1, examining both block sets shows no discriminatory effect. Thus, non-discrimination can be claimed. For the example shown in Table 2, although examin",
        "context_after": "To calculate the number of tuples to be modified within each subpopulation q, we express $\\Delta P \\vert _ { \\mathbf { q } }$ as $n _ { \\mathbf { q } } ^ { c ^ { + } e ^ { + } } / n _ { \\mathbf { q } } ^ { c ^ { + } } -$ $n _ { \\mathbf { q } } ^ { c ^ { - } e ^ { + } } / n _ { \\mathbf { q } } ^ { c ^ { - } }$ /. Please refer to Table 3 for the meaning of the /notations. For subpopulations with $\\Delta P | _ { \\mathbf { q } } \\ge \\tau$ , by selecting $\\lceil n _ { \\mathbf { q } } ^ { c ^ { - } } "
      },
      "element_b": {
        "element_id": "1611.07438_figure_1",
        "element_type": "figure",
        "caption": "Figure 1: Causal graph of an example university admission system.",
        "content": "Figure 1: Causal graph of an example university admission system.",
        "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1611.07438/1611.07438/hybrid_auto/images/1611.07438_page0_fig0.jpg",
        "context_before": "For the example shown in Table 2, although examining {major} shows no discriminatory effect, when examining {major,test score} we observe $| \\Delta P | _ { \\mathrm { \\{ m a t h , B \\} } } | = 0 .\n\nThe constructed causal graph is shown in Figure 2.\n\nWe make use of the above connection to identify the direct causal effect of $C$ on $E$ . We construct a new DAG $\\scriptstyle { G ^ { \\prime } }$ by deleting the arc $C E$ from $\\mathcal { G }$ and keeping everything else unchanged. Thus, the possible",
        "context_after": "give the following theorem. The proof follows the above analysis.\n\nTheorem 2.1. A node set B forms a meaningful partition for measuring discrimination $i f$ and only if B is a block set, i.e., B satisfies: (1) $( C \\texttt { \\ \" } \\bot { E } | \\texttt { B } ) _ { \\mathcal { G } ^ { \\prime } }$ holds; (2) B contains none of $E$ ’s decedents, where $\\scriptstyle { G ^ { \\prime } }$ is the graph constructed by deleting arc $C \\ \\ E$ from $\\mathcal { G }$ . Discriminatory effect is considered to pre"
      },
      "edge_contexts": [
        {
          "source": "1611.07438_table_3",
          "target": "1611.07438_figure_1",
          "ref_text": "Figure 1",
          "context_snippet": "1 to show how the criterion works. The causal graph of the examples is shown in Figure 1. There are two block sets in this graph: {major}, and {major,test score}. Note "
        }
      ]
    },
    {
      "pair_id": "1611.07438_pair_4",
      "doc_id": "1611.07438",
      "element_a_id": "1611.07438_table_3",
      "element_b_id": "1611.07438_figure_3",
      "element_a_type": "table",
      "element_b_type": "figure",
      "pair_type": "figure+table",
      "hop_distance": 1,
      "path": [
        "1611.07438_table_3",
        "1611.07438_figure_3"
      ],
      "quality_score": 1.0,
      "overlap_with_existing_l1": false,
      "element_a": {
        "element_id": "1611.07438_table_3",
        "element_type": "table",
        "caption": "Table 3: Contingency table within subpopulation q.",
        "content": "Table 3: Contingency table within subpopulation q.",
        "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1611.07438/1611.07438/hybrid_auto/images/70ad0c655da5ed8499636b5de3224e6deb769b0861f3a77939cc7f55a89a4053.jpg",
        "context_before": "We use the illustrative examples in Section 1 to show how the criterion works. The causal graph of the examples is shown in Figure 1. There are two block sets in this graph: {major}, and {major,test score}. Note that test score alone is not a block set. That is why conditioning on it will produce misleading results. For the example shown in Table 1, examining both block sets shows no discriminatory effect. Thus, non-discrimination can be claimed. For the example shown in Table 2, although examin",
        "context_after": "To calculate the number of tuples to be modified within each subpopulation q, we express $\\Delta P \\vert _ { \\mathbf { q } }$ as $n _ { \\mathbf { q } } ^ { c ^ { + } e ^ { + } } / n _ { \\mathbf { q } } ^ { c ^ { + } } -$ $n _ { \\mathbf { q } } ^ { c ^ { - } e ^ { + } } / n _ { \\mathbf { q } } ^ { c ^ { - } }$ /. Please refer to Table 3 for the meaning of the /notations. For subpopulations with $\\Delta P | _ { \\mathbf { q } } \\ge \\tau$ , by selecting $\\lceil n _ { \\mathbf { q } } ^ { c ^ { - } } "
      },
      "element_b": {
        "element_id": "1611.07438_figure_3",
        "element_type": "figure",
        "caption": "Figure 3: Causal graph for Dutch Census dataset: the red node represents the protected attribute, the blue node represents the decision, the green nodes represent set Q, and the black nodes represent the others.",
        "content": "Figure 3: Causal graph for Dutch Census dataset: the red node represents the protected attribute, the blue node represents the decision, the green nodes represent set Q, and the black nodes represent the others.",
        "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1611.07438/1611.07438/hybrid_auto/images/1611.07438_page0_fig2.jpg",
        "context_before": "and MData, in terms of the utility of the modified data is shown in Table 4. We also report the results from the Naive method used in [8] in which we completely reshuffle the gender information. We measure the utility by three metrics: the Euclidean distance $( d \\mathbf { \\theta } )$ , the number of modified tuples $( n _ { T } )$ , and the utility loss $( \\chi ^ { 2 } )$ . We can observe from χTable 4 that the MGraph algorithm retains the highest utility. Both MGraph and MData algorithms signi",
        "context_after": "the unexplainable (bad) discrimination when one of the attributes is considered to be explanatory for the discrimination. However, their methods do not distinguish whether a partition is meaningful or not. Therefore, they cannot find the correct partitions to measure the direct discriminatory effects. Our experiments show that, their methods cannot completely remove discrimination conditioning on any single attribute. The results are skipped due to space limitation. In addition, even if we remov"
      },
      "edge_contexts": [
        {
          "source": "1611.07438_figure_3",
          "target": "1611.07438_table_3",
          "ref_text": "Table 3",
          "context_snippet": "^ { - } e ^ { + } } / n _ { \\mathbf { q } } ^ { c ^ { - } }$ /. Please refer to Table 3 for the meaning of the /notations. For subpopulations with $\\Delta P | _ { \\mat"
        }
      ]
    },
    {
      "pair_id": "1611.07438_pair_5",
      "doc_id": "1611.07438",
      "element_a_id": "1611.07438_figure_2",
      "element_b_id": "1611.07438_table_2",
      "element_a_type": "figure",
      "element_b_type": "table",
      "pair_type": "figure+table",
      "hop_distance": 1,
      "path": [
        "1611.07438_figure_2",
        "1611.07438_table_2"
      ],
      "quality_score": 1.0,
      "overlap_with_existing_l1": false,
      "element_a": {
        "element_id": "1611.07438_figure_2",
        "element_type": "figure",
        "caption": "Figure 2: Causal graph for Adult dataset: the red node represents the protected attribute, the blue node represents the decision, the green nodes represent set Q.",
        "content": "Figure 2: Causal graph for Adult dataset: the red node represents the protected attribute, the blue node represents the decision, the green nodes represent set Q.",
        "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1611.07438/1611.07438/hybrid_auto/images/1611.07438_page0_fig1.jpg",
        "context_before": "αAnother dataset Dutch census consists of 60421 tuples with 12 attributes. Similarly, we binarize the domain values of attribute age due to its large domain size. Three tiers are used in the partial order for temporal priority: sex, age, country birth are defined in the first tire, education level is defined in the second tire, and all other attributes are defined in the third tire. The constructed causal graph is shown in Figure 3. We treat sex (female and male) as the protected attribute and o",
        "context_after": "Second, when there are multiple meaningful partitions, examining one partition showing no bias does not guarantee no bias based on other partitions. Consider a different example on the same toy model shown in Table 2. The average admission rate now becomes $43 \\%$ equally for both females and males. Further conditioning on major still shows that females and males have the same chance to be admitted in the two subpopulations. However, when partitioning the data based on the combination {major, te"
      },
      "element_b": {
        "element_id": "1611.07438_table_2",
        "element_type": "table",
        "caption": "Table 2: Summary statistics of Example 2.",
        "content": "Table 2: Summary statistics of Example 2.",
        "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1611.07438/1611.07438/hybrid_auto/images/7f7397e73e6c5668f821f78d526c1b92de2e41da8da2858fa7326406964ecc6a.jpg",
        "context_before": "tected attribute, and admission is the decision. We assume there is no correlation between gender and test score. The summary statistics of the admission rate is shown in Table 1. It can be observed that the average admission rate is $37 \\%$ for females and $46 \\%$ for males. It is already known that the judgment of discrimination cannot be made simply based on the average admission rates in the whole population and further partitioning is needed. If we partition the data conditioning on test sc",
        "context_after": "For a quantitative measurement of discrimination, a general legal principle is to measure the difference in the proportion of positive decisions between the protected group and non-protected group [24]. Discovering and preventing discrimination is not trivial. As shown by Zliobait ˇ e et ˙ al. [28], simply considering the difference measured in the whole population fails to take into account the part of differences that are explainable by other attributes, and removing all the differences will r"
      },
      "edge_contexts": [
        {
          "source": "1611.07438_figure_2",
          "target": "1611.07438_table_2",
          "ref_text": "Table 2",
          "context_snippet": "n other partitions. Consider a different example on the same toy model shown in Table 2. The average admission rate now becomes $43 \\%$ equally for both females and ma"
        }
      ]
    },
    {
      "pair_id": "1611.07509_pair_3",
      "doc_id": "1611.07509",
      "element_a_id": "1611.07509_figure_2",
      "element_b_id": "1611.07509_table_1",
      "element_a_type": "figure",
      "element_b_type": "table",
      "pair_type": "figure+table",
      "hop_distance": 1,
      "path": [
        "1611.07509_figure_2",
        "1611.07509_table_1"
      ],
      "quality_score": 1.0,
      "overlap_with_existing_l1": false,
      "element_a": {
        "element_id": "1611.07509_figure_2",
        "element_type": "figure",
        "caption": "Figure 2: An example with the recanting witness criterion satisfied.",
        "content": "Figure 2: An example with the recanting witness criterion satisfied.",
        "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1611.07509/1611.07509/hybrid_auto/images/1611.07509_page0_fig1.jpg",
        "context_before": "When applied to the example in Figure 1, it means the expected change in loan approval of the disadvantage group if they had the same racial makeups shown in the Zip code as the advantage group.\n\nχThe results are shown in Table 1.\n\nThe path-specific effect is an extension to the total causal effect in the sense that the effect of the intervention is transmitted only along a subset of causal paths from $X$ to Y. Denote a subset of causal paths by $\\pi$ . The $\\pi$ -specific effect conπ πsiders a ",
        "context_after": "The authors in (Avin, Shpitser, and Pearl 2005) have given the condition under which the path-specific effect can be estimated from the observational data, known as the recanting witness criterion.\n\nDefinition 3 (Recanting witness criterion) Given a path set , let Z be a node in $\\mathcal { G }$ such that: 1) there exists a path πfrom X to Z which is a segment of a path in ; 2) there exπists a path from Z to Y which is a segment of a path in ; 3) πthere exists another path from Z to Y which is n"
      },
      "element_b": {
        "element_id": "1611.07509_table_1",
        "element_type": "table",
        "caption": "Table 1: Direct/indirect discriminatory effects in the modified training data and predictions for the testing data. Values violating the discrimination criterion are marked in bold.",
        "content": "Table 1: Direct/indirect discriminatory effects in the modified training data and predictions for the testing data. Values violating the discrimination criterion are marked in bold.",
        "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1611.07509/1611.07509/hybrid_auto/images/fdd1004c513835da188353f3f94f92476f68856a2ce4d146ecfe558bdc171fea.jpg",
        "context_before": "The Dutch dataset consists of 60421 tuples with 12 attributes. Similarly, we binarize the domain values of attribute age due to its large domain size. Three tiers are used in the partial order for temporal priority: sex, age, country birth are defined in the first tire, edu level and marital status are defined in the second tire, and all other attributes are defined in the third tire. The causal graph is shown in Figure 4. Similarly we treat sex as the protected attribute, occupation as the deci",
        "context_after": "represents full discrimination removal. However, has no λdirect connection with the threshold . In our experiments, we execute $D I$ τmultiple times with different s and report the one that is closest to achieve $\\tau \\ : = \\ : 0 . 0 5$ λ. As shown in τ .the column “DI”, it indeed removes direct and indirect discrimination from the training data. However, as indicated by the bold values 0.167/0.168, significant amount of indirect discrimination exists in the predictions of both classifiers. In a"
      },
      "edge_contexts": [
        {
          "source": "1611.07509_figure_2",
          "target": "1611.07509_table_1",
          "ref_text": "Table 1",
          "context_snippet": "akeups shown in the Zip code as the advantage group.\n\nχThe results are shown in Table 1.\n\nThe path-specific effect is an extension to the total causal effect in the se"
        }
      ]
    },
    {
      "pair_id": "1611.07509_pair_4",
      "doc_id": "1611.07509",
      "element_a_id": "1611.07509_figure_4",
      "element_b_id": "1611.07509_table_1",
      "element_a_type": "figure",
      "element_b_type": "table",
      "pair_type": "figure+table",
      "hop_distance": 1,
      "path": [
        "1611.07509_figure_4",
        "1611.07509_table_1"
      ],
      "quality_score": 1.0,
      "overlap_with_existing_l1": false,
      "element_a": {
        "element_id": "1611.07509_figure_4",
        "element_type": "figure",
        "caption": "Figure 4: Causal network for Dutch dataset: the green path represents the direct path, and the blue paths represent the indirect paths passing through marital status.",
        "content": "Figure 4: Causal network for Dutch dataset: the green path represents the direct path, and the blue paths represent the indirect paths passing through marital status.",
        "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1611.07509/1611.07509/hybrid_auto/images/1611.07509_page0_fig4.jpg",
        "context_before": "The Adult dataset consists of 65123 tuples with 11 attributes such as age, education, sex, occupation, income, marital status etc. Since the computational complexity of the PC algorithm is an exponential function of the number of attributes and their domain sizes, for computational feasibility we binarize each attribute’s domain values into two classes to reduce the domain sizes. We use three tiers in the partial order for temporal priority: sex, age, native country, race are defined in the firs",
        "context_after": "The Dutch dataset consists of 60421 tuples with 12 attributes. Similarly, we binarize the domain values of attribute age due to its large domain size. Three tiers are used in the partial order for temporal priority: sex, age, country birth are defined in the first tire, edu level and marital status are defined in the second tire, and all other attributes are defined in the third tire. The causal graph is shown in Figure 4. Similarly we treat sex as the protected attribute, occupation as the deci"
      },
      "element_b": {
        "element_id": "1611.07509_table_1",
        "element_type": "table",
        "caption": "Table 1: Direct/indirect discriminatory effects in the modified training data and predictions for the testing data. Values violating the discrimination criterion are marked in bold.",
        "content": "Table 1: Direct/indirect discriminatory effects in the modified training data and predictions for the testing data. Values violating the discrimination criterion are marked in bold.",
        "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1611.07509/1611.07509/hybrid_auto/images/fdd1004c513835da188353f3f94f92476f68856a2ce4d146ecfe558bdc171fea.jpg",
        "context_before": "The Dutch dataset consists of 60421 tuples with 12 attributes. Similarly, we binarize the domain values of attribute age due to its large domain size. Three tiers are used in the partial order for temporal priority: sex, age, country birth are defined in the first tire, edu level and marital status are defined in the second tire, and all other attributes are defined in the third tire. The causal graph is shown in Figure 4. Similarly we treat sex as the protected attribute, occupation as the deci",
        "context_after": "represents full discrimination removal. However, has no λdirect connection with the threshold . In our experiments, we execute $D I$ τmultiple times with different s and report the one that is closest to achieve $\\tau \\ : = \\ : 0 . 0 5$ λ. As shown in τ .the column “DI”, it indeed removes direct and indirect discrimination from the training data. However, as indicated by the bold values 0.167/0.168, significant amount of indirect discrimination exists in the predictions of both classifiers. In a"
      },
      "edge_contexts": [
        {
          "source": "1611.07509_table_1",
          "target": "1611.07509_figure_4",
          "ref_text": "Figure 4",
          "context_snippet": "ll other attributes are defined in the third tire. The causal graph is shown in Figure 4. Similarly we treat sex as the protected attribute, occupation as the deci\n\nThe"
        }
      ]
    },
    {
      "pair_id": "1611.07509_pair_5",
      "doc_id": "1611.07509",
      "element_a_id": "1611.07509_table_1",
      "element_b_id": "1611.07509_figure_1",
      "element_a_type": "table",
      "element_b_type": "figure",
      "pair_type": "figure+table",
      "hop_distance": 1,
      "path": [
        "1611.07509_table_1",
        "1611.07509_figure_1"
      ],
      "quality_score": 1.0,
      "overlap_with_existing_l1": false,
      "element_a": {
        "element_id": "1611.07509_table_1",
        "element_type": "table",
        "caption": "Table 1: Direct/indirect discriminatory effects in the modified training data and predictions for the testing data. Values violating the discrimination criterion are marked in bold.",
        "content": "Table 1: Direct/indirect discriminatory effects in the modified training data and predictions for the testing data. Values violating the discrimination criterion are marked in bold.",
        "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1611.07509/1611.07509/hybrid_auto/images/fdd1004c513835da188353f3f94f92476f68856a2ce4d146ecfe558bdc171fea.jpg",
        "context_before": "The Dutch dataset consists of 60421 tuples with 12 attributes. Similarly, we binarize the domain values of attribute age due to its large domain size. Three tiers are used in the partial order for temporal priority: sex, age, country birth are defined in the first tire, edu level and marital status are defined in the second tire, and all other attributes are defined in the third tire. The causal graph is shown in Figure 4. Similarly we treat sex as the protected attribute, occupation as the deci",
        "context_after": "represents full discrimination removal. However, has no λdirect connection with the threshold . In our experiments, we execute $D I$ τmultiple times with different s and report the one that is closest to achieve $\\tau \\ : = \\ : 0 . 0 5$ λ. As shown in τ .the column “DI”, it indeed removes direct and indirect discrimination from the training data. However, as indicated by the bold values 0.167/0.168, significant amount of indirect discrimination exists in the predictions of both classifiers. In a"
      },
      "element_b": {
        "element_id": "1611.07509_figure_1",
        "element_type": "figure",
        "caption": "Figure 1: The toy model.",
        "content": "Figure 1: The toy model.",
        "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1611.07509/1611.07509/hybrid_auto/images/1611.07509_page0_fig0.jpg",
        "context_before": "The causal modeling based discrimination detection has been proposed most recently (Bonchi et al. 2015; Zhang,\n\nCopyright $©$ 2017, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.\n\narXiv:1611.07509v1 [cs.LG] 22 Nov 2016",
        "context_after": "Wu, and Wu 2016b; 2016a) for improving the correlation based approaches. In this paper, we develop a framework for discovering and removing both direct and indirect discrimination based on the causal network. A causal network is a directed acyclic graph (DAG) widely used for causal representation, reasoning and inference (Pearl 2009), where causal effects are carried by the paths that trace arrows pointing from the cause to the effect which are referred to as the causal paths. Using this model, "
      },
      "edge_contexts": [
        {
          "source": "1611.07509_table_1",
          "target": "1611.07509_figure_1",
          "ref_text": "Figure 1",
          "context_snippet": " ^ { + }$ and everything else remains unchanged. When applied to the example in Figure 1, it means the expected change in loan approval of applications actually from th"
        }
      ]
    },
    {
      "pair_id": "1701.08230_pair_1",
      "doc_id": "1701.08230",
      "element_a_id": "1701.08230_table_1",
      "element_b_id": "1701.08230_figure_1",
      "element_a_type": "table",
      "element_b_type": "figure",
      "pair_type": "figure+table",
      "hop_distance": 1,
      "path": [
        "1701.08230_table_1",
        "1701.08230_figure_1"
      ],
      "quality_score": 1.0,
      "overlap_with_existing_l1": false,
      "element_a": {
        "element_id": "1701.08230_table_1",
        "element_type": "table",
        "caption": "Table 1: Based on the Broward County data, satisfying common fairness denitions results in detaining low-risk defendants while reducing public safety. For each fairness constraint, we estimate the increase in violent crime committed by released defendants, relative to a rule that optimizes for public safety alone; and the proportion of detained defendants that are low risk (i.e., would be released if we again considered only public safety).",
        "content": "Table 1: Based on the Broward County data, satisfying common fairness denitions results in detaining low-risk defendants while reducing public safety. For each fairness constraint, we estimate the increase in violent crime committed by released defendants, relative to a rule that optimizes for public safety alone; and the proportion of detained defendants that are low risk (i.e., would be released if we again considered only public safety).",
        "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1701.08230/1701.08230/hybrid_auto/images/1f4572e64663f787874618af9f17e4c4996cb864282415c743c3aab762053005.jpg",
        "context_before": "We use data from Broward County, Florida originally compiled by ProPublica [30]. Following their analysis, we only consider black and white defendants who were assigned COMPAS risk scores within 30 days of their arrest, and were not arrested for an ordinary trac crime. We further restrict to only those defendants who spent at least two years (aer their COMPAS evaluation) outside a correctional facility without being arrested for a violent crime, or were arrested for a violent crime within this",
        "context_after": "safety subject to (1) and (2). e proportion of defendants detained is chosen to match the fraction of defendants classied as medium or high risk by COMPAS (scoring 5 or greater). Conditional statistical parity requires that one dene the “legitimate” factors $\\ell ( X )$ , and ` Xthis choice signicantly impacts results. For example, if all variables are deemed legitimate, then this fairness condition imposes no constraint on the algorithm. In our application, we consider only a defendant’s nu"
      },
      "element_b": {
        "element_id": "1701.08230_figure_1",
        "element_type": "figure",
        "caption": "Figure 1: Top: distribution of risk scores for Broward County data (le ), and simulated data drawn from two beta distributions with equal means (right). Bottom: using a single threshold which detains $3 0 \\%$ of defendants in Broward County violates statistical parity (as measured by detention rate), predictive equality (false positive rate), and conditional statistical parity (detention rate conditional on number of prior arrests). We omit the last measure for the simulated data since that would require making additional assumptions about the relationship of priors and risk in the hypothetical populations.",
        "content": "Figure 1: Top: distribution of risk scores for Broward County data (le ), and simulated data drawn from two beta distributions with equal means (right). Bottom: using a single threshold which detains $3 0 \\%$ of defendants in Broward County violates statistical parity (as measured by detention rate), predictive equality (false positive rate), and conditional statistical parity (detention rate conditional on number of prior arrests). We omit the last measure for the simulated data since that would require making additional assumptions about the relationship of priors and risk in the hypothetical populations.",
        "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1701.08230/1701.08230/hybrid_auto/images/1701.08230_page0_fig3.jpg",
        "context_before": "",
        "context_after": "e reason for these disparities is that white and black defendants in Broward County have dierent distributions of risk, $\\ p _ { Y \\vert X }$ , pY Xas shown in Figure 1. In particular, a greater fraction of black defendants have relatively high risk scores, in part because black defendants are more likely to have prior arrests, which is a strong indicator of reoending. Importantly, while an algorithm designer can choose dierent decision rules based on these risk scores, the algorithm cannot "
      },
      "edge_contexts": [
        {
          "source": "1701.08230_figure_1",
          "target": "1701.08230_table_1",
          "ref_text": "Table 1",
          "context_snippet": "ntion and false positive rates in the red group.\n\nFor each fairness constraint, Table 1 shows that violent recidivism increases while low risk defendants are detained."
        }
      ]
    },
    {
      "pair_id": "1701.08230_pair_2",
      "doc_id": "1701.08230",
      "element_a_id": "1701.08230_figure_2",
      "element_b_id": "1701.08230_table_1",
      "element_a_type": "figure",
      "element_b_type": "table",
      "pair_type": "figure+table",
      "hop_distance": 1,
      "path": [
        "1701.08230_figure_2",
        "1701.08230_table_1"
      ],
      "quality_score": 1.0,
      "overlap_with_existing_l1": false,
      "element_a": {
        "element_id": "1701.08230_figure_2",
        "element_type": "figure",
        "caption": "Figure 2: Recidivism rate by COMPAS risk score and race. White and black defendants with the same risk score are roughly equally likely to reoend, indicating that the scores are calibrated. e $y$ -axis shows the proportion of defenydants re-arrested for any crime, including non-violent offenses; the gray bands show $9 5 \\%$ condence intervals.",
        "content": "Figure 2: Recidivism rate by COMPAS risk score and race. White and black defendants with the same risk score are roughly equally likely to reoend, indicating that the scores are calibrated. e $y$ -axis shows the proportion of defenydants re-arrested for any crime, including non-violent offenses; the gray bands show $9 5 \\%$ condence intervals.",
        "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1701.08230/1701.08230/hybrid_auto/images/1701.08230_page0_fig4.jpg",
        "context_before": "For each denition, we nd the set of thresholds that produce a decision rule that: (1) satises the fairness denition; (2) detains $3 0 \\%$ of defendants; and (3) maximizes expected public\n\nTable 1: Based on the Broward County data, satisfying common fairness denitions results in detaining low-risk defendants while reducing public safety.\n\nough intuitively appealing, detention rates and false positive rates are poor proxies for the thresholds: these infra-marginal statistics consider average",
        "context_after": "34, 37]. In that work, taste-based discrimination [6] is equated with applying decision thresholds that dier by race. eir seing is human, not algorithmic, decision making, and so one cannot directly observe the thresholds being applied; the goal is thus to infer the thresholds from observable statistics. ough intuitively appealing, detention rates and false positive rates are poor proxies for the thresholds: these infra-marginal statistics consider average risk above the thresholds, and so c"
      },
      "element_b": {
        "element_id": "1701.08230_table_1",
        "element_type": "table",
        "caption": "Table 1: Based on the Broward County data, satisfying common fairness denitions results in detaining low-risk defendants while reducing public safety. For each fairness constraint, we estimate the increase in violent crime committed by released defendants, relative to a rule that optimizes for public safety alone; and the proportion of detained defendants that are low risk (i.e., would be released if we again considered only public safety).",
        "content": "Table 1: Based on the Broward County data, satisfying common fairness denitions results in detaining low-risk defendants while reducing public safety. For each fairness constraint, we estimate the increase in violent crime committed by released defendants, relative to a rule that optimizes for public safety alone; and the proportion of detained defendants that are low risk (i.e., would be released if we again considered only public safety).",
        "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1701.08230/1701.08230/hybrid_auto/images/1f4572e64663f787874618af9f17e4c4996cb864282415c743c3aab762053005.jpg",
        "context_before": "We use data from Broward County, Florida originally compiled by ProPublica [30]. Following their analysis, we only consider black and white defendants who were assigned COMPAS risk scores within 30 days of their arrest, and were not arrested for an ordinary trac crime. We further restrict to only those defendants who spent at least two years (aer their COMPAS evaluation) outside a correctional facility without being arrested for a violent crime, or were arrested for a violent crime within this",
        "context_after": "safety subject to (1) and (2). e proportion of defendants detained is chosen to match the fraction of defendants classied as medium or high risk by COMPAS (scoring 5 or greater). Conditional statistical parity requires that one dene the “legitimate” factors $\\ell ( X )$ , and ` Xthis choice signicantly impacts results. For example, if all variables are deemed legitimate, then this fairness condition imposes no constraint on the algorithm. In our application, we consider only a defendant’s nu"
      },
      "edge_contexts": [
        {
          "source": "1701.08230_figure_2",
          "target": "1701.08230_table_1",
          "ref_text": "Table 1",
          "context_snippet": "nition; (2) detains $3 0 \\%$ of defendants; and (3) maximizes expected public\n\nTable 1: Based on the Broward County data, satisfying common fairness denitions result"
        }
      ]
    },
    {
      "pair_id": "1703.06856_pair_1",
      "doc_id": "1703.06856",
      "element_a_id": "1703.06856_figure_2",
      "element_b_id": "1703.06856_table_1",
      "element_a_type": "figure",
      "element_b_type": "table",
      "pair_type": "figure+table",
      "hop_distance": 1,
      "path": [
        "1703.06856_figure_2",
        "1703.06856_table_1"
      ],
      "quality_score": 1.0,
      "overlap_with_existing_l1": false,
      "element_a": {
        "element_id": "1703.06856_figure_2",
        "element_type": "figure",
        "caption": "Figure 2: Left: A causal model for the problem of predicting law school success fairly. Right: Density plots of predicted $\\mathrm { F Y A } _ { a }$ and $\\mathrm { F Y A } _ { a ^ { \\prime } }$ .",
        "content": "Figure 2: Left: A causal model for the problem of predicting law school success fairly. Right: Density plots of predicted $\\mathrm { F Y A } _ { a }$ and $\\mathrm { F Y A } _ { a ^ { \\prime } }$ .",
        "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1703.06856/1703.06856/hybrid_auto/images/1703.06856_page0_fig6.jpg",
        "context_before": "",
        "context_after": "In Level 3, we model GPA, LSAT, and FYA as continuous variables with additive error terms independent of race and sex (that may in turn be correlated with one-another). This model is shown\n\nIn Level 2, we postulate that a latent variable: a student’s knowledge (K), affects GPA, LSAT, and FYA scores. The causal graph corresponding to this model is shown in Figure 2, (Level 2). This is a short-hand for the distributions:\n\nin Figure 2, (Level 3), and is expressed by:"
      },
      "element_b": {
        "element_id": "1703.06856_table_1",
        "element_type": "table",
        "caption": "Table 1: Prediction results using logistic regression. Note that we must sacrifice a small amount of accuracy to ensuring counterfactually fair prediction (Fair $K$ , Fair Add), versus the models that use unfair features: GPA, LSAT, race, sex (Full, Unaware).",
        "content": "Table 1: Prediction results using logistic regression. Note that we must sacrifice a small amount of accuracy to ensuring counterfactually fair prediction (Fair $K$ , Fair Add), versus the models that use unfair features: GPA, LSAT, race, sex (Full, Unaware).",
        "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1703.06856/1703.06856/hybrid_auto/images/08fd2af4f3f4e209bae332b4b0dae6871b3687d4b71fedd1b9f90aaf83211b6f.jpg",
        "context_before": "The causal graph corresponding to this model is shown in Figure 2, (Level 2).\n\nNote that in this case, this model is not fair even if the data was generated by one of the models shown in Figure 2 as it corresponds to Scenario 3.\n\nIn Level 3, we model GPA, LSAT, and FYA as continuous variables with additive error terms independent of race and sex (that may in turn be correlated with one-another). This model is shown",
        "context_after": "in Figure 2, (Level 3), and is expressed by:\n\n$$ \\mathbf {G P A} = b _ {G} + w _ {G} ^ {R} R + w _ {G} ^ {S} S + \\epsilon_ {G}, \\epsilon_ {G} \\sim p (\\epsilon_ {G}) $$\n\n$$ \\mathrm {L S A T} = b _ {L} + w _ {L} ^ {R} R + w _ {L} ^ {S} S + \\epsilon_ {L}, \\quad \\epsilon_ {L} \\sim p (\\epsilon_ {L}) $$\n\nTo provide an intuition for counterfactual fairness, we will consider two real-world fair prediction scenarios: insurance pricing and crime prediction. Each of these correspond to one of the two causa"
      },
      "edge_contexts": [
        {
          "source": "1703.06856_table_1",
          "target": "1703.06856_figure_2",
          "ref_text": "Figure 2",
          "context_snippet": "The causal graph corresponding to this model is shown in Figure 2, (Level 2).\n\nNote that in this case, this model is not fair even if the data wa"
        }
      ]
    },
    {
      "pair_id": "1703.06856_pair_4",
      "doc_id": "1703.06856",
      "element_a_id": "1703.06856_table_1",
      "element_b_id": "1703.06856_figure_1",
      "element_a_type": "table",
      "element_b_type": "figure",
      "pair_type": "figure+table",
      "hop_distance": 1,
      "path": [
        "1703.06856_table_1",
        "1703.06856_figure_1"
      ],
      "quality_score": 1.0,
      "overlap_with_existing_l1": false,
      "element_a": {
        "element_id": "1703.06856_table_1",
        "element_type": "table",
        "caption": "Table 1: Prediction results using logistic regression. Note that we must sacrifice a small amount of accuracy to ensuring counterfactually fair prediction (Fair $K$ , Fair Add), versus the models that use unfair features: GPA, LSAT, race, sex (Full, Unaware).",
        "content": "Table 1: Prediction results using logistic regression. Note that we must sacrifice a small amount of accuracy to ensuring counterfactually fair prediction (Fair $K$ , Fair Add), versus the models that use unfair features: GPA, LSAT, race, sex (Full, Unaware).",
        "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1703.06856/1703.06856/hybrid_auto/images/08fd2af4f3f4e209bae332b4b0dae6871b3687d4b71fedd1b9f90aaf83211b6f.jpg",
        "context_before": "The causal graph corresponding to this model is shown in Figure 2, (Level 2).\n\nNote that in this case, this model is not fair even if the data was generated by one of the models shown in Figure 2 as it corresponds to Scenario 3.\n\nIn Level 3, we model GPA, LSAT, and FYA as continuous variables with additive error terms independent of race and sex (that may in turn be correlated with one-another). This model is shown",
        "context_after": "in Figure 2, (Level 3), and is expressed by:\n\n$$ \\mathbf {G P A} = b _ {G} + w _ {G} ^ {R} R + w _ {G} ^ {S} S + \\epsilon_ {G}, \\epsilon_ {G} \\sim p (\\epsilon_ {G}) $$\n\n$$ \\mathrm {L S A T} = b _ {L} + w _ {L} ^ {R} R + w _ {L} ^ {S} S + \\epsilon_ {L}, \\quad \\epsilon_ {L} \\sim p (\\epsilon_ {L}) $$\n\nTo provide an intuition for counterfactual fairness, we will consider two real-world fair prediction scenarios: insurance pricing and crime prediction. Each of these correspond to one of the two causa"
      },
      "element_b": {
        "element_id": "1703.06856_figure_1",
        "element_type": "figure",
        "caption": "(e) Figure 1: (a), (b) Two causal models for different real-world fair prediction scenarios. See Section 3.1 for discussion. (c) The graph corresponding to a causal model with $A$ being the protected attribute and $Y$ some outcome of interest, with background variables assumed to be independent. (d) Expanding the model to include an intermediate variable indicating whether the individual is employed with two (latent) background variables Prejudiced (if the person offering the job is prejudiced) and Qualifications (a measure of the individual’s qualifications). (e) A twin network representation of this system [28] under two different counterfactual levels for $A$ . This is created by copying nodes descending from $A$ , which inherit unaffected parents from the factual world.",
        "content": "(e) Figure 1: (a), (b) Two causal models for different real-world fair prediction scenarios. See Section 3.1 for discussion. (c) The graph corresponding to a causal model with $A$ being the protected attribute and $Y$ some outcome of interest, with background variables assumed to be independent. (d) Expanding the model to include an intermediate variable indicating whether the individual is employed with two (latent) background variables Prejudiced (if the person offering the job is prejudiced) and Qualifications (a measure of the individual’s qualifications). (e) A twin network representation of this system [28] under two different counterfactual levels for $A$ . This is created by copying nodes descending from $A$ , which inherit unaffected parents from the factual world.",
        "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1703.06856/1703.06856/hybrid_auto/images/1703.06856_page0_fig3.jpg",
        "context_before": "",
        "context_after": "our examples to follow, then IF can be defined by treating equally two individuals with the same $W$ in a way that is also counterfactually fair.\n\nRelation to Pearl et al. [29]. In Example 4.4.4 of [29], the authors condition instead on $X , A$ , and the observed realization of $\\hat { Y }$ , and calculate the probability of the counterfactual realization $\\hat { Y } _ { A a ^ { \\prime } }$ differing from the factual. This example conflates the predictor $\\hat { Y }$ with the outcome $Y$ , of wh"
      },
      "edge_contexts": [
        {
          "source": "1703.06856_table_1",
          "target": "1703.06856_figure_1",
          "ref_text": "Figure 1",
          "context_snippet": "d crime prediction. Each of these correspond to one of the two causal graphs in Figure 1(a),(b). The Supplementary Material provides a more mathematical discussion of t"
        }
      ]
    },
    {
      "pair_id": "1706.02409_pair_1",
      "doc_id": "1706.02409",
      "element_a_id": "1706.02409_table_1",
      "element_b_id": "1706.02409_figure_1",
      "element_a_type": "table",
      "element_b_type": "figure",
      "pair_type": "figure+table",
      "hop_distance": 1,
      "path": [
        "1706.02409_table_1",
        "1706.02409_figure_1"
      ],
      "quality_score": 1.0,
      "overlap_with_existing_l1": false,
      "element_a": {
        "element_id": "1706.02409_table_1",
        "element_type": "table",
        "caption": "Table 1: Summary of datasets. Type indicates whether regression is logistic or linear; $n$ is total number of data points; $d$ is dimensionality; Minority $n$ is the number of data points in the smaller population; Protected indicates which feature is protected or fairness-sensitive.",
        "content": "Table 1: Summary of datasets. Type indicates whether regression is logistic or linear; $n$ is total number of data points; $d$ is dimensionality; Minority $n$ is the number of data points in the smaller population; Protected indicates which feature is protected or fairness-sensitive.",
        "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1706.02409/1706.02409/hybrid_auto/images/834287260aa1d3dd87e59dcd6cda5d48342d6c4f5fdb2b7c9a35846e53a127fd.jpg",
        "context_before": "5We only used the data in Adult.data in our experiments.\n\n6http://www2.law.ucla.edu/sander/Systemic/Data.htm\n\ngoal is to predict the sentence length given by the judge based on factors such as previous criminal records and the crimes for which the conviction was obtained. The protected attribute is gender.",
        "context_after": "4.1 Accuracy-Fairness Efficient Frontiers\n\nWe begin by examining the efficient frontier of accuracy vs. fairness for the six datasets. These curves are shown in Figure 1, and are obtained by varying the weight $\\lambda$ on the fairness regularizer, and for each value of $\\lambda$ finding the model which minimizes the associated regularized loss function. For the logistic regression cases, we extract probabilities from the learned model w as ${ \\mathrm { P r } } [ y _ { i } = 1 ] =$ $\\exp ( \\math"
      },
      "element_b": {
        "element_id": "1706.02409_figure_1",
        "element_type": "figure",
        "caption": "Figure 1: Efficient frontiers of accuracy vs. fairness for each dataset. For datasets with binary-valued targets (logistic regression), we consider three fairness notions (group, individual and hybrid), and for each examine building a single model or separate models for each group, yielding a total of six curves. For real-valued targets (linear regression), we consider two fairness notions (group and individual), and again single or separate models, yielding a total of four curves.",
        "content": "Figure 1: Efficient frontiers of accuracy vs. fairness for each dataset. For datasets with binary-valued targets (logistic regression), we consider three fairness notions (group, individual and hybrid), and for each examine building a single model or separate models for each group, yielding a total of six curves. For real-valued targets (linear regression), we consider two fairness notions (group and individual), and again single or separate models, yielding a total of four curves.",
        "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1706.02409/1706.02409/hybrid_auto/images/1706.02409_page0_fig5.jpg",
        "context_before": "",
        "context_after": "4.2 Price of Fairness\n\nThe efficient fairness/accuracy frontiers pictured in Figure 1 can be compared across data sets in a qualitative sense — e.g. to see that in some datasets, the fairness penalty can be substantially decreased with little cost to accuracy. However, they are difficult to compare quantitatively, because the scale of the fairness loss differs substantially from data set to data set. In this section, we give a cross-dataset comparison using a measure (which we call Price of Fair"
      },
      "edge_contexts": [
        {
          "source": "1706.02409_figure_1",
          "target": "1706.02409_table_1",
          "ref_text": "Table 1",
          "context_snippet": "raining a predictor to have fairness\n\nThe datasets themselves are summarized in Table 1, where we specify the size and dimensionality of each, along with the “protecte"
        }
      ]
    },
    {
      "pair_id": "1707.09457_pair_1",
      "doc_id": "1707.09457",
      "element_a_id": "1707.09457_table_1",
      "element_b_id": "1707.09457_figure_1",
      "element_a_type": "table",
      "element_b_type": "figure",
      "pair_type": "figure+table",
      "hop_distance": 1,
      "path": [
        "1707.09457_table_1",
        "1707.09457_figure_1"
      ],
      "quality_score": 1.0,
      "overlap_with_existing_l1": false,
      "element_a": {
        "element_id": "1707.09457_table_1",
        "element_type": "table",
        "caption": "Table 1: Statistics for the two recognition problems. In vSRL, we consider gender bias relating to verbs, while in MLC we consider the gender bias related to objects.",
        "content": "Table 1: Statistics for the two recognition problems. In vSRL, we consider gender bias relating to verbs, while in MLC we consider the gender bias related to objects.",
        "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1707.09457/1707.09457/hybrid_auto/images/ec39e21fb44a922a487a63da2ae6305d9fa5edf16eb972b8a32abe434971542b.jpg",
        "context_before": "For example, one can represent the problem as an\n\nTable 1: Statistics for the two recognition problems.\n\nIn this section, we provide details about the two visual recognition tasks we evaluated for bias: visual semantic role labeling (vSRL), and multi-label classification (MLC). We focus on gender, defining $G = \\{ \\mathrm { m a n } , \\mathrm { w o m a n } \\}$ and focus on the agent\n\nrole in vSRL, and any occurrence in text associated with the images in MLC. Problem statistics are summarized in T",
        "context_after": "integer linear program and solve it using an offthe-shelf solver (e.g., Gurobi (Gurobi Optimization, 2016)). However, Eq. (3) involves all test instances. Solving a constrained optimization problem on such a scale is difficult. Therefore, we consider relaxing the constraints and solve Eq. (3) using a Lagrangian relaxation technique (Rush and Collins, 2012). We introduce a Lagrangian multiplier $\\lambda _ { j } \\geq 0$ for each corpus-level constraint. The Lagrangian is\n\n$$ \\begin{array}{l} L (\\l"
      },
      "element_b": {
        "element_id": "1707.09457_figure_1",
        "element_type": "figure",
        "caption": "Figure 1: Five example images from the imSitu visual semantic role labeling (vSRL) dataset. Each image is paired with a table describing a situation: the verb, cooking, its semantic roles, i.e agent, and noun values filling that role, i.e. woman. In the imSitu training set, $33 \\%$ of cooking images have man in the agent role while the rest have woman. After training a Conditional Random Field (CRF), bias is amplified: man fills $16 \\%$ of agent roles in cooking images. To reduce this bias amplification our calibration method adjusts weights of CRF potentials associated with biased predictions. After applying our methods, man appears in the agent role of $20 \\%$ of cooking images, reducing the bias amplification by $2 5 \\%$ , while keeping the CRF vSRL performance unchanged.",
        "content": "Figure 1: Five example images from the imSitu visual semantic role labeling (vSRL) dataset. Each image is paired with a table describing a situation: the verb, cooking, its semantic roles, i.e agent, and noun values filling that role, i.e. woman. In the imSitu training set, $33 \\%$ of cooking images have man in the agent role while the rest have woman. After training a Conditional Random Field (CRF), bias is amplified: man fills $16 \\%$ of agent roles in cooking images. To reduce this bias amplification our calibration method adjusts weights of CRF potentials associated with biased predictions. After applying our methods, man appears in the agent role of $20 \\%$ of cooking images, reducing the bias amplification by $2 5 \\%$ , while keeping the CRF vSRL performance unchanged.",
        "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1707.09457/1707.09457/hybrid_auto/images/1707.09457_page0_fig0.jpg",
        "context_before": "Our analysis reveals that over $45 \\%$ and $37 \\%$ of verbs and objects, respectively, exhibit bias toward a gender greater than 2:1. For example, as seen in Figure 1, the cooking activity in imSitu is a heavily biased verb. Furthermore, we show that after training state-of-the-art structured predictors, models amplify the existing bias, by $5 . 0 \\%$ for vSRL, and $3 . 6 \\%$ in MLC.\n\narXiv:1707.09457v1 [cs.AI] 29 Jul 2017\n\n1To simplify our analysis, we only consider a gender binary as perceived",
        "context_after": "To mitigate the role of bias amplification when training models on biased corpora, we propose a novel constrained inference framework, called RBA, for Reducing Bias Amplification in predictions. Our method introduces corpus-level constraints so that gender indicators co-occur no more often together with elements of the prediction task than in the original training distribution. For example, as seen in Figure 1, we would like noun man to occur in the agent role of the cooking as often as it occur"
      },
      "edge_contexts": [
        {
          "source": "1707.09457_table_1",
          "target": "1707.09457_figure_1",
          "ref_text": "Figure 1",
          "context_snippet": "at otherwise would have weak visual support. For example, in the first image of Figure 1, it is possible to predict a spatula by considering that it is a common tool us"
        }
      ]
    },
    {
      "pair_id": "1711.07076_pair_1",
      "doc_id": "1711.07076",
      "element_a_id": "1711.07076_figure_1",
      "element_b_id": "1711.07076_table_1",
      "element_a_type": "figure",
      "element_b_type": "table",
      "pair_type": "figure+table",
      "hop_distance": 1,
      "path": [
        "1711.07076_figure_1",
        "1711.07076_table_1"
      ],
      "quality_score": 1.0,
      "overlap_with_existing_l1": false,
      "element_a": {
        "element_id": "1711.07076_figure_1",
        "element_type": "figure",
        "caption": "Figure 1: Demonstration of a DLP’s undesirable side effects on a simple example of hiring data (see §4.1). An unconstrained classifier (vertical line) hires candidates based on work experience, yielding higher hiring rates for men than for women. A DLP (dashed diagonal) achieves near-parity by differentiating based on an irrelevant attribute (hair length). The DLP hurts some short-haired women, flipping their decisions to reject, and helps some long-haired men.",
        "content": "Figure 1: Demonstration of a DLP’s undesirable side effects on a simple example of hiring data (see §4.1). An unconstrained classifier (vertical line) hires candidates based on work experience, yielding higher hiring rates for men than for women. A DLP (dashed diagonal) achieves near-parity by differentiating based on an irrelevant attribute (hair length). The DLP hurts some short-haired women, flipping their decisions to reject, and helps some long-haired men.",
        "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1711.07076/1711.07076/hybrid_auto/images/1711.07076_page0_fig1.jpg",
        "context_before": "",
        "context_after": "classifier should be blind to the protected characteristic. Within the technical literature, these criteria are commonly referred to as disparate impact and disparate treatment, respectively.\n\nIn this paper, we call these technical criteria impact parity and treatment parity to distinguish them from their legal antecedents. The distinction between technical and legal terminology is important to maintain. While impact and treatment parity are inspired by legal concepts, technical approaches that "
      },
      "element_b": {
        "element_id": "1711.07076_table_1",
        "element_type": "table",
        "caption": "Table 1: Statistics of public datasets.",
        "content": "Table 1: Statistics of public datasets.",
        "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1711.07076/1711.07076/hybrid_auto/images/73253ae8394dedc7c7f2c1c9f3baecda271905c3ae24a045329e78298413e562.jpg",
        "context_before": "Figure 2 (left) shapes our basic intuition for what is happening: Considering the probability of admission for the unconstrained classifier (y-axis), students whose decisions are ‘flipped’ (after applying the fairness constraint) tend to be those close to the decision boundary.\n\nthe protected characteristic (Table 2), so no synthetic discrimination is applied.\n\nFigure 2 (left) shapes our basic intuition for what is happening: Considering the probability of admission for the unconstrained classif",
        "context_after": "accuracy-maximizing classifications $\\hat { y }$ (thresholding at .5), we then flip those predictions which close the gap fastest:\n\n1. Assign each example with $\\{ \\tilde { y } _ { i } = 0 , z _ { i } = b \\}$ or $\\{ \\tilde { y } _ { i } = 1 , z _ { i } = a \\}$ , a score $c _ { i }$ equal to the reduction in the p-gap divided by the reduction in accuracy:\n\n2. Flip examples in descending order according to this score until the desired CV-score is reached.\n\nFigure 1 shows the test set results of ap"
      },
      "edge_contexts": [
        {
          "source": "1711.07076_table_1",
          "target": "1711.07076_figure_1",
          "ref_text": "Figure 1 s",
          "context_snippet": "escending order according to this score until the desired CV-score is reached.\n\nFigure 1 shows the test set results of applying a DLP to the available historical data to "
        }
      ]
    },
    {
      "pair_id": "1711.07076_pair_3",
      "doc_id": "1711.07076",
      "element_a_id": "1711.07076_figure_2",
      "element_b_id": "1711.07076_table_2",
      "element_a_type": "figure",
      "element_b_type": "table",
      "pair_type": "figure+table",
      "hop_distance": 1,
      "path": [
        "1711.07076_figure_2",
        "1711.07076_table_2"
      ],
      "quality_score": 1.0,
      "overlap_with_existing_l1": false,
      "element_a": {
        "element_id": "1711.07076_figure_2",
        "element_type": "figure",
        "caption": "Figure 2: (left) probability of the sensitive variable versus (unconstrained) admission probability, on unseen test data. Downward triangles indicate individuals rejected only after applying the DLP (“treatment”), while upward triangles indicate individuals accepted only by the DLP. The remaining ${ \\sim } 4 { , } 0 0 0$ blue/yellow dots indicate people whose decisions are not altered. Many students benefiting from the DLP are males who ‘look like’ females based on other features, whereas females who ‘look like’ males are hurt by the DLP. Detail view (center) and summary statistics (right) of the same plot.",
        "content": "Figure 2: (left) probability of the sensitive variable versus (unconstrained) admission probability, on unseen test data. Downward triangles indicate individuals rejected only after applying the DLP (“treatment”), while upward triangles indicate individuals accepted only by the DLP. The remaining ${ \\sim } 4 { , } 0 0 0$ blue/yellow dots indicate people whose decisions are not altered. Many students benefiting from the DLP are males who ‘look like’ females based on other features, whereas females who ‘look like’ males are hurt by the DLP. Detail view (center) and summary statistics (right) of the same plot.",
        "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1711.07076/1711.07076/hybrid_auto/images/1711.07076_page0_fig4.jpg",
        "context_before": "",
        "context_after": "reasonable policies: For example, by applying per-group thresholds, we could hire the highest rated individuals in each group, rather than distorting rankings within groups based on how female/male individuals appear to be from their other features.\n\n4.2 Case study: Gender bias in CS graduate admissions\n\nFor our next example, we demonstrate a similar result but this time by analyzing real data with synthetic discrimination, to empirically demonstrate our arguments. We consider a sample of ${ \\si"
      },
      "element_b": {
        "element_id": "1711.07076_table_2",
        "element_type": "table",
        "caption": "Table 2: Comparison between unconstrained classification, DLPs, and thresholding schemes. Note that the $p \\%$ rules from [5] were the strongest that could be obtained with their method; on complex datasets $p \\%$ rules of $100 \\%$ are rarely obtained in practice, due to their specific approximation scheme. Employee and Customer datasets are from IBM, the others are UCI datasets.",
        "content": "Table 2: Comparison between unconstrained classification, DLPs, and thresholding schemes. Note that the $p \\%$ rules from [5] were the strongest that could be obtained with their method; on complex datasets $p \\%$ rules of $100 \\%$ are rarely obtained in practice, due to their specific approximation scheme. Employee and Customer datasets are from IBM, the others are UCI datasets.",
        "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1711.07076/1711.07076/hybrid_auto/images/0fbfcf39dc88d96ca575ba08fe2933a6560752dae4588585fbe2e534623dfe81.jpg",
        "context_before": "Starting from the\n\nTable 1: Statistics of public datasets.\n\nBasic info about these datasets (including the prediction target and protected feature) is shown in Table 1.\n\nThe protocol we follow is the same as in Section 4.2. Each of these datasets exhibits a certain degree of bias w.r.t. the protected characteristic (Table 2), so no synthetic discrimination is applied. In Table 2, we compare (1) The $p \\%$ rule obtained using the classifier of [5] compared to that of a naïve classifier (column k ",
        "context_after": "Coming to terms with treatment disparity. Legal considerations aside, treatment disparity approaches have three advantages over DLPs: they optimally trade accuracy for representativeness, preserve rankings among members of each group, and do no harm to members of the disadvantaged group. In addition, treatment disparity has another advantage: by setting class-dependent thresholds, it’s easier to understand how treatment disparity impacts individuals. It seems plausible that policy-makers could r"
      },
      "edge_contexts": [
        {
          "source": "1711.07076_table_2",
          "target": "1711.07076_figure_2",
          "ref_text": "Figure 2",
          "context_snippet": " about protected characteristics for several of these other fairness criteria.\n\nFigure 2 (left) shapes our basic intuition for what is happening: Considering the probab"
        }
      ]
    },
    {
      "pair_id": "1711.07076_pair_4",
      "doc_id": "1711.07076",
      "element_a_id": "1711.07076_table_1",
      "element_b_id": "1711.07076_figure_2",
      "element_a_type": "table",
      "element_b_type": "figure",
      "pair_type": "figure+table",
      "hop_distance": 1,
      "path": [
        "1711.07076_table_1",
        "1711.07076_figure_2"
      ],
      "quality_score": 1.0,
      "overlap_with_existing_l1": false,
      "element_a": {
        "element_id": "1711.07076_table_1",
        "element_type": "table",
        "caption": "Table 1: Statistics of public datasets.",
        "content": "Table 1: Statistics of public datasets.",
        "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1711.07076/1711.07076/hybrid_auto/images/73253ae8394dedc7c7f2c1c9f3baecda271905c3ae24a045329e78298413e562.jpg",
        "context_before": "Figure 2 (left) shapes our basic intuition for what is happening: Considering the probability of admission for the unconstrained classifier (y-axis), students whose decisions are ‘flipped’ (after applying the fairness constraint) tend to be those close to the decision boundary.\n\nthe protected characteristic (Table 2), so no synthetic discrimination is applied.\n\nFigure 2 (left) shapes our basic intuition for what is happening: Considering the probability of admission for the unconstrained classif",
        "context_after": "accuracy-maximizing classifications $\\hat { y }$ (thresholding at .5), we then flip those predictions which close the gap fastest:\n\n1. Assign each example with $\\{ \\tilde { y } _ { i } = 0 , z _ { i } = b \\}$ or $\\{ \\tilde { y } _ { i } = 1 , z _ { i } = a \\}$ , a score $c _ { i }$ equal to the reduction in the p-gap divided by the reduction in accuracy:\n\n2. Flip examples in descending order according to this score until the desired CV-score is reached.\n\nFigure 1 shows the test set results of ap"
      },
      "element_b": {
        "element_id": "1711.07076_figure_2",
        "element_type": "figure",
        "caption": "Figure 2: (left) probability of the sensitive variable versus (unconstrained) admission probability, on unseen test data. Downward triangles indicate individuals rejected only after applying the DLP (“treatment”), while upward triangles indicate individuals accepted only by the DLP. The remaining ${ \\sim } 4 { , } 0 0 0$ blue/yellow dots indicate people whose decisions are not altered. Many students benefiting from the DLP are males who ‘look like’ females based on other features, whereas females who ‘look like’ males are hurt by the DLP. Detail view (center) and summary statistics (right) of the same plot.",
        "content": "Figure 2: (left) probability of the sensitive variable versus (unconstrained) admission probability, on unseen test data. Downward triangles indicate individuals rejected only after applying the DLP (“treatment”), while upward triangles indicate individuals accepted only by the DLP. The remaining ${ \\sim } 4 { , } 0 0 0$ blue/yellow dots indicate people whose decisions are not altered. Many students benefiting from the DLP are males who ‘look like’ females based on other features, whereas females who ‘look like’ males are hurt by the DLP. Detail view (center) and summary statistics (right) of the same plot.",
        "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1711.07076/1711.07076/hybrid_auto/images/1711.07076_page0_fig4.jpg",
        "context_before": "",
        "context_after": "reasonable policies: For example, by applying per-group thresholds, we could hire the highest rated individuals in each group, rather than distorting rankings within groups based on how female/male individuals appear to be from their other features.\n\n4.2 Case study: Gender bias in CS graduate admissions\n\nFor our next example, we demonstrate a similar result but this time by analyzing real data with synthetic discrimination, to empirically demonstrate our arguments. We consider a sample of ${ \\si"
      },
      "edge_contexts": [
        {
          "source": "1711.07076_table_1",
          "target": "1711.07076_figure_2",
          "ref_text": "Figure 2",
          "context_snippet": "Figure 2 (left) shapes our basic intuition for what is happening: Considering the probab"
        }
      ]
    },
    {
      "pair_id": "1801.04385_pair_1",
      "doc_id": "1801.04385",
      "element_a_id": "1801.04385_table_1",
      "element_b_id": "1801.04385_figure_1",
      "element_a_type": "table",
      "element_b_type": "figure",
      "pair_type": "figure+table",
      "hop_distance": 1,
      "path": [
        "1801.04385_table_1",
        "1801.04385_figure_1"
      ],
      "quality_score": 1.0,
      "overlap_with_existing_l1": false,
      "element_a": {
        "element_id": "1801.04385_table_1",
        "element_type": "table",
        "caption": "Table 1: Examples of Simpson’s paradox in Stack Exchange data. For these variables, the trend in the outcome variable (answer acceptance) as a function of $X _ { p }$ in the aggregate data Xpreverses when the data disaggregated on $X _ { c }$ .",
        "content": "Table 1: Examples of Simpson’s paradox in Stack Exchange data. For these variables, the trend in the outcome variable (answer acceptance) as a function of $X _ { p }$ in the aggregate data Xpreverses when the data disaggregated on $X _ { c }$ .",
        "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1801.04385/1801.04385/hybrid_auto/images/7f42ddc2f4b37f32cafbcc5e1409bb37f84145852e4aca3b348893549be9fe45.jpg",
        "context_before": "Words: Number of words in the answer.\n\nLines of codes: Number of lines of codes in the answer.\n\n1hps://archive.org/details/stackexchange",
        "context_after": "URLs: Number of hyperlinks in the answer.\n\nReadability: Answer’s Flesch Reading Ease [14] score.\n\n4.2 Simpson’s Paradoxes on Stack Exchange\n\ne eleven variables in Stack Exchange data, result in 110 possible Simpson’s pairs. Among these, our method identies seven as instance of paradox. ese are listed in Table 1.\n\nOur approach automatically identies this example as Simpson’s paradox, as illustrated in Fig. 1. e gure shows average acceptance probability for an answer as a function of its pos"
      },
      "element_b": {
        "element_id": "1801.04385_figure_1",
        "element_type": "figure",
        "caption": "Figure 1: Simpson’s paradox in Stack Exchange data. Both plots show the probability an answer is accepted as the best answer to a question as a function of its position within user’s activity session. (a) Acceptance probability calculated over aggregated data has an upward trend, suggesting that answers written later in a session are more likely to be accepted as best answers. However, when data is disaggregated by session length (b), the trend reverses. Among answers produced during sessions of the same length (dierent colors represent dierent-length sessions), later answers are less likely to be accepted as best answers. (a) Aggregated Data",
        "content": "Figure 1: Simpson’s paradox in Stack Exchange data. Both plots show the probability an answer is accepted as the best answer to a question as a function of its position within user’s activity session. (a) Acceptance probability calculated over aggregated data has an upward trend, suggesting that answers written later in a session are more likely to be accepted as best answers. However, when data is disaggregated by session length (b), the trend reverses. Among answers produced during sessions of the same length (dierent colors represent dierent-length sessions), later answers are less likely to be accepted as best answers. (a) Aggregated Data",
        "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1801.04385/1801.04385/hybrid_auto/images/1801.04385_page0_fig2.jpg",
        "context_before": "",
        "context_after": "e eleven variables in Stack Exchange data, result in 110 possible Simpson’s pairs. Among these, our method identies seven as instance of paradox. ese are listed in Table 1.\n\nOur approach automatically identies this example as Simpson’s paradox, as illustrated in Fig. 1. e gure shows average acceptance probability for an answer as a function of its position (or\n\nindex) within a session. According to Fig. 1a, which reports aggregate acceptance probability, answers wrien later in a session a"
      },
      "edge_contexts": [
        {
          "source": "1801.04385_figure_1",
          "target": "1801.04385_table_1",
          "ref_text": "Table 1",
          "context_snippet": "ng these, our method identies seven as instance of paradox. ese are listed in Table 1.\n\nOur approach automatically identies this example as Simpson’s paradox, as il"
        }
      ]
    },
    {
      "pair_id": "1801.04385_pair_2",
      "doc_id": "1801.04385",
      "element_a_id": "1801.04385_figure_2",
      "element_b_id": "1801.04385_table_1",
      "element_a_type": "figure",
      "element_b_type": "table",
      "pair_type": "figure+table",
      "hop_distance": 1,
      "path": [
        "1801.04385_figure_2",
        "1801.04385_table_1"
      ],
      "quality_score": 1.0,
      "overlap_with_existing_l1": false,
      "element_a": {
        "element_id": "1801.04385_figure_2",
        "element_type": "figure",
        "caption": "(b) Disaggregated Data Figure 2: Novel Simpson’s paradox discovered in Stack Exchange data. Plots show the probability an answer is accepted as best answer as a function of the number of lifetime answers written by user over his or her tenure. (a) Acceptance probability calculated over aggregated data has an upward trend, with answers written by more experienced users (who have already posted more answers) more likely to be accepted as best answers. However, when data is disaggregated by reputation (b), the trend reverses. Among answers written by users with the same reputation (dierent colors represent reputation bins), those posted by users who had already written more answers are less likely to be accepted as best answers.",
        "content": "(b) Disaggregated Data Figure 2: Novel Simpson’s paradox discovered in Stack Exchange data. Plots show the probability an answer is accepted as best answer as a function of the number of lifetime answers written by user over his or her tenure. (a) Acceptance probability calculated over aggregated data has an upward trend, with answers written by more experienced users (who have already posted more answers) more likely to be accepted as best answers. However, when data is disaggregated by reputation (b), the trend reverses. Among answers written by users with the same reputation (dierent colors represent reputation bins), those posted by users who had already written more answers are less likely to be accepted as best answers.",
        "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1801.04385/1801.04385/hybrid_auto/images/1801.04385_page0_fig3.jpg",
        "context_before": "e eleven variables in Stack Exchange data, result in 110 possible Simpson’s pairs. Among these, our method identies seven as instance of paradox. ese are listed in Table 1.\n\nOur approach automatically identies this example as Simpson’s paradox, as illustrated in Fig. 1. e gure shows average acceptance probability for an answer as a function of its position (or\n\nindex) within a session. According to Fig. 1a, which reports aggregate acceptance probability, answers wrien later in a session a",
        "context_after": "produced during shorter sessions. In addition, there are many more shorter sessions than longer ones. Table 2 reports the number of sessions of dierent length. By far, the most common session has length one: users write only one answer during these sessions. Each longer session is about half as common as a session that is one answer shorter.\n\nWhat happens to the trend in the aggregated data? When calculating acceptance probability as a function of answer position, all sessions contribute to acc"
      },
      "element_b": {
        "element_id": "1801.04385_table_1",
        "element_type": "table",
        "caption": "Table 1: Examples of Simpson’s paradox in Stack Exchange data. For these variables, the trend in the outcome variable (answer acceptance) as a function of $X _ { p }$ in the aggregate data Xpreverses when the data disaggregated on $X _ { c }$ .",
        "content": "Table 1: Examples of Simpson’s paradox in Stack Exchange data. For these variables, the trend in the outcome variable (answer acceptance) as a function of $X _ { p }$ in the aggregate data Xpreverses when the data disaggregated on $X _ { c }$ .",
        "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1801.04385/1801.04385/hybrid_auto/images/7f42ddc2f4b37f32cafbcc5e1409bb37f84145852e4aca3b348893549be9fe45.jpg",
        "context_before": "Words: Number of words in the answer.\n\nLines of codes: Number of lines of codes in the answer.\n\n1hps://archive.org/details/stackexchange",
        "context_after": "URLs: Number of hyperlinks in the answer.\n\nReadability: Answer’s Flesch Reading Ease [14] score.\n\n4.2 Simpson’s Paradoxes on Stack Exchange\n\ne eleven variables in Stack Exchange data, result in 110 possible Simpson’s pairs. Among these, our method identies seven as instance of paradox. ese are listed in Table 1.\n\nOur approach automatically identies this example as Simpson’s paradox, as illustrated in Fig. 1. e gure shows average acceptance probability for an answer as a function of its pos"
      },
      "edge_contexts": [
        {
          "source": "1801.04385_figure_2",
          "target": "1801.04385_table_1",
          "ref_text": "Table 1",
          "context_snippet": "ng these, our method identies seven as instance of paradox. ese are listed in Table 1.\n\nOur approach automatically identies this example as Simpson’s paradox, as il"
        }
      ]
    },
    {
      "pair_id": "1801.04385_pair_3",
      "doc_id": "1801.04385",
      "element_a_id": "1801.04385_figure_2",
      "element_b_id": "1801.04385_table_2",
      "element_a_type": "figure",
      "element_b_type": "table",
      "pair_type": "figure+table",
      "hop_distance": 1,
      "path": [
        "1801.04385_figure_2",
        "1801.04385_table_2"
      ],
      "quality_score": 1.0,
      "overlap_with_existing_l1": false,
      "element_a": {
        "element_id": "1801.04385_figure_2",
        "element_type": "figure",
        "caption": "(b) Disaggregated Data Figure 2: Novel Simpson’s paradox discovered in Stack Exchange data. Plots show the probability an answer is accepted as best answer as a function of the number of lifetime answers written by user over his or her tenure. (a) Acceptance probability calculated over aggregated data has an upward trend, with answers written by more experienced users (who have already posted more answers) more likely to be accepted as best answers. However, when data is disaggregated by reputation (b), the trend reverses. Among answers written by users with the same reputation (dierent colors represent reputation bins), those posted by users who had already written more answers are less likely to be accepted as best answers.",
        "content": "(b) Disaggregated Data Figure 2: Novel Simpson’s paradox discovered in Stack Exchange data. Plots show the probability an answer is accepted as best answer as a function of the number of lifetime answers written by user over his or her tenure. (a) Acceptance probability calculated over aggregated data has an upward trend, with answers written by more experienced users (who have already posted more answers) more likely to be accepted as best answers. However, when data is disaggregated by reputation (b), the trend reverses. Among answers written by users with the same reputation (dierent colors represent reputation bins), those posted by users who had already written more answers are less likely to be accepted as best answers.",
        "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1801.04385/1801.04385/hybrid_auto/images/1801.04385_page0_fig3.jpg",
        "context_before": "e eleven variables in Stack Exchange data, result in 110 possible Simpson’s pairs. Among these, our method identies seven as instance of paradox. ese are listed in Table 1.\n\nOur approach automatically identies this example as Simpson’s paradox, as illustrated in Fig. 1. e gure shows average acceptance probability for an answer as a function of its position (or\n\nindex) within a session. According to Fig. 1a, which reports aggregate acceptance probability, answers wrien later in a session a",
        "context_after": "produced during shorter sessions. In addition, there are many more shorter sessions than longer ones. Table 2 reports the number of sessions of dierent length. By far, the most common session has length one: users write only one answer during these sessions. Each longer session is about half as common as a session that is one answer shorter.\n\nWhat happens to the trend in the aggregated data? When calculating acceptance probability as a function of answer position, all sessions contribute to acc"
      },
      "element_b": {
        "element_id": "1801.04385_table_2",
        "element_type": "table",
        "caption": "Table 2: Number of data points in each group   ",
        "content": "<table><tr><td>Session Length</td><td>1</td><td>2</td><td>3</td><td>4</td><td>5</td><td>6</td><td>7</td><td>8</td></tr><tr><td>Data points</td><td>7.2M</td><td>2.6M</td><td>1.3M</td><td>0.7M</td><td>0.4M</td><td>0.3M</td><td>0.2M</td><td>0.1M</td></tr></table>",
        "image_path": null,
        "context_before": "produced during shorter sessions. In addition, there are many more shorter sessions than longer ones. Table 2 reports the number of sessions of dierent length. By far, the most common session has length one: users write only one answer during these sessions. Each longer session is about half as common as a session that is one answer shorter.\n\nWhat happens to the trend in the aggregated data? When calculating acceptance probability as a function of answer position, all sessions contribute to acc",
        "context_after": "produced during shorter sessions. In addition, there are many more shorter sessions than longer ones. Table 2 reports the number of sessions of dierent length. By far, the most common session has length one: users write only one answer during these sessions. Each longer session is about half as common as a session that is one answer shorter.\n\nWhat happens to the trend in the aggregated data? When calculating acceptance probability as a function of answer position, all sessions contribute to acc"
      },
      "edge_contexts": [
        {
          "source": "1801.04385_figure_2",
          "target": "1801.04385_table_2",
          "ref_text": "Table 2",
          "context_snippet": "r sessions. In addition, there are many more shorter sessions than longer ones. Table 2 reports the number of sessions of dierent length. By far, the most common sess"
        }
      ]
    },
    {
      "pair_id": "1801.07593_pair_1",
      "doc_id": "1801.07593",
      "element_a_id": "1801.07593_figure_2",
      "element_b_id": "1801.07593_table_1",
      "element_a_type": "figure",
      "element_b_type": "table",
      "pair_type": "figure+table",
      "hop_distance": 1,
      "path": [
        "1801.07593_figure_2",
        "1801.07593_table_1"
      ],
      "quality_score": 1.0,
      "overlap_with_existing_l1": false,
      "element_a": {
        "element_id": "1801.07593_figure_2",
        "element_type": "figure",
        "caption": "Figure 2: Diagram illustrating the gradients in Eqn. 1 and the relevance of the projection term $\\mathrm { p r o j } _ { h } g$ . Without the projection term, in the pictured scenario, the predictor would move in the direction labelled $g + h$ in the diagram, which actually helps the adversary. With the projection term, the predictor will never move in a direction that helps the adversary.",
        "content": "Figure 2: Diagram illustrating the gradients in Eqn. 1 and the relevance of the projection term $\\mathrm { p r o j } _ { h } g$ . Without the projection term, in the pictured scenario, the predictor would move in the direction labelled $g + h$ in the diagram, which actually helps the adversary. With the projection term, the predictor will never move in a direction that helps the adversary.",
        "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1801.07593/1801.07593/hybrid_auto/images/1801.07593_page0_fig1.jpg",
        "context_before": "As in Figure 1, we assume that the model is trained by attempting to modify weights $W$ to minimize some loss $L _ { P } ( \\hat { y } , y )$ , using a gradient-based method such as stochastic gradient descent.\n\n(2016), the\n\nTable 1: Completions for he : she :: doctor : ?\n\nWe update $U$ to minimize $L _ { A }$ at each training time step, according to the gradient $\\nabla _ { U } L _ { A }$ . We modify $W$ according\n\n2Achieving equality of odds and demographic parity are generally incongruent goal",
        "context_after": "$$ \\nabla_ {W} L _ {P} - \\operatorname {p r o j} _ {\\nabla_ {W} L _ {A}} \\nabla_ {W} L _ {P} - \\alpha \\nabla_ {W} L _ {A} \\tag {1} $$\n\nwhere $\\alpha$ is a tuneable hyperparameter that can vary at each time step and we define $\\mathrm { p r o j } _ { v } x = 0$ if $v = 0$ .\n\nThe middle term p $\\mathbf { r o j } _ { \\nabla _ { W } L _ { A } } \\nabla _ { W } L _ { P }$ prevents the predictor from moving in a direction that helps the adversary decrease its loss while the last term, $\\alpha \\nabla _ "
      },
      "element_b": {
        "element_id": "1801.07593_table_1",
        "element_type": "table",
        "caption": "Table 1: Completions for he : she :: doctor : ?",
        "content": "Table 1: Completions for he : she :: doctor : ?",
        "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1801.07593/1801.07593/hybrid_auto/images/a9cae1e5ef3a65c7111c90a7f5a24a9469357bff317b2c966c9bb2effe311d98.jpg",
        "context_before": "Note that, in this proof, the adversary can be operating in a few different ways, as long as it is given $\\hat { Y }$ as one of its inputs; for example, for demographic parity, it could be given only $\\hat { Y }$ ; for equality of odds, it can be given both $\\hat { Y }$ and $Y$ .\n\nWe will show in the next propositions that the adversary gaining no advantage from information about $\\hat { Y }$ is exactly the condition needed to guarantee that desired definitions of equality are satisfied.\n\nPropos",
        "context_after": "technique generalizes to other protected variables and other forms of embeddings. Following Bolukbasi et al. (2016), we pick 10 (male, female) word pairs, and define the and define the bias subspace to be the space spanned by the top $k$ principal components of the differences, where $k$ is a tuneable parameter. In our experiments, we find that $k = 1$ gives reasonable results, so we did not experiment further.\n\nWe use embeddings trained from Wikipedia to generate input data from the Google anal"
      },
      "edge_contexts": [
        {
          "source": "1801.07593_figure_2",
          "target": "1801.07593_table_1",
          "ref_text": "Table 1",
          "context_snippet": "sing a gradient-based method such as stochastic gradient descent.\n\n(2016), the\n\nTable 1: Completions for he : she :: doctor : ?\n\nWe update $U$ to minimize $L _ { A }$ "
        }
      ]
    },
    {
      "pair_id": "1801.07593_pair_2",
      "doc_id": "1801.07593",
      "element_a_id": "1801.07593_figure_2",
      "element_b_id": "1801.07593_table_2",
      "element_a_type": "figure",
      "element_b_type": "table",
      "pair_type": "figure+table",
      "hop_distance": 1,
      "path": [
        "1801.07593_figure_2",
        "1801.07593_table_2"
      ],
      "quality_score": 1.0,
      "overlap_with_existing_l1": false,
      "element_a": {
        "element_id": "1801.07593_figure_2",
        "element_type": "figure",
        "caption": "Figure 2: Diagram illustrating the gradients in Eqn. 1 and the relevance of the projection term $\\mathrm { p r o j } _ { h } g$ . Without the projection term, in the pictured scenario, the predictor would move in the direction labelled $g + h$ in the diagram, which actually helps the adversary. With the projection term, the predictor will never move in a direction that helps the adversary.",
        "content": "Figure 2: Diagram illustrating the gradients in Eqn. 1 and the relevance of the projection term $\\mathrm { p r o j } _ { h } g$ . Without the projection term, in the pictured scenario, the predictor would move in the direction labelled $g + h$ in the diagram, which actually helps the adversary. With the projection term, the predictor will never move in a direction that helps the adversary.",
        "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1801.07593/1801.07593/hybrid_auto/images/1801.07593_page0_fig1.jpg",
        "context_before": "As in Figure 1, we assume that the model is trained by attempting to modify weights $W$ to minimize some loss $L _ { P } ( \\hat { y } , y )$ , using a gradient-based method such as stochastic gradient descent.\n\n(2016), the\n\nTable 1: Completions for he : she :: doctor : ?\n\nWe update $U$ to minimize $L _ { A }$ at each training time step, according to the gradient $\\nabla _ { U } L _ { A }$ . We modify $W$ according\n\n2Achieving equality of odds and demographic parity are generally incongruent goal",
        "context_after": "$$ \\nabla_ {W} L _ {P} - \\operatorname {p r o j} _ {\\nabla_ {W} L _ {A}} \\nabla_ {W} L _ {P} - \\alpha \\nabla_ {W} L _ {A} \\tag {1} $$\n\nwhere $\\alpha$ is a tuneable hyperparameter that can vary at each time step and we define $\\mathrm { p r o j } _ { v } x = 0$ if $v = 0$ .\n\nThe middle term p $\\mathbf { r o j } _ { \\nabla _ { W } L _ { A } } \\nabla _ { W } L _ { P }$ prevents the predictor from moving in a direction that helps the adversary decrease its loss while the last term, $\\alpha \\nabla _ "
      },
      "element_b": {
        "element_id": "1801.07593_table_2",
        "element_type": "table",
        "caption": "Table 2: Features in the UCI dataset per individual. Features are either continuous (Cont) or Categorical (Cat). Categorical features are converted to sparse tensors for the model.",
        "content": "Table 2: Features in the UCI dataset per individual. Features are either continuous (Cont) or Categorical (Cat). Categorical features are converted to sparse tensors for the model.",
        "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1801.07593/1801.07593/hybrid_auto/images/24ed65306e56dfae5a76f82264e0d84dfadd62bf9e63acafad5c5b29d78a1bb9.jpg",
        "context_before": "As in Figure 1, we assume that the model is trained by attempting to modify weights $W$ to minimize some loss $L _ { P } ( \\hat { y } , y )$ , using a gradient-based method such as stochastic gradient descent.\n\n(2016), the\n\nTable 1: Completions for he : she :: doctor : ?\n\nWe use embeddings trained from Wikipedia to generate input data from the Google analogy data set (Mikolov et al. 2013). For each analogy in the dataset, we let $x =$ $\\left( { { x } _ { 1 } } , { { x } _ { 2 } } , { { x } _ { 3",
        "context_after": "To better align with the work in Beutel et al. (2017), we attempt to enforce EQUALITY OF ODDS on a model for the task of predicting the income of a person – in particular, predicting whether the income is $> { \\mathfrak { S } } 5 0 k$ – given various attributes about the person, as made available in the UCI Adult dataset (Asuncion and Newman 2007).\n\nDetails on the features that the dataset provides are available in Table 2. We use both categorical and continuous columns as given, with exception "
      },
      "edge_contexts": [
        {
          "source": "1801.07593_table_2",
          "target": "1801.07593_figure_2",
          "ref_text": "Fig. 2",
          "context_snippet": "ion term, it is possible for the predictor to end up helping the adversary (see Fig. 2). Without the last term, the predictor will never try to hurt the adversary, an"
        }
      ]
    },
    {
      "pair_id": "1801.07593_pair_7",
      "doc_id": "1801.07593",
      "element_a_id": "1801.07593_figure_3",
      "element_b_id": "1801.07593_table_2",
      "element_a_type": "figure",
      "element_b_type": "table",
      "pair_type": "figure+table",
      "hop_distance": 1,
      "path": [
        "1801.07593_figure_3",
        "1801.07593_table_2"
      ],
      "quality_score": 1.0,
      "overlap_with_existing_l1": false,
      "element_a": {
        "element_id": "1801.07593_figure_3",
        "element_type": "figure",
        "caption": "Note that, in this proof, the adversary can be operating in a few different ways, as long as it is given $\\hat { Y }$ as one of its inputs; for example, for demographic parity, it could be given only $\\hat { Y }$ ; for e",
        "content": "Note that, in this proof, the adversary can be operating in a few different ways, as long as it is given $\\hat { Y }$ as one of its inputs; for example, for demographic parity, it could be given only $\\hat { Y }$ ; for e",
        "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1801.07593/1801.07593/hybrid_auto/images/1801.07593_page0_fig2.jpg",
        "context_before": "Details on the features that the dataset provides are available in Table 2. We use both categorical and continuous columns as given, with exception to the fnlwgt feature, which we discard. We convert the remaining columns into tensors where the categorical columns are sparse tensors, age is bucketized at boundaries [18, 25, 30, 35, 40, 45, 50, 55, 60, 65], and the rest of the continuous columns are real-valued.\n\nDetails on the features that the dataset provides are available in Table 2.\n\nProof. ",
        "context_after": "Note that, in this proof, the adversary can be operating in a few different ways, as long as it is given $\\hat { Y }$ as one of its inputs; for example, for demographic parity, it could be given only $\\hat { Y }$ ; for equality of odds, it can be given both $\\hat { Y }$ and $Y$ .\n\nWe will show in the next propositions that the adversary gaining no advantage from information about $\\hat { Y }$ is exactly the condition needed to guarantee that desired definitions of equality are satisfied.\n\nPropos"
      },
      "element_b": {
        "element_id": "1801.07593_table_2",
        "element_type": "table",
        "caption": "Table 2: Features in the UCI dataset per individual. Features are either continuous (Cont) or Categorical (Cat). Categorical features are converted to sparse tensors for the model.",
        "content": "Table 2: Features in the UCI dataset per individual. Features are either continuous (Cont) or Categorical (Cat). Categorical features are converted to sparse tensors for the model.",
        "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1801.07593/1801.07593/hybrid_auto/images/24ed65306e56dfae5a76f82264e0d84dfadd62bf9e63acafad5c5b29d78a1bb9.jpg",
        "context_before": "As in Figure 1, we assume that the model is trained by attempting to modify weights $W$ to minimize some loss $L _ { P } ( \\hat { y } , y )$ , using a gradient-based method such as stochastic gradient descent.\n\n(2016), the\n\nTable 1: Completions for he : she :: doctor : ?\n\nWe use embeddings trained from Wikipedia to generate input data from the Google analogy data set (Mikolov et al. 2013). For each analogy in the dataset, we let $x =$ $\\left( { { x } _ { 1 } } , { { x } _ { 2 } } , { { x } _ { 3",
        "context_after": "To better align with the work in Beutel et al. (2017), we attempt to enforce EQUALITY OF ODDS on a model for the task of predicting the income of a person – in particular, predicting whether the income is $> { \\mathfrak { S } } 5 0 k$ – given various attributes about the person, as made available in the UCI Adult dataset (Asuncion and Newman 2007).\n\nDetails on the features that the dataset provides are available in Table 2. We use both categorical and continuous columns as given, with exception "
      },
      "edge_contexts": [
        {
          "source": "1801.07593_figure_3",
          "target": "1801.07593_table_2",
          "ref_text": "Table 2",
          "context_snippet": "Details on the features that the dataset provides are available in Table 2. We use both categorical and continuous columns as given, with exception to the"
        }
      ]
    },
    {
      "pair_id": "1801.07593_pair_8",
      "doc_id": "1801.07593",
      "element_a_id": "1801.07593_table_1",
      "element_b_id": "1801.07593_figure_1",
      "element_a_type": "table",
      "element_b_type": "figure",
      "pair_type": "figure+table",
      "hop_distance": 1,
      "path": [
        "1801.07593_table_1",
        "1801.07593_figure_1"
      ],
      "quality_score": 1.0,
      "overlap_with_existing_l1": false,
      "element_a": {
        "element_id": "1801.07593_table_1",
        "element_type": "table",
        "caption": "Table 1: Completions for he : she :: doctor : ?",
        "content": "Table 1: Completions for he : she :: doctor : ?",
        "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1801.07593/1801.07593/hybrid_auto/images/a9cae1e5ef3a65c7111c90a7f5a24a9469357bff317b2c966c9bb2effe311d98.jpg",
        "context_before": "Note that, in this proof, the adversary can be operating in a few different ways, as long as it is given $\\hat { Y }$ as one of its inputs; for example, for demographic parity, it could be given only $\\hat { Y }$ ; for equality of odds, it can be given both $\\hat { Y }$ and $Y$ .\n\nWe will show in the next propositions that the adversary gaining no advantage from information about $\\hat { Y }$ is exactly the condition needed to guarantee that desired definitions of equality are satisfied.\n\nPropos",
        "context_after": "technique generalizes to other protected variables and other forms of embeddings. Following Bolukbasi et al. (2016), we pick 10 (male, female) word pairs, and define the and define the bias subspace to be the space spanned by the top $k$ principal components of the differences, where $k$ is a tuneable parameter. In our experiments, we find that $k = 1$ gives reasonable results, so we did not experiment further.\n\nWe use embeddings trained from Wikipedia to generate input data from the Google anal"
      },
      "element_b": {
        "element_id": "1801.07593_figure_1",
        "element_type": "figure",
        "caption": "Figure 1: The architecture of the adversarial network.",
        "content": "Figure 1: The architecture of the adversarial network.",
        "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1801.07593/1801.07593/hybrid_auto/images/1801.07593_page0_fig0.jpg",
        "context_before": "There has been significant work done in the area of debiasing various specific types of data or predictor.\n\nDebiasing word embeddings: Bolukbasi et al. (2016) devises a method to remove gender bias from word embeddings. The method relies on a lot of human input; namely, it needs a large “training set” of gender-specific words.\n\nSimple models: Lum and Johndrow (2016) demonstrate that removing the protected variable from the training data fails to yield a debiased model (since other variables can ",
        "context_after": "(2016) discuss the shortcomings of focusing solely on DE-MOGRAPHIC PARITY, present alternate definitions of fairness, and devise a method for deriving an unbiased predictor from a biased one, in cases when both the output variable and the protected variable are discrete.\n\nAdversarial training: Goodfellow et al. (2014) pioneered the technique of using multiple networks with competing goals to force the first network to “deceive” the second network, applying this method to the problem of creating "
      },
      "edge_contexts": [
        {
          "source": "1801.07593_table_1",
          "target": "1801.07593_figure_1",
          "ref_text": "Figure 1",
          "context_snippet": "e predictor, trained to accomplish the task of predicting $Y$ given $X$ . As in Figure 1, we assume that the model is trained by attempting to modify weights $W$ to min"
        }
      ]
    },
    {
      "pair_id": "1801.07593_pair_9",
      "doc_id": "1801.07593",
      "element_a_id": "1801.07593_table_2",
      "element_b_id": "1801.07593_figure_1",
      "element_a_type": "table",
      "element_b_type": "figure",
      "pair_type": "figure+table",
      "hop_distance": 1,
      "path": [
        "1801.07593_table_2",
        "1801.07593_figure_1"
      ],
      "quality_score": 1.0,
      "overlap_with_existing_l1": false,
      "element_a": {
        "element_id": "1801.07593_table_2",
        "element_type": "table",
        "caption": "Table 2: Features in the UCI dataset per individual. Features are either continuous (Cont) or Categorical (Cat). Categorical features are converted to sparse tensors for the model.",
        "content": "Table 2: Features in the UCI dataset per individual. Features are either continuous (Cont) or Categorical (Cat). Categorical features are converted to sparse tensors for the model.",
        "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1801.07593/1801.07593/hybrid_auto/images/24ed65306e56dfae5a76f82264e0d84dfadd62bf9e63acafad5c5b29d78a1bb9.jpg",
        "context_before": "As in Figure 1, we assume that the model is trained by attempting to modify weights $W$ to minimize some loss $L _ { P } ( \\hat { y } , y )$ , using a gradient-based method such as stochastic gradient descent.\n\n(2016), the\n\nTable 1: Completions for he : she :: doctor : ?\n\nWe use embeddings trained from Wikipedia to generate input data from the Google analogy data set (Mikolov et al. 2013). For each analogy in the dataset, we let $x =$ $\\left( { { x } _ { 1 } } , { { x } _ { 2 } } , { { x } _ { 3",
        "context_after": "To better align with the work in Beutel et al. (2017), we attempt to enforce EQUALITY OF ODDS on a model for the task of predicting the income of a person – in particular, predicting whether the income is $> { \\mathfrak { S } } 5 0 k$ – given various attributes about the person, as made available in the UCI Adult dataset (Asuncion and Newman 2007).\n\nDetails on the features that the dataset provides are available in Table 2. We use both categorical and continuous columns as given, with exception "
      },
      "element_b": {
        "element_id": "1801.07593_figure_1",
        "element_type": "figure",
        "caption": "Figure 1: The architecture of the adversarial network.",
        "content": "Figure 1: The architecture of the adversarial network.",
        "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1801.07593/1801.07593/hybrid_auto/images/1801.07593_page0_fig0.jpg",
        "context_before": "There has been significant work done in the area of debiasing various specific types of data or predictor.\n\nDebiasing word embeddings: Bolukbasi et al. (2016) devises a method to remove gender bias from word embeddings. The method relies on a lot of human input; namely, it needs a large “training set” of gender-specific words.\n\nSimple models: Lum and Johndrow (2016) demonstrate that removing the protected variable from the training data fails to yield a debiased model (since other variables can ",
        "context_after": "(2016) discuss the shortcomings of focusing solely on DE-MOGRAPHIC PARITY, present alternate definitions of fairness, and devise a method for deriving an unbiased predictor from a biased one, in cases when both the output variable and the protected variable are discrete.\n\nAdversarial training: Goodfellow et al. (2014) pioneered the technique of using multiple networks with competing goals to force the first network to “deceive” the second network, applying this method to the problem of creating "
      },
      "edge_contexts": [
        {
          "source": "1801.07593_table_2",
          "target": "1801.07593_figure_1",
          "ref_text": "Figure 1",
          "context_snippet": "As in Figure 1, we assume that the model is trained by attempting to modify weights $W$ to min"
        }
      ]
    },
    {
      "pair_id": "1802.08139_pair_8",
      "doc_id": "1802.08139",
      "element_a_id": "1802.08139_table_1",
      "element_b_id": "1802.08139_figure_4",
      "element_a_type": "table",
      "element_b_type": "figure",
      "pair_type": "figure+table",
      "hop_distance": 1,
      "path": [
        "1802.08139_table_1",
        "1802.08139_figure_4"
      ],
      "quality_score": 1.0,
      "overlap_with_existing_l1": false,
      "element_a": {
        "element_id": "1802.08139_table_1",
        "element_type": "table",
        "caption": "Table 1. In order columns represent: unfair test accuracy, fair test accuracy, and MMD values for $H _ { m }$ , $H _ { l }$ , and $H _ { r }$ $( \\times \\ 1 0 { , } 0 0 0 )$ for the UCI Adult dataset. Rows represent values after 5,000, 8,000, 15,000, and 20,000 training steps.",
        "content": "Table 1. In order columns represent: unfair test accuracy, fair test accuracy, and MMD values for $H _ { m }$ , $H _ { l }$ , and $H _ { r }$ $( \\times \\ 1 0 { , } 0 0 0 )$ for the UCI Adult dataset. Rows represent values after 5,000, 8,000, 15,000, and 20,000 training steps.",
        "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1802.08139/1802.08139/hybrid_auto/images/7aba8c4fe5f535c8fe5ad4312414bb3fed9b54ea616ef261d8df2528fab733e4.jpg",
        "context_before": "This could be preferable to other causal latent variable approaches such as the FairLearning algorithm proposed in Kusner et al. (2017), which separately learns a predictor of $Y$ using samples from the previously inferred latent variables and from the non-descendants of $A$ .\n\nIn order for $\\mathcal { F } _ { \\theta , \\phi }$ to be tractable conjugacy is required, which heavily restricts the family of models that can be used. This issue can be addressed with a Monte-Carlo approximation recently",
        "context_after": "week are continuous, whilst sex, nationality, marital status, working class, occupation, and income are categorical. Besides the direct effect $A Y$ , we would like to remove the effect of $A$ on $Y$ through marital status, namely along the paths $A \\to M \\to , . . . , \\to Y$ . This GCM is similar to the one analyzed in $\\ S 3 . 2$ and, except for the latent variables, is the same as the one used in Nabi & Shpitser (2018).\n\nNabi & Shpitser (2018) assume that all variables, except $A$ and $Y$ are"
      },
      "element_b": {
        "element_id": "1802.08139_figure_4",
        "element_type": "figure",
        "caption": "Figure 4. (a): GCM for the UCI Adult dataset. (b): GCM for the UCI German Credit dataset.",
        "content": "Figure 4. (a): GCM for the UCI Adult dataset. (b): GCM for the UCI German Credit dataset.",
        "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1802.08139/1802.08139/hybrid_auto/images/1802.08139_page0_fig9.jpg",
        "context_before": "",
        "context_after": "This could be preferable to other causal latent variable approaches such as the FairLearning algorithm proposed in Kusner et al. (2017), which separately learns a predictor of $Y$ using samples from the previously inferred latent variables and from the non-descendants of $A$ .\n\nIn order for $\\mathcal { F } _ { \\theta , \\phi }$ to be tractable conjugacy is required, which heavily restricts the family of models that can be used. This issue can be addressed with a Monte-Carlo approximation recently"
      },
      "edge_contexts": [
        {
          "source": "1802.08139_table_1",
          "target": "1802.08139_figure_4",
          "ref_text": "Fig. 4",
          "context_snippet": "edit risk, i.e. as likely or not likely to repay the loan. We assume the GCM in Fig. 4(b), where $A$ corresponds to the protected attribute sex, $C$ to age, $S$ to th"
        }
      ]
    },
    {
      "pair_id": "1802.08139_pair_9",
      "doc_id": "1802.08139",
      "element_a_id": "1802.08139_table_1",
      "element_b_id": "1802.08139_figure_1",
      "element_a_type": "table",
      "element_b_type": "figure",
      "pair_type": "figure+table",
      "hop_distance": 1,
      "path": [
        "1802.08139_table_1",
        "1802.08139_figure_1"
      ],
      "quality_score": 1.0,
      "overlap_with_existing_l1": false,
      "element_a": {
        "element_id": "1802.08139_table_1",
        "element_type": "table",
        "caption": "Table 1. In order columns represent: unfair test accuracy, fair test accuracy, and MMD values for $H _ { m }$ , $H _ { l }$ , and $H _ { r }$ $( \\times \\ 1 0 { , } 0 0 0 )$ for the UCI Adult dataset. Rows represent values after 5,000, 8,000, 15,000, and 20,000 training steps.",
        "content": "Table 1. In order columns represent: unfair test accuracy, fair test accuracy, and MMD values for $H _ { m }$ , $H _ { l }$ , and $H _ { r }$ $( \\times \\ 1 0 { , } 0 0 0 )$ for the UCI Adult dataset. Rows represent values after 5,000, 8,000, 15,000, and 20,000 training steps.",
        "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1802.08139/1802.08139/hybrid_auto/images/7aba8c4fe5f535c8fe5ad4312414bb3fed9b54ea616ef261d8df2528fab733e4.jpg",
        "context_before": "This could be preferable to other causal latent variable approaches such as the FairLearning algorithm proposed in Kusner et al. (2017), which separately learns a predictor of $Y$ using samples from the previously inferred latent variables and from the non-descendants of $A$ .\n\nIn order for $\\mathcal { F } _ { \\theta , \\phi }$ to be tractable conjugacy is required, which heavily restricts the family of models that can be used. This issue can be addressed with a Monte-Carlo approximation recently",
        "context_after": "week are continuous, whilst sex, nationality, marital status, working class, occupation, and income are categorical. Besides the direct effect $A Y$ , we would like to remove the effect of $A$ on $Y$ through marital status, namely along the paths $A \\to M \\to , . . . , \\to Y$ . This GCM is similar to the one analyzed in $\\ S 3 . 2$ and, except for the latent variables, is the same as the one used in Nabi & Shpitser (2018).\n\nNabi & Shpitser (2018) assume that all variables, except $A$ and $Y$ are"
      },
      "element_b": {
        "element_id": "1802.08139_figure_1",
        "element_type": "figure",
        "caption": "(c) Figure 1. (a): GCM with a confounder $C$ for the causal effect of $A$ on $Y$ . (b): GCM with one direct and one indirect causal path from $A$ to $Y$ . (c): GCM with a confounder $C$ for the effect of $M$ on $Y$ .",
        "content": "(c) Figure 1. (a): GCM with a confounder $C$ for the causal effect of $A$ on $Y$ . (b): GCM with one direct and one indirect causal path from $A$ to $Y$ . (c): GCM with a confounder $C$ for the effect of $M$ on $Y$ .",
        "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1802.08139/1802.08139/hybrid_auto/images/1802.08139_page0_fig2.jpg",
        "context_before": "",
        "context_after": "gender through department choice is not unfair.\n\nTo deal with such scenarios, we propose a novel fairness definition called path-specific counterfactual fairness, which states that a decision is fair toward an individual if it coincides with the one that would have been taken in a counterfactual world in which the sensitive attribute along the unfair pathways were different.\n\nIn order to achieve path-specific counterfactual fairness, a decision system needs to be able to discern the causal effec"
      },
      "edge_contexts": [
        {
          "source": "1802.08139_table_1",
          "target": "1802.08139_figure_1",
          "ref_text": "Fig. 1",
          "context_snippet": "f an open undirected path from $A$ to $Y$ is given by $A \\left. C \\right. Y$ in Fig. 1(a): the variable\n\nIf confounders are present, then the causal effect can be ret"
        }
      ]
    },
    {
      "pair_id": "1802.08139_pair_11",
      "doc_id": "1802.08139",
      "element_a_id": "1802.08139_figure_6",
      "element_b_id": "1802.08139_table_2",
      "element_a_type": "figure",
      "element_b_type": "table",
      "pair_type": "figure+table",
      "hop_distance": 1,
      "path": [
        "1802.08139_figure_6",
        "1802.08139_table_2"
      ],
      "quality_score": 1.0,
      "overlap_with_existing_l1": false,
      "element_a": {
        "element_id": "1802.08139_figure_6",
        "element_type": "figure",
        "caption": "Figure 6. Histograms of $\\widetilde { q } ( H _ { s } | A )$ after 2,000 and 8,000 training steps for one dimension of the variable housing.",
        "content": "Figure 6. Histograms of $\\widetilde { q } ( H _ { s } | A )$ after 2,000 and 8,000 training steps for one dimension of the variable housing.",
        "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1802.08139/1802.08139/hybrid_auto/images/1802.08139_page0_fig14.jpg",
        "context_before": "In Table 2, we show the unfair and fair test accuracy and the MMD values for $H _ { s }$ after 2,000, 4,000, and 8,000 training steps (the results remain similar with a higher number of training steps). As we can see, unfair and fair accuracy, and MMD values are similar for all iterations. This indicates that, unlike the Adult dataset, model-observations mismatch is not problematic. This is confirmed by $\\widetilde { q } ( H _ { s } | A )$ ; we learn a structured distribution which does not diff",
        "context_after": ""
      },
      "element_b": {
        "element_id": "1802.08139_table_2",
        "element_type": "table",
        "caption": "Table 2. In order columns represent: unfair test accuracy, fair test accuracy, and MMD values for $H _ { s }$ $( \\times ~ 1 0 0 )$ for the UCI German Credit dataset. Rows represent values after 2,000, 4,000, and 8,000 training steps.",
        "content": "Table 2. In order columns represent: unfair test accuracy, fair test accuracy, and MMD values for $H _ { s }$ $( \\times ~ 1 0 0 )$ for the UCI German Credit dataset. Rows represent values after 2,000, 4,000, and 8,000 training steps.",
        "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1802.08139/1802.08139/hybrid_auto/images/b3fb2ac1c0dbc2e93cde0695817bae19a24c605c323fd094e8312e08228f5035.jpg",
        "context_before": "males and females for increasing numbers of training steps. The remaining variables are shown in the Appendix. As can be seen, the addition of the MMD penalization to the variational bound for more training steps has the effect of reducing the number of modes in the posterior. From the evidence available, it is unclear if the shape changes are a necessary consequence of enforcing them to be similar, or if a simplification of the latent space is a more fundamental drawback of the MMD method. We l",
        "context_after": "In Table 2, we show the unfair and fair test accuracy and the MMD values for $H _ { s }$ after 2,000, 4,000, and 8,000 training steps (the results remain similar with a higher number of training steps). As we can see, unfair and fair accuracy, and MMD values are similar for all iterations. This indicates that, unlike the Adult dataset, model-observations mismatch is not problematic. This is confirmed by $\\widetilde { q } ( H _ { s } | A )$ ; we learn a structured distribution which does not diff"
      },
      "edge_contexts": [
        {
          "source": "1802.08139_table_2",
          "target": "1802.08139_figure_6",
          "ref_text": "Fig. 6",
          "context_snippet": "ured distribution which does not differ significantly for females and males. In Fig. 6, we show $\\widetilde { q } ( H _ { s } | A )$ for one dimension of the variable"
        }
      ]
    },
    {
      "pair_id": "1802.08139_pair_12",
      "doc_id": "1802.08139",
      "element_a_id": "1802.08139_table_2",
      "element_b_id": "1802.08139_figure_4",
      "element_a_type": "table",
      "element_b_type": "figure",
      "pair_type": "figure+table",
      "hop_distance": 1,
      "path": [
        "1802.08139_table_2",
        "1802.08139_figure_4"
      ],
      "quality_score": 1.0,
      "overlap_with_existing_l1": false,
      "element_a": {
        "element_id": "1802.08139_table_2",
        "element_type": "table",
        "caption": "Table 2. In order columns represent: unfair test accuracy, fair test accuracy, and MMD values for $H _ { s }$ $( \\times ~ 1 0 0 )$ for the UCI German Credit dataset. Rows represent values after 2,000, 4,000, and 8,000 training steps.",
        "content": "Table 2. In order columns represent: unfair test accuracy, fair test accuracy, and MMD values for $H _ { s }$ $( \\times ~ 1 0 0 )$ for the UCI German Credit dataset. Rows represent values after 2,000, 4,000, and 8,000 training steps.",
        "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1802.08139/1802.08139/hybrid_auto/images/b3fb2ac1c0dbc2e93cde0695817bae19a24c605c323fd094e8312e08228f5035.jpg",
        "context_before": "males and females for increasing numbers of training steps. The remaining variables are shown in the Appendix. As can be seen, the addition of the MMD penalization to the variational bound for more training steps has the effect of reducing the number of modes in the posterior. From the evidence available, it is unclear if the shape changes are a necessary consequence of enforcing them to be similar, or if a simplification of the latent space is a more fundamental drawback of the MMD method. We l",
        "context_after": "In Table 2, we show the unfair and fair test accuracy and the MMD values for $H _ { s }$ after 2,000, 4,000, and 8,000 training steps (the results remain similar with a higher number of training steps). As we can see, unfair and fair accuracy, and MMD values are similar for all iterations. This indicates that, unlike the Adult dataset, model-observations mismatch is not problematic. This is confirmed by $\\widetilde { q } ( H _ { s } | A )$ ; we learn a structured distribution which does not diff"
      },
      "element_b": {
        "element_id": "1802.08139_figure_4",
        "element_type": "figure",
        "caption": "Figure 4. (a): GCM for the UCI Adult dataset. (b): GCM for the UCI German Credit dataset.",
        "content": "Figure 4. (a): GCM for the UCI Adult dataset. (b): GCM for the UCI German Credit dataset.",
        "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1802.08139/1802.08139/hybrid_auto/images/1802.08139_page0_fig9.jpg",
        "context_before": "",
        "context_after": "This could be preferable to other causal latent variable approaches such as the FairLearning algorithm proposed in Kusner et al. (2017), which separately learns a predictor of $Y$ using samples from the previously inferred latent variables and from the non-descendants of $A$ .\n\nIn order for $\\mathcal { F } _ { \\theta , \\phi }$ to be tractable conjugacy is required, which heavily restricts the family of models that can be used. This issue can be addressed with a Monte-Carlo approximation recently"
      },
      "edge_contexts": [
        {
          "source": "1802.08139_table_2",
          "target": "1802.08139_figure_4",
          "ref_text": "Fig. 4",
          "context_snippet": "edit risk, i.e. as likely or not likely to repay the loan. We assume the GCM in Fig. 4(b), where $A$ corresponds to the protected attribute sex, $C$ to age, $S$ to th"
        }
      ]
    },
    {
      "pair_id": "1802.08139_pair_13",
      "doc_id": "1802.08139",
      "element_a_id": "1802.08139_table_2",
      "element_b_id": "1802.08139_figure_5",
      "element_a_type": "table",
      "element_b_type": "figure",
      "pair_type": "figure+table",
      "hop_distance": 1,
      "path": [
        "1802.08139_table_2",
        "1802.08139_figure_5"
      ],
      "quality_score": 1.0,
      "overlap_with_existing_l1": false,
      "element_a": {
        "element_id": "1802.08139_table_2",
        "element_type": "table",
        "caption": "Table 2. In order columns represent: unfair test accuracy, fair test accuracy, and MMD values for $H _ { s }$ $( \\times ~ 1 0 0 )$ for the UCI German Credit dataset. Rows represent values after 2,000, 4,000, and 8,000 training steps.",
        "content": "Table 2. In order columns represent: unfair test accuracy, fair test accuracy, and MMD values for $H _ { s }$ $( \\times ~ 1 0 0 )$ for the UCI German Credit dataset. Rows represent values after 2,000, 4,000, and 8,000 training steps.",
        "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1802.08139/1802.08139/hybrid_auto/images/b3fb2ac1c0dbc2e93cde0695817bae19a24c605c323fd094e8312e08228f5035.jpg",
        "context_before": "males and females for increasing numbers of training steps. The remaining variables are shown in the Appendix. As can be seen, the addition of the MMD penalization to the variational bound for more training steps has the effect of reducing the number of modes in the posterior. From the evidence available, it is unclear if the shape changes are a necessary consequence of enforcing them to be similar, or if a simplification of the latent space is a more fundamental drawback of the MMD method. We l",
        "context_after": "In Table 2, we show the unfair and fair test accuracy and the MMD values for $H _ { s }$ after 2,000, 4,000, and 8,000 training steps (the results remain similar with a higher number of training steps). As we can see, unfair and fair accuracy, and MMD values are similar for all iterations. This indicates that, unlike the Adult dataset, model-observations mismatch is not problematic. This is confirmed by $\\widetilde { q } ( H _ { s } | A )$ ; we learn a structured distribution which does not diff"
      },
      "element_b": {
        "element_id": "1802.08139_figure_5",
        "element_type": "figure",
        "caption": "Figure 5. Histograms of (one dimension of) $\\tilde { q } ( H _ { m } | A )$ after 5,000, 8,000, 15,000 and 20,000 training steps.",
        "content": "Figure 5. Histograms of (one dimension of) $\\tilde { q } ( H _ { m } | A )$ after 5,000, 8,000, 15,000 and 20,000 training steps.",
        "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1802.08139/1802.08139/hybrid_auto/images/1802.08139_page0_fig15.jpg",
        "context_before": "",
        "context_after": "males and females for increasing numbers of training steps. The remaining variables are shown in the Appendix. As can be seen, the addition of the MMD penalization to the variational bound for more training steps has the effect of reducing the number of modes in the posterior. From the evidence available, it is unclear if the shape changes are a necessary consequence of enforcing them to be similar, or if a simplification of the latent space is a more fundamental drawback of the MMD method. We l"
      },
      "edge_contexts": [
        {
          "source": "1802.08139_table_2",
          "target": "1802.08139_figure_5",
          "ref_text": "Fig. 5",
          "context_snippet": "hidden variable $H _ { s }$ for $S$ , as $R$ does not need to be corrected.\n\nIn Fig. 5, we show histograms of $\\tilde { q } ( H _ { m } | A )$ separately for\n\n5.3. Th"
        }
      ]
    },
    {
      "pair_id": "1804.06876_pair_1",
      "doc_id": "1804.06876",
      "element_a_id": "1804.06876_figure_1",
      "element_b_id": "1804.06876_table_1",
      "element_a_type": "figure",
      "element_b_type": "table",
      "pair_type": "figure+table",
      "hop_distance": 1,
      "path": [
        "1804.06876_figure_1",
        "1804.06876_table_1"
      ],
      "quality_score": 1.0,
      "overlap_with_existing_l1": false,
      "element_a": {
        "element_id": "1804.06876_figure_1",
        "element_type": "figure",
        "caption": "Figure 1: Pairs of gender balanced co-reference tests in the WinoBias dataset. Male and female entities are marked in solid blue and dashed orange, respectively. For each example, the gender of the pronominal reference is irrelevant for the co-reference decision. Systems must be able to make correct linking predictions in pro-stereotypical scenarios (solid purple lines) and anti-stereotypical scenarios (dashed purple lines) equally well to pass the test. Importantly, stereotypical occupations are considered based on US Department of Labor statistics.",
        "content": "Figure 1: Pairs of gender balanced co-reference tests in the WinoBias dataset. Male and female entities are marked in solid blue and dashed orange, respectively. For each example, the gender of the pronominal reference is irrelevant for the co-reference decision. Systems must be able to make correct linking predictions in pro-stereotypical scenarios (solid purple lines) and anti-stereotypical scenarios (dashed purple lines) equally well to pass the test. Importantly, stereotypical occupations are considered based on US Department of Labor statistics.",
        "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1804.06876/1804.06876/hybrid_auto/images/1804.06876_page0_fig2.jpg",
        "context_before": "",
        "context_after": "nouns to either male or female stereotypical occupations (see the illustrative examples in Figure 1). None of the examples can be disambiguated by the gender of the pronoun but this cue can potentially distract the model. We consider a system to be gender biased if it links pronouns to occupations dominated by the gender of the pronoun (pro-stereotyped condition) more accurately than occupations not dominated by the gender of the pronoun (anti-stereotyped condition). The corpus can be used to ce"
      },
      "element_b": {
        "element_id": "1804.06876_table_1",
        "element_type": "table",
        "caption": "Table 1: Occupations statistics used in WinoBias dataset, organized by the percent of people in the occupation who are reported as female. When woman dominate profession, we call linking the noun phrase referring to the job with female and male pronoun as ‘pro-stereotypical‘, and ‘anti-stereotypical‘, respectively. Similarly, if the occupation is male dominated, linking the noun phrase with the male and female pronoun is called, ‘pro-stereotypical‘ and ‘anti-steretypical‘, respectively.",
        "content": "Table 1: Occupations statistics used in WinoBias dataset, organized by the percent of people in the occupation who are reported as female. When woman dominate profession, we call linking the noun phrase referring to the job with female and male pronoun as ‘pro-stereotypical‘, and ‘anti-stereotypical‘, respectively. Similarly, if the occupation is male dominated, linking the noun phrase with the male and female pronoun is called, ‘pro-stereotypical‘ and ‘anti-steretypical‘, respectively.",
        "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1804.06876/1804.06876/hybrid_auto/images/89b88d06df9094145068b49a5780a0e3991fd584bd525a9c805e78d1df1a638b.jpg",
        "context_before": "Type 2: [entity1] [interacts with] [entity2] and then [interacts with] [pronoun] for [circumstances]. These tests can be resolved using syntactic information and understanding of the pronoun (Figure 1; Type 2). We expect systems to do well on such cases because both semantic and syntactic cues help disambiguation.\n\nPrototypical WinoCoRef style sentences, where co-reference decisions must be made using world knowledge about given circumstances (Figure 1; Type 1).\n\nBerkeley Coreference Resolution ",
        "context_after": "and designed to cover cases requiring semantics and syntax separately.4\n\nType 1: [entity1] [interacts with] [entity2] [conjunction] [pronoun] [circumstances]. Prototypical WinoCoRef style sentences, where co-reference decisions must be made using world knowledge about given circumstances (Figure 1; Type 1). Such examples are challenging because they contain no syntactic cues.\n\nType 2: [entity1] [interacts with] [entity2] and then [interacts with] [pronoun] for [circumstances]. These tests can be"
      },
      "edge_contexts": [
        {
          "source": "1804.06876_table_1",
          "target": "1804.06876_figure_1",
          "ref_text": "Figure 1",
          "context_snippet": "s can be resolved using syntactic information and understanding of the pronoun (Figure 1; Type 2). We expect systems to do well on such cases because both semantic and "
        }
      ]
    },
    {
      "pair_id": "1804.09301_pair_1",
      "doc_id": "1804.09301",
      "element_a_id": "1804.09301_figure_2",
      "element_b_id": "1804.09301_table_1",
      "element_a_type": "figure",
      "element_b_type": "table",
      "pair_type": "figure+table",
      "hop_distance": 1,
      "path": [
        "1804.09301_figure_2",
        "1804.09301_table_1"
      ],
      "quality_score": 1.0,
      "overlap_with_existing_l1": false,
      "element_a": {
        "element_id": "1804.09301_figure_2",
        "element_type": "figure",
        "caption": "Figure 2: A “Winogender” schema for the occupation paramedic. Correct answers in bold. In general, OC-CUPATION and PARTICIPANT may appear in either order in the sentence.",
        "content": "Figure 2: A “Winogender” schema for the occupation paramedic. Correct answers in bold. In general, OC-CUPATION and PARTICIPANT may appear in either order in the sentence.",
        "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1804.09301/1804.09301/hybrid_auto/images/1804.09301_page0_fig1.jpg",
        "context_before": "Our intent is to reveal cases where coreference systems may be more or less likely to recognize a pronoun as coreferent with a particular occupation based on pronoun gender, as observed in Figure 1.\n\nBureau of Labor Statistics) and the gender statistics from text (Bergsma and Lin, 2006) which these systems access directly; correlation values are in Table 1.\n\nValidation Like Winograd schemas, each sentence template is written with one intended correct answer (here, either OCCUPATION or PAR-\n\n3Thi",
        "context_after": "TICIPANT).5 We aimed to write sentences where (1) pronoun resolution was as unambiguous for humans as possible (in the absence of additional context), and (2) the resolution would not be affected by changing pronoun gender. (See Figure 2.) Nonetheless, to ensure that our own judgments are shared by other English speakers, we validated all 720 sentences on Mechanical Turk, with 10-way redundancy. Each MTurk task included 5 sentences from our dataset, and 5 sentences from the Winograd Schema Chall"
      },
      "element_b": {
        "element_id": "1804.09301_table_1",
        "element_type": "table",
        "caption": "Table 1: Correlation values for Figures 3 and 4.",
        "content": "Table 1: Correlation values for Figures 3 and 4.",
        "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1804.09301/1804.09301/hybrid_auto/images/1f7c7ec514efeaeb6fe748921e08e0547aff05d496fb0a8573b2f718083c6998.jpg",
        "context_before": "Bureau of Labor Statistics.4 For each occupation, we wrote two similar sentence templates: one in which PRONOUN is coreferent with OCCUPATION, and one in which it is coreferent with PARTICIPANT (see Figure 2).\n\nWe also identify so-called “gotcha” sentences in which pronoun gender does not match the occupation’s majority gender (BLS) if OCCUPATION is the correct answer; all systems perform worse on these “gotchas.”8 (See Table 2.)\n\nBecause coreference systems need to make discrete choices about w",
        "context_after": "As the riddle reveals, the task of coreference resolution in English is tightly bound with questions of gender, for humans and automated systems alike (see Figure 1). As awareness grows of the ways in which data-driven AI technologies may acquire and amplify human-like biases (Caliskan et al., 2017; Barocas and Selbst, 2016; Hovy and Spruit, 2016), this work investigates how gender biases manifest in coreference resolution systems.\n\nOur intent is to reveal cases where coreference systems may be "
      },
      "edge_contexts": [
        {
          "source": "1804.09301_table_1",
          "target": "1804.09301_figure_2",
          "ref_text": "Figure 2",
          "context_snippet": "ferent with OCCUPATION, and one in which it is coreferent with PARTICIPANT (see Figure 2).\n\nWe also identify so-called “gotcha” sentences in which pronoun gender does n"
        }
      ]
    },
    {
      "pair_id": "1804.09301_pair_2",
      "doc_id": "1804.09301",
      "element_a_id": "1804.09301_table_1",
      "element_b_id": "1804.09301_figure_1",
      "element_a_type": "table",
      "element_b_type": "figure",
      "pair_type": "figure+table",
      "hop_distance": 1,
      "path": [
        "1804.09301_table_1",
        "1804.09301_figure_1"
      ],
      "quality_score": 1.0,
      "overlap_with_existing_l1": false,
      "element_a": {
        "element_id": "1804.09301_table_1",
        "element_type": "table",
        "caption": "Table 1: Correlation values for Figures 3 and 4.",
        "content": "Table 1: Correlation values for Figures 3 and 4.",
        "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1804.09301/1804.09301/hybrid_auto/images/1f7c7ec514efeaeb6fe748921e08e0547aff05d496fb0a8573b2f718083c6998.jpg",
        "context_before": "Bureau of Labor Statistics.4 For each occupation, we wrote two similar sentence templates: one in which PRONOUN is coreferent with OCCUPATION, and one in which it is coreferent with PARTICIPANT (see Figure 2).\n\nWe also identify so-called “gotcha” sentences in which pronoun gender does not match the occupation’s majority gender (BLS) if OCCUPATION is the correct answer; all systems perform worse on these “gotchas.”8 (See Table 2.)\n\nBecause coreference systems need to make discrete choices about w",
        "context_after": "As the riddle reveals, the task of coreference resolution in English is tightly bound with questions of gender, for humans and automated systems alike (see Figure 1). As awareness grows of the ways in which data-driven AI technologies may acquire and amplify human-like biases (Caliskan et al., 2017; Barocas and Selbst, 2016; Hovy and Spruit, 2016), this work investigates how gender biases manifest in coreference resolution systems.\n\nOur intent is to reveal cases where coreference systems may be "
      },
      "element_b": {
        "element_id": "1804.09301_figure_1",
        "element_type": "figure",
        "caption": "Figure 1: Stanford CoreNLP rule-based coreference system resolves a male and neutral pronoun as coreferent with “The surgeon,” but does not for the corresponding female pronoun.",
        "content": "Figure 1: Stanford CoreNLP rule-based coreference system resolves a male and neutral pronoun as coreferent with “The surgeon,” but does not for the corresponding female pronoun.",
        "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1804.09301/1804.09301/hybrid_auto/images/1804.09301_page0_fig0.jpg",
        "context_before": "That a majority of people are reportedly unable to solve this riddle1 is taken as evidence of underlying implicit gender bias (Wapman and Belle, 2014): many first-time listeners have difficulty assigning both the role of “mother” and “surgeon” to the same entity.\n\nAs the riddle reveals, the task of coreference resolution in English is tightly bound with questions of gender, for humans and automated systems alike (see Figure 1). As awareness grows of the ways in which data-driven AI technologies ",
        "context_after": "the style of Winograd schemas, wherein a pronoun must be resolved to one of two previouslymentioned entities in a sentence designed to be easy for humans to interpret, but challenging for data-driven systems (Levesque et al., 2011). In our setting, one of these mentions is a person referred to by their occupation; by varying only the pronoun’s gender, we are able to test the impact of gender on resolution. With these “Winogender schemas,” we demonstrate the presence of systematic gender bias in "
      },
      "edge_contexts": [
        {
          "source": "1804.09301_table_1",
          "target": "1804.09301_figure_1",
          "ref_text": "Figure 1",
          "context_snippet": "tly bound with questions of gender, for humans and automated systems alike (see Figure 1). As awareness grows of the ways in which data-driven AI technologies may acqui"
        }
      ]
    },
    {
      "pair_id": "1804.09301_pair_3",
      "doc_id": "1804.09301",
      "element_a_id": "1804.09301_table_1",
      "element_b_id": "1804.09301_figure_4",
      "element_a_type": "table",
      "element_b_type": "figure",
      "pair_type": "figure+table",
      "hop_distance": 1,
      "path": [
        "1804.09301_table_1",
        "1804.09301_figure_4"
      ],
      "quality_score": 1.0,
      "overlap_with_existing_l1": false,
      "element_a": {
        "element_id": "1804.09301_table_1",
        "element_type": "table",
        "caption": "Table 1: Correlation values for Figures 3 and 4.",
        "content": "Table 1: Correlation values for Figures 3 and 4.",
        "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1804.09301/1804.09301/hybrid_auto/images/1f7c7ec514efeaeb6fe748921e08e0547aff05d496fb0a8573b2f718083c6998.jpg",
        "context_before": "Bureau of Labor Statistics.4 For each occupation, we wrote two similar sentence templates: one in which PRONOUN is coreferent with OCCUPATION, and one in which it is coreferent with PARTICIPANT (see Figure 2).\n\nWe also identify so-called “gotcha” sentences in which pronoun gender does not match the occupation’s majority gender (BLS) if OCCUPATION is the correct answer; all systems perform worse on these “gotchas.”8 (See Table 2.)\n\nBecause coreference systems need to make discrete choices about w",
        "context_after": "As the riddle reveals, the task of coreference resolution in English is tightly bound with questions of gender, for humans and automated systems alike (see Figure 1). As awareness grows of the ways in which data-driven AI technologies may acquire and amplify human-like biases (Caliskan et al., 2017; Barocas and Selbst, 2016; Hovy and Spruit, 2016), this work investigates how gender biases manifest in coreference resolution systems.\n\nOur intent is to reveal cases where coreference systems may be "
      },
      "element_b": {
        "element_id": "1804.09301_figure_4",
        "element_type": "figure",
        "caption": "Figure 4: These two plots show how gender bias in coreference systems corresponds with occupational gender statistics from the U.S Bureau of Labor Statistics (left) and from text as computed by Bergsma and Lin (2006) (right); each point represents one occupation. The y-axes measure the extent to which a coref system prefers to match female pronouns with a given occupation over male pronouns, as tested by our Winogender schemas. A value of 100 (maximum female bias) means the system always resolved female pronouns to the given occupation and never male pronouns $( 1 0 0 \\% - 0 \\% )$ ; a score of -100 (maximum male bias) is the reverse; and a value of 0 indicates no gender differential. Recall the Winogender evaluation set is gender-balanced for each occupation; thus the horizontal dotted black line $\\scriptstyle ( \\mathrm { y = 0 } )$ in both plots represents a hypothetical system with $100 \\%$ accuracy. Regression lines with $9 5 \\%$ confidence intervals are shown.",
        "content": "Figure 4: These two plots show how gender bias in coreference systems corresponds with occupational gender statistics from the U.S Bureau of Labor Statistics (left) and from text as computed by Bergsma and Lin (2006) (right); each point represents one occupation. The y-axes measure the extent to which a coref system prefers to match female pronouns with a given occupation over male pronouns, as tested by our Winogender schemas. A value of 100 (maximum female bias) means the system always resolved female pronouns to the given occupation and never male pronouns $( 1 0 0 \\% - 0 \\% )$ ; a score of -100 (maximum male bias) is the reverse; and a value of 0 indicates no gender differential. Recall the Winogender evaluation set is gender-balanced for each occupation; thus the horizontal dotted black line $\\scriptstyle ( \\mathrm { y = 0 } )$ in both plots represents a hypothetical system with $100 \\%$ accuracy. Regression lines with $9 5 \\%$ confidence intervals are shown.",
        "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1804.09301/1804.09301/hybrid_auto/images/1804.09301_page0_fig4.jpg",
        "context_before": "",
        "context_after": "When these systems’ predictions diverge based on pronoun gender, they do so in ways that reinforce and magnify real-world occupational gender disparities. Figure 4 shows that systems’ gender preferences for occupations correlate with realworld employment statistics (U.S. Bureau of Labor Statistics) and the gender statistics from text (Bergsma and Lin, 2006) which these systems access directly; correlation values are in Table 1. We also identify so-called “gotcha” sentences in which pronoun gende"
      },
      "edge_contexts": [
        {
          "source": "1804.09301_figure_4",
          "target": "1804.09301_table_1",
          "ref_text": "Table 1",
          "context_snippet": "a and Lin, 2006) which these systems access directly; correlation values are in Table 1. We also identify so-called “gotcha” sentences in which pronoun gender does not"
        }
      ]
    },
    {
      "pair_id": "1804.09301_pair_4",
      "doc_id": "1804.09301",
      "element_a_id": "1804.09301_figure_3",
      "element_b_id": "1804.09301_table_1",
      "element_a_type": "figure",
      "element_b_type": "table",
      "pair_type": "figure+table",
      "hop_distance": 1,
      "path": [
        "1804.09301_figure_3",
        "1804.09301_table_1"
      ],
      "quality_score": 1.0,
      "overlap_with_existing_l1": false,
      "element_a": {
        "element_id": "1804.09301_figure_3",
        "element_type": "figure",
        "caption": "Figure 3: Gender statistics from Bergsma and Lin (2006) correlate with Bureau of Labor Statistics 2015. However, the former has systematically lower female percentages; most points lie well below the 45-degree line (dotted). Regression line and $9 5 \\%$ confidence interval in blue. Pearson $\\Gamma = 0 . 6 7$ .",
        "content": "Figure 3: Gender statistics from Bergsma and Lin (2006) correlate with Bureau of Labor Statistics 2015. However, the former has systematically lower female percentages; most points lie well below the 45-degree line (dotted). Regression line and $9 5 \\%$ confidence interval in blue. Pearson $\\Gamma = 0 . 6 7$ .",
        "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1804.09301/1804.09301/hybrid_auto/images/1804.09301_page0_fig2.jpg",
        "context_before": "As the riddle reveals, the task of coreference resolution in English is tightly bound with questions of gender, for humans and automated systems alike (see Figure 1).\n\nOur intent is to reveal cases where coreference systems may be more or less likely to recognize a pronoun as coreferent with a particular occupation based on pronoun gender, as observed in Figure 1.\n\nBureau of Labor Statistics) and the gender statistics from text (Bergsma and Lin, 2006) which these systems access directly; correla",
        "context_after": "4 Results and Discussion\n\nWe evaluate examples of each of the three coreference system architectures described in 2: the Lee et al. (2011) sieve system from the rulebased paradigm (referred to as RULE), Durrett and Klein (2013) from the statistical paradigm (STAT), and the Clark and Manning (2016a) deep reinforcement system from the neural paradigm (NEURAL).\n\nBy multiple measures, the Winogender schemas reveal varying degrees of gender bias in all three systems. First we observe that these syste"
      },
      "element_b": {
        "element_id": "1804.09301_table_1",
        "element_type": "table",
        "caption": "Table 1: Correlation values for Figures 3 and 4.",
        "content": "Table 1: Correlation values for Figures 3 and 4.",
        "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1804.09301/1804.09301/hybrid_auto/images/1f7c7ec514efeaeb6fe748921e08e0547aff05d496fb0a8573b2f718083c6998.jpg",
        "context_before": "Bureau of Labor Statistics.4 For each occupation, we wrote two similar sentence templates: one in which PRONOUN is coreferent with OCCUPATION, and one in which it is coreferent with PARTICIPANT (see Figure 2).\n\nWe also identify so-called “gotcha” sentences in which pronoun gender does not match the occupation’s majority gender (BLS) if OCCUPATION is the correct answer; all systems perform worse on these “gotchas.”8 (See Table 2.)\n\nBecause coreference systems need to make discrete choices about w",
        "context_after": "As the riddle reveals, the task of coreference resolution in English is tightly bound with questions of gender, for humans and automated systems alike (see Figure 1). As awareness grows of the ways in which data-driven AI technologies may acquire and amplify human-like biases (Caliskan et al., 2017; Barocas and Selbst, 2016; Hovy and Spruit, 2016), this work investigates how gender biases manifest in coreference resolution systems.\n\nOur intent is to reveal cases where coreference systems may be "
      },
      "edge_contexts": [
        {
          "source": "1804.09301_figure_3",
          "target": "1804.09301_table_1",
          "ref_text": "Table 1",
          "context_snippet": "a and Lin, 2006) which these systems access directly; correlation values are in Table 1."
        }
      ]
    },
    {
      "pair_id": "1804.09301_pair_5",
      "doc_id": "1804.09301",
      "element_a_id": "1804.09301_figure_4",
      "element_b_id": "1804.09301_table_2",
      "element_a_type": "figure",
      "element_b_type": "table",
      "pair_type": "figure+table",
      "hop_distance": 1,
      "path": [
        "1804.09301_figure_4",
        "1804.09301_table_2"
      ],
      "quality_score": 1.0,
      "overlap_with_existing_l1": false,
      "element_a": {
        "element_id": "1804.09301_figure_4",
        "element_type": "figure",
        "caption": "Figure 4: These two plots show how gender bias in coreference systems corresponds with occupational gender statistics from the U.S Bureau of Labor Statistics (left) and from text as computed by Bergsma and Lin (2006) (right); each point represents one occupation. The y-axes measure the extent to which a coref system prefers to match female pronouns with a given occupation over male pronouns, as tested by our Winogender schemas. A value of 100 (maximum female bias) means the system always resolved female pronouns to the given occupation and never male pronouns $( 1 0 0 \\% - 0 \\% )$ ; a score of -100 (maximum male bias) is the reverse; and a value of 0 indicates no gender differential. Recall the Winogender evaluation set is gender-balanced for each occupation; thus the horizontal dotted black line $\\scriptstyle ( \\mathrm { y = 0 } )$ in both plots represents a hypothetical system with $100 \\%$ accuracy. Regression lines with $9 5 \\%$ confidence intervals are shown.",
        "content": "Figure 4: These two plots show how gender bias in coreference systems corresponds with occupational gender statistics from the U.S Bureau of Labor Statistics (left) and from text as computed by Bergsma and Lin (2006) (right); each point represents one occupation. The y-axes measure the extent to which a coref system prefers to match female pronouns with a given occupation over male pronouns, as tested by our Winogender schemas. A value of 100 (maximum female bias) means the system always resolved female pronouns to the given occupation and never male pronouns $( 1 0 0 \\% - 0 \\% )$ ; a score of -100 (maximum male bias) is the reverse; and a value of 0 indicates no gender differential. Recall the Winogender evaluation set is gender-balanced for each occupation; thus the horizontal dotted black line $\\scriptstyle ( \\mathrm { y = 0 } )$ in both plots represents a hypothetical system with $100 \\%$ accuracy. Regression lines with $9 5 \\%$ confidence intervals are shown.",
        "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1804.09301/1804.09301/hybrid_auto/images/1804.09301_page0_fig4.jpg",
        "context_before": "",
        "context_after": "When these systems’ predictions diverge based on pronoun gender, they do so in ways that reinforce and magnify real-world occupational gender disparities. Figure 4 shows that systems’ gender preferences for occupations correlate with realworld employment statistics (U.S. Bureau of Labor Statistics) and the gender statistics from text (Bergsma and Lin, 2006) which these systems access directly; correlation values are in Table 1. We also identify so-called “gotcha” sentences in which pronoun gende"
      },
      "element_b": {
        "element_id": "1804.09301_table_2",
        "element_type": "table",
        "caption": "Table 2: System accuracy $( \\% )$ bucketed by gender and difficulty (so-called “gotchas,” shaded in purple). For female pronouns, a “gotcha” sentence is one where either (1) the correct answer is OCCUPATION but the occupation is $< 5 0 \\%$ female (according to BLS); or (2) the occupation is $\\geq 5 0 \\%$ female but the correct answer is PARTICIPANT; this is reversed for male pronouns. Systems do uniformly worse on “gotchas.”",
        "content": "Table 2: System accuracy $( \\% )$ bucketed by gender and difficulty (so-called “gotchas,” shaded in purple). For female pronouns, a “gotcha” sentence is one where either (1) the correct answer is OCCUPATION but the occupation is $< 5 0 \\%$ female (according to BLS); or (2) the occupation is $\\geq 5 0 \\%$ female but the correct answer is PARTICIPANT; this is reversed for male pronouns. Systems do uniformly worse on “gotchas.”",
        "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1804.09301/1804.09301/hybrid_auto/images/d21b28b9a0e5f04b69a329b6b24cbf7f1edd5af127e6790983b6a9569dad2a79.jpg",
        "context_before": "When these systems’ predictions diverge based on pronoun gender, they do so in ways that reinforce and magnify real-world occupational gender disparities. Figure 4 shows that systems’ gender preferences for occupations correlate with realworld employment statistics (U.S. Bureau of Labor Statistics) and the gender statistics from text (Bergsma and Lin, 2006) which these systems access directly; correlation values are in Table 1. We also identify so-called “gotcha” sentences in which pronoun gende",
        "context_after": "Here we give a brief (and non-exhaustive) overview of prior work on gender bias in NLP systems and datasets. A number of papers explore (gender) bias in English word embeddings:\n\n8“ The librarian helped the child pick out a book because he liked to encourage reading.” is an example of a “gotcha” sentence; librarians are $> 5 0 \\%$ female (BLS).\n\nhow they capture implicit human biases in modern (Caliskan et al., 2017) and historical (Garg et al., 2018) text, and methods for debiasing them (Bolukb"
      },
      "edge_contexts": [
        {
          "source": "1804.09301_table_2",
          "target": "1804.09301_figure_4",
          "ref_text": "Figure 4 s",
          "context_snippet": " in ways that reinforce and magnify real-world occupational gender disparities. Figure 4 shows that systems’ gender preferences for occupations correlate with realworld e"
        }
      ]
    },
    {
      "pair_id": "1805.03094_pair_1",
      "doc_id": "1805.03094",
      "element_a_id": "1805.03094_table_1",
      "element_b_id": "1805.03094_figure_1",
      "element_a_type": "table",
      "element_b_type": "figure",
      "pair_type": "figure+table",
      "hop_distance": 1,
      "path": [
        "1805.03094_table_1",
        "1805.03094_figure_1"
      ],
      "quality_score": 1.0,
      "overlap_with_existing_l1": false,
      "element_a": {
        "element_id": "1805.03094_table_1",
        "element_type": "table",
        "caption": "Table 1: Variables defining important disaggregations of Stack Exchange data, along with their pseudo- $\\bar { . } R ^ { 2 }$ scores.",
        "content": "Table 1: Variables defining important disaggregations of Stack Exchange data, along with their pseudo- $\\bar { . } R ^ { 2 }$ scores.",
        "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1805.03094/1805.03094/hybrid_auto/images/8b918a9fb2bde89fe1ee51f0bfd3c145b309679ad9c2ebf0a652c051648d9492.jpg",
        "context_before": "First, we study answerer performance on Stack Exchange (SE). Launched in 2008 as a forum for asking computer programming questions, Stack Exchange has grown to encompass a variety of technical and non-technical topics. Any user can ask a question, which others may answer. Users can vote for answers they find helpful, but only the asker can accept one of the answers as the best answer to the question. We used anonymized data representing all answers to questions posted on Stack Exchange from Augu",
        "context_after": "denoting whether the answer written by a user is accepted by the asker as best answer to his or her question. To this end, for each answer written by a user, we create a list of features describing the answer and the user. Features include the numbers of words, hyperlinks, and lines of code the answer contains, and its Flesch readability score (Kincaid et al. 1975). Features describing answerers are their reputation, tenure on SE (in seconds and in terms of percentile rank) and the total number "
      },
      "element_b": {
        "element_id": "1805.03094_figure_1",
        "element_type": "figure",
        "caption": "(b) Number of samples (d) Aggregate trend Figure 1: Disaggregation of Stack Exchange data. (a) The heat map shows the probability the answer is accepted as a function of its answer position within a session, with the horizontal bands corresponding to the different subgroups, conditioned on total number of answers the user has written. (b) Number of data samples within each bin of the heat map. Note that the outcome becomes noisy when there are few samples. The trends in performance as a function of answer position in (c) disaggregated data and (d) aggregate data. Error bars in (c) and (d) show $9 5 \\%$ confidence interval.",
        "content": "(b) Number of samples (d) Aggregate trend Figure 1: Disaggregation of Stack Exchange data. (a) The heat map shows the probability the answer is accepted as a function of its answer position within a session, with the horizontal bands corresponding to the different subgroups, conditioned on total number of answers the user has written. (b) Number of data samples within each bin of the heat map. Note that the outcome becomes noisy when there are few samples. The trends in performance as a function of answer position in (c) disaggregated data and (d) aggregate data. Error bars in (c) and (d) show $9 5 \\%$ confidence interval.",
        "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1805.03094/1805.03094/hybrid_auto/images/1805.03094_page0_fig3.jpg",
        "context_before": "",
        "context_after": "scores. For example, when examining how performance— probability to solve a problem correctly—changes over the course of a day $X _ { j }$ is hour24), the relevant disaggregation conditions the data on all first attempts, i.e., the number of all problems the user solved correctly on their first attempt. On the other hand, several disaggregations can explain the trends in performance as a function of month. Conditioning on first five attempts has the most explanatory power, followed by disaggrega"
      },
      "edge_contexts": [
        {
          "source": "1805.03094_figure_1",
          "target": "1805.03094_table_1",
          "ref_text": "Table 1",
          "context_snippet": " from all possible pairs of covariates, our method identified 8 as significant. Table 1 ranks these disaggregations along with their pseudo- $R ^ { 2 }$ scores. Note t"
        }
      ]
    },
    {
      "pair_id": "1805.03094_pair_2",
      "doc_id": "1805.03094",
      "element_a_id": "1805.03094_figure_2",
      "element_b_id": "1805.03094_table_2",
      "element_a_type": "figure",
      "element_b_type": "table",
      "pair_type": "figure+table",
      "hop_distance": 1,
      "path": [
        "1805.03094_figure_2",
        "1805.03094_table_2"
      ],
      "quality_score": 1.0,
      "overlap_with_existing_l1": false,
      "element_a": {
        "element_id": "1805.03094_figure_2",
        "element_type": "figure",
        "caption": "(b) Number of samples (d) Aggregate trend Figure 2: Disaggregation of Stack Exchange data similar to Fig. 1, but instead disaggreagted on user reputation. (a) The heat map shows acceptance probability as a function of its answer position within a session. (b) Number of data samples within each bin of the heat map. Note that the outcome becomes noisy when there are few samples. The trends in (c) disaggregated data and (d) aggregate data.",
        "content": "(b) Number of samples (d) Aggregate trend Figure 2: Disaggregation of Stack Exchange data similar to Fig. 1, but instead disaggreagted on user reputation. (a) The heat map shows acceptance probability as a function of its answer position within a session. (b) Number of data samples within each bin of the heat map. Note that the outcome becomes noisy when there are few samples. The trends in (c) disaggregated data and (d) aggregate data.",
        "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1805.03094/1805.03094/hybrid_auto/images/1805.03094_page0_fig7.jpg",
        "context_before": "",
        "context_after": "covariate month, conditioned on five first attempts. When data is aggregated over the entire population, there appears to be a slight seasonal variation, with performance higher on average during the summer months (Fig. 4(d)). Once data is disaggregated by five first attempts, the seasonal trends are no longer so obvious in several subgroups (Fig. 4(c)). Interestingly, it appears to be the high achieving users (who correctly answer more of the five first problems), who perform better during the "
      },
      "element_b": {
        "element_id": "1805.03094_table_2",
        "element_type": "table",
        "caption": "Therefore, we define performance in a more stringent way, as a binary variable, which is equal to one if the user had perfect performance (i.e., correctly recalled all words in a lesson), and zero oth-\n\nTable 2: Variables defining important disaggregations of the Khan Academy data, along with their ",
        "content": "Therefore, we define performance in a more stringent way, as a binary variable, which is equal to one if the user had perfect performance (i.e., correctly recalled all words in a lesson), and zero oth-\n\nTable 2: Variables defining important disaggregations of the Khan Academy data, along with their ",
        "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1805.03094/1805.03094/hybrid_auto/images/1805.03094_page0_fig8.jpg",
        "context_before": "Figure 2 shows an alternate disaggregation of SE data for the covariate answer position, here conditioned on user reputation.\n\nSome of these are presented in Table 2.\n\nTherefore, we define performance in a more stringent way, as a binary variable, which is equal to one if the user had perfect performance (i.e., correctly recalled all words in a lesson), and zero oth-\n\nTable 2: Variables defining important disaggregations of the Khan Academy data, along with their pseudo- $\\bar { \\boldsymbol { R ",
        "context_after": ""
      },
      "edge_contexts": [
        {
          "source": "1805.03094_table_2",
          "target": "1805.03094_figure_2",
          "ref_text": "Figure 2 s",
          "context_snippet": "Figure 2 shows an alternate disaggregation of SE data for the covariate answer position, h"
        }
      ]
    },
    {
      "pair_id": "1805.03094_pair_3",
      "doc_id": "1805.03094",
      "element_a_id": "1805.03094_figure_3",
      "element_b_id": "1805.03094_table_3",
      "element_a_type": "figure",
      "element_b_type": "table",
      "pair_type": "figure+table",
      "hop_distance": 1,
      "path": [
        "1805.03094_figure_3",
        "1805.03094_table_3"
      ],
      "quality_score": 1.0,
      "overlap_with_existing_l1": false,
      "element_a": {
        "element_id": "1805.03094_figure_3",
        "element_type": "figure",
        "caption": "Figure 3: A disaggregation of the Khan Academy data showing performance as a function of hour of day, conditioned on all first attempts. (a) The heat map shows average performance within a subgroup as a function of the hour of day. (b) Number of data samples within each subgroup. The trends in (c) t",
        "content": "Figure 3: A disaggregation of the Khan Academy data showing performance as a function of hour of day, conditioned on all first attempts. (a) The heat map shows average performance within a subgroup as a function of the hour of day. (b) Number of data samples within each subgroup. The trends in (c) t",
        "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1805.03094/1805.03094/hybrid_auto/images/1805.03094_page0_fig16.jpg",
        "context_before": "Of the 462 potential disaggregations of DL data, 51 were found to be significant using the $\\chi ^ { 2 }$ test. Table 3 reports disaggregations associated with select covariates, including lesson’s position within a session, lesson index in user’s history, the number of lessons the user completed, etc. The trends with respect to some of the covariates could be explained by several different disaggregations, with some of them having relatively high values of pseudo- $R ^ { 2 }$ . Again, user expe",
        "context_after": ""
      },
      "element_b": {
        "element_id": "1805.03094_table_3",
        "element_type": "table",
        "caption": "Table 3: Variables defining important disaggregations of Duolingo data, along with their pseudo- $R ^ { 2 }$ scores.",
        "content": "Table 3: Variables defining important disaggregations of Duolingo data, along with their pseudo- $R ^ { 2 }$ scores.",
        "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1805.03094/1805.03094/hybrid_auto/images/92ee48bdf21a8197424a5607949a67cea5f830f08da8366a8957711fda894d90.jpg",
        "context_before": "Figure 4 shows the disaggregation corresponding to the\n\ncovariate month, conditioned on five first attempts. When data is aggregated over the entire population, there appears to be a slight seasonal variation, with performance higher on average during the summer months (Fig. 4(d)). Once data is disaggregated by five first attempts, the seasonal trends are no longer so obvious in several subgroups (Fig. 4(c)). Interestingly, it appears to be the high achieving users (who correctly answer more of ",
        "context_after": "Figure 3 takes a closer look at the disaggregation corresponding to covariate hour24. In the aggregate data (Fig. 3(d)), there is a small but significant upward trend in performance over the course of a day. It looks like performance is higher at night than during the day. However, when data is disaggregated by all first attempts, only a couple of subgroups have the up-trend: the rest stay flat or even decline in performance. All first attempts, which represents how many of all problems users so"
      },
      "edge_contexts": [
        {
          "source": "1805.03094_table_3",
          "target": "1805.03094_figure_3",
          "ref_text": "Figure 3 t",
          "context_snippet": "Figure 3 takes a closer look at the disaggregation corresponding to covariate hour24. In t"
        }
      ]
    },
    {
      "pair_id": "1805.03094_pair_4",
      "doc_id": "1805.03094",
      "element_a_id": "1805.03094_figure_5",
      "element_b_id": "1805.03094_table_3",
      "element_a_type": "figure",
      "element_b_type": "table",
      "pair_type": "figure+table",
      "hop_distance": 1,
      "path": [
        "1805.03094_figure_5",
        "1805.03094_table_3"
      ],
      "quality_score": 1.0,
      "overlap_with_existing_l1": false,
      "element_a": {
        "element_id": "1805.03094_figure_5",
        "element_type": "figure",
        "caption": "Figure 5: Disaggregation of Duolingo data. (a) The heat map shows performance, as a function of how many lessons the user completed, conditioned on how many of the five first lessons were answered correctly. (b) Number of data samples within each bin of the heat map. Trends in (c) the disaggregated ",
        "content": "Figure 5: Disaggregation of Duolingo data. (a) The heat map shows performance, as a function of how many lessons the user completed, conditioned on how many of the five first lessons were answered correctly. (b) Number of data samples within each bin of the heat map. Trends in (c) the disaggregated ",
        "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1805.03094/1805.03094/hybrid_auto/images/1805.03094_page0_fig21.jpg",
        "context_before": "Figure 5 examines the impact of experience on performance. In the aggregate data Fig. 5(d), performance appears to increase as function of experience (lesson index): users who have more practice perform better. However, once the data is disaggregated by initial performance (five first lessons), or skill, in Fig. 5(c), a subtler picture emerges. Users who initially performed the worst (bottom bins in Fig. 5(a)) improve their performance as they have more lessons, while the best performers initial",
        "context_after": ""
      },
      "element_b": {
        "element_id": "1805.03094_table_3",
        "element_type": "table",
        "caption": "Table 3: Variables defining important disaggregations of Duolingo data, along with their pseudo- $R ^ { 2 }$ scores.",
        "content": "Table 3: Variables defining important disaggregations of Duolingo data, along with their pseudo- $R ^ { 2 }$ scores.",
        "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1805.03094/1805.03094/hybrid_auto/images/92ee48bdf21a8197424a5607949a67cea5f830f08da8366a8957711fda894d90.jpg",
        "context_before": "Figure 4 shows the disaggregation corresponding to the\n\ncovariate month, conditioned on five first attempts. When data is aggregated over the entire population, there appears to be a slight seasonal variation, with performance higher on average during the summer months (Fig. 4(d)). Once data is disaggregated by five first attempts, the seasonal trends are no longer so obvious in several subgroups (Fig. 4(c)). Interestingly, it appears to be the high achieving users (who correctly answer more of ",
        "context_after": "Figure 3 takes a closer look at the disaggregation corresponding to covariate hour24. In the aggregate data (Fig. 3(d)), there is a small but significant upward trend in performance over the course of a day. It looks like performance is higher at night than during the day. However, when data is disaggregated by all first attempts, only a couple of subgroups have the up-trend: the rest stay flat or even decline in performance. All first attempts, which represents how many of all problems users so"
      },
      "edge_contexts": [
        {
          "source": "1805.03094_table_3",
          "target": "1805.03094_figure_5",
          "ref_text": "Figure 5 e",
          "context_snippet": "itial skill (five first lessons) appear as significant conditioning variables.\n\nFigure 5 examines the impact of experience on performance. In the aggregate data Fig. 5(d)"
        }
      ]
    },
    {
      "pair_id": "1805.03094_pair_5",
      "doc_id": "1805.03094",
      "element_a_id": "1805.03094_figure_6",
      "element_b_id": "1805.03094_table_3",
      "element_a_type": "figure",
      "element_b_type": "table",
      "pair_type": "figure+table",
      "hop_distance": 1,
      "path": [
        "1805.03094_figure_6",
        "1805.03094_table_3"
      ],
      "quality_score": 1.0,
      "overlap_with_existing_l1": false,
      "element_a": {
        "element_id": "1805.03094_figure_6",
        "element_type": "figure",
        "caption": "(b) Number of samples (c) Subgroup trends (d) Aggregate trend Figure 6: Disaggregation of Duolingo data showing performance as a function of lesson Correct (a) The heat map shows performance, i.e., probability to answer all the words correctly, conditioned on the number of distinct words in the lesson. (b) Number of data samples in each bin of the heat map. Trends in (c) the disaggregated data and in (d) aggregate data. Errors bars show $9 5 \\%$ confidence interval.",
        "content": "(b) Number of samples (c) Subgroup trends (d) Aggregate trend Figure 6: Disaggregation of Duolingo data showing performance as a function of lesson Correct (a) The heat map shows performance, i.e., probability to answer all the words correctly, conditioned on the number of distinct words in the lesson. (b) Number of data samples in each bin of the heat map. Trends in (c) the disaggregated data and in (d) aggregate data. Errors bars show $9 5 \\%$ confidence interval.",
        "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1805.03094/1805.03094/hybrid_auto/images/1805.03094_page0_fig23.jpg",
        "context_before": "",
        "context_after": "teresting is a region of lower performance starting around values of lesson correct near 20 and distinct words between 3 and 10, and continues upwards and to the right. For some reason user performance drops in this regime.\n\nThere are several commonalities emerging from the three data sets we studied. Across platforms, initial performance, captured by first five attempts in the KA data or first five lessons in the DL data, appeared as an important conditioning variable differentiating the subgro"
      },
      "element_b": {
        "element_id": "1805.03094_table_3",
        "element_type": "table",
        "caption": "Table 3: Variables defining important disaggregations of Duolingo data, along with their pseudo- $R ^ { 2 }$ scores.",
        "content": "Table 3: Variables defining important disaggregations of Duolingo data, along with their pseudo- $R ^ { 2 }$ scores.",
        "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1805.03094/1805.03094/hybrid_auto/images/92ee48bdf21a8197424a5607949a67cea5f830f08da8366a8957711fda894d90.jpg",
        "context_before": "Figure 4 shows the disaggregation corresponding to the\n\ncovariate month, conditioned on five first attempts. When data is aggregated over the entire population, there appears to be a slight seasonal variation, with performance higher on average during the summer months (Fig. 4(d)). Once data is disaggregated by five first attempts, the seasonal trends are no longer so obvious in several subgroups (Fig. 4(c)). Interestingly, it appears to be the high achieving users (who correctly answer more of ",
        "context_after": "Figure 3 takes a closer look at the disaggregation corresponding to covariate hour24. In the aggregate data (Fig. 3(d)), there is a small but significant upward trend in performance over the course of a day. It looks like performance is higher at night than during the day. However, when data is disaggregated by all first attempts, only a couple of subgroups have the up-trend: the rest stay flat or even decline in performance. All first attempts, which represents how many of all problems users so"
      },
      "edge_contexts": [
        {
          "source": "1805.03094_table_3",
          "target": "1805.03094_figure_6",
          "ref_text": "Figure 6",
          "context_snippet": " the initially worst performers.\n\nAnother disaggregation of DL data is shown in Figure 6. The plots show performance as a function of lesson correct, the number of word"
        }
      ]
    },
    {
      "pair_id": "1805.03677_pair_1",
      "doc_id": "1805.03677",
      "element_a_id": "1805.03677_figure_1",
      "element_b_id": "1805.03677_table_1",
      "element_a_type": "figure",
      "element_b_type": "table",
      "pair_type": "figure+table",
      "hop_distance": 1,
      "path": [
        "1805.03677_figure_1",
        "1805.03677_table_1"
      ],
      "quality_score": 1.0,
      "overlap_with_existing_l1": false,
      "element_a": {
        "element_id": "1805.03677_figure_1",
        "element_type": "figure",
        "caption": "Figure 1.​ Model Development Pipeline",
        "content": "Figure 1.​ Model Development Pipeline",
        "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1805.03677/1805.03677/hybrid_auto/images/1805.03677_page0_fig0.jpg",
        "context_before": "2 http://datanutrition.media.mit.edu/demo.html\n\nData driven decision making systems play an increasingly important and impactful role in our lives. These frameworks are built on increasingly sophisticated artificial intelligence (AI) systems and are tuned by a growing population of data specialists to infer a vast diversity of outcomes: the song that plays next3 on your playlist, the type of advertisement you are most likely to see, or whether you qualify for a mortgage and at what rate [1]. The",
        "context_after": "Models often come under scrutiny only after they are built, trained, and deployed. If a model is found to perpetuate a bias - for example, over-indexing for a particular race or gender - the data specialist returns to the development stage in order to identify and address the issue. This feedback loop is inefficient, costly, and does not always mitigate harm; the time and energy of the data specialist is a sunk cost, and if in use, the model may have already caused harm. Some of this harm could "
      },
      "element_b": {
        "element_id": "1805.03677_table_1",
        "element_type": "table",
        "caption": "Table 1.​ Table illustrating 7 modules of the Dataset Nutrition Label, together with their description, role, and contents.",
        "content": "Table 1.​ Table illustrating 7 modules of the Dataset Nutrition Label, together with their description, role, and contents.",
        "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1805.03677/1805.03677/hybrid_auto/images/19c066fc428ea874c5a58b80ccef1c6ff4f5434de74568c7e9175b48f508971d.jpg",
        "context_before": "Figure 2​) starts to offer a glimpse into the dataset distributions.\n\nOrdinal   \nNominal   \n\n<table><tr><td>name</td><td>type</td><td>count</td><td>uniqueEntries</td><td>mostFrequent</td><td>leastFrequent</td><td>missing</td></tr><tr><td>id</td><td>number</td><td>500</td><td>488 including mi...</td><td>missing value (13)</td><td>multiple detected</td><td>2.60%</td></tr><tr><td>applicable_man...</td><td>number</td><td>500</td><td>4</td><td>100000000232 (...)</td><td>multiple detected</td><td>0%</",
        "context_after": "The list of modules currently examined in this study, while not exhaustive, provides a solid representation of the kinds of flexibility supported by the Label framework. Other modules considered for future iterations or additional datasets include but are not limited to: a comments section for users to interact with authors of the Label for feedback or other purposes; an extension of the Provenance section that includes the versioning history and change logs of the dataset and associated Labels "
      },
      "edge_contexts": [
        {
          "source": "1805.03677_table_1",
          "target": "1805.03677_figure_1",
          "ref_text": "Figure 1",
          "context_snippet": "n modules (Table 1, 2​). The Metadata, Provenance, and Variables modules (Supp. Figure 1​) provide as-is dataset information. They mirror information submitted by the L"
        }
      ]
    },
    {
      "pair_id": "1805.03677_pair_2",
      "doc_id": "1805.03677",
      "element_a_id": "1805.03677_figure_2",
      "element_b_id": "1805.03677_table_2",
      "element_a_type": "figure",
      "element_b_type": "table",
      "pair_type": "figure+table",
      "hop_distance": 1,
      "path": [
        "1805.03677_figure_2",
        "1805.03677_table_2"
      ],
      "quality_score": 1.0,
      "overlap_with_existing_l1": false,
      "element_a": {
        "element_id": "1805.03677_figure_2",
        "element_type": "figure",
        "caption": "Figure 2.​ (A) Survey results about data analysis best practices in respondents’ organizations and (B) Survey results about how respondents learned to analyze data",
        "content": "Figure 2.​ (A) Survey results about data analysis best practices in respondents’ organizations and (B) Survey results about how respondents learned to analyze data",
        "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1805.03677/1805.03677/hybrid_auto/images/1805.03677_page0_fig2.jpg",
        "context_before": "",
        "context_after": "To improve the accuracy and fairness of AI systems, it is imperative that data specialists are able to more quickly assess the viability and fitness of datasets, and more easily find and use better quality data to train their models. As a proposed solution, we introduce the Dataset Nutrition Label, a diagnostic framework to address and mitigate some of these challenges by providing critical information to data specialists at the point of data analysis.\n\nThis study begins with a review of related"
      },
      "element_b": {
        "element_id": "1805.03677_table_2",
        "element_type": "table",
        "caption": "Table 2.​ Variability of attributes across prototype modules highlights the potential diversity of information included in a Label   ",
        "content": "<table><tr><td></td><td colspan=\"5\">Module Characteristic - Level Required</td></tr><tr><td>Module Name</td><td>Technical Expertise</td><td>Manual Effort</td><td>Subjectivity</td><td>Interactivity</td><td>Data Exposure</td></tr><tr><td>Metadata</td><td>Low</td><td>High</td><td>Low</td><td>Low</td><td>Low</td></tr><tr><td>Provenance</td><td>Low</td><td>High</td><td>Low</td><td>Low</td><td>Low</td></tr><tr><td>Variables</td><td>Low</td><td>High</td><td>Medium</td><td>Low</td><td>Medium</td></tr><tr><td>Statistics</td><td>Medium</td><td>Low</td><td>Low</td><td>Low</td><td>Medium</td></tr><tr><td>Pair Plots</td><td>Medium</td><td>Low</td><td>Low</td><td>High</td><td>High</td></tr><tr><td>Probabilistic Modeling</td><td>High</td><td>Medium</td><td>High</td><td>Low</td><td>High</td></tr><tr><td>Ground Truth Correlations</td><td>Medium</td><td>Medium</td><td>Low</td><td>Low</td><td>High</td></tr></table>",
        "image_path": null,
        "context_before": "The resulting prototype successfully demonstrates how disparate modules can be built on a specific dataset in order to highlight multiple, complementary facets of the data, ideally to be leveraged for further investigation by data specialists through the use of additional tools and strategies. The prototype Label includes seven modules (Table 1, 2​). The Metadata, Provenance, and Variables modules (Supp. Figure 1​) provide as-is dataset information. They mirror information submitted by the Label",
        "context_after": "While the generation of some modules is fully automated, some require human input (Table 2​).\n\nFigure 2​) starts to offer a glimpse into the dataset distributions.\n\nOrdinal   \nNominal   \n\n<table><tr><td>name</td><td>type</td><td>count</td><td>uniqueEntries</td><td>mostFrequent</td><td>leastFrequent</td><td>missing</td></tr><tr><td>id</td><td>number</td><td>500</td><td>488 including mi...</td><td>missing value (13)</td><td>multiple detected</td><td>2.60%</td></tr><tr><td>applicable_man...</td><td"
      },
      "edge_contexts": [
        {
          "source": "1805.03677_table_2",
          "target": "1805.03677_figure_2",
          "ref_text": "figure 2",
          "context_snippet": "dataset information. They mirror information submitted by the Label\n\nSupplement figure 2.​ Prototype Label demonstrating the Statistics module, splitting the variables "
        }
      ]
    },
    {
      "pair_id": "1805.03677_pair_3",
      "doc_id": "1805.03677",
      "element_a_id": "1805.03677_table_2",
      "element_b_id": "1805.03677_figure_1",
      "element_a_type": "table",
      "element_b_type": "figure",
      "pair_type": "figure+table",
      "hop_distance": 1,
      "path": [
        "1805.03677_table_2",
        "1805.03677_figure_1"
      ],
      "quality_score": 1.0,
      "overlap_with_existing_l1": false,
      "element_a": {
        "element_id": "1805.03677_table_2",
        "element_type": "table",
        "caption": "Table 2.​ Variability of attributes across prototype modules highlights the potential diversity of information included in a Label   ",
        "content": "<table><tr><td></td><td colspan=\"5\">Module Characteristic - Level Required</td></tr><tr><td>Module Name</td><td>Technical Expertise</td><td>Manual Effort</td><td>Subjectivity</td><td>Interactivity</td><td>Data Exposure</td></tr><tr><td>Metadata</td><td>Low</td><td>High</td><td>Low</td><td>Low</td><td>Low</td></tr><tr><td>Provenance</td><td>Low</td><td>High</td><td>Low</td><td>Low</td><td>Low</td></tr><tr><td>Variables</td><td>Low</td><td>High</td><td>Medium</td><td>Low</td><td>Medium</td></tr><tr><td>Statistics</td><td>Medium</td><td>Low</td><td>Low</td><td>Low</td><td>Medium</td></tr><tr><td>Pair Plots</td><td>Medium</td><td>Low</td><td>Low</td><td>High</td><td>High</td></tr><tr><td>Probabilistic Modeling</td><td>High</td><td>Medium</td><td>High</td><td>Low</td><td>High</td></tr><tr><td>Ground Truth Correlations</td><td>Medium</td><td>Medium</td><td>Low</td><td>Low</td><td>High</td></tr></table>",
        "image_path": null,
        "context_before": "The resulting prototype successfully demonstrates how disparate modules can be built on a specific dataset in order to highlight multiple, complementary facets of the data, ideally to be leveraged for further investigation by data specialists through the use of additional tools and strategies. The prototype Label includes seven modules (Table 1, 2​). The Metadata, Provenance, and Variables modules (Supp. Figure 1​) provide as-is dataset information. They mirror information submitted by the Label",
        "context_after": "While the generation of some modules is fully automated, some require human input (Table 2​).\n\nFigure 2​) starts to offer a glimpse into the dataset distributions.\n\nOrdinal   \nNominal   \n\n<table><tr><td>name</td><td>type</td><td>count</td><td>uniqueEntries</td><td>mostFrequent</td><td>leastFrequent</td><td>missing</td></tr><tr><td>id</td><td>number</td><td>500</td><td>488 including mi...</td><td>missing value (13)</td><td>multiple detected</td><td>2.60%</td></tr><tr><td>applicable_man...</td><td"
      },
      "element_b": {
        "element_id": "1805.03677_figure_1",
        "element_type": "figure",
        "caption": "Figure 1.​ Model Development Pipeline",
        "content": "Figure 1.​ Model Development Pipeline",
        "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1805.03677/1805.03677/hybrid_auto/images/1805.03677_page0_fig0.jpg",
        "context_before": "2 http://datanutrition.media.mit.edu/demo.html\n\nData driven decision making systems play an increasingly important and impactful role in our lives. These frameworks are built on increasingly sophisticated artificial intelligence (AI) systems and are tuned by a growing population of data specialists to infer a vast diversity of outcomes: the song that plays next3 on your playlist, the type of advertisement you are most likely to see, or whether you qualify for a mortgage and at what rate [1]. The",
        "context_after": "Models often come under scrutiny only after they are built, trained, and deployed. If a model is found to perpetuate a bias - for example, over-indexing for a particular race or gender - the data specialist returns to the development stage in order to identify and address the issue. This feedback loop is inefficient, costly, and does not always mitigate harm; the time and energy of the data specialist is a sunk cost, and if in use, the model may have already caused harm. Some of this harm could "
      },
      "edge_contexts": [
        {
          "source": "1805.03677_table_2",
          "target": "1805.03677_figure_1",
          "ref_text": "Figure 1",
          "context_snippet": "n modules (Table 1, 2​). The Metadata, Provenance, and Variables modules (Supp. Figure 1​) provide as-is dataset information. They mirror information submitted by the L"
        }
      ]
    },
    {
      "pair_id": "1805.03677_pair_4",
      "doc_id": "1805.03677",
      "element_a_id": "1805.03677_table_2",
      "element_b_id": "1805.03677_figure_3",
      "element_a_type": "table",
      "element_b_type": "figure",
      "pair_type": "figure+table",
      "hop_distance": 1,
      "path": [
        "1805.03677_table_2",
        "1805.03677_figure_3"
      ],
      "quality_score": 1.0,
      "overlap_with_existing_l1": false,
      "element_a": {
        "element_id": "1805.03677_table_2",
        "element_type": "table",
        "caption": "Table 2.​ Variability of attributes across prototype modules highlights the potential diversity of information included in a Label   ",
        "content": "<table><tr><td></td><td colspan=\"5\">Module Characteristic - Level Required</td></tr><tr><td>Module Name</td><td>Technical Expertise</td><td>Manual Effort</td><td>Subjectivity</td><td>Interactivity</td><td>Data Exposure</td></tr><tr><td>Metadata</td><td>Low</td><td>High</td><td>Low</td><td>Low</td><td>Low</td></tr><tr><td>Provenance</td><td>Low</td><td>High</td><td>Low</td><td>Low</td><td>Low</td></tr><tr><td>Variables</td><td>Low</td><td>High</td><td>Medium</td><td>Low</td><td>Medium</td></tr><tr><td>Statistics</td><td>Medium</td><td>Low</td><td>Low</td><td>Low</td><td>Medium</td></tr><tr><td>Pair Plots</td><td>Medium</td><td>Low</td><td>Low</td><td>High</td><td>High</td></tr><tr><td>Probabilistic Modeling</td><td>High</td><td>Medium</td><td>High</td><td>Low</td><td>High</td></tr><tr><td>Ground Truth Correlations</td><td>Medium</td><td>Medium</td><td>Low</td><td>Low</td><td>High</td></tr></table>",
        "image_path": null,
        "context_before": "The resulting prototype successfully demonstrates how disparate modules can be built on a specific dataset in order to highlight multiple, complementary facets of the data, ideally to be leveraged for further investigation by data specialists through the use of additional tools and strategies. The prototype Label includes seven modules (Table 1, 2​). The Metadata, Provenance, and Variables modules (Supp. Figure 1​) provide as-is dataset information. They mirror information submitted by the Label",
        "context_after": "While the generation of some modules is fully automated, some require human input (Table 2​).\n\nFigure 2​) starts to offer a glimpse into the dataset distributions.\n\nOrdinal   \nNominal   \n\n<table><tr><td>name</td><td>type</td><td>count</td><td>uniqueEntries</td><td>mostFrequent</td><td>leastFrequent</td><td>missing</td></tr><tr><td>id</td><td>number</td><td>500</td><td>488 including mi...</td><td>missing value (13)</td><td>multiple detected</td><td>2.60%</td></tr><tr><td>applicable_man...</td><td"
      },
      "element_b": {
        "element_id": "1805.03677_figure_3",
        "element_type": "figure",
        "caption": "Figure 3.​ Architecture of the proposed Data Nutrition Label ecosystem.",
        "content": "Figure 3.​ Architecture of the proposed Data Nutrition Label ecosystem.",
        "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1805.03677/1805.03677/hybrid_auto/images/1805.03677_page0_fig3.jpg",
        "context_before": "Figure 2​) starts to offer a glimpse into the dataset distributions.\n\nOrdinal   \nNominal   \n\n<table><tr><td>name</td><td>type</td><td>count</td><td>uniqueEntries</td><td>mostFrequent</td><td>leastFrequent</td><td>missing</td></tr><tr><td>id</td><td>number</td><td>500</td><td>488 including mi...</td><td>missing value (13)</td><td>multiple detected</td><td>2.60%</td></tr><tr><td>applicable_man...</td><td>number</td><td>500</td><td>4</td><td>100000000232 (...)</td><td>multiple detected</td><td>0%</",
        "context_after": "Simple statistical analyses involving the generation of histograms, distribution information, and linear correlations are carried out directly in the browser, given tabular datasets of ${ < } 1 0 0 \\mathrm { K }$ rows. Server-side processing is thus reserved for more specialized and sophisticated analyses requiring additional computational power. Such processing could run multiple backends with the ultimate aim of providing the Label authors with a diverse set of options, fueled by the plethora "
      },
      "edge_contexts": [
        {
          "source": "1805.03677_figure_3",
          "target": "1805.03677_table_2",
          "ref_text": "Table 2",
          "context_snippet": "le the generation of some modules is fully automated, some require human input (Table 2​). For instance, the Metadata module mainly requires explicit input, while the "
        }
      ]
    },
    {
      "pair_id": "1805.03677_pair_8",
      "doc_id": "1805.03677",
      "element_a_id": "1805.03677_table_1",
      "element_b_id": "1805.03677_figure_2",
      "element_a_type": "table",
      "element_b_type": "figure",
      "pair_type": "figure+table",
      "hop_distance": 1,
      "path": [
        "1805.03677_table_1",
        "1805.03677_figure_2"
      ],
      "quality_score": 1.0,
      "overlap_with_existing_l1": false,
      "element_a": {
        "element_id": "1805.03677_table_1",
        "element_type": "table",
        "caption": "Table 1.​ Table illustrating 7 modules of the Dataset Nutrition Label, together with their description, role, and contents.",
        "content": "Table 1.​ Table illustrating 7 modules of the Dataset Nutrition Label, together with their description, role, and contents.",
        "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1805.03677/1805.03677/hybrid_auto/images/19c066fc428ea874c5a58b80ccef1c6ff4f5434de74568c7e9175b48f508971d.jpg",
        "context_before": "Figure 2​) starts to offer a glimpse into the dataset distributions.\n\nOrdinal   \nNominal   \n\n<table><tr><td>name</td><td>type</td><td>count</td><td>uniqueEntries</td><td>mostFrequent</td><td>leastFrequent</td><td>missing</td></tr><tr><td>id</td><td>number</td><td>500</td><td>488 including mi...</td><td>missing value (13)</td><td>multiple detected</td><td>2.60%</td></tr><tr><td>applicable_man...</td><td>number</td><td>500</td><td>4</td><td>100000000232 (...)</td><td>multiple detected</td><td>0%</",
        "context_after": "The list of modules currently examined in this study, while not exhaustive, provides a solid representation of the kinds of flexibility supported by the Label framework. Other modules considered for future iterations or additional datasets include but are not limited to: a comments section for users to interact with authors of the Label for feedback or other purposes; an extension of the Provenance section that includes the versioning history and change logs of the dataset and associated Labels "
      },
      "element_b": {
        "element_id": "1805.03677_figure_2",
        "element_type": "figure",
        "caption": "Figure 2.​ (A) Survey results about data analysis best practices in respondents’ organizations and (B) Survey results about how respondents learned to analyze data",
        "content": "Figure 2.​ (A) Survey results about data analysis best practices in respondents’ organizations and (B) Survey results about how respondents learned to analyze data",
        "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1805.03677/1805.03677/hybrid_auto/images/1805.03677_page0_fig2.jpg",
        "context_before": "",
        "context_after": "To improve the accuracy and fairness of AI systems, it is imperative that data specialists are able to more quickly assess the viability and fitness of datasets, and more easily find and use better quality data to train their models. As a proposed solution, we introduce the Dataset Nutrition Label, a diagnostic framework to address and mitigate some of these challenges by providing critical information to data specialists at the point of data analysis.\n\nThis study begins with a review of related"
      },
      "edge_contexts": [
        {
          "source": "1805.03677_figure_2",
          "target": "1805.03677_table_1",
          "ref_text": "Figure 2 ... Table 1",
          "context_snippet": "To test the concept generally and the modular framework specifically, we built a prototype with a dataset that included information about people and was maintained by an organization invested in better understanding the data. This combination of factors provides necessary information and access to b"
        }
      ]
    },
    {
      "pair_id": "1805.11202_pair_1",
      "doc_id": "1805.11202",
      "element_a_id": "1805.11202_figure_1",
      "element_b_id": "1805.11202_table_1",
      "element_a_type": "figure",
      "element_b_type": "table",
      "pair_type": "figure+table",
      "hop_distance": 1,
      "path": [
        "1805.11202_figure_1",
        "1805.11202_table_1"
      ],
      "quality_score": 1.0,
      "overlap_with_existing_l1": false,
      "element_a": {
        "element_id": "1805.11202_figure_1",
        "element_type": "figure",
        "caption": "Figure 1: Illustration of generative adversarial networks",
        "content": "Figure 1: Illustration of generative adversarial networks",
        "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1805.11202/1805.11202/hybrid_auto/images/1805.11202_page0_fig0.jpg",
        "context_before": "disc η P η s P η sThe classification fairness on a dataset is achieved if both the disparate treatment and disparate impact are removed from the data. To remove the disparate treatment, the classifier cannot use the protected attribute to make decisions. As for the disparate impact, research in [9] proposed the concept of $\\epsilon$ -fairness to examine the potential disparate impact.\n\nDefinition 3 $\\epsilon$ -fairness [9]). A labeled dataset $\\mathcal { D } = ( \\boldsymbol { \\chi } , \\boldsymbo",
        "context_after": "with empirical probabilities estimated from $\\mathcal { D }$ , where (balanced error rate) is defined as\n\n$$ B E R (f (\\mathcal {X}), \\mathcal {S}) = \\frac {P [ f (\\mathcal {X}) = 0 | \\mathcal {S} = 1 ] + P [ f (\\mathcal {X}) = 1 | \\mathcal {S} = 0 ]}{2}. $$\n\nindicates the average class-conditioned error of $f$ on distribu-BERtion $\\mathcal { D }$ over the pair $( \\chi , s )$ .\n\nFigure 1 illustrates the structure of GAN. Theoretical analysis shows that GAN aims to minimize the Jensen-Shannon div"
      },
      "element_b": {
        "element_id": "1805.11202_table_1",
        "element_type": "table",
        "caption": "Table 1: Risk differences of real and synthetic datasets   ",
        "content": "<table><tr><td></td><td>Real Data</td><td>SYN1-GAN</td><td>SYN2-NFGANI</td><td>SYN3-NFGANII</td><td>SYN4-FairGAN</td></tr><tr><td>\\( disk(D) \\)</td><td>0.1989</td><td>0.1798±0.0026</td><td>0.0025±0.0007</td><td>0.0062±0.0037</td><td>0.0411±0.0295</td></tr></table>",
        "image_path": null,
        "context_before": "Figure 1 illustrates the structure of GAN. Theoretical analysis shows that GAN aims to minimize the Jensen-Shannon divergence (JSD) between $P _ { \\mathrm { d a t a } }$ and $P _ { G }$ [10]. Minimization of the JSD is achieved when $P _ { G } = P _ { \\mathrm { d a t a } }$ .\n\nFairness. We adopt the risk difference in a labeled dataset $( d i s c ( \\mathscr { D } ) =$ $P ( y = 1 | s = 1 ) - P ( y = 1 | s = 0 ) ,$ disc as the metric to compare the per-P y s P y sformance of different GAN models o",
        "context_after": "shows the risk differences in the real and synthetic datasets. The risk difference in the Adult dataset is 0.1989, which indicates discrimination against female. The SYN-GAN, which is trained to be close to the real dataset, has the similar risk difference to the real dataset. On the contrary, SYN2-NFGANI, SYN3-NFGANII, and SYN4-FairGAN have lower risk differences than the real dataset. In particular, both SYN2-NFGANI and SYN3-NFGANII have extremely small risk differences. This is because the pr"
      },
      "edge_contexts": [
        {
          "source": "1805.11202_table_1",
          "target": "1805.11202_figure_1",
          "ref_text": "Figure 1 i",
          "context_snippet": "Figure 1 illustrates the structure of GAN. Theoretical analysis shows that GAN aims to min"
        }
      ]
    },
    {
      "pair_id": "1805.11202_pair_4",
      "doc_id": "1805.11202",
      "element_a_id": "1805.11202_table_1",
      "element_b_id": "1805.11202_figure_4",
      "element_a_type": "table",
      "element_b_type": "figure",
      "pair_type": "figure+table",
      "hop_distance": 1,
      "path": [
        "1805.11202_table_1",
        "1805.11202_figure_4"
      ],
      "quality_score": 1.0,
      "overlap_with_existing_l1": false,
      "element_a": {
        "element_id": "1805.11202_table_1",
        "element_type": "table",
        "caption": "Table 1: Risk differences of real and synthetic datasets   ",
        "content": "<table><tr><td></td><td>Real Data</td><td>SYN1-GAN</td><td>SYN2-NFGANI</td><td>SYN3-NFGANII</td><td>SYN4-FairGAN</td></tr><tr><td>\\( disk(D) \\)</td><td>0.1989</td><td>0.1798±0.0026</td><td>0.0025±0.0007</td><td>0.0062±0.0037</td><td>0.0411±0.0295</td></tr></table>",
        "image_path": null,
        "context_before": "Figure 1 illustrates the structure of GAN. Theoretical analysis shows that GAN aims to minimize the Jensen-Shannon divergence (JSD) between $P _ { \\mathrm { d a t a } }$ and $P _ { G }$ [10]. Minimization of the JSD is achieved when $P _ { G } = P _ { \\mathrm { d a t a } }$ .\n\nFairness. We adopt the risk difference in a labeled dataset $( d i s c ( \\mathscr { D } ) =$ $P ( y = 1 | s = 1 ) - P ( y = 1 | s = 0 ) ,$ disc as the metric to compare the per-P y s P y sformance of different GAN models o",
        "context_after": "shows the risk differences in the real and synthetic datasets. The risk difference in the Adult dataset is 0.1989, which indicates discrimination against female. The SYN-GAN, which is trained to be close to the real dataset, has the similar risk difference to the real dataset. On the contrary, SYN2-NFGANI, SYN3-NFGANII, and SYN4-FairGAN have lower risk differences than the real dataset. In particular, both SYN2-NFGANI and SYN3-NFGANII have extremely small risk differences. This is because the pr"
      },
      "element_b": {
        "element_id": "1805.11202_figure_4",
        "element_type": "figure",
        "caption": "(e) SYN4-FairGAN Figure 4: Dimension-wise conditional probability distributions $P ( \\mathbf { x } , y | s = 1 )$ vs. $P ( \\mathbf { x } , y | s = 0 )$ ). Each dot represents one attribute. The $\\mathbf { x }$ -axis represents the conditional probability given $s = 1$ P ,y s P ,y s. The y-axis represents the conditional probability given $s = 0$ . sThe diagonal line indicates the ideal fairness, where data have identical conditional probability distributions given .",
        "content": "(e) SYN4-FairGAN Figure 4: Dimension-wise conditional probability distributions $P ( \\mathbf { x } , y | s = 1 )$ vs. $P ( \\mathbf { x } , y | s = 0 )$ ). Each dot represents one attribute. The $\\mathbf { x }$ -axis represents the conditional probability given $s = 1$ P ,y s P ,y s. The y-axis represents the conditional probability given $s = 0$ . sThe diagonal line indicates the ideal fairness, where data have identical conditional probability distributions given .",
        "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1805.11202/1805.11202/hybrid_auto/images/1805.11202_page0_fig11.jpg",
        "context_before": "",
        "context_after": "In this paper, we don’t compare with the pre-process methods, because the classical methods like Massaging cannot remove disparate treatment and disparate impact [14]. Although the certifying framework proposed algorithms to remove disparate impact, they only work on numerical attributes [9].\n\nDatasets. We evaluate FairGAN and baselines on the UCI Adult income dataset which contains 48,842 instances [6]. The decision indicates whether the income is higher than $\\$ 50 k$ per year, and the protect"
      },
      "edge_contexts": [
        {
          "source": "1805.11202_table_1",
          "target": "1805.11202_figure_4",
          "ref_text": "Figure 4",
          "context_snippet": "s 0.0411, which shows the effectiveness of FairGAN on fair data generation.\n\nIn Figure 4, we compare the dimension-wise conditional probability distributions between $P"
        }
      ]
    },
    {
      "pair_id": "1805.11202_pair_7",
      "doc_id": "1805.11202",
      "element_a_id": "1805.11202_figure_2",
      "element_b_id": "1805.11202_table_1",
      "element_a_type": "figure",
      "element_b_type": "table",
      "pair_type": "figure+table",
      "hop_distance": 1,
      "path": [
        "1805.11202_figure_2",
        "1805.11202_table_1"
      ],
      "quality_score": 1.0,
      "overlap_with_existing_l1": false,
      "element_a": {
        "element_id": "1805.11202_figure_2",
        "element_type": "figure",
        "caption": "Figure 2: The Structure of FairGAN",
        "content": "Figure 2: The Structure of FairGAN",
        "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1805.11202/1805.11202/hybrid_auto/images/1805.11202_page0_fig1.jpg",
        "context_before": "Figure 1 illustrates the structure of GAN.\n\nTable 1\n\nTable 1: Risk differences of real and synthetic datasets   \n\n<table><tr><td></td><td>Real Data</td><td>SYN1-GAN</td><td>SYN2-NFGANI</td><td>SYN3-NFGANII</td><td>SYN4-FairGAN</td></tr><tr><td>\\( disk(D) \\)</td><td>0.1989</td><td>0.1798±0.0026</td><td>0.0025±0.0007</td><td>0.0062±0.0037</td><td>0.0411±0.0295</td></tr></table>\n\nshows the risk differences in the real and synthetic datasets.\n\n$$ V (G, D) = \\mathbb {E} _ {\\mathbf {x} \\sim P _ {\\math",
        "context_after": "Autoencoder is a feedforward neural network used for unsupervised learning. A basic autoencoder consists of an encoder and Enca decoder . Both the encoder and decoder are multilayer neural networks. Given an input $\\mathbf { x } \\in \\mathbb { R } ^ { n }$ , the encoder computes the hidden representation of an input $E n c ( \\mathbf { x } ) \\in \\mathbb { R } ^ { h }$ , and the decoder computes the reconstructed input $D e c ( E n c ( \\mathbf { x } ) ) \\in \\mathbb { R } ^ { n }$ based on the hidde"
      },
      "element_b": {
        "element_id": "1805.11202_table_1",
        "element_type": "table",
        "caption": "Table 1: Risk differences of real and synthetic datasets   ",
        "content": "<table><tr><td></td><td>Real Data</td><td>SYN1-GAN</td><td>SYN2-NFGANI</td><td>SYN3-NFGANII</td><td>SYN4-FairGAN</td></tr><tr><td>\\( disk(D) \\)</td><td>0.1989</td><td>0.1798±0.0026</td><td>0.0025±0.0007</td><td>0.0062±0.0037</td><td>0.0411±0.0295</td></tr></table>",
        "image_path": null,
        "context_before": "Figure 1 illustrates the structure of GAN. Theoretical analysis shows that GAN aims to minimize the Jensen-Shannon divergence (JSD) between $P _ { \\mathrm { d a t a } }$ and $P _ { G }$ [10]. Minimization of the JSD is achieved when $P _ { G } = P _ { \\mathrm { d a t a } }$ .\n\nFairness. We adopt the risk difference in a labeled dataset $( d i s c ( \\mathscr { D } ) =$ $P ( y = 1 | s = 1 ) - P ( y = 1 | s = 0 ) ,$ disc as the metric to compare the per-P y s P y sformance of different GAN models o",
        "context_after": "shows the risk differences in the real and synthetic datasets. The risk difference in the Adult dataset is 0.1989, which indicates discrimination against female. The SYN-GAN, which is trained to be close to the real dataset, has the similar risk difference to the real dataset. On the contrary, SYN2-NFGANI, SYN3-NFGANII, and SYN4-FairGAN have lower risk differences than the real dataset. In particular, both SYN2-NFGANI and SYN3-NFGANII have extremely small risk differences. This is because the pr"
      },
      "edge_contexts": [
        {
          "source": "1805.11202_figure_2",
          "target": "1805.11202_table_1",
          "ref_text": "Table 1",
          "context_snippet": "Figure 1 illustrates the structure of GAN.\n\nTable 1\n\nTable 1: Risk differences of real and synthetic datasets   \n\n<table><tr><td></"
        }
      ]
    },
    {
      "pair_id": "1805.11202_pair_8",
      "doc_id": "1805.11202",
      "element_a_id": "1805.11202_figure_2",
      "element_b_id": "1805.11202_table_2",
      "element_a_type": "figure",
      "element_b_type": "table",
      "pair_type": "figure+table",
      "hop_distance": 1,
      "path": [
        "1805.11202_figure_2",
        "1805.11202_table_2"
      ],
      "quality_score": 1.0,
      "overlap_with_existing_l1": false,
      "element_a": {
        "element_id": "1805.11202_figure_2",
        "element_type": "figure",
        "caption": "Figure 2: The Structure of FairGAN",
        "content": "Figure 2: The Structure of FairGAN",
        "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1805.11202/1805.11202/hybrid_auto/images/1805.11202_page0_fig1.jpg",
        "context_before": "Figure 1 illustrates the structure of GAN.\n\nTable 1\n\nTable 1: Risk differences of real and synthetic datasets   \n\n<table><tr><td></td><td>Real Data</td><td>SYN1-GAN</td><td>SYN2-NFGANI</td><td>SYN3-NFGANII</td><td>SYN4-FairGAN</td></tr><tr><td>\\( disk(D) \\)</td><td>0.1989</td><td>0.1798±0.0026</td><td>0.0025±0.0007</td><td>0.0062±0.0037</td><td>0.0411±0.0295</td></tr></table>\n\nshows the risk differences in the real and synthetic datasets.\n\n$$ V (G, D) = \\mathbb {E} _ {\\mathbf {x} \\sim P _ {\\math",
        "context_after": "Autoencoder is a feedforward neural network used for unsupervised learning. A basic autoencoder consists of an encoder and Enca decoder . Both the encoder and decoder are multilayer neural networks. Given an input $\\mathbf { x } \\in \\mathbb { R } ^ { n }$ , the encoder computes the hidden representation of an input $E n c ( \\mathbf { x } ) \\in \\mathbb { R } ^ { h }$ , and the decoder computes the reconstructed input $D e c ( E n c ( \\mathbf { x } ) ) \\in \\mathbb { R } ^ { n }$ based on the hidde"
      },
      "element_b": {
        "element_id": "1805.11202_table_2",
        "element_type": "table",
        "caption": "Table 2: Euclidean distances of joint and conditional probabilities between synthetic datasets and real dataset",
        "content": "Table 2: Euclidean distances of joint and conditional probabilities between synthetic datasets and real dataset",
        "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1805.11202/1805.11202/hybrid_auto/images/f7292ad4b93a243e252756e4f5181f42f624b01d4e8e8ce40ff1b813d55322ab.jpg",
        "context_before": "disparate impact in the real dataset. On the contrary, the BER in SYN4-FairGAN is $0 . 3 8 6 2 { \\scriptstyle \\pm 0 . 0 0 3 6 }$ , which indicates using the generated xˆ in SYN4-FairGAN to predict the real has much higher error srate. The disparate impact in SYN4-FairGAN is small. It shows the effectiveness of FairGAN on removal of the disparate impact in terms of the real . Note that we adopt a linear SVM as a classifier to predict .\n\nUtility. We then evaluate the data utility of synthetic data",
        "context_after": "5.3 Fair Classification\n\nIn this subsection, we adopt the real and synthetic datasets to train several classifiers and check whether the classifiers can achieve fairness. We evaluate the classifiers with three settings: 1) the classifiers are trained and tested on the real dataset, called REAL2REAL; 2) the classifiers are trained and tested on the synthetic datasets, called\n\nFairGAN consists of one generator $G _ { D e c }$ and two discriminators $D _ { 1 }$ and $D _ { 2 }$ GDec. We adopt the re"
      },
      "edge_contexts": [
        {
          "source": "1805.11202_table_2",
          "target": "1805.11202_figure_2",
          "ref_text": "Figure 2 s",
          "context_snippet": "sed generator from medGAN [5] to D Dgenerate both discrete and continuous data. Figure 2 shows the structure of FairGAN. In FairGAN, every generated sample has a correspo"
        }
      ]
    },
    {
      "pair_id": "1805.11202_pair_13",
      "doc_id": "1805.11202",
      "element_a_id": "1805.11202_figure_3",
      "element_b_id": "1805.11202_table_3",
      "element_a_type": "figure",
      "element_b_type": "table",
      "pair_type": "figure+table",
      "hop_distance": 1,
      "path": [
        "1805.11202_figure_3",
        "1805.11202_table_3"
      ],
      "quality_score": 1.0,
      "overlap_with_existing_l1": false,
      "element_a": {
        "element_id": "1805.11202_figure_3",
        "element_type": "figure",
        "caption": "(d) FairGAN Figure 3: Comparing FairGAN, NaïveFairGAN-I and NaïveFairGAN-II on a toy dataset. (a) shows the distributions $P _ { \\mathbf { d a t a } } ( x )$ (black), $P _ { \\bf d a t a } ( x | s = 1 )$ (green) and $P _ { \\bf d a t a } ( x | s = 0 )$ (red) of real data; (b), (c) and (d) are distributions $P _ { G } ( x ) , P _ { G } ( x | s = 1 )$ and $P _ { G } ( x | s = 0 )$ P x s P x s Pof synthetic datasets generated by NaïveFairGAN-I, NaïveFairGAN-II and FairGAN separately.",
        "content": "(d) FairGAN Figure 3: Comparing FairGAN, NaïveFairGAN-I and NaïveFairGAN-II on a toy dataset. (a) shows the distributions $P _ { \\mathbf { d a t a } } ( x )$ (black), $P _ { \\bf d a t a } ( x | s = 1 )$ (green) and $P _ { \\bf d a t a } ( x | s = 0 )$ (red) of real data; (b), (c) and (d) are distributions $P _ { G } ( x ) , P _ { G } ( x | s = 1 )$ and $P _ { G } ( x | s = 0 )$ P x s P x s Pof synthetic datasets generated by NaïveFairGAN-I, NaïveFairGAN-II and FairGAN separately.",
        "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1805.11202/1805.11202/hybrid_auto/images/1805.11202_page0_fig6.jpg",
        "context_before": "",
        "context_after": "We compare FairGAN with NaïveFairGAN-I and NaïveFairGAN-II on a toy dataset which consists of one unprotected attribute $x \\in \\mathbb { R }$ and one protected attribute $s \\in \\{ 0 , 1 \\}$ . The toy dataset is drawn from $x \\sim 0 . 5 * N ( 1 , 0 . 5 ) + 0 . 5 * N ( 3 , 0 . 5 )$ , where $P _ { \\mathrm { d a t a } } ( x | s = 1 ) = N ( 1 , 0 . 5 )$ xand $P _ { \\mathrm { d a t a } } ( x | s = 0 ) = N ( 3 , 0 . 5 )$ . P x s , .. Hence, the unprotected attribute $x$ P x s , .is strong correlated wi"
      },
      "element_b": {
        "element_id": "1805.11202_table_3",
        "element_type": "table",
        "caption": "Table 3: Risk differences in classifiers and classification accuracies on various training and testing settings   ",
        "content": "<table><tr><td rowspan=\"2\"></td><td rowspan=\"2\">Classifier</td><td>REAL2REAL</td><td colspan=\"4\">SYN2SYN</td><td colspan=\"4\">SYN2REAL</td></tr><tr><td></td><td>SYN1-GAN</td><td>SYN2-NFGANI</td><td>SYN3-NFGANII</td><td>SYN4-FairGAN</td><td>SYN1-GAN</td><td>SYN2-NFGANI</td><td>SYN3-NFGANII</td><td>SYN4-FairGAN</td></tr><tr><td rowspan=\"3\">Risk Difference</td><td>SVM (Linear)</td><td>0.1784</td><td>0.1341±0.0023</td><td>0.0018±0.0021</td><td>0.0073±0.0039</td><td>0.0371±0.0189</td><td>0.1712±0.0062</td><td>0.1580±0.0076</td><td>0.1579±0.0079</td><td>0.0461±0.0424</td></tr><tr><td>SVM (RBF)</td><td>0.1788</td><td>0.1292±0.0049</td><td>0.0018±0.0025</td><td>0.0074±0.0028</td><td>0.0354±0.0206</td><td>0.1623±0.0050</td><td>0.1602±0.0053</td><td>0.1603±0.0087</td><td>0.0526±0.0353</td></tr><tr><td>Decision Tree</td><td>0.1547</td><td>0.1396±0.0089</td><td>0.0015±0.0035</td><td>0.0115±0.0061</td><td>0.0535±0.0209</td><td>0.1640±0.0077</td><td>0.1506±0.0070</td><td>0.1588±0.0264</td><td>0.0754±",
        "image_path": null,
        "context_before": "sWe train FairGAN and NaïveFairGAN models to approximate the distribution of $P _ { \\mathrm { d a t a } } ( x )$ . Figure 3 shows the data probability $P ( x )$ P xand two conditional probabilities $P ( x | s = 1 )$ and $P ( x | s = 0 )$ P x of the P x s P x stoy dataset (shown in Figure 3a) and synthetic datasets (Figures 3b to 3d) from FairGAN and NaïveFairGAN models.\n\nFairness. We adopt the risk difference in a classifier $( d i s c ( \\eta ) =$ $P ( \\eta ( \\mathbf { x } ) = 1 | s = 1 ) - P ( ",
        "context_after": "xˆ that don’t have correlations with the real , i.e. free from the dissparate impact, the classifier trained on SYN4-FairGAN can achieve fair classification on the real dataset. It demonstrates the advantage of FairGAN over the NaïveFairGAN models on fair classification.\n\nClassification accuracy. Table 3 further shows the classification accuracies of different classifiers on various training and testing settings. We can observe that the accuracies of classifiers on the SYN2REAL setting are close"
      },
      "edge_contexts": [
        {
          "source": "1805.11202_table_3",
          "target": "1805.11202_figure_3",
          "ref_text": "Figure 3 s",
          "context_snippet": "models to approximate the distribution of $P _ { \\mathrm { d a t a } } ( x )$ . Figure 3 shows the data probability $P ( x )$ P xand two conditional probabilities $P ( x "
        }
      ]
    },
    {
      "pair_id": "1808.08166_pair_1",
      "doc_id": "1808.08166",
      "element_a_id": "1808.08166_figure_1",
      "element_b_id": "1808.08166_table_1",
      "element_a_type": "figure",
      "element_b_type": "table",
      "pair_type": "figure+table",
      "hop_distance": 1,
      "path": [
        "1808.08166_figure_1",
        "1808.08166_table_1"
      ],
      "quality_score": 1.0,
      "overlap_with_existing_l1": false,
      "element_a": {
        "element_id": "1808.08166_figure_1",
        "element_type": "figure",
        "caption": "Figure 1: Fairness Gerrymandering: A Toy Example [Kearns et al., 2018]",
        "content": "Figure 1: Fairness Gerrymandering: A Toy Example [Kearns et al., 2018]",
        "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1808.08166/1808.08166/hybrid_auto/images/1808.08166_page0_fig0.jpg",
        "context_before": "The most common definitions of fairness in machine learning are statistical in nature. They proceed by fixing a small number of “protected subgroups” (such as racial or gender groups), and then ask that some statistic of interest be approximately equalized across groups. Standard choices for these statistics include positive classification rates [Calders and Verwer, 2010], false positive or false negative rates [Hardt et al., 2016, Kleinberg et al., 2017, Chouldechova, 2017] and positive predict",
        "context_after": "Suppose individuals each have two sensitive attributes: race (say blue and green) and gender (say male and female). Suppose that these two attributes are distributed independently and uniformly at random, and are uncorrelated with a binary label that is also distributed uniformly at random. If we view gender and race as defining classes of people that we wish to protect, we could take a standard statistical fairness definition from the literature — say the equal odds condition of Hardt et al. [2"
      },
      "element_b": {
        "element_id": "1808.08166_table_1",
        "element_type": "table",
        "caption": "Table 1: Description of Data Sets.",
        "content": "Table 1: Description of Data Sets.",
        "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1808.08166/1808.08166/hybrid_auto/images/0316712cab10fb743166e741482c718fc19c00e3e8610fd406a4a41dd4162e0a.jpg",
        "context_before": "They illustrate the idea with the following toy example shown in Figure 1, described as follows.\n\nThe heuristic finds a linear threshold function as follows:\n\nTable 1: Description of Data Sets.\n\n$$ \\hat {h} \\in \\underset {h \\in \\mathcal {H}} {\\operatorname {a r g m i n}} \\sum_ {i = 1} ^ {n} \\left[ h \\left(X _ {i}\\right) c _ {i} ^ {1} + \\left(1 - h \\left(X _ {i}\\right)\\right) c _ {i} ^ {0} \\right] \\tag {1} $$\n\nFollowing both Agarwal et al. [2018] and Kearns et al. [2018], in all of the experiment",
        "context_after": "We leave the precise descriptions of the algorithm from Kearns et al. [2018] — which we will refer to as the SUBGROUP algorithm — to the appendix. We refer the reader to Kearns et al. [2018] for details about its derivation and guarantees.2 At this point we remark only that the algorithm operates by expressing the optimization problem to be solved (minimize error, subject to subgroup fairness constraints) as solving for the equilibrium in a two player zero-sum game, between a Learner and an Audi"
      },
      "edge_contexts": [
        {
          "source": "1808.08166_table_1",
          "target": "1808.08166_figure_1",
          "ref_text": "Figure 1",
          "context_snippet": "They illustrate the idea with the following toy example shown in Figure 1, described as follows.\n\nThe heuristic finds a linear threshold function as foll"
        }
      ]
    },
    {
      "pair_id": "1809.01496_pair_1",
      "doc_id": "1809.01496",
      "element_a_id": "1809.01496_figure_1",
      "element_b_id": "1809.01496_table_1",
      "element_a_type": "figure",
      "element_b_type": "table",
      "pair_type": "figure+table",
      "hop_distance": 1,
      "path": [
        "1809.01496_figure_1",
        "1809.01496_table_1"
      ],
      "quality_score": 1.0,
      "overlap_with_existing_l1": false,
      "element_a": {
        "element_id": "1809.01496_figure_1",
        "element_type": "figure",
        "caption": "(c) Gender-neutral profession words projected to gender direction in GN-GloVe Figure 1: Cosine similarity between the gender direction and the embeddings of gender-neutral words. In each figure, negative values represent a bias towards female, otherwise male.",
        "content": "(c) Gender-neutral profession words projected to gender direction in GN-GloVe Figure 1: Cosine similarity between the gender direction and the embeddings of gender-neutral words. In each figure, negative values represent a bias towards female, otherwise male.",
        "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1809.01496/1809.01496/hybrid_auto/images/1809.01496_page0_fig2.jpg",
        "context_before": "",
        "context_after": "the gender information has been substantially diminished from $w ^ { ( a ) }$ in the GN-GloVe embedding.\n\nWe further quantify the gender information exhibited in the embedding models. For each model, we project the word vectors of occupational words into the gender sub-space defined by “he-she” and compute their average size. A larger projection indicates an embedding model is more biased. Results show that the average projection of GloVe is 0.080, the projection of Hard-GloVe is 0.019, and the "
      },
      "element_b": {
        "element_id": "1809.01496_table_1",
        "element_type": "table",
        "caption": "Table 1: Percentage of predictions for each category on gender relational analogy task.",
        "content": "Table 1: Percentage of predictions for each category on gender relational analogy task.",
        "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1809.01496/1809.01496/hybrid_auto/images/cce2b0bde6ccb8fc8cf3d0e7374aaaa19f4605fa8353f73b5dbc00931c5b44ac.jpg",
        "context_before": "the gender information has been substantially diminished from $w ^ { ( a ) }$ in the GN-GloVe embedding.\n\nWe further quantify the gender information exhibited in the embedding models. For each model, we project the word vectors of occupational words into the gender sub-space defined by “he-she” and compute their average size. A larger projection indicates an embedding model is more biased. Results show that the average projection of GloVe is 0.080, the projection of Hard-GloVe is 0.019, and the ",
        "context_after": "are not used as a seed word during the training. To test the generalization ability of the model, we generate a subset of data (SemBias (subset)) of 40 instances associated with these 2 pairs.\n\nTable 1 lists the percentage of times that each class of pair is on the top based on a word embedding model (Mikolov et al., 2013c). GN-GloVe achieves $9 7 . 7 \\%$ accuracy in identifying genderdefinition word pairs as an analogy to “he - she”. In contrast, GloVe and Hard-GloVe makes significantly more mi"
      },
      "edge_contexts": [
        {
          "source": "1809.01496_figure_1",
          "target": "1809.01496_table_1",
          "ref_text": "Table 1",
          "context_snippet": "stances. Among the 22 genderdefinition word pairs, there are 2 word pairs that\n\nTable 1 lists the percentage of times that each class of pair is on the top based on a "
        }
      ]
    },
    {
      "pair_id": "1809.02208_pair_1",
      "doc_id": "1809.02208",
      "element_a_id": "1809.02208_table_1",
      "element_b_id": "1809.02208_figure_1",
      "element_a_type": "table",
      "element_b_type": "figure",
      "pair_type": "figure+table",
      "hop_distance": 1,
      "path": [
        "1809.02208_table_1",
        "1809.02208_figure_1"
      ],
      "quality_score": 1.0,
      "overlap_with_existing_l1": false,
      "element_a": {
        "element_id": "1809.02208_table_1",
        "element_type": "table",
        "caption": "4.1 for further explanation.",
        "content": "4.1 for further explanation.",
        "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1809.02208/1809.02208/hybrid_auto/images/e4fcd5d62aeba2df429ee96eb9f61b0581e6af843d097ef524695c63a5150ecc.jpg",
        "context_before": "As Figure 1 exemplifies, this approach produces results consistent with the hypothesis that sentences about stereotypical gender roles are translated accordingly with high probability: nurse and baker are translated with female pronouns while engineer and $\\it C E O$ are translated with male ones.\n\nAs Figure 1 clearly shows, the same template yields a male pronoun when “nurse” is replaced by “engineer”.\n\nWe shall assume and then show that the phenomenon of gender bias in machine translation can ",
        "context_after": "4.1 for further explanation.\n\n4.1 for further explanation."
      },
      "element_b": {
        "element_id": "1809.02208_figure_1",
        "element_type": "figure",
        "caption": "Figure 1: Translating sentences from a gender neutral language such as Hungarian to English provides a glimpse into the phenomenon of gender bias in machine translation. This screenshot from Google Translate shows how occupations from traditionally male-dominated fields [40] such as scholar, engineer and CEO are interpreted as male, while occupations such as nurse, baker and wedding organizer are interpreted as female.",
        "content": "Figure 1: Translating sentences from a gender neutral language such as Hungarian to English provides a glimpse into the phenomenon of gender bias in machine translation. This screenshot from Google Translate shows how occupations from traditionally male-dominated fields [40] such as scholar, engineer and CEO are interpreted as male, while occupations such as nurse, baker and wedding organizer are interpreted as female.",
        "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1809.02208/1809.02208/hybrid_auto/images/1809.02208_page0_fig0.jpg",
        "context_before": "On a 2014 article, Londa Schiebinger suggested that scientific research fails to take gender issues into account, arguing that the phenomenon of male defaults on new technologies such as Google Translate provides a window into this asymmetry [35]. Since then, recent worrisome results in machine learning have somewhat supported Schiebinger’s view. Not only Google photos’ statistical image labeling algorithm has been found to classify darkskinned people as gorillas [15] and purportedly intelligent",
        "context_after": "algorithm with promising results: they were able to cut the proportion of stereotypical analogies from 19% to 6% without any significant compromise in the performance of the word embedding technique. They are not alone: there is a growing effort to systematically discover and resolve issues of algorithmic bias in black-box algorithms[18]. The success of these results suggest that a similar technique could be used to remove gender bias from Google Translate outputs, should it exist. This paper in"
      },
      "edge_contexts": [
        {
          "source": "1809.02208_table_1",
          "target": "1809.02208_figure_1",
          "ref_text": "Figure 1 e",
          "context_snippet": "As Figure 1 exemplifies, this approach produces results consistent with the hypothesis that s"
        }
      ]
    },
    {
      "pair_id": "1809.02208_pair_2",
      "doc_id": "1809.02208",
      "element_a_id": "1809.02208_table_3",
      "element_b_id": "1809.02208_figure_2",
      "element_a_type": "table",
      "element_b_type": "figure",
      "pair_type": "figure+table",
      "hop_distance": 1,
      "path": [
        "1809.02208_table_3",
        "1809.02208_figure_2"
      ],
      "quality_score": 1.0,
      "overlap_with_existing_l1": false,
      "element_a": {
        "element_id": "1809.02208_table_3",
        "element_type": "table",
        "caption": "Table 3: Templates used to infer gender biases in the translation of job occupations and adjectives to the English language.",
        "content": "Table 3: Templates used to infer gender biases in the translation of job occupations and adjectives to the English language.",
        "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1809.02208/1809.02208/hybrid_auto/images/623aace85d297f2ef89881cd6eed80405fa061ebb9c33ec0f7fb12967d5607bf.jpg",
        "context_before": "",
        "context_after": "We shall assume and then show that the phenomenon of gender bias in machine translation can be assessed by mapping sentences constructed in gender neutral languages to English by the means of an automated translation tool. Specifically, we can translate sentences such as the Hungarian “˝o egy ´apol´on˝o”, where “´apol´on˝o” translates to “nurse” and “˝o” is a gender-neutral pronoun meaning either he, she or it, to English, yielding in this example the result “she’s a nurse” on Google Translate.\n"
      },
      "element_b": {
        "element_id": "1809.02208_figure_2",
        "element_type": "figure",
        "caption": "Figure 2: The data for the number of translated female pronouns per merged occupation category totaled among languages suggests and inverse distribution. STEM fields are nearly exclusively concentrated at $X = 0$ , while more evenly distributed in fields such as production and healthcare (See Table",
        "content": "Figure 2: The data for the number of translated female pronouns per merged occupation category totaled among languages suggests and inverse distribution. STEM fields are nearly exclusively concentrated at $X = 0$ , while more evenly distributed in fields such as production and healthcare (See Table",
        "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1809.02208/1809.02208/hybrid_auto/images/1809.02208_page0_fig1.jpg",
        "context_before": "Table 6 summarizes these data, and Table 7 summarizes it even further by coalescing occupation categories into broader groups to ease interpretation.\n\nThe bar plots in Figure 7 help us visualize how much of the distribution of each occupation category is composed of female, male and gender-neutral pronouns.\n\n1. Note that rows do not in general add up to $1 0 0 \\%$ , as there is a fair amount of translated sentences for which we cannot obtain a gender pronoun.\n\nPlotting histograms for the number ",
        "context_after": "There is a prohibitively large class of nouns and adjectives that could in principle be substituted into our templates. To simplify our dataset, we have decided to focus our work on job positions – which, we believe, are an interesting window into the nature of gender bias –, and were able to obtain a comprehensive list of professional occupations from the Bureau of Labor Statistics’ detailed occupations table [7], from the United States Department of Labor. The values inside, however, had to be"
      },
      "edge_contexts": [
        {
          "source": "1809.02208_table_3",
          "target": "1809.02208_figure_2",
          "ref_text": "Figure 2 s",
          "context_snippet": " male and gender-neutral pronouns are differently distributed. The histogram in Figure 2 suggests that the number of female pronouns is inversely distributed – which is m"
        }
      ]
    },
    {
      "pair_id": "1809.02208_pair_3",
      "doc_id": "1809.02208",
      "element_a_id": "1809.02208_table_3",
      "element_b_id": "1809.02208_figure_4",
      "element_a_type": "table",
      "element_b_type": "figure",
      "pair_type": "figure+table",
      "hop_distance": 1,
      "path": [
        "1809.02208_table_3",
        "1809.02208_figure_4"
      ],
      "quality_score": 1.0,
      "overlap_with_existing_l1": false,
      "element_a": {
        "element_id": "1809.02208_table_3",
        "element_type": "table",
        "caption": "Table 3: Templates used to infer gender biases in the translation of job occupations and adjectives to the English language.",
        "content": "Table 3: Templates used to infer gender biases in the translation of job occupations and adjectives to the English language.",
        "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1809.02208/1809.02208/hybrid_auto/images/623aace85d297f2ef89881cd6eed80405fa061ebb9c33ec0f7fb12967d5607bf.jpg",
        "context_before": "",
        "context_after": "We shall assume and then show that the phenomenon of gender bias in machine translation can be assessed by mapping sentences constructed in gender neutral languages to English by the means of an automated translation tool. Specifically, we can translate sentences such as the Hungarian “˝o egy ´apol´on˝o”, where “´apol´on˝o” translates to “nurse” and “˝o” is a gender-neutral pronoun meaning either he, she or it, to English, yielding in this example the result “she’s a nurse” on Google Translate.\n"
      },
      "element_b": {
        "element_id": "1809.02208_figure_4",
        "element_type": "figure",
        "caption": "Figure 4: The scarcity of gender-neutral pronouns is manifest in their histogram. Once again, STEM fields are predominantly concentrated at $X = 0$ .",
        "content": "Figure 4: The scarcity of gender-neutral pronouns is manifest in their histogram. Once again, STEM fields are predominantly concentrated at $X = 0$ .",
        "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1809.02208/1809.02208/hybrid_auto/images/1809.02208_page0_fig3.jpg",
        "context_before": "The histogram in Figure 2 suggests that the number of female pronouns is inversely distributed – which is mirrored in the data for gender-neutral pronouns in Figure 4 –, while the same data for male pronouns (shown in Figure 3) suggests a skew normal distribution.\n\n7) extends to higher values.   \nFigure 3: In contrast to Figure   \n2 male pronouns are seemingly skew normally distributed, with a peak at $X = 6$ . One can see how STEM fields concentrate mainly to the right ( $X \\geq 6$ ).\n\nThe temp",
        "context_after": "We can also visualize male, female, and gender neutral histograms side by side, in which context is useful to compare the dissimilar distributions of translated STEM and Healthcare occupations (Figures 5 and 6 respectively). The number of translated female pronouns among languages is not normally distributed for any of the individual categories in Table 2, but Healthcare is in many ways the most balanced category, which can be seen in comparison with STEM – in which male defaults are second to m"
      },
      "edge_contexts": [
        {
          "source": "1809.02208_figure_4",
          "target": "1809.02208_table_3",
          "ref_text": "Table 3",
          "context_snippet": "is similar to that used for occupations, and is provided again for reference in Table 3."
        }
      ]
    },
    {
      "pair_id": "1809.02208_pair_4",
      "doc_id": "1809.02208",
      "element_a_id": "1809.02208_table_3",
      "element_b_id": "1809.02208_figure_3",
      "element_a_type": "table",
      "element_b_type": "figure",
      "pair_type": "figure+table",
      "hop_distance": 1,
      "path": [
        "1809.02208_table_3",
        "1809.02208_figure_3"
      ],
      "quality_score": 1.0,
      "overlap_with_existing_l1": false,
      "element_a": {
        "element_id": "1809.02208_table_3",
        "element_type": "table",
        "caption": "Table 3: Templates used to infer gender biases in the translation of job occupations and adjectives to the English language.",
        "content": "Table 3: Templates used to infer gender biases in the translation of job occupations and adjectives to the English language.",
        "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1809.02208/1809.02208/hybrid_auto/images/623aace85d297f2ef89881cd6eed80405fa061ebb9c33ec0f7fb12967d5607bf.jpg",
        "context_before": "",
        "context_after": "We shall assume and then show that the phenomenon of gender bias in machine translation can be assessed by mapping sentences constructed in gender neutral languages to English by the means of an automated translation tool. Specifically, we can translate sentences such as the Hungarian “˝o egy ´apol´on˝o”, where “´apol´on˝o” translates to “nurse” and “˝o” is a gender-neutral pronoun meaning either he, she or it, to English, yielding in this example the result “she’s a nurse” on Google Translate.\n"
      },
      "element_b": {
        "element_id": "1809.02208_figure_3",
        "element_type": "figure",
        "caption": "7) extends to higher values. Figure 3: In contrast to Figure 2 male pronouns are seemingly skew normally distributed, with a peak at $X = 6$ . One can see how STEM fields concentrate mainly to the right ( $X \\geq 6$ ).",
        "content": "7) extends to higher values. Figure 3: In contrast to Figure 2 male pronouns are seemingly skew normally distributed, with a peak at $X = 6$ . One can see how STEM fields concentrate mainly to the right ( $X \\geq 6$ ).",
        "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1809.02208/1809.02208/hybrid_auto/images/1809.02208_page0_fig2.jpg",
        "context_before": "7) extends to higher values.   \nFigure 3: In contrast to Figure   \n2 male pronouns are seemingly skew normally distributed, with a peak at $X = 6$ . One can see how STEM fields concentrate mainly to the right ( $X \\geq 6$ ).\n\nThe number of translated female pronouns among languages is not normally distributed for any of the individual categories in Table 2, but Healthcare is in many ways the most balanced category, which can be seen in comparison with STEM – in which male defaults are second to ",
        "context_after": "We shall assume and then show that the phenomenon of gender bias in machine translation can be assessed by mapping sentences constructed in gender neutral languages to English by the means of an automated translation tool. Specifically, we can translate sentences such as the Hungarian “˝o egy ´apol´on˝o”, where “´apol´on˝o” translates to “nurse” and “˝o” is a gender-neutral pronoun meaning either he, she or it, to English, yielding in this example the result “she’s a nurse” on Google Translate.\n"
      },
      "edge_contexts": [
        {
          "source": "1809.02208_figure_3",
          "target": "1809.02208_table_3",
          "ref_text": "Table 3",
          "context_snippet": "is similar to that used for occupations, and is provided again for reference in Table 3."
        }
      ]
    },
    {
      "pair_id": "1809.02208_pair_5",
      "doc_id": "1809.02208",
      "element_a_id": "1809.02208_table_4",
      "element_b_id": "1809.02208_figure_2",
      "element_a_type": "table",
      "element_b_type": "figure",
      "pair_type": "figure+table",
      "hop_distance": 1,
      "path": [
        "1809.02208_table_4",
        "1809.02208_figure_2"
      ],
      "quality_score": 1.0,
      "overlap_with_existing_l1": false,
      "element_a": {
        "element_id": "1809.02208_table_4",
        "element_type": "table",
        "caption": "Table 4: A randomly selected example subset of thirty occupations obtained from our dataset with a total of 1019 different occupations.",
        "content": "Table 4: A randomly selected example subset of thirty occupations obtained from our dataset with a total of 1019 different occupations.",
        "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1809.02208/1809.02208/hybrid_auto/images/e5972c74aaa26dae21310e3352657e78fe977238057864a2475ce03e85151257.jpg",
        "context_before": "The histogram in Figure 2 suggests that the number of female pronouns is inversely distributed – which is mirrored in the data for gender-neutral pronouns in Figure 4 –, while the same data for male pronouns (shown in Figure 3) suggests a skew normal distribution.\n\n7) extends to higher values.   \nFigure 3: In contrast to Figure   \n2 male pronouns are seemingly skew normally distributed, with a peak at $X = 6$ . One can see how STEM fields concentrate mainly to the right ( $X \\geq 6$ ).\n\nThe temp",
        "context_after": "There is a prohibitively large class of nouns and adjectives that could in principle be substituted into our templates. To simplify our dataset, we have decided to focus our work on job positions – which, we believe, are an interesting window into the nature of gender bias –, and were able to obtain a comprehensive list of professional occupations from the Bureau of Labor Statistics’ detailed occupations table [7], from the United States Department of Labor. The values inside, however, had to be"
      },
      "element_b": {
        "element_id": "1809.02208_figure_2",
        "element_type": "figure",
        "caption": "Figure 2: The data for the number of translated female pronouns per merged occupation category totaled among languages suggests and inverse distribution. STEM fields are nearly exclusively concentrated at $X = 0$ , while more evenly distributed in fields such as production and healthcare (See Table",
        "content": "Figure 2: The data for the number of translated female pronouns per merged occupation category totaled among languages suggests and inverse distribution. STEM fields are nearly exclusively concentrated at $X = 0$ , while more evenly distributed in fields such as production and healthcare (See Table",
        "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1809.02208/1809.02208/hybrid_auto/images/1809.02208_page0_fig1.jpg",
        "context_before": "Table 6 summarizes these data, and Table 7 summarizes it even further by coalescing occupation categories into broader groups to ease interpretation.\n\nThe bar plots in Figure 7 help us visualize how much of the distribution of each occupation category is composed of female, male and gender-neutral pronouns.\n\n1. Note that rows do not in general add up to $1 0 0 \\%$ , as there is a fair amount of translated sentences for which we cannot obtain a gender pronoun.\n\nPlotting histograms for the number ",
        "context_after": "There is a prohibitively large class of nouns and adjectives that could in principle be substituted into our templates. To simplify our dataset, we have decided to focus our work on job positions – which, we believe, are an interesting window into the nature of gender bias –, and were able to obtain a comprehensive list of professional occupations from the Bureau of Labor Statistics’ detailed occupations table [7], from the United States Department of Labor. The values inside, however, had to be"
      },
      "edge_contexts": [
        {
          "source": "1809.02208_table_4",
          "target": "1809.02208_figure_2",
          "ref_text": "Figure 2 s",
          "context_snippet": "The histogram in Figure 2 suggests that the number of female pronouns is inversely distributed – which is m"
        }
      ]
    },
    {
      "pair_id": "1809.02244_pair_1",
      "doc_id": "1809.02244",
      "element_a_id": "1809.02244_figure_2",
      "element_b_id": "1809.02244_table_1",
      "element_a_type": "figure",
      "element_b_type": "table",
      "pair_type": "figure+table",
      "hop_distance": 1,
      "path": [
        "1809.02244_figure_2",
        "1809.02244_table_1"
      ],
      "quality_score": 1.0,
      "overlap_with_existing_l1": false,
      "element_a": {
        "element_id": "1809.02244_figure_2",
        "element_type": "figure",
        "caption": "Figure 2. Group-level incarceration rates for the COMPAS data as a function of the utility parameter $\\theta$ .",
        "content": "Figure 2. Group-level incarceration rates for the COMPAS data as a function of the utility parameter $\\theta$ .",
        "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1809.02244/1809.02244/hybrid_auto/images/1809.02244_page0_fig3.jpg",
        "context_before": "We generated synthetic data for a two-stage decision problem according to the causal model shown in Fig. 1(c) $(K = 2)$ , where all variables are binary except for the continuous response utility $Y \\equiv Y_{2}$ . Details on the specific models used are reported in the Supplement. We generated a dataset of size 5,000, with 100 bootstrap replications, where the sensitive variable $S$ is randomly assigned and where $S$ is chosen to be an informative covariate in estimating $Y$ .\n\nA third method f",
        "context_after": "$(A = 1)$ is a function of $\\theta$ , which we plot in Fig. 2 stratified by racial group. See the Supplement for results on overall incarceration rates, which also vary among the policies. The region of particular interest is between $\\theta = 2$ and 3, where fair and unrestricted optimal policies differ and both recommend lower-than-observed overall incarceration rates (see Supplement). For most $\\theta$ values, the fair policy recommends a decision rule which narrows the racial gap in incarcer"
      },
      "element_b": {
        "element_id": "1809.02244_table_1",
        "element_type": "table",
        "caption": "Table 1. Comparison of population outcomes $\\mathbb{E}[Y]$ under policies learned by different methods. The value under the observed policy was $0.24 \\pm 0.006$ .",
        "content": "Table 1. Comparison of population outcomes $\\mathbb{E}[Y]$ under policies learned by different methods. The value under the observed policy was $0.24 \\pm 0.006$ .",
        "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1809.02244/1809.02244/hybrid_auto/images/fb2916a64f5c6a8bc2f95d4d77e04208ef3c6c2c8c431253f728775492340976.jpg",
        "context_before": "$(A = 1)$ is a function of $\\theta$ , which we plot in Fig. 2 stratified by racial group. See the Supplement for results on overall incarceration rates, which also vary among the policies. The region of particular interest is between $\\theta = 2$ and 3, where fair and unrestricted optimal policies differ and both recommend lower-than-observed overall incarceration rates (see Supplement). For most $\\theta$ values, the fair policy recommends a decision rule which narrows the racial gap in incarcer",
        "context_after": "For this two-stage setting we estimated the optimal policies using Q-learning and value search. In value search, we considered restricted class of polices of the form $p(A_1 = 1|X,S,M) = -1 + \\alpha_xX + \\alpha_sS + \\alpha_mM + \\alpha_{sx}SX + \\alpha_{sm}SM + \\alpha_{mx}MX$ , and $p(A_2 = 1|X,S,M,A_1,Y_1) = -1 + \\alpha_xX + \\alpha_sS + \\alpha_mM + \\alpha_aA + \\alpha_{y_1}Y_1 + \\alpha_{sx}SX + \\alpha_{sm}SM + \\alpha_{mx}MX + \\alpha_{as}AS + \\alpha_{ax}AX$ where all $\\alpha$ s range from -3 to 3 b"
      },
      "edge_contexts": [
        {
          "source": "1809.02244_table_1",
          "target": "1809.02244_figure_2",
          "ref_text": "Fig. 2 s",
          "context_snippet": "$(A = 1)$ is a function of $\\theta$ , which we plot in Fig. 2 stratified by racial group. See the Supplement for results on overall incarcerati"
        }
      ]
    },
    {
      "pair_id": "1809.02244_pair_2",
      "doc_id": "1809.02244",
      "element_a_id": "1809.02244_table_1",
      "element_b_id": "1809.02244_figure_1",
      "element_a_type": "table",
      "element_b_type": "figure",
      "pair_type": "figure+table",
      "hop_distance": 1,
      "path": [
        "1809.02244_table_1",
        "1809.02244_figure_1"
      ],
      "quality_score": 1.0,
      "overlap_with_existing_l1": false,
      "element_a": {
        "element_id": "1809.02244_table_1",
        "element_type": "table",
        "caption": "Table 1. Comparison of population outcomes $\\mathbb{E}[Y]$ under policies learned by different methods. The value under the observed policy was $0.24 \\pm 0.006$ .",
        "content": "Table 1. Comparison of population outcomes $\\mathbb{E}[Y]$ under policies learned by different methods. The value under the observed policy was $0.24 \\pm 0.006$ .",
        "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1809.02244/1809.02244/hybrid_auto/images/fb2916a64f5c6a8bc2f95d4d77e04208ef3c6c2c8c431253f728775492340976.jpg",
        "context_before": "$(A = 1)$ is a function of $\\theta$ , which we plot in Fig. 2 stratified by racial group. See the Supplement for results on overall incarceration rates, which also vary among the policies. The region of particular interest is between $\\theta = 2$ and 3, where fair and unrestricted optimal policies differ and both recommend lower-than-observed overall incarceration rates (see Supplement). For most $\\theta$ values, the fair policy recommends a decision rule which narrows the racial gap in incarcer",
        "context_after": "For this two-stage setting we estimated the optimal policies using Q-learning and value search. In value search, we considered restricted class of polices of the form $p(A_1 = 1|X,S,M) = -1 + \\alpha_xX + \\alpha_sS + \\alpha_mM + \\alpha_{sx}SX + \\alpha_{sm}SM + \\alpha_{mx}MX$ , and $p(A_2 = 1|X,S,M,A_1,Y_1) = -1 + \\alpha_xX + \\alpha_sS + \\alpha_mM + \\alpha_aA + \\alpha_{y_1}Y_1 + \\alpha_{sx}SX + \\alpha_{sm}SM + \\alpha_{mx}MX + \\alpha_{as}AS + \\alpha_{ax}AX$ where all $\\alpha$ s range from -3 to 3 b"
      },
      "element_b": {
        "element_id": "1809.02244_figure_1",
        "element_type": "figure",
        "caption": "(c) Figure 1. (a) A simple causal DAG, with a single treatment $A$ , a single outcome $Y$ , a vector $X$ of baseline variables, and a single mediator $M$ . (b) A causal DAG corresponding to our (simplified) child welfare example with baseline factors $X$ , sensitive feature $S$ , action $A$ , vector of mediators (including e.g. socioeconomic variables, histories of drug treatment) $M$ , an indicator $Y_{1}$ of whether a child is separated from their parents, and an indicator of child hospitalization $Y_{2}$ . (d) A multistage decision problem, which corresponds to a complete DAG over vertices $X, S, M, A_{1}, Y_{1}, \\dots, A_{K}, Y_{K}$ .",
        "content": "(c) Figure 1. (a) A simple causal DAG, with a single treatment $A$ , a single outcome $Y$ , a vector $X$ of baseline variables, and a single mediator $M$ . (b) A causal DAG corresponding to our (simplified) child welfare example with baseline factors $X$ , sensitive feature $S$ , action $A$ , vector of mediators (including e.g. socioeconomic variables, histories of drug treatment) $M$ , an indicator $Y_{1}$ of whether a child is separated from their parents, and an indicator of child hospitalization $Y_{2}$ . (d) A multistage decision problem, which corresponds to a complete DAG over vertices $X, S, M, A_{1}, Y_{1}, \\dots, A_{K}, Y_{K}$ .",
        "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1809.02244/1809.02244/hybrid_auto/images/1809.02244_page0_fig2.jpg",
        "context_before": "",
        "context_after": "would be $p(Y_1, Y_2, A, M, S, X)$ . The proposal from Nabi & Shpitser (2018) is that fairness corresponds to the impermissibility of certain path-specific effects, and so fair inference requires decisions to be made from a counterfactual distribution $p^*(Y_1, Y_2, A, M, S, X)$ which is \"nearby\" to $p$ (in the sense of minimal Kullback-Leibler divergence) but where these PSEs are constrained to be zero. They call $p^*$ the distribution generated by a \"fair world.\"\n\nMultiple fairness concerns ha"
      },
      "edge_contexts": [
        {
          "source": "1809.02244_table_1",
          "target": "1809.02244_figure_1",
          "ref_text": "Fig. 1",
          "context_snippet": "y, see Shpitser (2018). As an example, the distribution of $Y(a)$ in the DAG in Fig. 1(a) is identified by $\\sum_{X,M} p(Y|a, M, X)p(M|a, X)p(X)$ . Note that some cau"
        }
      ]
    },
    {
      "pair_id": "1809.02244_pair_5",
      "doc_id": "1809.02244",
      "element_a_id": "1809.02244_figure_3",
      "element_b_id": "1809.02244_table_1",
      "element_a_type": "figure",
      "element_b_type": "table",
      "pair_type": "figure+table",
      "hop_distance": 1,
      "path": [
        "1809.02244_figure_3",
        "1809.02244_table_1"
      ],
      "quality_score": 1.0,
      "overlap_with_existing_l1": false,
      "element_a": {
        "element_id": "1809.02244_figure_3",
        "element_type": "figure",
        "caption": "Figure 3. Overall incarceration rates for the COMPAS data as a function of the utility parameter $\\theta$ .",
        "content": "Figure 3. Overall incarceration rates for the COMPAS data as a function of the utility parameter $\\theta$ .",
        "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1809.02244/1809.02244/hybrid_auto/images/1809.02244_page0_fig4.jpg",
        "context_before": "We generated synthetic data for a two-stage decision problem according to the causal model shown in Fig. 1(c) $(K = 2)$ , where all variables are binary except for the continuous response utility $Y \\equiv Y_{2}$ . Details on the specific models used are reported in the Supplement. We generated a dataset of size 5,000, with 100 bootstrap replications, where the sensitive variable $S$ is randomly assigned and where $S$ is chosen to be an informative covariate in estimating $Y$ .\n\nA third method f",
        "context_after": "In Fig. 3, we compare the overall incarceration rates recommended by the optimal fair and unconstrained policies on the COMPAS data, as a function of the utility parameter $\\theta$ . For low values of $\\theta$ the incarceration rate is zero, and becomes higher as $\\theta$ increases, but differentially for the fair and unconstrained optimal policies. The difference between the policies depends crucially on the utility function. For some values of the utility parameter, the unfair and fair policie"
      },
      "element_b": {
        "element_id": "1809.02244_table_1",
        "element_type": "table",
        "caption": "Table 1. Comparison of population outcomes $\\mathbb{E}[Y]$ under policies learned by different methods. The value under the observed policy was $0.24 \\pm 0.006$ .",
        "content": "Table 1. Comparison of population outcomes $\\mathbb{E}[Y]$ under policies learned by different methods. The value under the observed policy was $0.24 \\pm 0.006$ .",
        "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1809.02244/1809.02244/hybrid_auto/images/fb2916a64f5c6a8bc2f95d4d77e04208ef3c6c2c8c431253f728775492340976.jpg",
        "context_before": "$(A = 1)$ is a function of $\\theta$ , which we plot in Fig. 2 stratified by racial group. See the Supplement for results on overall incarceration rates, which also vary among the policies. The region of particular interest is between $\\theta = 2$ and 3, where fair and unrestricted optimal policies differ and both recommend lower-than-observed overall incarceration rates (see Supplement). For most $\\theta$ values, the fair policy recommends a decision rule which narrows the racial gap in incarcer",
        "context_after": "For this two-stage setting we estimated the optimal policies using Q-learning and value search. In value search, we considered restricted class of polices of the form $p(A_1 = 1|X,S,M) = -1 + \\alpha_xX + \\alpha_sS + \\alpha_mM + \\alpha_{sx}SX + \\alpha_{sm}SM + \\alpha_{mx}MX$ , and $p(A_2 = 1|X,S,M,A_1,Y_1) = -1 + \\alpha_xX + \\alpha_sS + \\alpha_mM + \\alpha_aA + \\alpha_{y_1}Y_1 + \\alpha_{sx}SX + \\alpha_{sm}SM + \\alpha_{mx}MX + \\alpha_{as}AS + \\alpha_{ax}AX$ where all $\\alpha$ s range from -3 to 3 b"
      },
      "edge_contexts": [
        {
          "source": "1809.02244_figure_3",
          "target": "1809.02244_table_1",
          "ref_text": "Table 1",
          "context_snippet": "pared the results with Q-learning and value search. The results are provided in Table 1. The data generating process for the single-stage decision problem matches the "
        }
      ]
    },
    {
      "pair_id": "1809.04737_pair_4",
      "doc_id": "1809.04737",
      "element_a_id": "1809.04737_table_1",
      "element_b_id": "1809.04737_figure_1",
      "element_a_type": "table",
      "element_b_type": "figure",
      "pair_type": "figure+table",
      "hop_distance": 1,
      "path": [
        "1809.04737_table_1",
        "1809.04737_figure_1"
      ],
      "quality_score": 1.0,
      "overlap_with_existing_l1": false,
      "element_a": {
        "element_id": "1809.04737_table_1",
        "element_type": "table",
        "caption": "Table 1: An example of admitting students.",
        "content": "Table 1: An example of admitting students.",
        "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1809.04737/1809.04737/hybrid_auto/images/495d8c4fb2630144c0e955d73ff0f6d19dbd2a0330ae67b2fb3e834cf0b8f831.jpg",
        "context_before": "Examples of $\\kappa ( \\cdot )$ and $\\delta ( \\cdot )$ are shown in Figure 1.\n\nThe statistics of the dataset is shown in Table 1.\n\n$$ \\begin{array}{l} \\mathbb {R D} (f) = \\mathbb {E} _ {\\mathbf {X}} \\left[ \\frac {P (S = s ^ {+} | \\mathbf {x})}{P (S = s ^ {+})} \\mathbb {1} _ {h (\\mathbf {x}) > 0} \\right. \\tag {3} \\\\ + \\frac {P (S = s ^ {-} | \\mathbf {x})}{P (S = s ^ {-})} \\mathbb {1} _ {h (\\mathbf {x}) < 0} - 1 ]. \\\\ \\end{array} $$\n\nFor simplicity, we may want to denote $P ( S = s ^ { + } | \\mathb",
        "context_after": "To sum up, we obtain the following convex optimization formulation for learning fair classifiers.\n\nProblem Formulation 1. The goal of the fairness-aware classification is to find a classifier $f$ which minimizes the empirical loss $\\mathbb { L } ( f )$ while satisfying fairness constraint $| \\mathbb { R } \\mathbb { D } ( f ) | \\leq \\tau$ . It can be approached by solving the following constrained optimization problem\n\n$$ \\min _ {h \\in \\mathcal {H}} \\quad \\mathbb {L} _ {\\phi} (h) $$\n\nFor simplici"
      },
      "element_b": {
        "element_id": "1809.04737_figure_1",
        "element_type": "figure",
        "caption": "Figure 1: Examples of $\\kappa ( \\cdot )$ and $\\delta ( \\cdot )$ .",
        "content": "Figure 1: Examples of $\\kappa ( \\cdot )$ and $\\delta ( \\cdot )$ .",
        "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1809.04737/1809.04737/hybrid_auto/images/1809.04737_page0_fig0.jpg",
        "context_before": "subject to $\\mathbb { R D } ( f ) \\leq \\tau , \\quad - \\mathbb { R D } ( f ) \\leq \\tau .$\n\nObviously, the above optimization problem is non-convex. Similar to the loss function, we adopt surrogate functions to convert the risk difference to convex constraints. By using predictive function $h$ and the indicator function, we can rewrite the risk difference as\n\n$$ \\begin{array}{l} \\mathbb {R} \\mathbb {D} (f) = \\mathbb {E} _ {\\mathbf {X} | S = s ^ {+}} \\left[ \\mathbb {1} \\left[ \\operatorname {s i g n",
        "context_after": "$$ \\begin{array}{l} \\mathbb {R D} (f) = \\mathbb {E} _ {\\mathbf {X}} \\left[ \\frac {P (S = s ^ {+} | \\mathbf {x})}{P (S = s ^ {+})} \\mathbb {1} _ {h (\\mathbf {x}) > 0} \\right. \\tag {3} \\\\ + \\frac {P (S = s ^ {-} | \\mathbf {x})}{P (S = s ^ {-})} \\mathbb {1} _ {h (\\mathbf {x}) < 0} - 1 ]. \\\\ \\end{array} $$\n\nFor simplicity, we may want to denote $P ( S = s ^ { + } | \\mathbf { x } )$ by $\\eta ( \\mathbf { x } )$ and $P ( \\bar { S } = s ^ { + } )$ by $p$ . Similarly, the indicator function in above equa"
      },
      "edge_contexts": [
        {
          "source": "1809.04737_table_1",
          "target": "1809.04737_figure_1",
          "ref_text": "Figure 1",
          "context_snippet": "Examples of $\\kappa ( \\cdot )$ and $\\delta ( \\cdot )$ are shown in Figure 1.\n\nThe statistics of the dataset is shown in Table 1.\n\n$$ \\begin{array}{l} \\math"
        }
      ]
    },
    {
      "pair_id": "1809.04737_pair_8",
      "doc_id": "1809.04737",
      "element_a_id": "1809.04737_figure_2",
      "element_b_id": "1809.04737_table_2",
      "element_a_type": "figure",
      "element_b_type": "table",
      "pair_type": "figure+table",
      "hop_distance": 1,
      "path": [
        "1809.04737_figure_2",
        "1809.04737_table_2"
      ],
      "quality_score": 1.0,
      "overlap_with_existing_l1": false,
      "element_a": {
        "element_id": "1809.04737_figure_2",
        "element_type": "figure",
        "caption": "(b) A classifier that doesn’t meet the $\\kappa , \\delta$ -risk difference constraint makes fair predictions. Figure 2: Two classifiers and their predictions.",
        "content": "(b) A classifier that doesn’t meet the $\\kappa , \\delta$ -risk difference constraint makes fair predictions. Figure 2: Two classifiers and their predictions.",
        "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1809.04737/1809.04737/hybrid_auto/images/1809.04737_page0_fig2.jpg",
        "context_before": "",
        "context_after": "Problem Formulation 2. A fair classifier $f = s i g n ( h )$ that achieves fairness constraint $- c _ { 2 } \\leq \\mathbb { R D } ( f ) \\leq c _ { 1 }$ can be obtained by solving the following constrained optimization\n\n$$ \\min _ {h \\in \\mathcal {H}} \\quad \\mathbb {L} _ {\\phi} (h) \\tag {6} $$\n\nsubject to $\\mathbb { R } \\mathbb { D } _ { \\kappa } ( h ) \\leq \\psi _ { \\kappa } \\big ( c _ { 1 } - \\mathbb { R } \\mathbb { D } ^ { - } \\big ) + \\mathbb { R } \\mathbb { D } _ { \\kappa } ^ { - } ,$\n\nFor our\n"
      },
      "element_b": {
        "element_id": "1809.04737_table_2",
        "element_type": "table",
        "caption": "Table 2: Some common surrogate functions for κ-δ and the corresponding $\\psi _ { \\kappa } ( \\mu )$ and $\\psi _ { \\delta } ( \\mu )$   ",
        "content": "<table><tr><td>Name of κ-δ</td><td>κ(α) for α ∈ R</td><td>δ(α) for α ∈ R</td><td>ψκ(μ) or ψδ(μ) for μ ∈ (0,1/p]</td></tr><tr><td>Hinge</td><td>max{α+1,0}</td><td>min{α,1}</td><td>μ</td></tr><tr><td>Square</td><td>(α+1)2</td><td>1-(1-α)2</td><td>μ2</td></tr><tr><td>Exponential</td><td>exp(α)</td><td>1-exp(-α)</td><td>(√(1-p)μ+1-√1-pμ)2</td></tr></table>",
        "image_path": null,
        "context_before": "",
        "context_after": ""
      },
      "edge_contexts": [
        {
          "source": "1809.04737_figure_2",
          "target": "1809.04737_table_2",
          "ref_text": "Table 2",
          "context_snippet": " ^ { - } \\big ) + \\mathbb { R } \\mathbb { D } _ { \\kappa } ^ { - } ,$\n\nFor our\n\nTable 2: Some common surrogate functions for κ-δ and the corresponding $\\psi _ { \\kappa"
        }
      ]
    },
    {
      "pair_id": "1809.10083_pair_1",
      "doc_id": "1809.10083",
      "element_a_id": "1809.10083_figure_1",
      "element_b_id": "1809.10083_table_1",
      "element_a_type": "figure",
      "element_b_type": "table",
      "pair_type": "figure+table",
      "hop_distance": 1,
      "path": [
        "1809.10083_figure_1",
        "1809.10083_table_1"
      ],
      "quality_score": 1.0,
      "overlap_with_existing_l1": false,
      "element_a": {
        "element_id": "1809.10083_figure_1",
        "element_type": "figure",
        "caption": "(b) Figure 1: (a) Unsupervised Invariance Induction Framework and (b) Adversarial Model Design",
        "content": "(b) Figure 1: (a) Unsupervised Invariance Induction Framework and (b) Adversarial Model Design",
        "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1809.10083/1809.10083/hybrid_auto/images/1809.10083_page0_fig1.jpg",
        "context_before": "",
        "context_after": "(4) a decoder Dec that reconstructs $x$ from $\\tilde { e } _ { 1 }$ and $e _ { 2 }$ . Additionally, the training objective contains a loss-term that enforces disentanglement between $E n c ( x ) _ { 1 } = e _ { 1 }$ and $E n c ( x ) _ { 2 } \\stackrel { - } { = } e _ { 2 }$ . Figure 1a shows our generalized framework. The training objective for this system can be written as Equation 1.\n\n$$ \\begin{array}{l} L = \\alpha L _ {p r e d} (y, P r e d (e _ {1})) + \\beta L _ {d e c} (x, D e c (\\psi (e _ {1"
      },
      "element_b": {
        "element_id": "1809.10083_table_1",
        "element_type": "table",
        "caption": "Table 1: Results on Extended Yale-B dataset   ",
        "content": "<table><tr><td>Metric</td><td>NN + MMD [13]</td><td>VFAE [14]</td><td>CAI [19]</td><td>Ours</td></tr><tr><td>Accuracy of predicting y from e1 (Ay)</td><td>0.82</td><td>0.85</td><td>0.89</td><td>0.95</td></tr><tr><td>Accuracy of predicting z from e1 (Az)</td><td>-</td><td>0.57</td><td>0.57</td><td>0.24</td></tr></table>",
        "image_path": null,
        "context_before": "player model ( $M _ { 1 }$ or $M _ { 2 }$ ) when we update the weights of the other. $M _ { 2 }$ should ideally be trained to convergence before updating $M _ { 1 }$ in each training epoch to backpropagate accurate and stable disentanglement-inducing gradients to Enc. However, this is not scalable in practice. We update $M _ { 1 }$ and $M _ { 2 }$ in the frequency ratio of $1 : k$ . We found $k = 5$ to perform well in our experiments.\n\nCompetition between prediction and reconstruction. The predi",
        "context_after": "Table 1 summarizes the results.\n\n5.1 Invariance to inherent nuisance factors\n\nWe provide results of our framework at the task of learning invariance to inherent nuisance factors on two datasets – Extended Yale-B [7] and Chairs [2].\n\nExtended Yale-B. This dataset contains face-images of 38 subjects under various lighting conditions. The target $y$ is the subject identity whereas the inherent nuisance factor $z$ is the lighting condition. We compare our framework to existing state-of-the-art super"
      },
      "edge_contexts": [
        {
          "source": "1809.10083_figure_1",
          "target": "1809.10083_table_1",
          "ref_text": "Table 1",
          "context_snippet": "y$ concentrate into $e _ { 1 }$ and all other factors migrate to $e _ { 2 }$ .\n\nTable 1 summarizes the results. The proposed unsupervised method outperforms existing s"
        }
      ]
    },
    {
      "pair_id": "1809.10083_pair_4",
      "doc_id": "1809.10083",
      "element_a_id": "1809.10083_table_1",
      "element_b_id": "1809.10083_figure_2",
      "element_a_type": "table",
      "element_b_type": "figure",
      "pair_type": "figure+table",
      "hop_distance": 1,
      "path": [
        "1809.10083_table_1",
        "1809.10083_figure_2"
      ],
      "quality_score": 1.0,
      "overlap_with_existing_l1": false,
      "element_a": {
        "element_id": "1809.10083_table_1",
        "element_type": "table",
        "caption": "Table 1: Results on Extended Yale-B dataset   ",
        "content": "<table><tr><td>Metric</td><td>NN + MMD [13]</td><td>VFAE [14]</td><td>CAI [19]</td><td>Ours</td></tr><tr><td>Accuracy of predicting y from e1 (Ay)</td><td>0.82</td><td>0.85</td><td>0.89</td><td>0.95</td></tr><tr><td>Accuracy of predicting z from e1 (Az)</td><td>-</td><td>0.57</td><td>0.57</td><td>0.24</td></tr></table>",
        "image_path": null,
        "context_before": "player model ( $M _ { 1 }$ or $M _ { 2 }$ ) when we update the weights of the other. $M _ { 2 }$ should ideally be trained to convergence before updating $M _ { 1 }$ in each training epoch to backpropagate accurate and stable disentanglement-inducing gradients to Enc. However, this is not scalable in practice. We update $M _ { 1 }$ and $M _ { 2 }$ in the frequency ratio of $1 : k$ . We found $k = 5$ to perform well in our experiments.\n\nCompetition between prediction and reconstruction. The predi",
        "context_after": "Table 1 summarizes the results.\n\n5.1 Invariance to inherent nuisance factors\n\nWe provide results of our framework at the task of learning invariance to inherent nuisance factors on two datasets – Extended Yale-B [7] and Chairs [2].\n\nExtended Yale-B. This dataset contains face-images of 38 subjects under various lighting conditions. The target $y$ is the subject identity whereas the inherent nuisance factor $z$ is the lighting condition. We compare our framework to existing state-of-the-art super"
      },
      "element_b": {
        "element_id": "1809.10083_figure_2",
        "element_type": "figure",
        "caption": "Figure 2: Extended Yale-B – t-SNE visualization of (a) raw data, (b) $e _ { 2 }$ labeled by lighting condition, (c) $e _ { 1 }$ labeled by lighting condition, and (d) $e _ { 1 }$ labeled by subject-ID (numerical markers, not colors).",
        "content": "Figure 2: Extended Yale-B – t-SNE visualization of (a) raw data, (b) $e _ { 2 }$ labeled by lighting condition, (c) $e _ { 1 }$ labeled by lighting condition, and (d) $e _ { 1 }$ labeled by subject-ID (numerical markers, not colors).",
        "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1809.10083/1809.10083/hybrid_auto/images/1809.10083_page0_fig6.jpg",
        "context_before": "",
        "context_after": "Table 1 summarizes the results. The proposed unsupervised method outperforms existing state-of-theart (supervised) invariance induction methods on both $A _ { y }$ and $A _ { z }$ metrics, providing a significant boost on $A _ { y }$ and complete removal of lighting information from $e _ { 1 }$ reflected by $A _ { z }$ . Furthermore, the accuracy of predicting $z$ from $e _ { 2 }$ is 0.89, which validates its automatic migration to $e _ { 2 }$ . Figure 2 shows t-SNE [15] visualization of raw dat"
      },
      "edge_contexts": [
        {
          "source": "1809.10083_figure_2",
          "target": "1809.10083_table_1",
          "ref_text": "Table 1",
          "context_snippet": "Table 1 summarizes the results. The proposed unsupervised method outperforms existing s"
        }
      ]
    },
    {
      "pair_id": "1611.07509_pair_1",
      "doc_id": "1611.07509",
      "element_a_id": "1611.07509_formula_1",
      "element_b_id": "1611.07509_figure_1",
      "element_a_type": "formula",
      "element_b_type": "figure",
      "pair_type": "figure+formula",
      "hop_distance": 1,
      "path": [
        "1611.07509_formula_1",
        "1611.07509_figure_1"
      ],
      "quality_score": 0.825,
      "overlap_with_existing_l1": false,
      "element_a": {
        "element_id": "1611.07509_formula_1",
        "element_type": "formula",
        "caption": "",
        "content": "$$S E _ {\\pi} \\left(x _ {2}, x _ {1}\\right) = P (y \\mid d o \\left(x _ {2} \\mid_ {\\pi}\\right)) - P (y \\mid d o \\left(x _ {1}\\right)).$$",
        "image_path": null,
        "context_before": "When applied to the example in Figure 1, it means the expected change in loan approval of applications actually from the disadvantage group (e.g., black), when the bank is instructed to treat the applicants as from the advantage group (e.g., white).\n\nWhen applied to the example in Figure 1, it means the expected change in loan approval of the disadvantage group if they had the same racial makeups shown in the Zip code as the advantage group.\n\nχThe results are shown in Table 1.",
        "context_after": ""
      },
      "element_b": {
        "element_id": "1611.07509_figure_1",
        "element_type": "figure",
        "caption": "Figure 1: The toy model.",
        "content": "Figure 1: The toy model.",
        "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1611.07509/1611.07509/hybrid_auto/images/1611.07509_page0_fig0.jpg",
        "context_before": "The causal modeling based discrimination detection has been proposed most recently (Bonchi et al. 2015; Zhang,\n\nCopyright $©$ 2017, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.\n\narXiv:1611.07509v1 [cs.LG] 22 Nov 2016",
        "context_after": "Wu, and Wu 2016b; 2016a) for improving the correlation based approaches. In this paper, we develop a framework for discovering and removing both direct and indirect discrimination based on the causal network. A causal network is a directed acyclic graph (DAG) widely used for causal representation, reasoning and inference (Pearl 2009), where causal effects are carried by the paths that trace arrows pointing from the cause to the effect which are referred to as the causal paths. Using this model, "
      },
      "edge_contexts": [
        {
          "source": "1611.07509_formula_1",
          "target": "1611.07509_figure_1",
          "ref_text": "Figure 1",
          "context_snippet": "When applied to the example in Figure 1, it means the expected change in loan approval of applications actually from th"
        }
      ]
    },
    {
      "pair_id": "1706.02744_pair_2",
      "doc_id": "1706.02744",
      "element_a_id": "1706.02744_formula_2",
      "element_b_id": "1706.02744_figure_4",
      "element_a_type": "formula",
      "element_b_type": "figure",
      "pair_type": "figure+formula",
      "hop_distance": 1,
      "path": [
        "1706.02744_formula_2",
        "1706.02744_figure_4"
      ],
      "quality_score": 0.825,
      "overlap_with_existing_l1": false,
      "element_a": {
        "element_id": "1706.02744_formula_2",
        "element_type": "formula",
        "caption": "",
        "content": "$$R _ {\\theta} = - \\lambda_ {X} \\beta P + \\lambda_ {X} X = \\lambda_ {X} (X - \\beta P),$$",
        "image_path": null,
        "context_before": "We proceed analogously to the previous subsection using the example graph in Figure 4. Instead of the proxy, we consider a resolving variable $E$ . The causal dependences are equivalent to the ones in Figure 3 and we again assume linear structural equations\n\n1. Intervene on $E$ by fixing it to a random variable $\\eta$ with $\\mathbb { P } ( \\eta ) = \\mathbb { P } ( E )$ , the marginal distribution of $E$ in $\\tilde { \\mathcal { G } }$ , see Figure 4. In the example we find\n\nLet us now try to adju",
        "context_after": ""
      },
      "element_b": {
        "element_id": "1706.02744_figure_4",
        "element_type": "figure",
        "caption": "Figure 4: A template graph $\\tilde { \\mathcal { G } }$ for unresolved discrimination (left) with its intervened version $\\mathcal { G }$ (right). While from the skeptical viewpoint we generically do not want $A$ to influence $R$ , we first intervene on $E$ interrupting all paths through $E$ and only cancel the remaining influence on $A$ to $R$ .",
        "content": "Figure 4: A template graph $\\tilde { \\mathcal { G } }$ for unresolved discrimination (left) with its intervened version $\\mathcal { G }$ (right). While from the skeptical viewpoint we generically do not want $A$ to influence $R$ , we first intervene on $E$ interrupting all paths through $E$ and only cancel the remaining influence on $A$ to $R$ .",
        "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1706.02744/1706.02744/hybrid_auto/images/1706.02744_page0_fig4.jpg",
        "context_before": "For the example in Figure 3,\n\nFigure 3 is an example thereof.\n\nIn the scenario of Figure 3, the direct effect of $P$ on $X$ along the arrow $P  X$ in the left graph cannot be estimated by $\\mathbb { E } [ X | P ]$ , because of the common confounder $A$ .",
        "context_after": "We will refer to the terminal ancestors of a node $V$ in a causal graph $\\mathcal { D }$ , denoted by $t a ^ { \\mathcal { D } } ( V )$ , which are those ancestors of $V$ that are also root nodes of $\\mathcal { D }$ . Moreover, in the procedure we clarify the notion of expressibility, which is an assumption about the relation of the given structural equations and the hypothesis class we choose for $R _ { \\theta }$ .\n\nProposition 2. If there is a choice of parameters $\\theta _ { 0 }$ such that $R "
      },
      "edge_contexts": [
        {
          "source": "1706.02744_formula_2",
          "target": "1706.02744_figure_4",
          "ref_text": "Figure 4",
          "context_snippet": "We proceed analogously to the previous subsection using the example graph in Figure 4. Instead of the proxy, we consider a resolving variable $E$ . The causal depend"
        }
      ]
    },
    {
      "pair_id": "1706.02744_pair_3",
      "doc_id": "1706.02744",
      "element_a_id": "1706.02744_formula_2",
      "element_b_id": "1706.02744_figure_3",
      "element_a_type": "formula",
      "element_b_type": "figure",
      "pair_type": "figure+formula",
      "hop_distance": 1,
      "path": [
        "1706.02744_formula_2",
        "1706.02744_figure_3"
      ],
      "quality_score": 0.825,
      "overlap_with_existing_l1": false,
      "element_a": {
        "element_id": "1706.02744_formula_2",
        "element_type": "formula",
        "caption": "",
        "content": "$$R _ {\\theta} = - \\lambda_ {X} \\beta P + \\lambda_ {X} X = \\lambda_ {X} (X - \\beta P),$$",
        "image_path": null,
        "context_before": "We proceed analogously to the previous subsection using the example graph in Figure 4. Instead of the proxy, we consider a resolving variable $E$ . The causal dependences are equivalent to the ones in Figure 3 and we again assume linear structural equations\n\n1. Intervene on $E$ by fixing it to a random variable $\\eta$ with $\\mathbb { P } ( \\eta ) = \\mathbb { P } ( E )$ , the marginal distribution of $E$ in $\\tilde { \\mathcal { G } }$ , see Figure 4. In the example we find\n\nLet us now try to adju",
        "context_after": ""
      },
      "element_b": {
        "element_id": "1706.02744_figure_3",
        "element_type": "figure",
        "caption": "Figure 3: A template graph $\\tilde { \\mathcal { G } }$ for proxy discrimination (left) with its intervened version $\\mathcal { G }$ (right). While from the benevolent viewpoint we do not generically prohibit any influence from $A$ on $R$ , we want to guarantee that the proxy $P$ has no overall influence on the prediction, by adjusting $P R$ to cancel the influence along $P $ $X R$ in the intervened graph.",
        "content": "Figure 3: A template graph $\\tilde { \\mathcal { G } }$ for proxy discrimination (left) with its intervened version $\\mathcal { G }$ (right). While from the benevolent viewpoint we do not generically prohibit any influence from $A$ on $R$ , we want to guarantee that the proxy $P$ has no overall influence on the prediction, by adjusting $P R$ to cancel the influence along $P $ $X R$ in the intervened graph.",
        "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1706.02744/1706.02744/hybrid_auto/images/1706.02744_page0_fig3.jpg",
        "context_before": "Proof. Let us consider the two graphs in Figure 2. First, we show that these graphs can generate the same joint distribution $\\mathbb { P } ( A , Y , X _ { 1 } , X _ { 2 } , R ^ { * } )$ for the Bayes optimal unconstrained predictor $R ^ { * }$ .\n\nLet us consider the two graphs in Figure 2.\n\nWe now work out a formal procedure to solve this task under specific assumptions and simultaneously illustrate it in a fully linear example, i.e. the structural equations are given by\n\n$$ P = \\alpha_ {P} A +",
        "context_after": "While presenting the general procedure, we illustrate each step in the example shown in Figure 3. A protected attribute $A$ affects a proxy $P$ as well as a feature $X$ . Both $P$ and $X$ have additional unobserved causes $N _ { P }$ and $N _ { X }$ , where $N _ { P } , N _ { X } , A$ are pairwise independent. Finally, the proxy also has an effect on the features $X$ and the predictor $R$ is a function of $P$ and $X$ . Given labeled training data, our task is to find a good predictor that exhibi"
      },
      "edge_contexts": [
        {
          "source": "1706.02744_formula_2",
          "target": "1706.02744_figure_3",
          "ref_text": "Figure 3 a",
          "context_snippet": "a resolving variable $E$ . The causal dependences are equivalent to the ones in Figure 3 and we again assume linear structural equations\n\n1. Intervene on $E$ by fixing it"
        }
      ]
    },
    {
      "pair_id": "1706.02744_pair_5",
      "doc_id": "1706.02744",
      "element_a_id": "1706.02744_formula_3",
      "element_b_id": "1706.02744_figure_3",
      "element_a_type": "formula",
      "element_b_type": "figure",
      "pair_type": "figure+formula",
      "hop_distance": 1,
      "path": [
        "1706.02744_formula_3",
        "1706.02744_figure_3"
      ],
      "quality_score": 0.825,
      "overlap_with_existing_l1": false,
      "element_a": {
        "element_id": "1706.02744_formula_3",
        "element_type": "formula",
        "caption": "",
        "content": "$$E = \\alpha_ {E} A + N _ {E}, \\qquad X = \\alpha_ {X} A + \\beta E + N _ {X}, \\qquad R _ {\\theta} = \\lambda_ {E} E + \\lambda_ {X} X.$$",
        "image_path": null,
        "context_before": "",
        "context_after": "Motivated by the algorithm to avoid proxy discrimination, we discuss some natural variants of the notion in this section that connect our interventional approach to individual fairness and other proposed criteria. We consider a generic graph structure as shown on the left in Figure 5. The proxy $P$ and the features $X$ could be multidimensional. The empty circle in the middle represents any number of variables forming a DAG that respects the drawn arrows. Figure 3 is an example thereof. All dash"
      },
      "element_b": {
        "element_id": "1706.02744_figure_3",
        "element_type": "figure",
        "caption": "Figure 3: A template graph $\\tilde { \\mathcal { G } }$ for proxy discrimination (left) with its intervened version $\\mathcal { G }$ (right). While from the benevolent viewpoint we do not generically prohibit any influence from $A$ on $R$ , we want to guarantee that the proxy $P$ has no overall influence on the prediction, by adjusting $P R$ to cancel the influence along $P $ $X R$ in the intervened graph.",
        "content": "Figure 3: A template graph $\\tilde { \\mathcal { G } }$ for proxy discrimination (left) with its intervened version $\\mathcal { G }$ (right). While from the benevolent viewpoint we do not generically prohibit any influence from $A$ on $R$ , we want to guarantee that the proxy $P$ has no overall influence on the prediction, by adjusting $P R$ to cancel the influence along $P $ $X R$ in the intervened graph.",
        "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1706.02744/1706.02744/hybrid_auto/images/1706.02744_page0_fig3.jpg",
        "context_before": "Proof. Let us consider the two graphs in Figure 2. First, we show that these graphs can generate the same joint distribution $\\mathbb { P } ( A , Y , X _ { 1 } , X _ { 2 } , R ^ { * } )$ for the Bayes optimal unconstrained predictor $R ^ { * }$ .\n\nLet us consider the two graphs in Figure 2.\n\nWe now work out a formal procedure to solve this task under specific assumptions and simultaneously illustrate it in a fully linear example, i.e. the structural equations are given by\n\n$$ P = \\alpha_ {P} A +",
        "context_after": "While presenting the general procedure, we illustrate each step in the example shown in Figure 3. A protected attribute $A$ affects a proxy $P$ as well as a feature $X$ . Both $P$ and $X$ have additional unobserved causes $N _ { P }$ and $N _ { X }$ , where $N _ { P } , N _ { X } , A$ are pairwise independent. Finally, the proxy also has an effect on the features $X$ and the predictor $R$ is a function of $P$ and $X$ . Given labeled training data, our task is to find a good predictor that exhibi"
      },
      "edge_contexts": [
        {
          "source": "1706.02744_formula_3",
          "target": "1706.02744_figure_3",
          "ref_text": "Figure 3 i",
          "context_snippet": "epresents any number of variables forming a DAG that respects the drawn arrows. Figure 3 is an example thereof. All dash\n\nFor an analysis of proxy discrimination, we need"
        }
      ]
    },
    {
      "pair_id": "1707.09457_pair_2",
      "doc_id": "1707.09457",
      "element_a_id": "1707.09457_formula_1",
      "element_b_id": "1707.09457_figure_1",
      "element_a_type": "formula",
      "element_b_type": "figure",
      "pair_type": "figure+formula",
      "hop_distance": 1,
      "path": [
        "1707.09457_formula_1",
        "1707.09457_figure_1"
      ],
      "quality_score": 0.825,
      "overlap_with_existing_l1": false,
      "element_a": {
        "element_id": "1707.09457_formula_1",
        "element_type": "formula",
        "caption": "",
        "content": "$$\\begin{array}{l} L (\\lambda , \\{y ^ {i} \\}) = \\\\ \\sum_ {i} f _ {\\theta} \\left(y ^ {i}\\right) - \\sum_ {j = 1} ^ {l} \\lambda_ {j} \\left(A _ {j} \\sum_ {i} y ^ {i} - b _ {j}\\right), \\tag {4} \\\\ \\end{array}$$",
        "image_path": null,
        "context_before": "",
        "context_after": "tics from images and require large quantities of labeled data, predominantly retrieved from the web. Methods often combine structured prediction and deep learning to model correlations between labels and images to make judgments that otherwise would have weak visual support. For example, in the first image of Figure 1, it is possible to predict a spatula by considering that it is a common tool used for the activity cooking. Yet such methods run the risk of discovering and exploiting societal bia"
      },
      "element_b": {
        "element_id": "1707.09457_figure_1",
        "element_type": "figure",
        "caption": "Figure 1: Five example images from the imSitu visual semantic role labeling (vSRL) dataset. Each image is paired with a table describing a situation: the verb, cooking, its semantic roles, i.e agent, and noun values filling that role, i.e. woman. In the imSitu training set, $33 \\%$ of cooking images have man in the agent role while the rest have woman. After training a Conditional Random Field (CRF), bias is amplified: man fills $16 \\%$ of agent roles in cooking images. To reduce this bias amplification our calibration method adjusts weights of CRF potentials associated with biased predictions. After applying our methods, man appears in the agent role of $20 \\%$ of cooking images, reducing the bias amplification by $2 5 \\%$ , while keeping the CRF vSRL performance unchanged.",
        "content": "Figure 1: Five example images from the imSitu visual semantic role labeling (vSRL) dataset. Each image is paired with a table describing a situation: the verb, cooking, its semantic roles, i.e agent, and noun values filling that role, i.e. woman. In the imSitu training set, $33 \\%$ of cooking images have man in the agent role while the rest have woman. After training a Conditional Random Field (CRF), bias is amplified: man fills $16 \\%$ of agent roles in cooking images. To reduce this bias amplification our calibration method adjusts weights of CRF potentials associated with biased predictions. After applying our methods, man appears in the agent role of $20 \\%$ of cooking images, reducing the bias amplification by $2 5 \\%$ , while keeping the CRF vSRL performance unchanged.",
        "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1707.09457/1707.09457/hybrid_auto/images/1707.09457_page0_fig0.jpg",
        "context_before": "Our analysis reveals that over $45 \\%$ and $37 \\%$ of verbs and objects, respectively, exhibit bias toward a gender greater than 2:1. For example, as seen in Figure 1, the cooking activity in imSitu is a heavily biased verb. Furthermore, we show that after training state-of-the-art structured predictors, models amplify the existing bias, by $5 . 0 \\%$ for vSRL, and $3 . 6 \\%$ in MLC.\n\narXiv:1707.09457v1 [cs.AI] 29 Jul 2017\n\n1To simplify our analysis, we only consider a gender binary as perceived",
        "context_after": "To mitigate the role of bias amplification when training models on biased corpora, we propose a novel constrained inference framework, called RBA, for Reducing Bias Amplification in predictions. Our method introduces corpus-level constraints so that gender indicators co-occur no more often together with elements of the prediction task than in the original training distribution. For example, as seen in Figure 1, we would like noun man to occur in the agent role of the cooking as often as it occur"
      },
      "edge_contexts": [
        {
          "source": "1707.09457_formula_1",
          "target": "1707.09457_figure_1",
          "ref_text": "Figure 1",
          "context_snippet": "at otherwise would have weak visual support. For example, in the first image of Figure 1, it is possible to predict a spatula by considering that it is a common tool us"
        }
      ]
    },
    {
      "pair_id": "1803.04383_pair_1",
      "doc_id": "1803.04383",
      "element_a_id": "1803.04383_formula_3",
      "element_b_id": "1803.04383_figure_3",
      "element_a_type": "formula",
      "element_b_type": "figure",
      "pair_type": "figure+formula",
      "hop_distance": 1,
      "path": [
        "1803.04383_formula_3",
        "1803.04383_figure_3"
      ],
      "quality_score": 0.825,
      "overlap_with_existing_l1": false,
      "element_a": {
        "element_id": "1803.04383_formula_3",
        "element_type": "formula",
        "caption": "",
        "content": "$$\\max _ {\\boldsymbol {\\tau} = (\\boldsymbol {\\tau} _ {\\mathrm {A}}, \\boldsymbol {\\tau} _ {\\mathrm {B}}) \\in [ 0, 1 ] ^ {2 C}, \\beta \\in [ 0, 1 ]} \\mathcal {U} (\\boldsymbol {\\tau}) \\quad \\mathrm {s . t .} \\quad \\beta = \\langle \\boldsymbol {\\pi} _ {\\mathrm {j}}, \\boldsymbol {\\tau} _ {\\mathrm {j}} \\rangle , \\mathrm {j} \\in \\{\\mathrm {A}, \\mathrm {B} \\}.$$",
        "image_path": null,
        "context_before": "where $\\begin{array} { r } { \\textstyle t _ { \\operatorname* { m a x } } = \\operatorname* { m i n } _ { \\mathrm { j } \\in \\{ \\mathsf { A } , \\mathsf { B } \\} } \\{ \\langle \\pi _ { \\mathrm { j } } , w _ { \\mathrm { j } } \\rangle \\} } \\end{array}$ is the largest possible TPR. The magenta EO curve in Figure 3 illustrates that feasible solutions to this optimization problem lie on a curve parametrized by $t$ . Note that the objective function decouples for $\\mathsf { j } \\in \\{ \\mathsf { A } , \\maths",
        "context_after": ""
      },
      "element_b": {
        "element_id": "1803.04383_figure_3",
        "element_type": "figure",
        "caption": "Utility Contour Plot Figure 3: Considering the utility as a function of selection rates, fairness constraints correspond to restricting the optimization to one-dimensional curves. The DemParity (DP) constraint is a straight line with slope 1, while the EqOpt (EO) constraint is a curve given by the graph of $G ^ { ( \\mathsf { A } \\to \\mathsf { B } ) }$ . The derivatives considered throughout Section 6 are taken with respect to the selection rate $\\beta _ { \\mathsf { A } }$ (horizontal axis); projecting the EO and DP constraint curves to the horizontal axis recovers concave utility curves such as those shown in the lower panel of Figure 2 (where MaxUtil in is represented by a horizontal line through the MU optimal solution).",
        "content": "Utility Contour Plot Figure 3: Considering the utility as a function of selection rates, fairness constraints correspond to restricting the optimization to one-dimensional curves. The DemParity (DP) constraint is a straight line with slope 1, while the EqOpt (EO) constraint is a curve given by the graph of $G ^ { ( \\mathsf { A } \\to \\mathsf { B } ) }$ . The derivatives considered throughout Section 6 are taken with respect to the selection rate $\\beta _ { \\mathsf { A } }$ (horizontal axis); projecting the EO and DP constraint curves to the horizontal axis recovers concave utility curves such as those shown in the lower panel of Figure 2 (where MaxUtil in is represented by a horizontal line through the MU optimal solution).",
        "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1803.04383/1803.04383/hybrid_auto/images/1803.04383_page0_fig5.jpg",
        "context_before": "5.1 Quantiles and Concavity of the Outcome Curve\n\nTo further our analysis, we now introduce left and right quantile functions, allowing us to specify thresholds in terms of both selection rate and score cutoffs.\n\nDefinition 5.2 (Upper quantile function). Define Q to be the upper quantile function corresponding to $\\pi$ , i.e.\n\nObserve that $\\Delta \\mu ( r _ { \\pi } ^ { - 1 } ( \\beta ) ) = \\langle \\Delta , \\pi \\circ r _ { \\pi } ^ { - 1 } ( \\beta ) \\rangle$ . By Lemma 5.3, $\\pi \\circ r _ { \\pi } ^",
        "context_after": "6 Proofs of Main Theorems\n\nWe are now ready to present and prove theorems that characterize the selection rates under fairness constraints, namely DemParity and EqOpt. These characterizations are crucial for proving the results in Section 3. Our computations also generalize readily to other linear constraints, in a way that will become clear in Section 6.2.\n\n6.1 A Characterization Theorem for DemParity\n\nLet us introduce the auxiliary variable $\\beta : = \\langle \\pi _ { \\mathsf { A } } , \\tau _ {"
      },
      "edge_contexts": [
        {
          "source": "1803.04383_formula_3",
          "target": "1803.04383_figure_3",
          "ref_text": "Figure 3 i",
          "context_snippet": " \\rangle \\} } \\end{array}$ is the largest possible TPR. The magenta EO curve in Figure 3 illustrates that feasible solutions to this optimization problem lie on a curve p"
        }
      ]
    },
    {
      "pair_id": "1808.08166_pair_2",
      "doc_id": "1808.08166",
      "element_a_id": "1808.08166_formula_1",
      "element_b_id": "1808.08166_figure_1",
      "element_a_type": "formula",
      "element_b_type": "figure",
      "pair_type": "figure+formula",
      "hop_distance": 1,
      "path": [
        "1808.08166_formula_1",
        "1808.08166_figure_1"
      ],
      "quality_score": 0.825,
      "overlap_with_existing_l1": false,
      "element_a": {
        "element_id": "1808.08166_formula_1",
        "element_type": "formula",
        "caption": "",
        "content": "$$\\hat {h} \\in \\underset {h \\in \\mathcal {H}} {\\operatorname {a r g m i n}} \\sum_ {i = 1} ^ {n} \\left[ h \\left(X _ {i}\\right) c _ {i} ^ {1} + \\left(1 - h \\left(X _ {i}\\right)\\right) c _ {i} ^ {0} \\right] \\tag {1}$$",
        "image_path": null,
        "context_before": "The properties of these datasets are summarized in Table 1, including the number of instances, the prediction being made, the overall number of features (which varies from 10 to 128), the number of protected features in the subgroup class (which varies from 3 to 18), the nature of the protected features, and the baseline (majority class) error rate.\n\nThey illustrate the idea with the following toy example shown in Figure 1, described as follows.\n\nThe heuristic finds a linear threshold function a",
        "context_after": ""
      },
      "element_b": {
        "element_id": "1808.08166_figure_1",
        "element_type": "figure",
        "caption": "Figure 1: Fairness Gerrymandering: A Toy Example [Kearns et al., 2018]",
        "content": "Figure 1: Fairness Gerrymandering: A Toy Example [Kearns et al., 2018]",
        "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1808.08166/1808.08166/hybrid_auto/images/1808.08166_page0_fig0.jpg",
        "context_before": "The most common definitions of fairness in machine learning are statistical in nature. They proceed by fixing a small number of “protected subgroups” (such as racial or gender groups), and then ask that some statistic of interest be approximately equalized across groups. Standard choices for these statistics include positive classification rates [Calders and Verwer, 2010], false positive or false negative rates [Hardt et al., 2016, Kleinberg et al., 2017, Chouldechova, 2017] and positive predict",
        "context_after": "Suppose individuals each have two sensitive attributes: race (say blue and green) and gender (say male and female). Suppose that these two attributes are distributed independently and uniformly at random, and are uncorrelated with a binary label that is also distributed uniformly at random. If we view gender and race as defining classes of people that we wish to protect, we could take a standard statistical fairness definition from the literature — say the equal odds condition of Hardt et al. [2"
      },
      "edge_contexts": [
        {
          "source": "1808.08166_formula_1",
          "target": "1808.08166_figure_1",
          "ref_text": "Figure 1",
          "context_snippet": ") error rate.\n\nThey illustrate the idea with the following toy example shown in Figure 1, described as follows.\n\nThe heuristic finds a linear threshold function as foll"
        }
      ]
    },
    {
      "pair_id": "1809.02244_pair_6",
      "doc_id": "1809.02244",
      "element_a_id": "1809.02244_formula_2",
      "element_b_id": "1809.02244_figure_4",
      "element_a_type": "formula",
      "element_b_type": "figure",
      "pair_type": "figure+formula",
      "hop_distance": 1,
      "path": [
        "1809.02244_formula_2",
        "1809.02244_figure_4"
      ],
      "quality_score": 0.825,
      "overlap_with_existing_l1": false,
      "element_a": {
        "element_id": "1809.02244_formula_2",
        "element_type": "formula",
        "caption": "",
        "content": "$$\\begin{array}{l} \\widehat {g} ^ {s y} (Z) = \\tag {2} \\\\ \\frac {1}{N} \\sum_ {n = 1} ^ {N} \\left\\{\\frac {\\mathbb {I} (S _ {n} = s)}{p (S _ {n} | X _ {n})} \\frac {p (M _ {n} | s ^ {\\prime} , X _ {n})}{p (M _ {n} | s , X _ {n})} - \\frac {\\mathbb {I} (S _ {n} = s ^ {\\prime})}{p (S _ {n} | X _ {n})} \\right\\} Y _ {n} \\\\ \\end{array}$$",
        "image_path": null,
        "context_before": "",
        "context_after": "In Fig. 4, we show the relative utility achieved by the optimal fair and unconstrained policies, as well as the utility of the observed decision pattern, as a function of $\\theta$ . As expected, choosing an optimal policy improves on the observed policy, with the unfair (unconstrained) choice being higher utility than the fair (constrained) choice; we sacrifice some optimality to satisfy the fairness constraints. However, the difference depends on the utility parameter and for a range of paramet"
      },
      "element_b": {
        "element_id": "1809.02244_figure_4",
        "element_type": "figure",
        "caption": "Figure 4. The relative utility of policies for the COMPAS data as a function of the utility parameter $\\theta$ .",
        "content": "Figure 4. The relative utility of policies for the COMPAS data as a function of the utility parameter $\\theta$ .",
        "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1809.02244/1809.02244/hybrid_auto/images/1809.02244_page0_fig5.jpg",
        "context_before": "In Fig. 3, we compare the overall incarceration rates recommended by the optimal fair and unconstrained policies on the COMPAS data, as a function of the utility parameter $\\theta$ . For low values of $\\theta$ the incarceration rate is zero, and becomes higher as $\\theta$ increases, but differentially for the fair and unconstrained optimal policies. The difference between the policies depends crucially on the utility function. For some values of the utility parameter, the unfair and fair policie",
        "context_after": "Theorem 1 Assume $S$ is binary. Under the causal model above, the following are consistent estimators of $PSE^{sy}$ and $PSE^{sak_k}$ , assuming all models are correctly specified:\n\n$$ \\begin{array}{l} \\widehat {g} ^ {s y} (Z) = \\tag {2} \\\\ \\frac {1}{N} \\sum_ {n = 1} ^ {N} \\left\\{\\frac {\\mathbb {I} (S _ {n} = s)}{p (S _ {n} | X _ {n})} \\frac {p (M _ {n} | s ^ {\\prime} , X _ {n})}{p (M _ {n} | s , X _ {n})} - \\frac {\\mathbb {I} (S _ {n} = s ^ {\\prime})}{p (S _ {n} | X _ {n})} \\right\\} Y _ {n} \\\\ "
      },
      "edge_contexts": [
        {
          "source": "1809.02244_formula_2",
          "target": "1809.02244_figure_4",
          "ref_text": "Fig. 4",
          "context_snippet": "In Fig. 4, we show the relative utility achieved by the optimal fair and unconstrained po"
        }
      ]
    },
    {
      "pair_id": "1809.02244_pair_7",
      "doc_id": "1809.02244",
      "element_a_id": "1809.02244_formula_3",
      "element_b_id": "1809.02244_figure_4",
      "element_a_type": "formula",
      "element_b_type": "figure",
      "pair_type": "figure+formula",
      "hop_distance": 1,
      "path": [
        "1809.02244_formula_3",
        "1809.02244_figure_4"
      ],
      "quality_score": 0.825,
      "overlap_with_existing_l1": false,
      "element_a": {
        "element_id": "1809.02244_formula_3",
        "element_type": "formula",
        "caption": "",
        "content": "$$\\widehat {g} ^ {s a _ {k}} (Z) = \\tag {3}$$",
        "image_path": null,
        "context_before": "",
        "context_after": "In Fig. 4, we show the relative utility achieved by the optimal fair and unconstrained policies, as well as the utility of the observed decision pattern, as a function of $\\theta$ . As expected, choosing an optimal policy improves on the observed policy, with the unfair (unconstrained) choice being higher utility than the fair (constrained) choice; we sacrifice some optimality to satisfy the fairness constraints. However, the difference depends on the utility parameter and for a range of paramet"
      },
      "element_b": {
        "element_id": "1809.02244_figure_4",
        "element_type": "figure",
        "caption": "Figure 4. The relative utility of policies for the COMPAS data as a function of the utility parameter $\\theta$ .",
        "content": "Figure 4. The relative utility of policies for the COMPAS data as a function of the utility parameter $\\theta$ .",
        "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1809.02244/1809.02244/hybrid_auto/images/1809.02244_page0_fig5.jpg",
        "context_before": "In Fig. 3, we compare the overall incarceration rates recommended by the optimal fair and unconstrained policies on the COMPAS data, as a function of the utility parameter $\\theta$ . For low values of $\\theta$ the incarceration rate is zero, and becomes higher as $\\theta$ increases, but differentially for the fair and unconstrained optimal policies. The difference between the policies depends crucially on the utility function. For some values of the utility parameter, the unfair and fair policie",
        "context_after": "Theorem 1 Assume $S$ is binary. Under the causal model above, the following are consistent estimators of $PSE^{sy}$ and $PSE^{sak_k}$ , assuming all models are correctly specified:\n\n$$ \\begin{array}{l} \\widehat {g} ^ {s y} (Z) = \\tag {2} \\\\ \\frac {1}{N} \\sum_ {n = 1} ^ {N} \\left\\{\\frac {\\mathbb {I} (S _ {n} = s)}{p (S _ {n} | X _ {n})} \\frac {p (M _ {n} | s ^ {\\prime} , X _ {n})}{p (M _ {n} | s , X _ {n})} - \\frac {\\mathbb {I} (S _ {n} = s ^ {\\prime})}{p (S _ {n} | X _ {n})} \\right\\} Y _ {n} \\\\ "
      },
      "edge_contexts": [
        {
          "source": "1809.02244_formula_3",
          "target": "1809.02244_figure_4",
          "ref_text": "Fig. 4",
          "context_snippet": "In Fig. 4, we show the relative utility achieved by the optimal fair and unconstrained po"
        }
      ]
    },
    {
      "pair_id": "1809.04737_pair_1",
      "doc_id": "1809.04737",
      "element_a_id": "1809.04737_formula_2",
      "element_b_id": "1809.04737_figure_1",
      "element_a_type": "formula",
      "element_b_type": "figure",
      "pair_type": "figure+formula",
      "hop_distance": 1,
      "path": [
        "1809.04737_formula_2",
        "1809.04737_figure_1"
      ],
      "quality_score": 0.825,
      "overlap_with_existing_l1": false,
      "element_a": {
        "element_id": "1809.04737_formula_2",
        "element_type": "formula",
        "caption": "",
        "content": "$$\\begin{array}{l} \\mathbb {R D} (f) = \\mathbb {E} _ {\\mathbf {X}} \\left[ \\frac {P (S = s ^ {+} | \\mathbf {x})}{P (S = s ^ {+})} \\mathbb {1} _ {h (\\mathbf {x}) > 0} \\right. \\tag {3} \\\\ + \\frac {P (S = s ^ {-} | \\mathbf {x})}{P (S = s ^ {-})} \\mathbb {1} _ {h (\\mathbf {x}) < 0} - 1 ]. \\\\ \\end{array}$$",
        "image_path": null,
        "context_before": "",
        "context_after": "For simplicity, we may want to denote $P ( S = s ^ { + } | \\mathbf { x } )$ by $\\eta ( \\mathbf { x } )$ and $P ( \\bar { S } = s ^ { + } )$ by $p$ . Similarly, the indicator function in above equation can be replaced with the surrogate function. The issue here is, two constraints $\\mathbb { R D } ( f ) \\leq \\tau$ and $- \\mathbb { R } \\mathbb { D } ( f ) \\leq \\tau$ are opposite to each other. Thus, replacing all indicator functions with a single surrogate function will result in a convex-concave p"
      },
      "element_b": {
        "element_id": "1809.04737_figure_1",
        "element_type": "figure",
        "caption": "Figure 1: Examples of $\\kappa ( \\cdot )$ and $\\delta ( \\cdot )$ .",
        "content": "Figure 1: Examples of $\\kappa ( \\cdot )$ and $\\delta ( \\cdot )$ .",
        "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1809.04737/1809.04737/hybrid_auto/images/1809.04737_page0_fig0.jpg",
        "context_before": "subject to $\\mathbb { R D } ( f ) \\leq \\tau , \\quad - \\mathbb { R D } ( f ) \\leq \\tau .$\n\nObviously, the above optimization problem is non-convex. Similar to the loss function, we adopt surrogate functions to convert the risk difference to convex constraints. By using predictive function $h$ and the indicator function, we can rewrite the risk difference as\n\n$$ \\begin{array}{l} \\mathbb {R} \\mathbb {D} (f) = \\mathbb {E} _ {\\mathbf {X} | S = s ^ {+}} \\left[ \\mathbb {1} \\left[ \\operatorname {s i g n",
        "context_after": "$$ \\begin{array}{l} \\mathbb {R D} (f) = \\mathbb {E} _ {\\mathbf {X}} \\left[ \\frac {P (S = s ^ {+} | \\mathbf {x})}{P (S = s ^ {+})} \\mathbb {1} _ {h (\\mathbf {x}) > 0} \\right. \\tag {3} \\\\ + \\frac {P (S = s ^ {-} | \\mathbf {x})}{P (S = s ^ {-})} \\mathbb {1} _ {h (\\mathbf {x}) < 0} - 1 ]. \\\\ \\end{array} $$\n\nFor simplicity, we may want to denote $P ( S = s ^ { + } | \\mathbf { x } )$ by $\\eta ( \\mathbf { x } )$ and $P ( \\bar { S } = s ^ { + } )$ by $p$ . Similarly, the indicator function in above equa"
      },
      "edge_contexts": [
        {
          "source": "1809.04737_formula_2",
          "target": "1809.04737_figure_1",
          "ref_text": "Figure 1",
          "context_snippet": "hown in Tab\n\nExamples of $\\kappa ( \\cdot )$ and $\\delta ( \\cdot )$ are shown in Figure 1."
        }
      ]
    },
    {
      "pair_id": "1809.04737_pair_2",
      "doc_id": "1809.04737",
      "element_a_id": "1809.04737_formula_3",
      "element_b_id": "1809.04737_figure_1",
      "element_a_type": "formula",
      "element_b_type": "figure",
      "pair_type": "figure+formula",
      "hop_distance": 1,
      "path": [
        "1809.04737_formula_3",
        "1809.04737_figure_1"
      ],
      "quality_score": 0.825,
      "overlap_with_existing_l1": false,
      "element_a": {
        "element_id": "1809.04737_formula_3",
        "element_type": "formula",
        "caption": "",
        "content": "$$\\begin{array}{l} \\mathbb {R D} (f) = \\mathbb {E} _ {\\mathbf {X}} \\left[ \\frac {P (S = s ^ {+} | \\mathbf {x})}{P (S = s ^ {+})} \\mathbb {1} _ {h (\\mathbf {x}) > 0} \\right. \\tag {3} \\\\ + \\frac {P (S = s ^ {-} | \\mathbf {x})}{P (S = s ^ {-})} \\mathbb {1} _ {h (\\mathbf {x}) < 0} - 1 ]. \\\\ \\end{array}$$",
        "image_path": null,
        "context_before": "Adding constraints into the classification models increases the computational complexity and also decreases the predictive accuracy. It is desired not to incorporate any fairness constraint if it is guaranteed that the classifier learned will be fair. This situation is possible. Consider an example of admitting students. The application profile contains two attributes, a sensitive attribute $\\operatorname { S e x }$ and a non-sensitive attribute GPA. The statistics of the dataset is shown in Tab",
        "context_after": ""
      },
      "element_b": {
        "element_id": "1809.04737_figure_1",
        "element_type": "figure",
        "caption": "Figure 1: Examples of $\\kappa ( \\cdot )$ and $\\delta ( \\cdot )$ .",
        "content": "Figure 1: Examples of $\\kappa ( \\cdot )$ and $\\delta ( \\cdot )$ .",
        "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1809.04737/1809.04737/hybrid_auto/images/1809.04737_page0_fig0.jpg",
        "context_before": "subject to $\\mathbb { R D } ( f ) \\leq \\tau , \\quad - \\mathbb { R D } ( f ) \\leq \\tau .$\n\nObviously, the above optimization problem is non-convex. Similar to the loss function, we adopt surrogate functions to convert the risk difference to convex constraints. By using predictive function $h$ and the indicator function, we can rewrite the risk difference as\n\n$$ \\begin{array}{l} \\mathbb {R} \\mathbb {D} (f) = \\mathbb {E} _ {\\mathbf {X} | S = s ^ {+}} \\left[ \\mathbb {1} \\left[ \\operatorname {s i g n",
        "context_after": "$$ \\begin{array}{l} \\mathbb {R D} (f) = \\mathbb {E} _ {\\mathbf {X}} \\left[ \\frac {P (S = s ^ {+} | \\mathbf {x})}{P (S = s ^ {+})} \\mathbb {1} _ {h (\\mathbf {x}) > 0} \\right. \\tag {3} \\\\ + \\frac {P (S = s ^ {-} | \\mathbf {x})}{P (S = s ^ {-})} \\mathbb {1} _ {h (\\mathbf {x}) < 0} - 1 ]. \\\\ \\end{array} $$\n\nFor simplicity, we may want to denote $P ( S = s ^ { + } | \\mathbf { x } )$ by $\\eta ( \\mathbf { x } )$ and $P ( \\bar { S } = s ^ { + } )$ by $p$ . Similarly, the indicator function in above equa"
      },
      "edge_contexts": [
        {
          "source": "1809.04737_formula_3",
          "target": "1809.04737_figure_1",
          "ref_text": "Figure 1",
          "context_snippet": "hown in Tab\n\nExamples of $\\kappa ( \\cdot )$ and $\\delta ( \\cdot )$ are shown in Figure 1.\n\nThe statistics of the dataset is shown in Table 1."
        }
      ]
    },
    {
      "pair_id": "1809.04737_pair_5",
      "doc_id": "1809.04737",
      "element_a_id": "1809.04737_formula_4",
      "element_b_id": "1809.04737_figure_1",
      "element_a_type": "formula",
      "element_b_type": "figure",
      "pair_type": "figure+formula",
      "hop_distance": 1,
      "path": [
        "1809.04737_formula_4",
        "1809.04737_figure_1"
      ],
      "quality_score": 0.825,
      "overlap_with_existing_l1": false,
      "element_a": {
        "element_id": "1809.04737_formula_4",
        "element_type": "formula",
        "caption": "",
        "content": "$$\\min _ {h \\in \\mathcal {H}} \\quad \\mathbb {L} _ {\\phi} (h)$$",
        "image_path": null,
        "context_before": "",
        "context_after": "For simplicity, we may want to denote $P ( S = s ^ { + } | \\mathbf { x } )$ by $\\eta ( \\mathbf { x } )$ and $P ( \\bar { S } = s ^ { + } )$ by $p$ . Similarly, the indicator function in above equation can be replaced with the surrogate function. The issue here is, two constraints $\\mathbb { R D } ( f ) \\leq \\tau$ and $- \\mathbb { R } \\mathbb { D } ( f ) \\leq \\tau$ are opposite to each other. Thus, replacing all indicator functions with a single surrogate function will result in a convex-concave p"
      },
      "element_b": {
        "element_id": "1809.04737_figure_1",
        "element_type": "figure",
        "caption": "Figure 1: Examples of $\\kappa ( \\cdot )$ and $\\delta ( \\cdot )$ .",
        "content": "Figure 1: Examples of $\\kappa ( \\cdot )$ and $\\delta ( \\cdot )$ .",
        "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1809.04737/1809.04737/hybrid_auto/images/1809.04737_page0_fig0.jpg",
        "context_before": "subject to $\\mathbb { R D } ( f ) \\leq \\tau , \\quad - \\mathbb { R D } ( f ) \\leq \\tau .$\n\nObviously, the above optimization problem is non-convex. Similar to the loss function, we adopt surrogate functions to convert the risk difference to convex constraints. By using predictive function $h$ and the indicator function, we can rewrite the risk difference as\n\n$$ \\begin{array}{l} \\mathbb {R} \\mathbb {D} (f) = \\mathbb {E} _ {\\mathbf {X} | S = s ^ {+}} \\left[ \\mathbb {1} \\left[ \\operatorname {s i g n",
        "context_after": "$$ \\begin{array}{l} \\mathbb {R D} (f) = \\mathbb {E} _ {\\mathbf {X}} \\left[ \\frac {P (S = s ^ {+} | \\mathbf {x})}{P (S = s ^ {+})} \\mathbb {1} _ {h (\\mathbf {x}) > 0} \\right. \\tag {3} \\\\ + \\frac {P (S = s ^ {-} | \\mathbf {x})}{P (S = s ^ {-})} \\mathbb {1} _ {h (\\mathbf {x}) < 0} - 1 ]. \\\\ \\end{array} $$\n\nFor simplicity, we may want to denote $P ( S = s ^ { + } | \\mathbf { x } )$ by $\\eta ( \\mathbf { x } )$ and $P ( \\bar { S } = s ^ { + } )$ by $p$ . Similarly, the indicator function in above equa"
      },
      "edge_contexts": [
        {
          "source": "1809.04737_formula_4",
          "target": "1809.04737_figure_1",
          "ref_text": "Figure 1",
          "context_snippet": "hown in Tab\n\nExamples of $\\kappa ( \\cdot )$ and $\\delta ( \\cdot )$ are shown in Figure 1."
        }
      ]
    },
    {
      "pair_id": "1907.06430_pair_3",
      "doc_id": "1907.06430",
      "element_a_id": "1907.06430_formula_3",
      "element_b_id": "1907.06430_figure_5",
      "element_a_type": "formula",
      "element_b_type": "figure",
      "pair_type": "figure+formula",
      "hop_distance": 1,
      "path": [
        "1907.06430_formula_3",
        "1907.06430_figure_5"
      ],
      "quality_score": 0.825,
      "overlap_with_existing_l1": false,
      "element_a": {
        "element_id": "1907.06430_formula_3",
        "element_type": "formula",
        "caption": "",
        "content": "$$\\int_ {C, M, L} p (Y | A = a, C, M, L) p (L | A = \\bar {a}, C, M) p (M | A = a, C) p (C).$$",
        "image_path": null,
        "context_before": "",
        "context_after": "To estimate the effect along a specific group of causal paths, we can generalize the formulas for the ADE and AIE by replacing the variable in the first term with the one resulting from performing the intervention $A = a$ along the group of interest and $A = { \\bar { a } }$ along the remaining causal paths. For example, consider the CBN of Fig. 5 (top) and assume that we are interested in isolating the effect of $A$ on $Y$ along the direct path $A Y$ and the paths passing through $M$ , $A \\to M\n"
      },
      "element_b": {
        "element_id": "1907.06430_figure_5",
        "element_type": "figure",
        "caption": "Fig. 5. Top: CBN with the direct path from $A$ to $Y$ and the indirect paths passing through $M$ highlighted in red. Bottom: CBN corresponding to Eq. (1).",
        "content": "Fig. 5. Top: CBN with the direct path from $A$ to $Y$ and the indirect paths passing through $M$ highlighted in red. Bottom: CBN corresponding to Eq. (1).",
        "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1907.06430/1907.06430/hybrid_auto/images/1907.06430_page0_fig7.jpg",
        "context_before": "Effect of Treatment on Treated. Consider the conditional distribution $p ( Y _ { a } | A = \\bar { a } )$ . This distribution measures the information travelling from $A$ to $Y$ along all open paths, when $A$ is set to $a$ along causal paths and to $a$ along non-causal paths. The effect of treatment on treated (ETT) of $A = a$ with respect to $A = \\bar { a }$ is defined as $\\mathrm { E T T } _ { \\bar { a } a } = \\langle Y _ { a } \\rangle _ { p ( Y _ { a } | A = \\bar { a } ) } - \\langle Y _ { \\bar",
        "context_after": "$$ \\int_ {C, M, L} p (Y | A = a, C, M, L) p (L | A = \\bar {a}, C, M) p (M | A = a, C) p (C). $$\n\nIn the simple case in which the CBN corresponds to a linear model, e.g.\n\n$$ A \\sim \\operatorname {B e r n} (\\pi), C = \\epsilon_ {c}, $$\n\nTo estimate the effect along a specific group of causal paths, we can generalize the formulas for the ADE and AIE by replacing the variable in the first term with the one resulting from performing the intervention $A = a$ along the group of interest and $A = { \\bar "
      },
      "edge_contexts": [
        {
          "source": "1907.06430_formula_3",
          "target": "1907.06430_figure_5",
          "ref_text": "Fig. 5",
          "context_snippet": "bar { a } }$ along the remaining causal paths. For example, consider the CBN of Fig. 5 (top) and assume that we are interested in isolating the effect of $A$ on $Y$ a"
        }
      ]
    },
    {
      "pair_id": "1907.06430_pair_4",
      "doc_id": "1907.06430",
      "element_a_id": "1907.06430_formula_4",
      "element_b_id": "1907.06430_figure_5",
      "element_a_type": "formula",
      "element_b_type": "figure",
      "pair_type": "figure+formula",
      "hop_distance": 1,
      "path": [
        "1907.06430_formula_4",
        "1907.06430_figure_5"
      ],
      "quality_score": 0.825,
      "overlap_with_existing_l1": false,
      "element_a": {
        "element_id": "1907.06430_formula_4",
        "element_type": "formula",
        "caption": "",
        "content": "$$A \\sim \\operatorname {B e r n} (\\pi), C = \\epsilon_ {c},$$",
        "image_path": null,
        "context_before": "",
        "context_after": "To estimate the effect along a specific group of causal paths, we can generalize the formulas for the ADE and AIE by replacing the variable in the first term with the one resulting from performing the intervention $A = a$ along the group of interest and $A = { \\bar { a } }$ along the remaining causal paths. For example, consider the CBN of Fig. 5 (top) and assume that we are interested in isolating the effect of $A$ on $Y$ along the direct path $A Y$ and the paths passing through $M$ , $A \\to M\n"
      },
      "element_b": {
        "element_id": "1907.06430_figure_5",
        "element_type": "figure",
        "caption": "Fig. 5. Top: CBN with the direct path from $A$ to $Y$ and the indirect paths passing through $M$ highlighted in red. Bottom: CBN corresponding to Eq. (1).",
        "content": "Fig. 5. Top: CBN with the direct path from $A$ to $Y$ and the indirect paths passing through $M$ highlighted in red. Bottom: CBN corresponding to Eq. (1).",
        "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1907.06430/1907.06430/hybrid_auto/images/1907.06430_page0_fig7.jpg",
        "context_before": "Effect of Treatment on Treated. Consider the conditional distribution $p ( Y _ { a } | A = \\bar { a } )$ . This distribution measures the information travelling from $A$ to $Y$ along all open paths, when $A$ is set to $a$ along causal paths and to $a$ along non-causal paths. The effect of treatment on treated (ETT) of $A = a$ with respect to $A = \\bar { a }$ is defined as $\\mathrm { E T T } _ { \\bar { a } a } = \\langle Y _ { a } \\rangle _ { p ( Y _ { a } | A = \\bar { a } ) } - \\langle Y _ { \\bar",
        "context_after": "$$ \\int_ {C, M, L} p (Y | A = a, C, M, L) p (L | A = \\bar {a}, C, M) p (M | A = a, C) p (C). $$\n\nIn the simple case in which the CBN corresponds to a linear model, e.g.\n\n$$ A \\sim \\operatorname {B e r n} (\\pi), C = \\epsilon_ {c}, $$\n\nTo estimate the effect along a specific group of causal paths, we can generalize the formulas for the ADE and AIE by replacing the variable in the first term with the one resulting from performing the intervention $A = a$ along the group of interest and $A = { \\bar "
      },
      "edge_contexts": [
        {
          "source": "1907.06430_formula_4",
          "target": "1907.06430_figure_5",
          "ref_text": "Fig. 5",
          "context_snippet": "bar { a } }$ along the remaining causal paths. For example, consider the CBN of Fig. 5 (top) and assume that we are interested in isolating the effect of $A$ on $Y$ a"
        }
      ]
    },
    {
      "pair_id": "1907.06430_pair_6",
      "doc_id": "1907.06430",
      "element_a_id": "1907.06430_formula_5",
      "element_b_id": "1907.06430_figure_7",
      "element_a_type": "formula",
      "element_b_type": "figure",
      "pair_type": "figure+formula",
      "hop_distance": 1,
      "path": [
        "1907.06430_formula_5",
        "1907.06430_figure_7"
      ],
      "quality_score": 0.825,
      "overlap_with_existing_l1": false,
      "element_a": {
        "element_id": "1907.06430_formula_5",
        "element_type": "formula",
        "caption": "",
        "content": "$$\\left( \\begin{array}{c} \\theta_ {X} \\\\ \\theta_ {A} \\end{array} \\right) = \\left( \\begin{array}{c c} \\alpha^ {2} + \\beta^ {2} & \\alpha \\\\ \\alpha & 1 \\end{array} \\right) ^ {- 1} \\left( \\begin{array}{c} \\gamma \\beta \\\\ 0 \\end{array} \\right) = \\left( \\begin{array}{c} \\gamma / \\beta \\\\ - \\alpha \\gamma / \\beta \\end{array} \\right),$$",
        "image_path": null,
        "context_before": "",
        "context_after": "As an example, assume the CBN in Fig. 6 representing the data-generation mechanism underlying a music degree scenario, where $A$ corresponds to gender, $M$ to music aptitude (unobserved, i.e. $M \\not \\in \\varDelta$ ), $X$ to the score obtained from an ability test taken at the beginning of the degree, and $Y$ to the score obtained from an ability test taken at the end of the degree. Individuals with higher music aptitude $M$ are more likely to obtain higher initial and final scores ( $M X$ , $M\n"
      },
      "element_b": {
        "element_id": "1907.06430_figure_7",
        "element_type": "figure",
        "caption": "Fig. 7. CBN underlying a college admission scenario.",
        "content": "Fig. 7. CBN underlying a college admission scenario.",
        "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1907.06430/1907.06430/hybrid_auto/images/1907.06430_page0_fig10.jpg",
        "context_before": "As an example, assume the CBN in Fig. 6 representing the data-generation mechanism underlying a music degree scenario, where $A$ corresponds to gender, $M$ to music aptitude (unobserved, i.e. $M \\not \\in \\varDelta$ ), $X$ to the score obtained from an ability test taken at the beginning of the degree, and $Y$ to the score obtained from an ability test taken at the end of the degree. Individuals with higher music aptitude $M$ are more likely to obtain higher initial and final scores ( $M X$ , $M ",
        "context_after": "In the more complex case in which the path $A $ $D Y$ is considered fair, unfairness can instead be quantified with the path-specific effect along the direct path $A Y$ , PSE $^ { a a }$ , given by\n\n$$ \\langle Y _ {a} (D _ {\\bar {a}}) \\rangle_ {p (Y _ {a} (D _ {\\bar {a}}))} - \\langle Y _ {\\bar {a}} \\rangle_ {p (Y _ {\\bar {a}})}. $$\n\nNotice that computing $p ( Y _ { a } ( D _ { \\bar { a } } ) )$ requires knowledge of the CBN. If the CBN structure is not known or\n\nConsider the college admission ex"
      },
      "edge_contexts": [
        {
          "source": "1907.06430_formula_5",
          "target": "1907.06430_figure_7",
          "ref_text": "Fig. 7",
          "context_snippet": "ores ( $M X$ , $M\n\nConsider the college admission example discussed in Sect. 2 (Fig. 7). In the case in which the path $A D$ , and therefore $A D Y$ , is considered u"
        }
      ]
    },
    {
      "pair_id": "1907.06430_pair_7",
      "doc_id": "1907.06430",
      "element_a_id": "1907.06430_formula_6",
      "element_b_id": "1907.06430_figure_7",
      "element_a_type": "formula",
      "element_b_type": "figure",
      "pair_type": "figure+formula",
      "hop_distance": 1,
      "path": [
        "1907.06430_formula_6",
        "1907.06430_figure_7"
      ],
      "quality_score": 0.825,
      "overlap_with_existing_l1": false,
      "element_a": {
        "element_id": "1907.06430_formula_6",
        "element_type": "formula",
        "caption": "",
        "content": "$$\\langle Y _ {a} (D _ {\\bar {a}}) \\rangle_ {p (Y _ {a} (D _ {\\bar {a}}))} - \\langle Y _ {\\bar {a}} \\rangle_ {p (Y _ {\\bar {a}})}.$$",
        "image_path": null,
        "context_before": "",
        "context_after": "Consider the college admission example discussed in Sect. 2 (Fig. 7). In the case in which the path $A D$ , and therefore $A D Y$ , is considered unfair, unfairness overall population can be quantified with $\\langle Y \\rangle _ { p ( Y | a ) } - \\langle Y \\rangle _ { p ( Y | \\bar { a } ) }$\n\nIn the college admission example of Fig. 7 in which the path $A D Y$ is considered fair, rather than measuring unfairness overall population, we might want to know e.g. whether a rejected female applicant $\\"
      },
      "element_b": {
        "element_id": "1907.06430_figure_7",
        "element_type": "figure",
        "caption": "Fig. 7. CBN underlying a college admission scenario.",
        "content": "Fig. 7. CBN underlying a college admission scenario.",
        "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1907.06430/1907.06430/hybrid_auto/images/1907.06430_page0_fig10.jpg",
        "context_before": "As an example, assume the CBN in Fig. 6 representing the data-generation mechanism underlying a music degree scenario, where $A$ corresponds to gender, $M$ to music aptitude (unobserved, i.e. $M \\not \\in \\varDelta$ ), $X$ to the score obtained from an ability test taken at the beginning of the degree, and $Y$ to the score obtained from an ability test taken at the end of the degree. Individuals with higher music aptitude $M$ are more likely to obtain higher initial and final scores ( $M X$ , $M ",
        "context_after": "In the more complex case in which the path $A $ $D Y$ is considered fair, unfairness can instead be quantified with the path-specific effect along the direct path $A Y$ , PSE $^ { a a }$ , given by\n\n$$ \\langle Y _ {a} (D _ {\\bar {a}}) \\rangle_ {p (Y _ {a} (D _ {\\bar {a}}))} - \\langle Y _ {\\bar {a}} \\rangle_ {p (Y _ {\\bar {a}})}. $$\n\nNotice that computing $p ( Y _ { a } ( D _ { \\bar { a } } ) )$ requires knowledge of the CBN. If the CBN structure is not known or\n\nConsider the college admission ex"
      },
      "edge_contexts": [
        {
          "source": "1907.06430_formula_6",
          "target": "1907.06430_figure_7",
          "ref_text": "Fig. 7",
          "context_snippet": "Consider the college admission example discussed in Sect. 2 (Fig. 7). In the case in which the path $A D$ , and therefore $A D Y$ , is considered u"
        }
      ]
    },
    {
      "pair_id": "1907.06430_pair_8",
      "doc_id": "1907.06430",
      "element_a_id": "1907.06430_formula_7",
      "element_b_id": "1907.06430_figure_7",
      "element_a_type": "formula",
      "element_b_type": "figure",
      "pair_type": "figure+formula",
      "hop_distance": 1,
      "path": [
        "1907.06430_formula_7",
        "1907.06430_figure_7"
      ],
      "quality_score": 0.825,
      "overlap_with_existing_l1": false,
      "element_a": {
        "element_id": "1907.06430_formula_7",
        "element_type": "formula",
        "caption": "",
        "content": "$$A \\sim \\operatorname {B e r n} (\\pi), \\quad Q = \\theta^ {q} + \\epsilon_ {q}, \\quad D = \\theta^ {d} + \\theta_ {a} ^ {d} A + \\epsilon_ {d},$$",
        "image_path": null,
        "context_before": "Consider the college admission example discussed in Sect. 2 (Fig. 7). In the case in which the path $A D$ , and therefore $A D Y$ , is considered unfair, unfairness overall population can be quantified with $\\langle Y \\rangle _ { p ( Y | a ) } - \\langle Y \\rangle _ { p ( Y | \\bar { a } ) }$\n\nIn the college admission example of Fig. 7 in which the path $A D Y$ is considered fair, rather than measuring unfairness overall population, we might want to know e.g. whether a rejected female applicant $\\",
        "context_after": ""
      },
      "element_b": {
        "element_id": "1907.06430_figure_7",
        "element_type": "figure",
        "caption": "Fig. 7. CBN underlying a college admission scenario.",
        "content": "Fig. 7. CBN underlying a college admission scenario.",
        "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1907.06430/1907.06430/hybrid_auto/images/1907.06430_page0_fig10.jpg",
        "context_before": "As an example, assume the CBN in Fig. 6 representing the data-generation mechanism underlying a music degree scenario, where $A$ corresponds to gender, $M$ to music aptitude (unobserved, i.e. $M \\not \\in \\varDelta$ ), $X$ to the score obtained from an ability test taken at the beginning of the degree, and $Y$ to the score obtained from an ability test taken at the end of the degree. Individuals with higher music aptitude $M$ are more likely to obtain higher initial and final scores ( $M X$ , $M ",
        "context_after": "In the more complex case in which the path $A $ $D Y$ is considered fair, unfairness can instead be quantified with the path-specific effect along the direct path $A Y$ , PSE $^ { a a }$ , given by\n\n$$ \\langle Y _ {a} (D _ {\\bar {a}}) \\rangle_ {p (Y _ {a} (D _ {\\bar {a}}))} - \\langle Y _ {\\bar {a}} \\rangle_ {p (Y _ {\\bar {a}})}. $$\n\nNotice that computing $p ( Y _ { a } ( D _ { \\bar { a } } ) )$ requires knowledge of the CBN. If the CBN structure is not known or\n\nConsider the college admission ex"
      },
      "edge_contexts": [
        {
          "source": "1907.06430_formula_7",
          "target": "1907.06430_figure_7",
          "ref_text": "Fig. 7",
          "context_snippet": "Consider the college admission example discussed in Sect. 2 (Fig. 7). In the case in which the path $A D$ , and therefore $A D Y$ , is considered u"
        }
      ]
    },
    {
      "pair_id": "1910.10872_pair_2",
      "doc_id": "1910.10872",
      "element_a_id": "1910.10872_formula_1",
      "element_b_id": "1910.10872_figure_1",
      "element_a_type": "formula",
      "element_b_type": "figure",
      "pair_type": "figure+formula",
      "hop_distance": 1,
      "path": [
        "1910.10872_formula_1",
        "1910.10872_figure_1"
      ],
      "quality_score": 0.825,
      "overlap_with_existing_l1": false,
      "element_a": {
        "element_id": "1910.10872_formula_1",
        "element_type": "formula",
        "caption": "",
        "content": "$$\\frac {\\sum_ {n \\in N _ {f}} f r e q _ {f} (n _ {t y p e} = \\emptyset)}{\\sum_ {n \\in N _ {f}} f r e q _ {f} (n)}$$",
        "image_path": null,
        "context_before": "Our benchmark dataset consists of nine templates listed in Table 1 which are templated sentences that start with the existing names in the census data followed by a sentence that represents a human-like activity. The aim was to test the performance of the NER from different perspectives. Template 1, containing just the name, purely tests the name itself and reveals something about the distribution of the training data. Template 4 is designed to direct the model to tag the name as a person. Templ",
        "context_after": ""
      },
      "element_b": {
        "element_id": "1910.10872_figure_1",
        "element_type": "figure",
        "caption": "Figure 1: Examples of PERSON entities that are wrongfully tagged as non-PERSON or NULL entities by CoreNLP.",
        "content": "Figure 1: Examples of PERSON entities that are wrongfully tagged as non-PERSON or NULL entities by CoreNLP.",
        "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1910.10872/1910.10872/hybrid_auto/images/1910.10872_page0_fig0.jpg",
        "context_before": "Machine learning and AI systems are becoming omnipresent in everyday lives. Recently, attention has been directed to problems concerning fairness and algorithmic bias. Some progress has been made on the analysis of gender stereotyping in different natural language processing (NLP) components, such as word embedding (Bolukbasi et al. 2016; Zhao et al. 2018b), co-reference resolution (Zhao et al. 2018a), machine translation (Font and Costa-jussa 2019) ` and sentence encoders (May et al. 2019). In ",
        "context_after": "instances of such cases throughout history in the real world, and that there are more female names than male names that are incorrectly tagged. Moreover, based on this same U.S. census data, we find that this miscategorization affects more women than men. This serves as our definition of bias which considers the differences between gender groups following the statistical parity notion of fairness (Kusner et al. 2017; Dwork et al. 2012).\n\nThe contributions of this paper are fourfold:\n\narXiv:1910."
      },
      "edge_contexts": [
        {
          "source": "1910.10872_formula_1",
          "target": "1910.10872_figure_1",
          "ref_text": "Figure 1 h",
          "context_snippet": " Template 4 is designed to direct the model to tag the name as a person. Templ\n\nFigure 1 has more examples with names that are either not recognized as an entity or wrong"
        }
      ]
    },
    {
      "pair_id": "1610.07524_pair_1",
      "doc_id": "1610.07524",
      "element_a_id": "1610.07524_formula_1",
      "element_b_id": "1610.07524_figure_1",
      "element_a_type": "formula",
      "element_b_type": "figure",
      "pair_type": "figure+formula",
      "hop_distance": 1,
      "path": [
        "1610.07524_formula_1",
        "1610.07524_figure_1"
      ],
      "quality_score": 0.8200000000000001,
      "overlap_with_existing_l1": false,
      "element_a": {
        "element_id": "1610.07524_formula_1",
        "element_type": "formula",
        "caption": "",
        "content": "$$S _ {c} (x) \\equiv \\left\\{ \\begin{array}{l l} \\mathrm {H R} & \\text {i f} S (x) > s _ {H R} \\\\ \\mathrm {L R} & \\text {i f} S (x) \\leq s _ {H R} \\end{array} \\right. \\tag {2.2}$$",
        "image_path": null,
        "context_before": "",
        "context_after": "Figure 1 shows a plot of the observed recidivism rates across all possible values of the COMPAS score. We can see that the COMPAS RPI appears to adhere well to the test fairness condition. In their response to the ProPublica investigation, Flores et al. [10] further verify this adherence using logistic regression.\n\nby thresholding $S$ at some cutoff $s H R$\n\n$$ S _ {c} (x) \\equiv \\left\\{ \\begin{array}{l l} \\mathrm {H R} & \\text {i f} S (x) > s _ {H R} \\\\ \\mathrm {L R} & \\text {i f} S (x) \\leq s "
      },
      "element_b": {
        "element_id": "1610.07524_figure_1",
        "element_type": "figure",
        "caption": "Figure 1: Plot shows $\\mathbb { P } ( Y = 1 \\mid S = s , R )$ for the COM-PAS decile score, with $R \\in \\{ \\mathrm { B l a c k } , \\mathrm { W h i t e } \\}$ . Error bars represent 95% confidence intervals.",
        "content": "Figure 1: Plot shows $\\mathbb { P } ( Y = 1 \\mid S = s , R )$ for the COM-PAS decile score, with $R \\in \\{ \\mathrm { B l a c k } , \\mathrm { W h i t e } \\}$ . Error bars represent 95% confidence intervals.",
        "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1610.07524/1610.07524/hybrid_auto/images/1610.07524_page0_fig0.jpg",
        "context_before": "Figure 1 shows a plot of the observed recidivism rates across all possible values of the COMPAS score. We can see that the COMPAS RPI appears to adhere well to the test fairness condition. In their response to the ProPublica investigation, Flores et al. [10] further verify this adherence using logistic regression.\n\n2.1 Implied constraints on the false positive and false negative rates\n\nTo facilitate a simpler discussion of error rates, we introduce the coarsened score $S _ { c }$ , which is obta",
        "context_after": "by thresholding $S$ at some cutoff $s H R$\n\n$$ S _ {c} (x) \\equiv \\left\\{ \\begin{array}{l l} \\mathrm {H R} & \\text {i f} S (x) > s _ {H R} \\\\ \\mathrm {L R} & \\text {i f} S (x) \\leq s _ {H R} \\end{array} \\right. \\tag {2.2} $$\n\nThe coarsened score simply assesses each defendant as being at high-risk or low-risk of recidivism. For the purpose of our discussion, we will think of $S _ { c }$ as a classifier used to predict the binary outcome $Y$ . This allows us to summarize $S _ { c }$ in terms of a"
      },
      "edge_contexts": [
        {
          "source": "1610.07524_formula_1",
          "target": "1610.07524_figure_1",
          "ref_text": "Figure 1 s",
          "context_snippet": "Figure 1 shows a plot of the observed recidivism rates across all possible values of the C"
        }
      ]
    },
    {
      "pair_id": "1610.07524_pair_2",
      "doc_id": "1610.07524",
      "element_a_id": "1610.07524_formula_2",
      "element_b_id": "1610.07524_figure_1",
      "element_a_type": "formula",
      "element_b_type": "figure",
      "pair_type": "figure+formula",
      "hop_distance": 1,
      "path": [
        "1610.07524_formula_2",
        "1610.07524_figure_1"
      ],
      "quality_score": 0.8200000000000001,
      "overlap_with_existing_l1": false,
      "element_a": {
        "element_id": "1610.07524_formula_2",
        "element_type": "formula",
        "caption": "",
        "content": "$$S _ {c} (x) \\equiv \\left\\{ \\begin{array}{l l} \\mathrm {H R} & \\text {i f} S (x) > s _ {H R} \\\\ \\mathrm {L R} & \\text {i f} S (x) \\leq s _ {H R} \\end{array} \\right. \\tag {2.2}$$",
        "image_path": null,
        "context_before": "by thresholding $S$ at some cutoff $s H R$\n\n$$ S _ {c} (x) \\equiv \\left\\{ \\begin{array}{l l} \\mathrm {H R} & \\text {i f} S (x) > s _ {H R} \\\\ \\mathrm {L R} & \\text {i f} S (x) \\leq s _ {H R} \\end{array} \\right. \\tag {2.2} $$\n\nThe coarsened score simply assesses each defendant as being at high-risk or low-risk of recidivism. For the purpose of our discussion, we will think of $S _ { c }$ as a classifier used to predict the binary outcome $Y$ . This allows us to summarize $S _ { c }$ in terms of a",
        "context_after": ""
      },
      "element_b": {
        "element_id": "1610.07524_figure_1",
        "element_type": "figure",
        "caption": "Figure 1: Plot shows $\\mathbb { P } ( Y = 1 \\mid S = s , R )$ for the COM-PAS decile score, with $R \\in \\{ \\mathrm { B l a c k } , \\mathrm { W h i t e } \\}$ . Error bars represent 95% confidence intervals.",
        "content": "Figure 1: Plot shows $\\mathbb { P } ( Y = 1 \\mid S = s , R )$ for the COM-PAS decile score, with $R \\in \\{ \\mathrm { B l a c k } , \\mathrm { W h i t e } \\}$ . Error bars represent 95% confidence intervals.",
        "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1610.07524/1610.07524/hybrid_auto/images/1610.07524_page0_fig0.jpg",
        "context_before": "Figure 1 shows a plot of the observed recidivism rates across all possible values of the COMPAS score. We can see that the COMPAS RPI appears to adhere well to the test fairness condition. In their response to the ProPublica investigation, Flores et al. [10] further verify this adherence using logistic regression.\n\n2.1 Implied constraints on the false positive and false negative rates\n\nTo facilitate a simpler discussion of error rates, we introduce the coarsened score $S _ { c }$ , which is obta",
        "context_after": "by thresholding $S$ at some cutoff $s H R$\n\n$$ S _ {c} (x) \\equiv \\left\\{ \\begin{array}{l l} \\mathrm {H R} & \\text {i f} S (x) > s _ {H R} \\\\ \\mathrm {L R} & \\text {i f} S (x) \\leq s _ {H R} \\end{array} \\right. \\tag {2.2} $$\n\nThe coarsened score simply assesses each defendant as being at high-risk or low-risk of recidivism. For the purpose of our discussion, we will think of $S _ { c }$ as a classifier used to predict the binary outcome $Y$ . This allows us to summarize $S _ { c }$ in terms of a"
      },
      "edge_contexts": [
        {
          "source": "1610.07524_formula_2",
          "target": "1610.07524_figure_1",
          "ref_text": "Figure 1 s",
          "context_snippet": "ws us to summarize $S _ { c }$ in terms of a confusion matrix, as shown below.\n\nFigure 1 shows a plot of the observed recidivism rates across all possible values of the C"
        }
      ]
    },
    {
      "pair_id": "1905.03674_pair_2",
      "doc_id": "1905.03674",
      "element_a_id": "1905.03674_formula_2",
      "element_b_id": "1905.03674_figure_2",
      "element_a_type": "formula",
      "element_b_type": "figure",
      "pair_type": "figure+formula",
      "hop_distance": 1,
      "path": [
        "1905.03674_formula_2",
        "1905.03674_figure_2"
      ],
      "quality_score": 0.8200000000000001,
      "overlap_with_existing_l1": false,
      "element_a": {
        "element_id": "1905.03674_formula_2",
        "element_type": "formula",
        "caption": "",
        "content": "$$\\begin{array}{l} \\min \\left(\\frac {d (i , x)}{d (i , y)}, \\frac {d (i ^ {*} , x)}{d (i ^ {*} , y)}\\right) \\\\ \\leq \\min \\left(\\frac {d (i , x)}{d (i , y)}, \\frac {r _ {y} + d (i , x) + d (i , y)}{r _ {y}}\\right) \\\\ \\leq \\min \\left(\\frac {r _ {y}}{d (i , y)}, 2 + \\frac {d (i , y)}{r _ {y}}\\right) \\\\ \\leq \\max _ {z \\geq 0} \\left(\\min \\left(z, 2 + 1 / z\\right)\\right) = 1 + \\sqrt {2} \\\\ \\end{array}$$",
        "image_path": null,
        "context_before": "",
        "context_after": "In the second case, $\\exists x \\in X$ and $\\exists i \\in N$ such that $i \\in B ( x , r _ { y } ) \\cap S$ . This case is drawn below in Figure 2. By the triangle inequality, $d ( x , y ) \\leq d ( i , x ) + d ( i , y )$ . Therefore, $d ( i ^ { * } , x ) \\leq r _ { y } + d ( i , x ) + d ( i , y )$ . Also, $d ( i , x ) \\leq r _ { y }$ , since $i \\in B ( x , r _ { y } )$ . Consider the minimum multiplicative improvement of $i$ and $i ^ { * }$ :\n\n$$ \\begin{array}{l} \\min \\left(\\frac {d (i , x)}{d (i ,"
      },
      "element_b": {
        "element_id": "1905.03674_figure_2",
        "element_type": "figure",
        "caption": "Figure 2: Diagram for Proof of Theorem 1",
        "content": "Figure 2: Diagram for Proof of Theorem 1",
        "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1905.03674/1905.03674/hybrid_auto/images/1905.03674_page0_fig1.jpg",
        "context_before": "Since $k = 5$ and there are three clusters, in a feasible solution there is a cluster in which we choose at most one center. In that cluster, suppose first that we choose a center of type $x _ { 1 }$ , $x _ { 2 }$ , or $x _ { 3 }$ . Since the instance is symmetric with respect to this choice, suppose without loss of generality that we choose $x _ { 1 }$ . Then the 200 points of types $a _ { 1 }$ and $a _ { 2 }$ could decrease their distance by a factor of 2 by switching to $x _ { 3 }$ . Since an",
        "context_after": "In the second case, $\\exists x \\in X$ and $\\exists i \\in N$ such that $i \\in B ( x , r _ { y } ) \\cap S$ . This case is drawn below in Figure 2. By the triangle inequality, $d ( x , y ) \\leq d ( i , x ) + d ( i , y )$ . Therefore, $d ( i ^ { * } , x ) \\leq r _ { y } + d ( i , x ) + d ( i , y )$ . Also, $d ( i , x ) \\leq r _ { y }$ , since $i \\in B ( x , r _ { y } )$ . Consider the minimum multiplicative improvement of $i$ and $i ^ { * }$ :\n\n$$ \\begin{array}{l} \\min \\left(\\frac {d (i , x)}{d (i ,"
      },
      "edge_contexts": [
        {
          "source": "1905.03674_formula_2",
          "target": "1905.03674_figure_2",
          "ref_text": "Figure 2",
          "context_snippet": "n N$ such that $i \\in B ( x , r _ { y } ) \\cap S$ . This case is drawn below in Figure 2. By the triangle inequality, $d ( x , y ) \\leq d ( i , x ) + d ( i , y )$ . The"
        }
      ]
    },
    {
      "pair_id": "1905.03674_pair_3",
      "doc_id": "1905.03674",
      "element_a_id": "1905.03674_formula_3",
      "element_b_id": "1905.03674_figure_2",
      "element_a_type": "formula",
      "element_b_type": "figure",
      "pair_type": "figure+formula",
      "hop_distance": 1,
      "path": [
        "1905.03674_formula_3",
        "1905.03674_figure_2"
      ],
      "quality_score": 0.8200000000000001,
      "overlap_with_existing_l1": false,
      "element_a": {
        "element_id": "1905.03674_formula_3",
        "element_type": "formula",
        "caption": "",
        "content": "$$\\begin{array}{l} \\min \\left(\\frac {d (i , x)}{d (i , y)}, \\frac {d (i ^ {*} , x)}{d (i ^ {*} , y)}\\right) \\\\ \\leq \\min \\left(\\frac {d (i , x)}{d (i , y)}, \\frac {r _ {y} + d (i , x) + d (i , y)}{r _ {y}}\\right) \\\\ \\leq \\min \\left(\\frac {r _ {y}}{d (i , y)}, 2 + \\frac {d (i , y)}{r _ {y}}\\right) \\\\ \\leq \\max _ {z \\geq 0} \\left(\\min \\left(z, 2 + 1 / z\\right)\\right) = 1 + \\sqrt {2} \\\\ \\end{array}$$",
        "image_path": null,
        "context_before": "In the second case, $\\exists x \\in X$ and $\\exists i \\in N$ such that $i \\in B ( x , r _ { y } ) \\cap S$ . This case is drawn below in Figure 2. By the triangle inequality, $d ( x , y ) \\leq d ( i , x ) + d ( i , y )$ . Therefore, $d ( i ^ { * } , x ) \\leq r _ { y } + d ( i , x ) + d ( i , y )$ . Also, $d ( i , x ) \\leq r _ { y }$ , since $i \\in B ( x , r _ { y } )$ . Consider the minimum multiplicative improvement of $i$ and $i ^ { * }$ :\n\n$$ \\begin{array}{l} \\min \\left(\\frac {d (i , x)}{d (i ,",
        "context_after": ""
      },
      "element_b": {
        "element_id": "1905.03674_figure_2",
        "element_type": "figure",
        "caption": "Figure 2: Diagram for Proof of Theorem 1",
        "content": "Figure 2: Diagram for Proof of Theorem 1",
        "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1905.03674/1905.03674/hybrid_auto/images/1905.03674_page0_fig1.jpg",
        "context_before": "Since $k = 5$ and there are three clusters, in a feasible solution there is a cluster in which we choose at most one center. In that cluster, suppose first that we choose a center of type $x _ { 1 }$ , $x _ { 2 }$ , or $x _ { 3 }$ . Since the instance is symmetric with respect to this choice, suppose without loss of generality that we choose $x _ { 1 }$ . Then the 200 points of types $a _ { 1 }$ and $a _ { 2 }$ could decrease their distance by a factor of 2 by switching to $x _ { 3 }$ . Since an",
        "context_after": "In the second case, $\\exists x \\in X$ and $\\exists i \\in N$ such that $i \\in B ( x , r _ { y } ) \\cap S$ . This case is drawn below in Figure 2. By the triangle inequality, $d ( x , y ) \\leq d ( i , x ) + d ( i , y )$ . Therefore, $d ( i ^ { * } , x ) \\leq r _ { y } + d ( i , x ) + d ( i , y )$ . Also, $d ( i , x ) \\leq r _ { y }$ , since $i \\in B ( x , r _ { y } )$ . Consider the minimum multiplicative improvement of $i$ and $i ^ { * }$ :\n\n$$ \\begin{array}{l} \\min \\left(\\frac {d (i , x)}{d (i ,"
      },
      "edge_contexts": [
        {
          "source": "1905.03674_formula_3",
          "target": "1905.03674_figure_2",
          "ref_text": "Figure 2",
          "context_snippet": "n N$ such that $i \\in B ( x , r _ { y } ) \\cap S$ . This case is drawn below in Figure 2. By the triangle inequality, $d ( x , y ) \\leq d ( i , x ) + d ( i , y )$ . The"
        }
      ]
    },
    {
      "pair_id": "1902.07823_pair_2",
      "doc_id": "1902.07823",
      "element_a_id": "1902.07823_formula_4",
      "element_b_id": "1902.07823_figure_1",
      "element_a_type": "formula",
      "element_b_type": "figure",
      "pair_type": "figure+formula",
      "hop_distance": 1,
      "path": [
        "1902.07823_formula_4",
        "1902.07823_figure_1"
      ],
      "quality_score": 0.78,
      "overlap_with_existing_l1": false,
      "element_a": {
        "element_id": "1902.07823_formula_4",
        "element_type": "formula",
        "caption": "",
        "content": "$$| T | \\cdot \\operatorname * {P r} _ {S, S ^ {\\prime} \\sim \\mathfrak {S} ^ {N}, X \\sim T, \\mathcal {A}} \\left[ \\mathrm {I} \\left[ \\mathcal {A} _ {S} (X) \\geq 0 \\right] \\neq \\mathrm {I} \\left[ \\mathcal {A} _ {S ^ {\\prime}} (X) \\geq 0 \\right] \\right].$$",
        "image_path": null,
        "context_before": "Our simulations indicate that introducing a stability-focused regularization term can make the algorithm more stable by slightly sacrificing accuracy. Table 1 summarizes the accuracy\n\nAdult dataset,race attribute   \nFigure 1: stab vs. $\\lambda$ for race attribute.\n\nTable 1 summarizes the accuracy\n\nand fairness metric under different regularization parameters $\\lambda$ .",
        "context_after": ""
      },
      "element_b": {
        "element_id": "1902.07823_figure_1",
        "element_type": "figure",
        "caption": "Adult dataset,race attribute Figure 1: stab vs. $\\lambda$ for race attribute.",
        "content": "Adult dataset,race attribute Figure 1: stab vs. $\\lambda$ for race attribute.",
        "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1902.07823/1902.07823/hybrid_auto/images/1902.07823_page0_fig1.jpg",
        "context_before": "Adult dataset,race attribute   \nFigure 1: stab vs. $\\lambda$ for race attribute.\n\nTable 1 summarizes the accuracy\n\nand fairness metric under different regularization parameters $\\lambda$ .\n\n$$ | T | \\cdot \\operatorname * {P r} _ {S, S ^ {\\prime} \\sim \\mathfrak {S} ^ {N}, X \\sim T, \\mathcal {A}} \\left[ \\mathrm {I} \\left[ \\mathcal {A} _ {S} (X) \\geq 0 \\right] \\neq \\mathrm {I} \\left[ \\mathcal {A} _ {S ^ {\\prime}} (X) \\geq 0 \\right] \\right]. $$\n\n4The codes are available on https://github.com/huanglx",
        "context_after": "Our framework can be easily extended to other fairness metrics; see a summary in Table 1 of [14].\n\nOur simulations indicate that introducing a stability-focused regularization term can make the algorithm more stable by slightly sacrificing accuracy. Table 1 summarizes the accuracy\n\nAdult dataset,race attribute   \nFigure 1: stab vs. $\\lambda$ for race attribute."
      },
      "edge_contexts": [
        {
          "source": "1902.07823_formula_4",
          "target": "1902.07823_figure_1",
          "ref_text": "Figure 1",
          "context_snippet": "cing accuracy. Table 1 summarizes the accuracy\n\nAdult dataset,race attribute   \nFigure 1: stab vs. $\\lambda$ for race attribute.\n\nTable 1 summarizes the accuracy\n\nand f"
        }
      ]
    },
    {
      "pair_id": "1602.05352_pair_4",
      "doc_id": "1602.05352",
      "element_a_id": "1602.05352_formula_1",
      "element_b_id": "1602.05352_figure_1",
      "element_a_type": "formula",
      "element_b_type": "figure",
      "pair_type": "figure+formula",
      "hop_distance": 1,
      "path": [
        "1602.05352_formula_1",
        "1602.05352_figure_1"
      ],
      "quality_score": 0.75,
      "overlap_with_existing_l1": false,
      "element_a": {
        "element_id": "1602.05352_formula_1",
        "element_type": "formula",
        "caption": "",
        "content": "$$\\begin{array}{l} \\mathbb {E} _ {O} \\Big [ \\hat {R} _ {I P S} (\\hat {Y} | P) \\Big ] = \\frac {1}{U \\cdot I} \\sum_ {u} \\sum_ {i} \\mathbb {E} _ {O _ {u, i}} \\bigg [ \\frac {\\delta_ {u , i} (Y , \\hat {Y})}{P _ {u , i}} O _ {u, i} \\bigg ] \\\\ = \\frac {1}{U \\cdot I} \\sum_ {u} \\sum_ {i} \\delta_ {u, i} (Y, \\hat {Y}) = R (\\hat {Y}). \\\\ \\end{array}$$",
        "image_path": null,
        "context_before": "",
        "context_after": "Consider a toy example adapted from Steck (2010) to illustrate the disastrous effect that selection bias can have on conventional evaluation using a test set of held-out ratings. Denote with $u ~ \\in ~ \\{ 1 , . . . , U \\}$ the users and with $i \\in \\{ 1 , . . . , I \\}$ the movies. Figure 1 shows the matrix of true ratings $\\dot { Y } \\in \\mathfrak { R } ^ { U \\times I }$ for our toy example, where a sub-\n\nset of users are “horror lovers” who rate all horror movies 5 and all romance movies 1. Sim"
      },
      "element_b": {
        "element_id": "1602.05352_figure_1",
        "element_type": "figure",
        "caption": "Figure 1. Movie-Lovers toy example. Top row: true rating matrix $Y$ , propensity matrix $P$ , observation indicator matrix $O$ . Bottom row: two rating prediction matrices $\\hat { Y _ { 1 } }$ and $\\hat { Y } _ { 2 }$ , and intervention indicator matrix $\\hat { Y } _ { 3 }$ .",
        "content": "Figure 1. Movie-Lovers toy example. Top row: true rating matrix $Y$ , propensity matrix $P$ , observation indicator matrix $O$ . Bottom row: two rating prediction matrices $\\hat { Y _ { 1 } }$ and $\\hat { Y } _ { 2 }$ , and intervention indicator matrix $\\hat { Y } _ { 3 }$ .",
        "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1602.05352/1602.05352/hybrid_auto/images/1602.05352_page0_fig2.jpg",
        "context_before": "",
        "context_after": "set of users are “horror lovers” who rate all horror movies 5 and all romance movies 1. Similarly, there is a subset of “romance lovers” who rate just the opposite way. However, both groups rate dramas as 3. The binary matrix ${ \\cal O } \\in \\{ 0 , 1 \\} ^ { \\bar { U } \\times \\bar { I } }$ in Figure 1 shows for which movies the users provided their rating to the system, $\\left[ O _ { u , i } \\right. = 1 ] \\Leftrightarrow$ $[ Y _ { u , i }$ observed]. Our toy example shows a strong correlation bet"
      },
      "edge_contexts": [
        {
          "source": "1602.05352_formula_1",
          "target": "1602.05352_figure_1",
          "ref_text": "Figure 1 s",
          "context_snippet": "\\{ 1 , . . . , U \\}$ the users and with $i \\in \\{ 1 , . . . , I \\}$ the movies. Figure 1 shows the matrix of true ratings $\\dot { Y } \\in \\mathfrak { R } ^ { U \\times I }"
        }
      ]
    },
    {
      "pair_id": "1607.06520_pair_4",
      "doc_id": "1607.06520",
      "element_a_id": "1607.06520_formula_3",
      "element_b_id": "1607.06520_figure_6",
      "element_a_type": "formula",
      "element_b_type": "figure",
      "pair_type": "figure+formula",
      "hop_distance": 1,
      "path": [
        "1607.06520_formula_3",
        "1607.06520_figure_6"
      ],
      "quality_score": 0.75,
      "overlap_with_existing_l1": false,
      "element_a": {
        "element_id": "1607.06520_formula_3",
        "element_type": "formula",
        "caption": "",
        "content": "$$\\operatorname {D i r e c t B i a s} _ {c} = \\frac {1}{| N |} \\sum_ {w \\in N} | \\cos (\\vec {w}, g) | ^ {c}$$",
        "image_path": null,
        "context_before": "",
        "context_after": "To identify the gender subspace, we took the ten gender pair difference vectors and computed its principal components (PCs). As Figure 6 shows, there is a single direction that explains the majority of variance in these vectors. The first eigenvalue is significantly larger than the rest. Note that, from the randomness in a finite sample of ten noisy vectors, one expects a decrease in eigenvalues. However, as also illustrated in 6, the decrease one observes due to random sampling is much more gra"
      },
      "element_b": {
        "element_id": "1607.06520_figure_6",
        "element_type": "figure",
        "caption": "Figure 6: Left: the percentage of variance explained in the PCA of these vector differences (each difference normalized to be a unit vector). The top component explains significantly more variance than any other. Right: for comparison, the corresponding percentages for random unit vectors (figure created by averaging over 1,000 draws of ten random unit vectors in 300 dimensions).",
        "content": "Figure 6: Left: the percentage of variance explained in the PCA of these vector differences (each difference normalized to be a unit vector). The top component explains significantly more variance than any other. Right: for comparison, the corresponding percentages for random unit vectors (figure created by averaging over 1,000 draws of ten random unit vectors in 300 dimensions).",
        "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1607.06520/1607.06520/hybrid_auto/images/1607.06520_page0_fig2.jpg",
        "context_before": "",
        "context_after": "$$ \\operatorname {D i r e c t B i a s} _ {c} = \\frac {1}{| N |} \\sum_ {w \\in N} | \\cos (\\vec {w}, g) | ^ {c} $$\n\nwhere $c$ is a parameter that determines how strict do we want to in measuring bias. If $c$ is $0$ , then $| \\mathrm { c o s } ( \\vec { w } - g ) | ^ { c } = 0$ only if $\\vec { w }$ has no overlap with $g$ and otherwise it is 1. Such strict measurement of bias might be desirable in settings such as the college admissions example from the Introduction, where it would be unacceptable fo"
      },
      "edge_contexts": [
        {
          "source": "1607.06520_formula_3",
          "target": "1607.06520_figure_6",
          "ref_text": "Figure 6 s",
          "context_snippet": " gender pair difference vectors and computed its principal components (PCs). As Figure 6 shows, there is a single direction that explains the majority of variance in thes"
        }
      ]
    },
    {
      "pair_id": "1607.06520_pair_5",
      "doc_id": "1607.06520",
      "element_a_id": "1607.06520_formula_4",
      "element_b_id": "1607.06520_figure_6",
      "element_a_type": "formula",
      "element_b_type": "figure",
      "pair_type": "figure+formula",
      "hop_distance": 1,
      "path": [
        "1607.06520_formula_4",
        "1607.06520_figure_6"
      ],
      "quality_score": 0.75,
      "overlap_with_existing_l1": false,
      "element_a": {
        "element_id": "1607.06520_formula_4",
        "element_type": "formula",
        "caption": "",
        "content": "$$\\beta (w, v) = \\left(w \\cdot v - \\frac {w _ {\\perp} \\cdot v _ {\\perp}}{\\| w _ {\\perp} \\| _ {2} \\| v _ {\\perp} \\| _ {2}}\\right) \\Bigg / w \\cdot v.$$",
        "image_path": null,
        "context_before": "$$ \\operatorname {D i r e c t B i a s} _ {c} = \\frac {1}{| N |} \\sum_ {w \\in N} | \\cos (\\vec {w}, g) | ^ {c} $$\n\nwhere $c$ is a parameter that determines how strict do we want to in measuring bias. If $c$ is $0$ , then $| \\mathrm { c o s } ( \\vec { w } - g ) | ^ { c } = 0$ only if $\\vec { w }$ has no overlap with $g$ and otherwise it is 1. Such strict measurement of bias might be desirable in settings such as the college admissions example from the Introduction, where it would be unacceptable fo",
        "context_after": ""
      },
      "element_b": {
        "element_id": "1607.06520_figure_6",
        "element_type": "figure",
        "caption": "Figure 6: Left: the percentage of variance explained in the PCA of these vector differences (each difference normalized to be a unit vector). The top component explains significantly more variance than any other. Right: for comparison, the corresponding percentages for random unit vectors (figure created by averaging over 1,000 draws of ten random unit vectors in 300 dimensions).",
        "content": "Figure 6: Left: the percentage of variance explained in the PCA of these vector differences (each difference normalized to be a unit vector). The top component explains significantly more variance than any other. Right: for comparison, the corresponding percentages for random unit vectors (figure created by averaging over 1,000 draws of ten random unit vectors in 300 dimensions).",
        "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1607.06520/1607.06520/hybrid_auto/images/1607.06520_page0_fig2.jpg",
        "context_before": "",
        "context_after": "$$ \\operatorname {D i r e c t B i a s} _ {c} = \\frac {1}{| N |} \\sum_ {w \\in N} | \\cos (\\vec {w}, g) | ^ {c} $$\n\nwhere $c$ is a parameter that determines how strict do we want to in measuring bias. If $c$ is $0$ , then $| \\mathrm { c o s } ( \\vec { w } - g ) | ^ { c } = 0$ only if $\\vec { w }$ has no overlap with $g$ and otherwise it is 1. Such strict measurement of bias might be desirable in settings such as the college admissions example from the Introduction, where it would be unacceptable fo"
      },
      "edge_contexts": [
        {
          "source": "1607.06520_formula_4",
          "target": "1607.06520_figure_6",
          "ref_text": "Figure 6 s",
          "context_snippet": " gender pair difference vectors and computed its principal components (PCs). As Figure 6 shows, there is a single direction that explains the majority of variance in thes"
        }
      ]
    },
    {
      "pair_id": "1610.02413_pair_1",
      "doc_id": "1610.02413",
      "element_a_id": "1610.02413_formula_4",
      "element_b_id": "1610.02413_figure_2",
      "element_a_type": "formula",
      "element_b_type": "figure",
      "pair_type": "figure+formula",
      "hop_distance": 1,
      "path": [
        "1610.02413_formula_4",
        "1610.02413_figure_2"
      ],
      "quality_score": 0.75,
      "overlap_with_existing_l1": false,
      "element_a": {
        "element_id": "1610.02413_formula_4",
        "element_type": "formula",
        "caption": "",
        "content": "$$\\widetilde {Y} = \\mathbb {I} \\left\\{R > T _ {a} \\right\\},$$",
        "image_path": null,
        "context_before": "",
        "context_after": "The feasible set of false/true positive rates of possible equalized odds predictors is thus the intersection of the areas under the $A$ -conditional ROC curves, and above the main diagonal (see Figure 2). Since for any loss function the optimal false/true-positive rate will always be on the upper-left boundary of this feasible set, this is effectively the ROC curve of the equalized odds predictors. This ROC curve is the pointwise minimum of all $A$ -conditional ROC curves. The performance of an\n"
      },
      "element_b": {
        "element_id": "1610.02413_figure_2",
        "element_type": "figure",
        "caption": "Figure 2: Finding the optimal equalized odds threshold predictor (middle), and equal opportunity threshold predictor (right). For the equal opportunity predictor, within each group the cost for a given true positive rate is proportional to the horizontal gap between the ROC curve and the profit-maximizing tangent line (i.e., the two curves on the left plot), so it is a convex function of the true positive rate (right). This lets us optimize it efficiently with ternary search.",
        "content": "Figure 2: Finding the optimal equalized odds threshold predictor (middle), and equal opportunity threshold predictor (right). For the equal opportunity predictor, within each group the cost for a given true positive rate is proportional to the horizontal gap between the ROC curve and the profit-maximizing tangent line (i.e., the two curves on the left plot), so it is a convex function of the true positive rate (right). This lets us optimize it efficiently with ternary search.",
        "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1610.02413/1610.02413/hybrid_auto/images/1610.02413_page0_fig5.jpg",
        "context_before": "",
        "context_after": "The definition of $D _ { a }$ is analogous to the polytope $P _ { a }$ in the previous section, except that here we do not consider points below the main diagonal (line from $( 0 , 0 )$ to (1, 1)), which are worse than “random guessing” and hence never desirable for any reasonable loss function.\n\nDeriving an optimal equalized odds threshold predictor. Any point in the convex hull $D _ { a }$ represents the false/true positive rates, conditioned on $A = a$ , of a randomized derived predictor base"
      },
      "edge_contexts": [
        {
          "source": "1610.02413_formula_4",
          "target": "1610.02413_figure_2",
          "ref_text": "Figure 2",
          "context_snippet": "e areas under the $A$ -conditional ROC curves, and above the main diagonal (see Figure 2). Since for any loss function the optimal false/true-positive rate will always "
        }
      ]
    },
    {
      "pair_id": "1610.07524_pair_4",
      "doc_id": "1610.07524",
      "element_a_id": "1610.07524_formula_4",
      "element_b_id": "1610.07524_figure_2",
      "element_a_type": "formula",
      "element_b_type": "figure",
      "pair_type": "figure+formula",
      "hop_distance": 1,
      "path": [
        "1610.07524_formula_4",
        "1610.07524_figure_2"
      ],
      "quality_score": 0.75,
      "overlap_with_existing_l1": false,
      "element_a": {
        "element_id": "1610.07524_formula_4",
        "element_type": "formula",
        "caption": "",
        "content": "$$\\Delta \\leq (t _ {H} - t _ {L}) d _ {\\mathrm {T V}} (f _ {b, y}, f _ {w, y}).$$",
        "image_path": null,
        "context_before": "",
        "context_after": "One might expect that differences in false positive rates are largely attributable to the subset of defendants who are charged with more serious offenses and who have a larger number of prior arrests/convictions. While it is true that the false positive rates within both racial groups are higher for defendants with worse criminal histories, considerable between-group differences in these error rates persist across low prior count subgroups. Figure 2 shows a plot of false positive rates across di"
      },
      "element_b": {
        "element_id": "1610.07524_figure_2",
        "element_type": "figure",
        "caption": "Figure 2: False positive rates across prior record count for defendants charged with a Misdemeanor offense. Plot is based on assessing a defendant as “high-risk” if their COMPAS decile score is $> 4$ . Error bars represent 95% confidence intervals. Figure 3: COMPAS decile score histograms for Black and White defendants. Cohen’s $d = 0 . 6 0$ , non-overlap $d _ { \\mathrm { T V } } ( f _ { b } , f _ { w } ) = 2 4 . 5 \\%$ .",
        "content": "Figure 2: False positive rates across prior record count for defendants charged with a Misdemeanor offense. Plot is based on assessing a defendant as “high-risk” if their COMPAS decile score is $> 4$ . Error bars represent 95% confidence intervals. Figure 3: COMPAS decile score histograms for Black and White defendants. Cohen’s $d = 0 . 6 0$ , non-overlap $d _ { \\mathrm { T V } } ( f _ { b } , f _ { w } ) = 2 4 . 5 \\%$ .",
        "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1610.07524/1610.07524/hybrid_auto/images/1610.07524_page0_fig2.jpg",
        "context_before": "",
        "context_after": "is equivalent to the total variation distance. Letting $f _ { r , y } ( s )$ denote the score distribution for race $r$ and recidivism outcome $y$ , one can establish the following sharp bound on $\\Delta$ .\n\nProposition 3.2 (Percent overlap bound). Under the MinMax policy,\n\n$$ \\Delta \\leq (t _ {H} - t _ {L}) d _ {\\mathrm {T V}} (f _ {b, y}, f _ {w, y}). $$\n\nOne might expect that differences in false positive rates are largely attributable to the subset of defendants who are charged with more ser"
      },
      "edge_contexts": [
        {
          "source": "1610.07524_formula_4",
          "target": "1610.07524_figure_2",
          "ref_text": "Figure 2 s",
          "context_snippet": "roup differences in these error rates persist across low prior count subgroups. Figure 2 shows a plot of false positive rates across di\n\nOur analysis indicates that there"
        }
      ]
    },
    {
      "pair_id": "1703.06856_pair_2",
      "doc_id": "1703.06856",
      "element_a_id": "1703.06856_formula_1",
      "element_b_id": "1703.06856_figure_1",
      "element_a_type": "formula",
      "element_b_type": "figure",
      "pair_type": "figure+formula",
      "hop_distance": 1,
      "path": [
        "1703.06856_formula_1",
        "1703.06856_figure_1"
      ],
      "quality_score": 0.75,
      "overlap_with_existing_l1": false,
      "element_a": {
        "element_id": "1703.06856_formula_1",
        "element_type": "formula",
        "caption": "",
        "content": "$$\\mathrm {F Y A} \\sim \\mathcal {N} \\left(w _ {F} ^ {K} K + w _ {F} ^ {R} R + w _ {F} ^ {S} S, 1\\right),$$",
        "image_path": null,
        "context_before": "Consider a linear model with the structure in Figure 1(a).\n\n□\n\nNote that if Figure 1(a) is the true model for the real world then ${ \\hat { Y } } ( X , A )$ will also satisfy demographic parity and equality of opportunity as $\\hat { Y }$ will be unaffected by $A$ .\n\nA contrast between the two approaches is left for future work, although we stress that they are in some sense complementary: we are motivated mostly by problems such as the one in Figure 1(d), where many of the mediators themselves a",
        "context_after": ""
      },
      "element_b": {
        "element_id": "1703.06856_figure_1",
        "element_type": "figure",
        "caption": "(e) Figure 1: (a), (b) Two causal models for different real-world fair prediction scenarios. See Section 3.1 for discussion. (c) The graph corresponding to a causal model with $A$ being the protected attribute and $Y$ some outcome of interest, with background variables assumed to be independent. (d) Expanding the model to include an intermediate variable indicating whether the individual is employed with two (latent) background variables Prejudiced (if the person offering the job is prejudiced) and Qualifications (a measure of the individual’s qualifications). (e) A twin network representation of this system [28] under two different counterfactual levels for $A$ . This is created by copying nodes descending from $A$ , which inherit unaffected parents from the factual world.",
        "content": "(e) Figure 1: (a), (b) Two causal models for different real-world fair prediction scenarios. See Section 3.1 for discussion. (c) The graph corresponding to a causal model with $A$ being the protected attribute and $Y$ some outcome of interest, with background variables assumed to be independent. (d) Expanding the model to include an intermediate variable indicating whether the individual is employed with two (latent) background variables Prejudiced (if the person offering the job is prejudiced) and Qualifications (a measure of the individual’s qualifications). (e) A twin network representation of this system [28] under two different counterfactual levels for $A$ . This is created by copying nodes descending from $A$ , which inherit unaffected parents from the factual world.",
        "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1703.06856/1703.06856/hybrid_auto/images/1703.06856_page0_fig3.jpg",
        "context_before": "",
        "context_after": "our examples to follow, then IF can be defined by treating equally two individuals with the same $W$ in a way that is also counterfactually fair.\n\nRelation to Pearl et al. [29]. In Example 4.4.4 of [29], the authors condition instead on $X , A$ , and the observed realization of $\\hat { Y }$ , and calculate the probability of the counterfactual realization $\\hat { Y } _ { A a ^ { \\prime } }$ differing from the factual. This example conflates the predictor $\\hat { Y }$ with the outcome $Y$ , of wh"
      },
      "edge_contexts": [
        {
          "source": "1703.06856_formula_1",
          "target": "1703.06856_figure_1",
          "ref_text": "Figure 1",
          "context_snippet": "Consider a linear model with the structure in Figure 1(a).\n\n□\n\nNote that if Figure 1(a) is the true model for the real world then ${ \\"
        }
      ]
    },
    {
      "pair_id": "1703.06856_pair_3",
      "doc_id": "1703.06856",
      "element_a_id": "1703.06856_formula_2",
      "element_b_id": "1703.06856_figure_1",
      "element_a_type": "formula",
      "element_b_type": "figure",
      "pair_type": "figure+formula",
      "hop_distance": 1,
      "path": [
        "1703.06856_formula_2",
        "1703.06856_figure_1"
      ],
      "quality_score": 0.75,
      "overlap_with_existing_l1": false,
      "element_a": {
        "element_id": "1703.06856_formula_2",
        "element_type": "formula",
        "caption": "",
        "content": "$$\\mathrm {K} \\sim \\mathcal {N} (0, 1)$$",
        "image_path": null,
        "context_before": "Consider a linear model with the structure in Figure 1(a).\n\n□\n\nNote that if Figure 1(a) is the true model for the real world then ${ \\hat { Y } } ( X , A )$ will also satisfy demographic parity and equality of opportunity as $\\hat { Y }$ will be unaffected by $A$ .\n\nA contrast between the two approaches is left for future work, although we stress that they are in some sense complementary: we are motivated mostly by problems such as the one in Figure 1(d), where many of the mediators themselves a",
        "context_after": ""
      },
      "element_b": {
        "element_id": "1703.06856_figure_1",
        "element_type": "figure",
        "caption": "(e) Figure 1: (a), (b) Two causal models for different real-world fair prediction scenarios. See Section 3.1 for discussion. (c) The graph corresponding to a causal model with $A$ being the protected attribute and $Y$ some outcome of interest, with background variables assumed to be independent. (d) Expanding the model to include an intermediate variable indicating whether the individual is employed with two (latent) background variables Prejudiced (if the person offering the job is prejudiced) and Qualifications (a measure of the individual’s qualifications). (e) A twin network representation of this system [28] under two different counterfactual levels for $A$ . This is created by copying nodes descending from $A$ , which inherit unaffected parents from the factual world.",
        "content": "(e) Figure 1: (a), (b) Two causal models for different real-world fair prediction scenarios. See Section 3.1 for discussion. (c) The graph corresponding to a causal model with $A$ being the protected attribute and $Y$ some outcome of interest, with background variables assumed to be independent. (d) Expanding the model to include an intermediate variable indicating whether the individual is employed with two (latent) background variables Prejudiced (if the person offering the job is prejudiced) and Qualifications (a measure of the individual’s qualifications). (e) A twin network representation of this system [28] under two different counterfactual levels for $A$ . This is created by copying nodes descending from $A$ , which inherit unaffected parents from the factual world.",
        "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1703.06856/1703.06856/hybrid_auto/images/1703.06856_page0_fig3.jpg",
        "context_before": "",
        "context_after": "our examples to follow, then IF can be defined by treating equally two individuals with the same $W$ in a way that is also counterfactually fair.\n\nRelation to Pearl et al. [29]. In Example 4.4.4 of [29], the authors condition instead on $X , A$ , and the observed realization of $\\hat { Y }$ , and calculate the probability of the counterfactual realization $\\hat { Y } _ { A a ^ { \\prime } }$ differing from the factual. This example conflates the predictor $\\hat { Y }$ with the outcome $Y$ , of wh"
      },
      "edge_contexts": [
        {
          "source": "1703.06856_formula_2",
          "target": "1703.06856_figure_1",
          "ref_text": "Figure 1",
          "context_snippet": "Consider a linear model with the structure in Figure 1(a).\n\n□\n\nNote that if Figure 1(a) is the true model for the real world then ${ \\"
        }
      ]
    },
    {
      "pair_id": "1703.06856_pair_5",
      "doc_id": "1703.06856",
      "element_a_id": "1703.06856_formula_3",
      "element_b_id": "1703.06856_figure_1",
      "element_a_type": "formula",
      "element_b_type": "figure",
      "pair_type": "figure+formula",
      "hop_distance": 1,
      "path": [
        "1703.06856_formula_3",
        "1703.06856_figure_1"
      ],
      "quality_score": 0.75,
      "overlap_with_existing_l1": false,
      "element_a": {
        "element_id": "1703.06856_formula_3",
        "element_type": "formula",
        "caption": "",
        "content": "$$\\mathbf {G P A} = b _ {G} + w _ {G} ^ {R} R + w _ {G} ^ {S} S + \\epsilon_ {G}, \\epsilon_ {G} \\sim p (\\epsilon_ {G})$$",
        "image_path": null,
        "context_before": "",
        "context_after": "To provide an intuition for counterfactual fairness, we will consider two real-world fair prediction scenarios: insurance pricing and crime prediction. Each of these correspond to one of the two causal graphs in Figure 1(a),(b). The Supplementary Material provides a more mathematical discussion of these examples with more detailed insights.\n\nScenario 1: The Red Car. A car insurance company wishes to price insurance for car owners by predicting their accident rate $Y$ . They assume there is an un"
      },
      "element_b": {
        "element_id": "1703.06856_figure_1",
        "element_type": "figure",
        "caption": "(e) Figure 1: (a), (b) Two causal models for different real-world fair prediction scenarios. See Section 3.1 for discussion. (c) The graph corresponding to a causal model with $A$ being the protected attribute and $Y$ some outcome of interest, with background variables assumed to be independent. (d) Expanding the model to include an intermediate variable indicating whether the individual is employed with two (latent) background variables Prejudiced (if the person offering the job is prejudiced) and Qualifications (a measure of the individual’s qualifications). (e) A twin network representation of this system [28] under two different counterfactual levels for $A$ . This is created by copying nodes descending from $A$ , which inherit unaffected parents from the factual world.",
        "content": "(e) Figure 1: (a), (b) Two causal models for different real-world fair prediction scenarios. See Section 3.1 for discussion. (c) The graph corresponding to a causal model with $A$ being the protected attribute and $Y$ some outcome of interest, with background variables assumed to be independent. (d) Expanding the model to include an intermediate variable indicating whether the individual is employed with two (latent) background variables Prejudiced (if the person offering the job is prejudiced) and Qualifications (a measure of the individual’s qualifications). (e) A twin network representation of this system [28] under two different counterfactual levels for $A$ . This is created by copying nodes descending from $A$ , which inherit unaffected parents from the factual world.",
        "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1703.06856/1703.06856/hybrid_auto/images/1703.06856_page0_fig3.jpg",
        "context_before": "",
        "context_after": "our examples to follow, then IF can be defined by treating equally two individuals with the same $W$ in a way that is also counterfactually fair.\n\nRelation to Pearl et al. [29]. In Example 4.4.4 of [29], the authors condition instead on $X , A$ , and the observed realization of $\\hat { Y }$ , and calculate the probability of the counterfactual realization $\\hat { Y } _ { A a ^ { \\prime } }$ differing from the factual. This example conflates the predictor $\\hat { Y }$ with the outcome $Y$ , of wh"
      },
      "edge_contexts": [
        {
          "source": "1703.06856_formula_3",
          "target": "1703.06856_figure_1",
          "ref_text": "Figure 1",
          "context_snippet": "d crime prediction. Each of these correspond to one of the two causal graphs in Figure 1(a),(b). The Supplementary Material provides a more mathematical discussion of t"
        }
      ]
    },
    {
      "pair_id": "1705.10378_pair_3",
      "doc_id": "1705.10378",
      "element_a_id": "1705.10378_formula_1",
      "element_b_id": "1705.10378_figure_2",
      "element_a_type": "formula",
      "element_b_type": "figure",
      "pair_type": "figure+formula",
      "hop_distance": 1,
      "path": [
        "1705.10378_formula_1",
        "1705.10378_figure_2"
      ],
      "quality_score": 0.75,
      "overlap_with_existing_l1": false,
      "element_a": {
        "element_id": "1705.10378_formula_1",
        "element_type": "formula",
        "caption": "",
        "content": "$$p (H = 1 | C = 1) = 0. 0 5 9 5 \\approx 0. 0 5 1 5 = p (H = 1 | C = 0).$$",
        "image_path": null,
        "context_before": "",
        "context_after": "Correctional Offender Management Profiling for Alternative Sanctions, or COMPAS, is a risk assessment tool, created by the company Northpointe, that is being used across the US to determine whether to release or detain a defendant before his or her trial. Each pretrial defendant receives several COMPAS scores based on factors including but not limited to demographics, criminal history, family history, and social status. Among these scores, we are primarily interested in “Risk of Recidivism\". Pro"
      },
      "element_b": {
        "element_id": "1705.10378_figure_2",
        "element_type": "figure",
        "caption": "(b) Figure 2: Causal graphs for (a) the COMPAS dataset, and (b) the Adult dataset.",
        "content": "(b) Figure 2: Causal graphs for (a) the COMPAS dataset, and (b) the Adult dataset.",
        "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1705.10378/1705.10378/hybrid_auto/images/1705.10378_page0_fig4.jpg",
        "context_before": "",
        "context_after": "had we changed race from Caucasian to African-American. In our experiment we restricted NDE to lie between 0.95 and 1.05. Using unconstrained BART, our prediction accuracy on the test set was $6 7 . 8 \\%$ , removing treatment from the outcome model dropped the accuracy to $6 4 . 0 \\%$ , and using constrained BART lead to the accuracy of $6 6 . 4 \\%$ . As expected, dropping race, an informative feature, led to a greater decrease in accuracy, compared to simply constraining the outcome model to ob"
      },
      "edge_contexts": [
        {
          "source": "1705.10378_formula_1",
          "target": "1705.10378_figure_2",
          "ref_text": "Figure 2",
          "context_snippet": "prediction outcome. The simplified causal graph model for this task is given in Figure 2 (a), where $A$ denotes race, prior convictions is the mediator $M$ , demographi"
        }
      ]
    },
    {
      "pair_id": "1706.02744_pair_1",
      "doc_id": "1706.02744",
      "element_a_id": "1706.02744_formula_1",
      "element_b_id": "1706.02744_figure_2",
      "element_a_type": "formula",
      "element_b_type": "figure",
      "pair_type": "figure+formula",
      "hop_distance": 1,
      "path": [
        "1706.02744_formula_1",
        "1706.02744_figure_2"
      ],
      "quality_score": 0.75,
      "overlap_with_existing_l1": false,
      "element_a": {
        "element_id": "1706.02744_formula_1",
        "element_type": "formula",
        "caption": "",
        "content": "$$P = \\alpha_ {P} A + N _ {P}, \\qquad X = \\alpha_ {X} A + \\beta P + N _ {X}, \\qquad R _ {\\theta} = \\lambda_ {P} P + \\lambda_ {X} X.$$",
        "image_path": null,
        "context_before": "The two graphs in Figure 2 are taken from [2], which we here reinterpret in the causal context to prove Theorem 1. We point out that there is an established set of conditions under which unresolved discrimination can, in fact, be determined from observational data. Note that the two graphs are not Markov equivalent. Therefore, to obtain the same joint distribution we must violate a condition called faithfulness.2 We later argue that violation of faithfulness is by no means pathological, but emer",
        "context_after": ""
      },
      "element_b": {
        "element_id": "1706.02744_figure_2",
        "element_type": "figure",
        "caption": "Figure 2: Two graphs that may generate the same joint distribution for the Bayes optimal unconstrained predictor $R ^ { * }$ . If $X _ { 1 }$ is a resolving variable, $R ^ { * }$ exhibits unresolved discrimination in the right graph (along the red paths), but not in the left one.",
        "content": "Figure 2: Two graphs that may generate the same joint distribution for the Bayes optimal unconstrained predictor $R ^ { * }$ . If $X _ { 1 }$ is a resolving variable, $R ^ { * }$ exhibits unresolved discrimination in the right graph (along the red paths), but not in the left one.",
        "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1706.02744/1706.02744/hybrid_auto/images/1706.02744_page0_fig2.jpg",
        "context_before": "",
        "context_after": "ditional) statistical independences $A \\perp \\perp R$ , and $A \\bot \\bot R \\vert Y$ , but well captured by only considering dependences mitigated along directed causal paths.\n\nWe will next show that observational criteria are fundamentally unable to determine whether a predictor exhibits unresolved discrimination or not. This is true even if the predictor is Bayes optimal. In passing, we also note that fairness criteria such as equalized odds may or may not exhibit unresolved discrimination, but"
      },
      "edge_contexts": [
        {
          "source": "1706.02744_formula_1",
          "target": "1706.02744_figure_2",
          "ref_text": "Figure 2 a",
          "context_snippet": "The two graphs in Figure 2 are taken from [2], which we here reinterpret in the causal context to prove Theo"
        }
      ]
    },
    {
      "pair_id": "1706.02744_pair_4",
      "doc_id": "1706.02744",
      "element_a_id": "1706.02744_formula_3",
      "element_b_id": "1706.02744_figure_5",
      "element_a_type": "formula",
      "element_b_type": "figure",
      "pair_type": "figure+formula",
      "hop_distance": 1,
      "path": [
        "1706.02744_formula_3",
        "1706.02744_figure_5"
      ],
      "quality_score": 0.75,
      "overlap_with_existing_l1": false,
      "element_a": {
        "element_id": "1706.02744_formula_3",
        "element_type": "formula",
        "caption": "",
        "content": "$$E = \\alpha_ {E} A + N _ {E}, \\qquad X = \\alpha_ {X} A + \\beta E + N _ {X}, \\qquad R _ {\\theta} = \\lambda_ {E} E + \\lambda_ {X} X.$$",
        "image_path": null,
        "context_before": "",
        "context_after": "Motivated by the algorithm to avoid proxy discrimination, we discuss some natural variants of the notion in this section that connect our interventional approach to individual fairness and other proposed criteria. We consider a generic graph structure as shown on the left in Figure 5. The proxy $P$ and the features $X$ could be multidimensional. The empty circle in the middle represents any number of variables forming a DAG that respects the drawn arrows. Figure 3 is an example thereof. All dash"
      },
      "element_b": {
        "element_id": "1706.02744_figure_5",
        "element_type": "figure",
        "caption": "Figure 5: Left: A generic graph $\\tilde { \\mathcal { G } }$ to describe proxy discrimination. Right: The graph corresponding to an intervention on $P$ . The circle labeled “DAG” represents any sub-DAG of $\\tilde { \\mathcal { G } }$ and $\\mathcal { G }$ containing an arbitrary number of variables that is compatible with the shown arrows. Dashed arrows can, but do not have to be present in a given scenario.",
        "content": "Figure 5: Left: A generic graph $\\tilde { \\mathcal { G } }$ to describe proxy discrimination. Right: The graph corresponding to an intervention on $P$ . The circle labeled “DAG” represents any sub-DAG of $\\tilde { \\mathcal { G } }$ and $\\mathcal { G }$ containing an arbitrary number of variables that is compatible with the shown arrows. Dashed arrows can, but do not have to be present in a given scenario.",
        "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1706.02744/1706.02744/hybrid_auto/images/1706.02744_page0_fig6.jpg",
        "context_before": "",
        "context_after": "4.2 Avoiding unresolved discrimination\n\nWe proceed analogously to the previous subsection using the example graph in Figure 4. Instead of the proxy, we consider a resolving variable $E$ . The causal dependences are equivalent to the ones in Figure 3 and we again assume linear structural equations\n\n$$ E = \\alpha_ {E} A + N _ {E}, \\qquad X = \\alpha_ {X} A + \\beta E + N _ {X}, \\qquad R _ {\\theta} = \\lambda_ {E} E + \\lambda_ {X} X. $$\n\nMotivated by the algorithm to avoid proxy discrimination, we dis"
      },
      "edge_contexts": [
        {
          "source": "1706.02744_formula_3",
          "target": "1706.02744_figure_5",
          "ref_text": "Figure 5",
          "context_snippet": "roposed criteria. We consider a generic graph structure as shown on the left in Figure 5. The proxy $P$ and the features $X$ could be multidimensional. The empty circle"
        }
      ]
    },
    {
      "pair_id": "1803.04383_pair_2",
      "doc_id": "1803.04383",
      "element_a_id": "1803.04383_formula_6",
      "element_b_id": "1803.04383_figure_6",
      "element_a_type": "formula",
      "element_b_type": "figure",
      "pair_type": "figure+formula",
      "hop_distance": 1,
      "path": [
        "1803.04383_formula_6",
        "1803.04383_figure_6"
      ],
      "quality_score": 0.75,
      "overlap_with_existing_l1": false,
      "element_a": {
        "element_id": "1803.04383_formula_6",
        "element_type": "formula",
        "caption": "",
        "content": "$$\\kappa \\cdot \\frac {\\rho \\left(\\mathrm {Q} _ {\\mathrm {A}} (\\beta)\\right)}{\\rho \\left(\\mathrm {Q} _ {\\mathrm {B}} \\left(\\beta_ {0}\\right)\\right)} < 1 \\tag {31}$$",
        "image_path": null,
        "context_before": "u+\n\nThese results are further examined in Figure 6, which displays the normalized outcome curves and the utility curves for both the white and the black group.\n\nFigure 6 highlights that the position of the utility optima in the lower panel determines the loan (selection) rates.\n\nselection rate   \nFigure 6: The outcome and utility curves are plotted for both groups against the group selection rates. The relative positions of the utility maxima determine the position of the decision rule threshold",
        "context_after": ""
      },
      "element_b": {
        "element_id": "1803.04383_figure_6",
        "element_type": "figure",
        "caption": "selection rate Figure 6: The outcome and utility curves are plotted for both groups against the group selection rates. The relative positions of the utility maxima determine the position of the decision rule thresholds. We hold $\\frac { u _ { - } } { u _ { + } } = - 4$ as fixed.",
        "content": "selection rate Figure 6: The outcome and utility curves are plotted for both groups against the group selection rates. The relative positions of the utility maxima determine the position of the decision rule thresholds. We hold $\\frac { u _ { - } } { u _ { + } } = - 4$ as fixed.",
        "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1803.04383/1803.04383/hybrid_auto/images/1803.04383_page0_fig15.jpg",
        "context_before": "",
        "context_after": "8 Conclusion and Future Work\n\nWe argue that without a careful model of delayed outcomes, we cannot foresee the impact a fairness criterion would have if enforced as a constraint on a classification system. However, if such an accurate outcome model is available, we show that there are more direct ways to optimize for positive outcomes than via existing fairness criteria.\n\nOur formal framework exposes a concise, yet expressive way to model outcomes via the expected change in a variable of interes"
      },
      "edge_contexts": [
        {
          "source": "1803.04383_formula_6",
          "target": "1803.04383_figure_6",
          "ref_text": "Figure 6",
          "context_snippet": "u+\n\nThese results are further examined in Figure 6, which displays the normalized outcome curves and the utility curves for both t"
        }
      ]
    },
    {
      "pair_id": "1805.05859_pair_1",
      "doc_id": "1805.05859",
      "element_a_id": "1805.05859_formula_3",
      "element_b_id": "1805.05859_figure_2",
      "element_a_type": "formula",
      "element_b_type": "figure",
      "pair_type": "figure+formula",
      "hop_distance": 1,
      "path": [
        "1805.05859_formula_3",
        "1805.05859_figure_2"
      ],
      "quality_score": 0.75,
      "overlap_with_existing_l1": false,
      "element_a": {
        "element_id": "1805.05859_formula_3",
        "element_type": "formula",
        "caption": "",
        "content": "$$\\begin{array}{l} P (\\hat {Y} (a ^ {\\prime}, X _ {1} (a ^ {\\prime}, X _ {2} (a)), X _ {2} (a)) \\mid X _ {1} = x _ {1}, X _ {2} = x _ {2}, A = a) = \\\\ P (\\hat {Y} / \\left. \\left(\\mathbf {X} _ {1} - \\mathbf {X} _ {2} (\\cdot)\\right), \\mathbf {X} _ {2} (\\cdot)\\right) \\mid \\mathbf {X} _ {1} = \\mathbf {X} _ {2} = A, \\end{array} \\tag {9}$$",
        "image_path": null,
        "context_before": "",
        "context_after": "Our own suggestion for path-specific counterfactual fairness builds directly on the original: just extract latent fair variables from observed variables that are known to be (path-specifically) fair and build a black-box predictor around them. For interpretation, it is easier to include $\\hat { Y }$ in the causal graph (removing $Y$ , which plays no role as an input to $\\hat { Y }$ ), adding edges from all other vertices into $\\hat { Y }$ . Figure 2(a) shows an example with three variables $A ,\n"
      },
      "element_b": {
        "element_id": "1805.05859_figure_2",
        "element_type": "figure",
        "caption": "Figure 2: (a) A causal graph linking protected attribute $A$ to predictor $\\hat { Y }$ , where only a subset of edges will “carry” counterfactual values of $A$ in order to represent the constraints of path-specific counterfactual fairness. (b) This diagram, inspired by [30], is a representation of how counterfactuals are propagated only through some edges. For other edges, inputs are based on the baseline value $a$ of an individual.",
        "content": "Figure 2: (a) A causal graph linking protected attribute $A$ to predictor $\\hat { Y }$ , where only a subset of edges will “carry” counterfactual values of $A$ in order to represent the constraints of path-specific counterfactual fairness. (b) This diagram, inspired by [30], is a representation of how counterfactuals are propagated only through some edges. For other edges, inputs are based on the baseline value $a$ of an individual.",
        "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1805.05859/1805.05859/hybrid_auto/images/1805.05859_page0_fig4.jpg",
        "context_before": "",
        "context_after": "path-specific scenario: we require\n\n$$ \\begin{array}{l} P (\\hat {Y} (a ^ {\\prime}, X _ {1} (a ^ {\\prime}, X _ {2} (a)), X _ {2} (a)) \\mid X _ {1} = x _ {1}, X _ {2} = x _ {2}, A = a) = \\\\ P (\\hat {Y} / \\left. \\left(\\mathbf {X} _ {1} - \\mathbf {X} _ {2} (\\cdot)\\right), \\mathbf {X} _ {2} (\\cdot)\\right) \\mid \\mathbf {X} _ {1} = \\mathbf {X} _ {2} = A, \\end{array} \\tag {9} $$\n\n$$ P (\\tilde {Y} (a, X _ {1} (a, X _ {2} (a)), X _ {2} (a)) \\mid X _ {1} = x _ {1}, X _ {2} = x _ {2}, A = a). $$\n\nOur own su"
      },
      "edge_contexts": [
        {
          "source": "1805.05859_formula_3",
          "target": "1805.05859_figure_2",
          "ref_text": "Figure 2",
          "context_snippet": "put to $\\hat { Y }$ ), adding edges from all other vertices into $\\hat { Y }$ . Figure 2(a) shows an example with three variables $A ,\n\nFigure 2(a) shows an example wit"
        }
      ]
    },
    {
      "pair_id": "1805.05859_pair_2",
      "doc_id": "1805.05859",
      "element_a_id": "1805.05859_formula_4",
      "element_b_id": "1805.05859_figure_2",
      "element_a_type": "formula",
      "element_b_type": "figure",
      "pair_type": "figure+formula",
      "hop_distance": 1,
      "path": [
        "1805.05859_formula_4",
        "1805.05859_figure_2"
      ],
      "quality_score": 0.75,
      "overlap_with_existing_l1": false,
      "element_a": {
        "element_id": "1805.05859_formula_4",
        "element_type": "formula",
        "caption": "",
        "content": "$$P (\\tilde {Y} (a, X _ {1} (a, X _ {2} (a)), X _ {2} (a)) \\mid X _ {1} = x _ {1}, X _ {2} = x _ {2}, A = a).$$",
        "image_path": null,
        "context_before": "",
        "context_after": "Our own suggestion for path-specific counterfactual fairness builds directly on the original: just extract latent fair variables from observed variables that are known to be (path-specifically) fair and build a black-box predictor around them. For interpretation, it is easier to include $\\hat { Y }$ in the causal graph (removing $Y$ , which plays no role as an input to $\\hat { Y }$ ), adding edges from all other vertices into $\\hat { Y }$ . Figure 2(a) shows an example with three variables $A ,\n"
      },
      "element_b": {
        "element_id": "1805.05859_figure_2",
        "element_type": "figure",
        "caption": "Figure 2: (a) A causal graph linking protected attribute $A$ to predictor $\\hat { Y }$ , where only a subset of edges will “carry” counterfactual values of $A$ in order to represent the constraints of path-specific counterfactual fairness. (b) This diagram, inspired by [30], is a representation of how counterfactuals are propagated only through some edges. For other edges, inputs are based on the baseline value $a$ of an individual.",
        "content": "Figure 2: (a) A causal graph linking protected attribute $A$ to predictor $\\hat { Y }$ , where only a subset of edges will “carry” counterfactual values of $A$ in order to represent the constraints of path-specific counterfactual fairness. (b) This diagram, inspired by [30], is a representation of how counterfactuals are propagated only through some edges. For other edges, inputs are based on the baseline value $a$ of an individual.",
        "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1805.05859/1805.05859/hybrid_auto/images/1805.05859_page0_fig4.jpg",
        "context_before": "",
        "context_after": "path-specific scenario: we require\n\n$$ \\begin{array}{l} P (\\hat {Y} (a ^ {\\prime}, X _ {1} (a ^ {\\prime}, X _ {2} (a)), X _ {2} (a)) \\mid X _ {1} = x _ {1}, X _ {2} = x _ {2}, A = a) = \\\\ P (\\hat {Y} / \\left. \\left(\\mathbf {X} _ {1} - \\mathbf {X} _ {2} (\\cdot)\\right), \\mathbf {X} _ {2} (\\cdot)\\right) \\mid \\mathbf {X} _ {1} = \\mathbf {X} _ {2} = A, \\end{array} \\tag {9} $$\n\n$$ P (\\tilde {Y} (a, X _ {1} (a, X _ {2} (a)), X _ {2} (a)) \\mid X _ {1} = x _ {1}, X _ {2} = x _ {2}, A = a). $$\n\nOur own su"
      },
      "edge_contexts": [
        {
          "source": "1805.05859_formula_4",
          "target": "1805.05859_figure_2",
          "ref_text": "Figure 2",
          "context_snippet": "put to $\\hat { Y }$ ), adding edges from all other vertices into $\\hat { Y }$ . Figure 2(a) shows an example with three variables $A ,\n\nFigure 2(a) shows an example wit"
        }
      ]
    },
    {
      "pair_id": "1811.00103_pair_1",
      "doc_id": "1811.00103",
      "element_a_id": "1811.00103_formula_1",
      "element_b_id": "1811.00103_figure_1",
      "element_a_type": "formula",
      "element_b_type": "figure",
      "pair_type": "figure+formula",
      "hop_distance": 1,
      "path": [
        "1811.00103_formula_1",
        "1811.00103_figure_1"
      ],
      "quality_score": 0.75,
      "overlap_with_existing_l1": false,
      "element_a": {
        "element_id": "1811.00103_formula_1",
        "element_type": "formula",
        "caption": "",
        "content": "$$e r r o r (Y, Z) = \\| Y - Z \\| _ {F} ^ {2}.$$",
        "image_path": null,
        "context_before": "Results We focus on projections into relatively few dimensions, as those are used ubiquitously in early phases of data exploration. As we already saw in Figure 1 left, at lower dimensions, there is a noticeable gap between PCA’s average reconstruction error for men and women on the LFW data set. This gap is at the scale of up to $10 \\%$ of the total reconstruction error when we project to 20 dimensions. This still holds when we subsample male and female faces with equal probability from the data",
        "context_after": ""
      },
      "element_b": {
        "element_id": "1811.00103_figure_1",
        "element_type": "figure",
        "caption": "Average reconstruction error (RE) of PCA on LFW (resampled) Figure 1: Left: Average reconstruction error of PCA on labeled faces in the wild data set (LFW), separated by gender. Right: The same, but sampling 1000 faces with men and women equiprobably (mean over 20 samples).",
        "content": "Average reconstruction error (RE) of PCA on LFW (resampled) Figure 1: Left: Average reconstruction error of PCA on labeled faces in the wild data set (LFW), separated by gender. Right: The same, but sampling 1000 faces with men and women equiprobably (mean over 20 samples).",
        "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1811.00103/1811.00103/hybrid_auto/images/1811.00103_page0_fig1.jpg",
        "context_before": "",
        "context_after": "and approaches have significant merit, but form an incomplete picture of the ML pipeline and where unfairness might be introduced therein. Our work takes another step in fleshing out this picture by analyzing when dimensionality reduction might inadvertently introduce bias. We focus on principal component analysis (henceforth PCA), perhaps the most fundamental dimensionality reduction technique in the sciences [Pearson, 1901; Hotelling, 1933; Jolliffe, 1986]. We show several real-world data sets"
      },
      "edge_contexts": [
        {
          "source": "1811.00103_formula_1",
          "target": "1811.00103_figure_1",
          "ref_text": "Figure 1 l",
          "context_snippet": "are used ubiquitously in early phases of data exploration. As we already saw in Figure 1 left, at lower dimensions, there is a noticeable gap between PCA’s average recons"
        }
      ]
    },
    {
      "pair_id": "1811.00103_pair_2",
      "doc_id": "1811.00103",
      "element_a_id": "1811.00103_formula_4",
      "element_b_id": "1811.00103_figure_1",
      "element_a_type": "formula",
      "element_b_type": "figure",
      "pair_type": "figure+formula",
      "hop_distance": 1,
      "path": [
        "1811.00103_formula_4",
        "1811.00103_figure_1"
      ],
      "quality_score": 0.75,
      "overlap_with_existing_l1": false,
      "element_a": {
        "element_id": "1811.00103_formula_4",
        "element_type": "formula",
        "caption": "",
        "content": "$$\\min _ {U \\in \\mathbb {R} ^ {m \\times n}, \\operatorname {r a n k} (U) \\leq d} \\max _ {i \\in \\{1, \\dots , k \\}} \\left\\{\\frac {1}{| A _ {i} |} \\operatorname {l o s s} \\left(A _ {i}, U _ {A _ {i}}\\right)\\right) \\Bigg \\}, \\tag {10}$$",
        "image_path": null,
        "context_before": "",
        "context_after": "We use two common human-centric data sets for our experiments. The first one is labeled faces in the wild (LFW) [Huang et al., 2007], the second is the Default Credit data set [Yeh and Lien, 2009]. We preprocess all data to have its mean at the origin. For the LFW data, we normalized each pixel value by $\\scriptstyle { \\frac { 1 } { 2 5 5 } }$ . The gender information for LFW was taken from Afifi and Abdelhamed [2017], who manually verified the correctness of these labels. For the credit data, s"
      },
      "element_b": {
        "element_id": "1811.00103_figure_1",
        "element_type": "figure",
        "caption": "Average reconstruction error (RE) of PCA on LFW (resampled) Figure 1: Left: Average reconstruction error of PCA on labeled faces in the wild data set (LFW), separated by gender. Right: The same, but sampling 1000 faces with men and women equiprobably (mean over 20 samples).",
        "content": "Average reconstruction error (RE) of PCA on LFW (resampled) Figure 1: Left: Average reconstruction error of PCA on labeled faces in the wild data set (LFW), separated by gender. Right: The same, but sampling 1000 faces with men and women equiprobably (mean over 20 samples).",
        "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1811.00103/1811.00103/hybrid_auto/images/1811.00103_page0_fig1.jpg",
        "context_before": "",
        "context_after": "and approaches have significant merit, but form an incomplete picture of the ML pipeline and where unfairness might be introduced therein. Our work takes another step in fleshing out this picture by analyzing when dimensionality reduction might inadvertently introduce bias. We focus on principal component analysis (henceforth PCA), perhaps the most fundamental dimensionality reduction technique in the sciences [Pearson, 1901; Hotelling, 1933; Jolliffe, 1986]. We show several real-world data sets"
      },
      "edge_contexts": [
        {
          "source": "1811.00103_formula_4",
          "target": "1811.00103_figure_1",
          "ref_text": "Figure 1 l",
          "context_snippet": "are used ubiquitously in early phases of data exploration. As we already saw in Figure 1 left, at lower dimensions, there is a noticeable gap between PCA’s average recons"
        }
      ]
    },
    {
      "pair_id": "1811.00103_pair_3",
      "doc_id": "1811.00103",
      "element_a_id": "1811.00103_formula_4",
      "element_b_id": "1811.00103_figure_3",
      "element_a_type": "formula",
      "element_b_type": "figure",
      "pair_type": "figure+formula",
      "hop_distance": 1,
      "path": [
        "1811.00103_formula_4",
        "1811.00103_figure_3"
      ],
      "quality_score": 0.75,
      "overlap_with_existing_l1": false,
      "element_a": {
        "element_id": "1811.00103_formula_4",
        "element_type": "formula",
        "caption": "",
        "content": "$$\\min _ {U \\in \\mathbb {R} ^ {m \\times n}, \\operatorname {r a n k} (U) \\leq d} \\max _ {i \\in \\{1, \\dots , k \\}} \\left\\{\\frac {1}{| A _ {i} |} \\operatorname {l o s s} \\left(A _ {i}, U _ {A _ {i}}\\right)\\right) \\Bigg \\}, \\tag {10}$$",
        "image_path": null,
        "context_before": "",
        "context_after": "We use two common human-centric data sets for our experiments. The first one is labeled faces in the wild (LFW) [Huang et al., 2007], the second is the Default Credit data set [Yeh and Lien, 2009]. We preprocess all data to have its mean at the origin. For the LFW data, we normalized each pixel value by $\\scriptstyle { \\frac { 1 } { 2 5 5 } }$ . The gender information for LFW was taken from Afifi and Abdelhamed [2017], who manually verified the correctness of these labels. For the credit data, s"
      },
      "element_b": {
        "element_id": "1811.00103_figure_3",
        "element_type": "figure",
        "caption": "Figure 3: Reconstruction error of PCA/Fair PCA on LFW and the Default Credit data set.",
        "content": "Figure 3: Reconstruction error of PCA/Fair PCA on LFW and the Default Credit data set.",
        "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1811.00103/1811.00103/hybrid_auto/images/1811.00103_page0_fig7.jpg",
        "context_before": "Figure 3 shows the average reconstruction error of each population (Male/Female, Higher/Lower education) as the result of running vanilla PCA and Fair PCA on LFW and Credit data. As we expect,\n\nFigure 3 shows the average reconstruction error of each population (Male/Female, Higher/Lower education) as the result of running vanilla PCA and Fair PCA on LFW and Credit data.",
        "context_after": ""
      },
      "edge_contexts": [
        {
          "source": "1811.00103_formula_4",
          "target": "1811.00103_figure_3",
          "ref_text": "Figure 3 s",
          "context_snippet": " women have equal magnitude in the objective function of PCA (Figure 1 right).\n\nFigure 3 shows the average reconstruction error of each population (Male/Female, Higher/Lo"
        }
      ]
    },
    {
      "pair_id": "1902.03519_pair_3",
      "doc_id": "1902.03519",
      "element_a_id": "1902.03519_formula_1",
      "element_b_id": "1902.03519_figure_2",
      "element_a_type": "formula",
      "element_b_type": "figure",
      "pair_type": "figure+formula",
      "hop_distance": 1,
      "path": [
        "1902.03519_formula_1",
        "1902.03519_figure_2"
      ],
      "quality_score": 0.75,
      "overlap_with_existing_l1": false,
      "element_a": {
        "element_id": "1902.03519_formula_1",
        "element_type": "formula",
        "caption": "",
        "content": "$$\\# \\text {p o i n t s} \\bar {c} \\geq \\frac {r}{b} \\sum_ {j \\in Q} b _ {j}. \\tag {3}$$",
        "image_path": null,
        "context_before": "In Figure 2 and both Table 1 and 3, the reported runtime for each sample size $S$ is the median runtime of our algorithm on 10 different sample sets from the given pointset each of size $S$ .\n\nAlthough the number of children of $T ( v )$ can be as large as $\\gamma ^ { d }$ , for each node $v$ in $T$ , MinHeavyPoints performs\n\nTable 1: The table compares the performance of our fairlet-decomposition algorithm and the algorithm of (Chierichetti et al., 2017).\n\nComparing the cost of the solution ret",
        "context_after": ""
      },
      "element_b": {
        "element_id": "1902.03519_figure_2",
        "element_type": "figure",
        "caption": "Figure 2: Each figure captures the running time of our fairlet decomposition algorithms with the specified balance parameter on different number of sample points from one of the four datasets: Diabetes, Bank, Census and Census II.",
        "content": "Figure 2: Each figure captures the running time of our fairlet decomposition algorithms with the specified balance parameter on different number of sample points from one of the four datasets: Diabetes, Bank, Census and Census II.",
        "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1902.03519/1902.03519/hybrid_auto/images/1902.03519_page0_fig9.jpg",
        "context_before": "",
        "context_after": "“balance”, “duration-of-account”) as attributes to represent the dimensions of the points in the space. Moreover, we consider “marital-status” as the sensitive information.\n\n6https://archive.ics.uci.edu/ml/datasets/adult\n\n7https://archive.ics.uci.edu/ml/datasets/US+Census+Data+(1990)\n\nWe complement our theoretical analysis with empirical evaluation. Our experiments show that the quality of the clustering obtained by our algorithm is comparable to that of Chierichetti et al. (2017). At the same t"
      },
      "edge_contexts": [
        {
          "source": "1902.03519_formula_1",
          "target": "1902.03519_figure_2",
          "ref_text": "Figure 2 a",
          "context_snippet": "In Figure 2 and both Table 1 and 3, the reported runtime for each sample size $S$ is the medi"
        }
      ]
    },
    {
      "pair_id": "1905.12843_pair_2",
      "doc_id": "1905.12843",
      "element_a_id": "1905.12843_formula_1",
      "element_b_id": "1905.12843_figure_1",
      "element_a_type": "formula",
      "element_b_type": "figure",
      "pair_type": "figure+formula",
      "hop_distance": 1,
      "path": [
        "1905.12843_formula_1",
        "1905.12843_figure_1"
      ],
      "quality_score": 0.75,
      "overlap_with_existing_l1": false,
      "element_a": {
        "element_id": "1905.12843_formula_1",
        "element_type": "formula",
        "caption": "",
        "content": "$$\\text {f o r a l l} i \\in [ n ]: t _ {i} \\geq \\frac {\\alpha}{2} - Y _ {i} \\langle \\beta , x _ {i} \\rangle ,$$",
        "image_path": null,
        "context_before": "Runtime comparison. We performed a comparison on the running time of a single call of the three supervised learning oracles. On a subsampled law school data set with 1,000 examples, we ran the oracles to solve an instance of the $\\boldsymbol { \\mathrm { B E S T } } _ { h }$ problem, optimizing over either the linear models or tree ensemble models. The details are listed in Table 1. We also compare the number of oracle calls for different specified values of fairness slackness.\n\nWe then evaluated",
        "context_after": ""
      },
      "element_b": {
        "element_id": "1905.12843_figure_1",
        "element_type": "figure",
        "caption": "Figure 1. Relative test loss versus the worst constraint violation with respect to SP. Relative losses are computed by subtracting the smallest baseline loss from the actual loss. For our algorithm and fair classification we plot the convex envelope of the predictors obtained on training data at various accuracy–fairness tradeoffs. We show $9 5 \\%$ confidence bands for the relative loss of our method and fair classification, and also show $9 5 \\%$ confidence intervals for constraint violation (the same for all methods). Our method dominates or matches the baselines up to statistical uncertainty on all datasets except adult, where fair classification is slightly better.",
        "content": "Figure 1. Relative test loss versus the worst constraint violation with respect to SP. Relative losses are computed by subtracting the smallest baseline loss from the actual loss. For our algorithm and fair classification we plot the convex envelope of the predictors obtained on training data at various accuracy–fairness tradeoffs. We show $9 5 \\%$ confidence bands for the relative loss of our method and fair classification, and also show $9 5 \\%$ confidence intervals for constraint violation (the same for all methods). Our method dominates or matches the baselines up to statistical uncertainty on all datasets except adult, where fair classification is slightly better.",
        "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1905.12843/1905.12843/hybrid_auto/images/1905.12843_page0_fig5.jpg",
        "context_before": "",
        "context_after": "from Section 4.4: reductions to cost-sensitive (CS) oracles, least-squares (LS) oracles, and logistic-loss minimization (LR) oracles. Our CS oracle sought the linear model minimizing weighted hinge-loss (as a surrogate for weighted classification error). Because of unfavorable scaling of the cost-sensitive problem sizes (see Section 4.4), we only ran the CS oracle on the three small datasets. We considered two variants of LS and LR oracles: linear learners from scikit-learn (Pedregosa et al., 20"
      },
      "edge_contexts": [
        {
          "source": "1905.12843_formula_1",
          "target": "1905.12843_figure_1",
          "ref_text": "Figure 1",
          "context_snippet": "the selected predictors on the test set, and show the resulting Pareto front in Figure 1.\n\nThe details are listed in Table 1."
        }
      ]
    },
    {
      "pair_id": "1905.12843_pair_4",
      "doc_id": "1905.12843",
      "element_a_id": "1905.12843_formula_2",
      "element_b_id": "1905.12843_figure_1",
      "element_a_type": "formula",
      "element_b_type": "figure",
      "pair_type": "figure+formula",
      "hop_distance": 1,
      "path": [
        "1905.12843_formula_2",
        "1905.12843_figure_1"
      ],
      "quality_score": 0.75,
      "overlap_with_existing_l1": false,
      "element_a": {
        "element_id": "1905.12843_formula_2",
        "element_type": "formula",
        "caption": "",
        "content": "$$f o r \\quad j \\in [ d ]: - 1 \\leq \\beta_ {j} \\leq 1.$$",
        "image_path": null,
        "context_before": "Runtime comparison. We performed a comparison on the running time of a single call of the three supervised learning oracles. On a subsampled law school data set with 1,000 examples, we ran the oracles to solve an instance of the $\\boldsymbol { \\mathrm { B E S T } } _ { h }$ problem, optimizing over either the linear models or tree ensemble models. The details are listed in Table 1. We also compare the number of oracle calls for different specified values of fairness slackness.\n\nWe then evaluated",
        "context_after": ""
      },
      "element_b": {
        "element_id": "1905.12843_figure_1",
        "element_type": "figure",
        "caption": "Figure 1. Relative test loss versus the worst constraint violation with respect to SP. Relative losses are computed by subtracting the smallest baseline loss from the actual loss. For our algorithm and fair classification we plot the convex envelope of the predictors obtained on training data at various accuracy–fairness tradeoffs. We show $9 5 \\%$ confidence bands for the relative loss of our method and fair classification, and also show $9 5 \\%$ confidence intervals for constraint violation (the same for all methods). Our method dominates or matches the baselines up to statistical uncertainty on all datasets except adult, where fair classification is slightly better.",
        "content": "Figure 1. Relative test loss versus the worst constraint violation with respect to SP. Relative losses are computed by subtracting the smallest baseline loss from the actual loss. For our algorithm and fair classification we plot the convex envelope of the predictors obtained on training data at various accuracy–fairness tradeoffs. We show $9 5 \\%$ confidence bands for the relative loss of our method and fair classification, and also show $9 5 \\%$ confidence intervals for constraint violation (the same for all methods). Our method dominates or matches the baselines up to statistical uncertainty on all datasets except adult, where fair classification is slightly better.",
        "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1905.12843/1905.12843/hybrid_auto/images/1905.12843_page0_fig5.jpg",
        "context_before": "",
        "context_after": "from Section 4.4: reductions to cost-sensitive (CS) oracles, least-squares (LS) oracles, and logistic-loss minimization (LR) oracles. Our CS oracle sought the linear model minimizing weighted hinge-loss (as a surrogate for weighted classification error). Because of unfavorable scaling of the cost-sensitive problem sizes (see Section 4.4), we only ran the CS oracle on the three small datasets. We considered two variants of LS and LR oracles: linear learners from scikit-learn (Pedregosa et al., 20"
      },
      "edge_contexts": [
        {
          "source": "1905.12843_formula_2",
          "target": "1905.12843_figure_1",
          "ref_text": "Figure 1",
          "context_snippet": "the selected predictors on the test set, and show the resulting Pareto front in Figure 1.\n\nThe details are listed in Table 1."
        }
      ]
    },
    {
      "pair_id": "1907.12059_pair_3",
      "doc_id": "1907.12059",
      "element_a_id": "1907.12059_formula_4",
      "element_b_id": "1907.12059_figure_1",
      "element_a_type": "formula",
      "element_b_type": "figure",
      "pair_type": "figure+formula",
      "hop_distance": 1,
      "path": [
        "1907.12059_formula_4",
        "1907.12059_figure_1"
      ],
      "quality_score": 0.75,
      "overlap_with_existing_l1": false,
      "element_a": {
        "element_id": "1907.12059_formula_4",
        "element_type": "formula",
        "caption": "",
        "content": "$$\\left| p _ {\\boldsymbol {a}} - \\hat {p} _ {\\boldsymbol {a}} \\right| \\leq \\frac {\\epsilon}{4 | \\mathcal {A} | \\max [ L , 1 ]}. \\tag {11}$$",
        "image_path": null,
        "context_before": "Figure 1 shows overlaying model belief histograms for four demographic groups and their barycenter in the Adult dataset. Wasserstein-1 Penalty effectively matches all group histograms to the barycenter after training for 10,000 steps with $\\beta = 1 0 0$ .\n\n(2016)) using a linear model as the underlying predictor and equal positive prediction rate between each group $\\mathcal { D } _ { a }$ and $\\mathcal { D }$\n\nTable 1: Adult Dataset – German Credit Dataset   \n\n<table><tr><td rowspan=\"2\"></td><",
        "context_after": ""
      },
      "element_b": {
        "element_id": "1907.12059_figure_1",
        "element_type": "figure",
        "caption": "Figure 1: Histograms of model beliefs for groups of Black females, Black males, White females, and White males, and their barycenter on the Adult dataset using Wass-1 Penalty. Top: Initial state. Bottom: After 10,000 training steps with $\\alpha = 0 , \\beta = 1 0 0$ each group histogram matches the barycenter. Figure 2: Err-Exp v.s. SDD, Err-Exp v.s. SPDD trade-off curves on Bank test set using Wass-1 Penalty DB, points plotted every 100 steps over 80,000 total training steps.",
        "content": "Figure 1: Histograms of model beliefs for groups of Black females, Black males, White females, and White males, and their barycenter on the Adult dataset using Wass-1 Penalty. Top: Initial state. Bottom: After 10,000 training steps with $\\alpha = 0 , \\beta = 1 0 0$ each group histogram matches the barycenter. Figure 2: Err-Exp v.s. SDD, Err-Exp v.s. SPDD trade-off curves on Bank test set using Wass-1 Penalty DB, points plotted every 100 steps over 80,000 total training steps.",
        "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1907.12059/1907.12059/hybrid_auto/images/1907.12059_page0_fig9.jpg",
        "context_before": "",
        "context_after": "Err-Exp. Though not always the case, often as the learning model moves towards the fairness goal of SDP, model accuracy decreases (Err-Exp increases).\n\nWe introduced an approach to ensure that the output of a classification system does not depend on sensitive information using the Wasserstein-1 distance. We demon-\n\nstrated that using the Wasserstein-1 barycenter enables us to reach independence with minimal modifications of the model decisions. We introduced two methods with different desirable "
      },
      "edge_contexts": [
        {
          "source": "1907.12059_formula_4",
          "target": "1907.12059_figure_1",
          "ref_text": "Figure 1 s",
          "context_snippet": "Figure 1 shows overlaying model belief histograms for four demographic groups and their ba"
        }
      ]
    },
    {
      "pair_id": "1907.12059_pair_5",
      "doc_id": "1907.12059",
      "element_a_id": "1907.12059_formula_5",
      "element_b_id": "1907.12059_figure_1",
      "element_a_type": "formula",
      "element_b_type": "figure",
      "pair_type": "figure+formula",
      "hop_distance": 1,
      "path": [
        "1907.12059_formula_5",
        "1907.12059_figure_1"
      ],
      "quality_score": 0.75,
      "overlap_with_existing_l1": false,
      "element_a": {
        "element_id": "1907.12059_formula_5",
        "element_type": "formula",
        "caption": "",
        "content": "$$\\sum_ {\\boldsymbol {a} \\in \\mathcal {A}} p _ {\\boldsymbol {a}} \\mathcal {W} _ {1} \\left(p _ {S _ {\\boldsymbol {a}}}, p _ {\\bar {S}}\\right) \\leq \\sum_ {\\boldsymbol {a} \\in \\mathcal {A}} \\hat {p} _ {\\boldsymbol {a}} \\mathcal {W} _ {1} \\left(\\hat {p} _ {S _ {\\boldsymbol {a}}}, \\hat {p} _ {\\bar {S}}\\right) + \\epsilon .$$",
        "image_path": null,
        "context_before": "Figure 1 shows overlaying model belief histograms for four demographic groups and their barycenter in the Adult dataset. Wasserstein-1 Penalty effectively matches all group histograms to the barycenter after training for 10,000 steps with $\\beta = 1 0 0$ .\n\n(2016)) using a linear model as the underlying predictor and equal positive prediction rate between each group $\\mathcal { D } _ { a }$ and $\\mathcal { D }$\n\nTable 1: Adult Dataset – German Credit Dataset   \n\n<table><tr><td rowspan=\"2\"></td><",
        "context_after": ""
      },
      "element_b": {
        "element_id": "1907.12059_figure_1",
        "element_type": "figure",
        "caption": "Figure 1: Histograms of model beliefs for groups of Black females, Black males, White females, and White males, and their barycenter on the Adult dataset using Wass-1 Penalty. Top: Initial state. Bottom: After 10,000 training steps with $\\alpha = 0 , \\beta = 1 0 0$ each group histogram matches the barycenter. Figure 2: Err-Exp v.s. SDD, Err-Exp v.s. SPDD trade-off curves on Bank test set using Wass-1 Penalty DB, points plotted every 100 steps over 80,000 total training steps.",
        "content": "Figure 1: Histograms of model beliefs for groups of Black females, Black males, White females, and White males, and their barycenter on the Adult dataset using Wass-1 Penalty. Top: Initial state. Bottom: After 10,000 training steps with $\\alpha = 0 , \\beta = 1 0 0$ each group histogram matches the barycenter. Figure 2: Err-Exp v.s. SDD, Err-Exp v.s. SPDD trade-off curves on Bank test set using Wass-1 Penalty DB, points plotted every 100 steps over 80,000 total training steps.",
        "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1907.12059/1907.12059/hybrid_auto/images/1907.12059_page0_fig9.jpg",
        "context_before": "",
        "context_after": "Err-Exp. Though not always the case, often as the learning model moves towards the fairness goal of SDP, model accuracy decreases (Err-Exp increases).\n\nWe introduced an approach to ensure that the output of a classification system does not depend on sensitive information using the Wasserstein-1 distance. We demon-\n\nstrated that using the Wasserstein-1 barycenter enables us to reach independence with minimal modifications of the model decisions. We introduced two methods with different desirable "
      },
      "edge_contexts": [
        {
          "source": "1907.12059_formula_5",
          "target": "1907.12059_figure_1",
          "ref_text": "Figure 1 s",
          "context_snippet": "Figure 1 shows overlaying model belief histograms for four demographic groups and their ba"
        }
      ]
    },
    {
      "pair_id": "1809.10083_pair_3",
      "doc_id": "1809.10083",
      "element_a_id": "1809.10083_formula_1",
      "element_b_id": "1809.10083_table_1",
      "element_a_type": "formula",
      "element_b_type": "table",
      "pair_type": "formula+table",
      "hop_distance": 1,
      "path": [
        "1809.10083_formula_1",
        "1809.10083_table_1"
      ],
      "quality_score": 0.825,
      "overlap_with_existing_l1": false,
      "element_a": {
        "element_id": "1809.10083_formula_1",
        "element_type": "formula",
        "caption": "",
        "content": "$$\\begin{array}{l} L = \\alpha L _ {p r e d} (y, P r e d (e _ {1})) + \\beta L _ {d e c} (x, D e c (\\psi (e _ {1}), e _ {2})) + \\gamma L _ {d i s} ((e _ {1}, e _ {2})) \\\\ = \\alpha L _ {p r e d} (y, P r e d (E n c (x) _ {1})) + \\beta L _ {d e c} (x, D e c (\\psi (E n c (x) _ {1}), E n c (x) _ {2})) + \\gamma L _ {d i s} (E n c (x)) \\tag {1} \\\\ \\end{array}$$",
        "image_path": null,
        "context_before": "",
        "context_after": "Table 1 summarizes the results. The proposed unsupervised method outperforms existing state-of-theart (supervised) invariance induction methods on both $A _ { y }$ and $A _ { z }$ metrics, providing a significant boost on $A _ { y }$ and complete removal of lighting information from $e _ { 1 }$ reflected by $A _ { z }$ . Furthermore, the accuracy of predicting $z$ from $e _ { 2 }$ is 0.89, which validates its automatic migration to $e _ { 2 }$ . Figure 2 shows t-SNE [15] visualization of raw dat"
      },
      "element_b": {
        "element_id": "1809.10083_table_1",
        "element_type": "table",
        "caption": "Table 1: Results on Extended Yale-B dataset   ",
        "content": "<table><tr><td>Metric</td><td>NN + MMD [13]</td><td>VFAE [14]</td><td>CAI [19]</td><td>Ours</td></tr><tr><td>Accuracy of predicting y from e1 (Ay)</td><td>0.82</td><td>0.85</td><td>0.89</td><td>0.95</td></tr><tr><td>Accuracy of predicting z from e1 (Az)</td><td>-</td><td>0.57</td><td>0.57</td><td>0.24</td></tr></table>",
        "image_path": null,
        "context_before": "player model ( $M _ { 1 }$ or $M _ { 2 }$ ) when we update the weights of the other. $M _ { 2 }$ should ideally be trained to convergence before updating $M _ { 1 }$ in each training epoch to backpropagate accurate and stable disentanglement-inducing gradients to Enc. However, this is not scalable in practice. We update $M _ { 1 }$ and $M _ { 2 }$ in the frequency ratio of $1 : k$ . We found $k = 5$ to perform well in our experiments.\n\nCompetition between prediction and reconstruction. The predi",
        "context_after": "Table 1 summarizes the results.\n\n5.1 Invariance to inherent nuisance factors\n\nWe provide results of our framework at the task of learning invariance to inherent nuisance factors on two datasets – Extended Yale-B [7] and Chairs [2].\n\nExtended Yale-B. This dataset contains face-images of 38 subjects under various lighting conditions. The target $y$ is the subject identity whereas the inherent nuisance factor $z$ is the lighting condition. We compare our framework to existing state-of-the-art super"
      },
      "edge_contexts": [
        {
          "source": "1809.10083_formula_1",
          "target": "1809.10083_table_1",
          "ref_text": "Table 1",
          "context_snippet": "Table 1 summarizes the results. The proposed unsupervised method outperforms existing s"
        }
      ]
    },
    {
      "pair_id": "1809.10083_pair_6",
      "doc_id": "1809.10083",
      "element_a_id": "1809.10083_formula_2",
      "element_b_id": "1809.10083_table_1",
      "element_a_type": "formula",
      "element_b_type": "table",
      "pair_type": "formula+table",
      "hop_distance": 1,
      "path": [
        "1809.10083_formula_2",
        "1809.10083_table_1"
      ],
      "quality_score": 0.825,
      "overlap_with_existing_l1": false,
      "element_a": {
        "element_id": "1809.10083_formula_2",
        "element_type": "formula",
        "caption": "",
        "content": "$$\\begin{array}{l} \\min _ {E n c, P r e d, D e c} \\max _ {D i s _ {1}, D i s _ {2}} J (E n c, P r e d, D e c, D i s _ {1}, D i s _ {2}); \\text {w h e r e :} \\\\ J (E n c, P r e d, D e c, D i s _ {1}, D i s _ {2}) \\\\ = \\alpha L _ {p r e d} (y, P r e d (e _ {1})) + \\beta L _ {d e c} (x, D e c (\\psi (e _ {1}), e _ {2})) + \\gamma \\tilde {L} _ {d i s} ((e _ {1}, e _ {2})) \\\\ = \\alpha L _ {p r e d} \\left(y, P r e d \\left(E n c (x) _ {1}\\right)\\right) + \\beta L _ {d e c} \\left(x, D e c \\left(\\psi \\left(E n c (x) _ {1}\\right), E n c (x) _ {2}\\right)\\right) \\\\ + \\gamma \\left\\{\\tilde {L} _ {d i s _ {1}} \\left(E n c (x) _ {2}, D i s _ {1} \\left(E n c (x) _ {1}\\right)\\right) + \\tilde {L} _ {d i s _ {2}} \\left(E n c (x) _ {1}, D i s _ {2} \\left(E n c (x) _ {2}\\right)\\right) \\right\\} \\tag {2} \\\\ \\end{array}$$",
        "image_path": null,
        "context_before": "Table 1 summarizes the results. The proposed unsupervised method outperforms existing state-of-theart (supervised) invariance induction methods on both $A _ { y }$ and $A _ { z }$ metrics, providing a significant boost on $A _ { y }$ and complete removal of lighting information from $e _ { 1 }$ reflected by $A _ { z }$ . Furthermore, the accuracy of predicting $z$ from $e _ { 2 }$ is 0.89, which validates its automatic migration to $e _ { 2 }$ . Figure 2 shows t-SNE [15] visualization of raw dat",
        "context_after": ""
      },
      "element_b": {
        "element_id": "1809.10083_table_1",
        "element_type": "table",
        "caption": "Table 1: Results on Extended Yale-B dataset   ",
        "content": "<table><tr><td>Metric</td><td>NN + MMD [13]</td><td>VFAE [14]</td><td>CAI [19]</td><td>Ours</td></tr><tr><td>Accuracy of predicting y from e1 (Ay)</td><td>0.82</td><td>0.85</td><td>0.89</td><td>0.95</td></tr><tr><td>Accuracy of predicting z from e1 (Az)</td><td>-</td><td>0.57</td><td>0.57</td><td>0.24</td></tr></table>",
        "image_path": null,
        "context_before": "player model ( $M _ { 1 }$ or $M _ { 2 }$ ) when we update the weights of the other. $M _ { 2 }$ should ideally be trained to convergence before updating $M _ { 1 }$ in each training epoch to backpropagate accurate and stable disentanglement-inducing gradients to Enc. However, this is not scalable in practice. We update $M _ { 1 }$ and $M _ { 2 }$ in the frequency ratio of $1 : k$ . We found $k = 5$ to perform well in our experiments.\n\nCompetition between prediction and reconstruction. The predi",
        "context_after": "Table 1 summarizes the results.\n\n5.1 Invariance to inherent nuisance factors\n\nWe provide results of our framework at the task of learning invariance to inherent nuisance factors on two datasets – Extended Yale-B [7] and Chairs [2].\n\nExtended Yale-B. This dataset contains face-images of 38 subjects under various lighting conditions. The target $y$ is the subject identity whereas the inherent nuisance factor $z$ is the lighting condition. We compare our framework to existing state-of-the-art super"
      },
      "edge_contexts": [
        {
          "source": "1809.10083_formula_2",
          "target": "1809.10083_table_1",
          "ref_text": "Table 1",
          "context_snippet": "Table 1 summarizes the results. The proposed unsupervised method outperforms existing s"
        }
      ]
    },
    {
      "pair_id": "1611.07509_pair_2",
      "doc_id": "1611.07509",
      "element_a_id": "1611.07509_formula_1",
      "element_b_id": "1611.07509_table_1",
      "element_a_type": "formula",
      "element_b_type": "table",
      "pair_type": "formula+table",
      "hop_distance": 1,
      "path": [
        "1611.07509_formula_1",
        "1611.07509_table_1"
      ],
      "quality_score": 0.7749999999999999,
      "overlap_with_existing_l1": false,
      "element_a": {
        "element_id": "1611.07509_formula_1",
        "element_type": "formula",
        "caption": "",
        "content": "$$S E _ {\\pi} \\left(x _ {2}, x _ {1}\\right) = P (y \\mid d o \\left(x _ {2} \\mid_ {\\pi}\\right)) - P (y \\mid d o \\left(x _ {1}\\right)).$$",
        "image_path": null,
        "context_before": "When applied to the example in Figure 1, it means the expected change in loan approval of applications actually from the disadvantage group (e.g., black), when the bank is instructed to treat the applicants as from the advantage group (e.g., white).\n\nWhen applied to the example in Figure 1, it means the expected change in loan approval of the disadvantage group if they had the same racial makeups shown in the Zip code as the advantage group.\n\nχThe results are shown in Table 1.",
        "context_after": ""
      },
      "element_b": {
        "element_id": "1611.07509_table_1",
        "element_type": "table",
        "caption": "Table 1: Direct/indirect discriminatory effects in the modified training data and predictions for the testing data. Values violating the discrimination criterion are marked in bold.",
        "content": "Table 1: Direct/indirect discriminatory effects in the modified training data and predictions for the testing data. Values violating the discrimination criterion are marked in bold.",
        "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1611.07509/1611.07509/hybrid_auto/images/fdd1004c513835da188353f3f94f92476f68856a2ce4d146ecfe558bdc171fea.jpg",
        "context_before": "The Dutch dataset consists of 60421 tuples with 12 attributes. Similarly, we binarize the domain values of attribute age due to its large domain size. Three tiers are used in the partial order for temporal priority: sex, age, country birth are defined in the first tire, edu level and marital status are defined in the second tire, and all other attributes are defined in the third tire. The causal graph is shown in Figure 4. Similarly we treat sex as the protected attribute, occupation as the deci",
        "context_after": "represents full discrimination removal. However, has no λdirect connection with the threshold . In our experiments, we execute $D I$ τmultiple times with different s and report the one that is closest to achieve $\\tau \\ : = \\ : 0 . 0 5$ λ. As shown in τ .the column “DI”, it indeed removes direct and indirect discrimination from the training data. However, as indicated by the bold values 0.167/0.168, significant amount of indirect discrimination exists in the predictions of both classifiers. In a"
      },
      "edge_contexts": [
        {
          "source": "1611.07509_formula_1",
          "target": "1611.07509_table_1",
          "ref_text": "Table 1",
          "context_snippet": "akeups shown in the Zip code as the advantage group.\n\nχThe results are shown in Table 1."
        }
      ]
    },
    {
      "pair_id": "1808.08166_pair_3",
      "doc_id": "1808.08166",
      "element_a_id": "1808.08166_formula_1",
      "element_b_id": "1808.08166_table_1",
      "element_a_type": "formula",
      "element_b_type": "table",
      "pair_type": "formula+table",
      "hop_distance": 1,
      "path": [
        "1808.08166_formula_1",
        "1808.08166_table_1"
      ],
      "quality_score": 0.7749999999999999,
      "overlap_with_existing_l1": false,
      "element_a": {
        "element_id": "1808.08166_formula_1",
        "element_type": "formula",
        "caption": "",
        "content": "$$\\hat {h} \\in \\underset {h \\in \\mathcal {H}} {\\operatorname {a r g m i n}} \\sum_ {i = 1} ^ {n} \\left[ h \\left(X _ {i}\\right) c _ {i} ^ {1} + \\left(1 - h \\left(X _ {i}\\right)\\right) c _ {i} ^ {0} \\right] \\tag {1}$$",
        "image_path": null,
        "context_before": "The properties of these datasets are summarized in Table 1, including the number of instances, the prediction being made, the overall number of features (which varies from 10 to 128), the number of protected features in the subgroup class (which varies from 3 to 18), the nature of the protected features, and the baseline (majority class) error rate.\n\nThey illustrate the idea with the following toy example shown in Figure 1, described as follows.\n\nThe heuristic finds a linear threshold function a",
        "context_after": ""
      },
      "element_b": {
        "element_id": "1808.08166_table_1",
        "element_type": "table",
        "caption": "Table 1: Description of Data Sets.",
        "content": "Table 1: Description of Data Sets.",
        "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1808.08166/1808.08166/hybrid_auto/images/0316712cab10fb743166e741482c718fc19c00e3e8610fd406a4a41dd4162e0a.jpg",
        "context_before": "They illustrate the idea with the following toy example shown in Figure 1, described as follows.\n\nThe heuristic finds a linear threshold function as follows:\n\nTable 1: Description of Data Sets.\n\n$$ \\hat {h} \\in \\underset {h \\in \\mathcal {H}} {\\operatorname {a r g m i n}} \\sum_ {i = 1} ^ {n} \\left[ h \\left(X _ {i}\\right) c _ {i} ^ {1} + \\left(1 - h \\left(X _ {i}\\right)\\right) c _ {i} ^ {0} \\right] \\tag {1} $$\n\nFollowing both Agarwal et al. [2018] and Kearns et al. [2018], in all of the experiment",
        "context_after": "We leave the precise descriptions of the algorithm from Kearns et al. [2018] — which we will refer to as the SUBGROUP algorithm — to the appendix. We refer the reader to Kearns et al. [2018] for details about its derivation and guarantees.2 At this point we remark only that the algorithm operates by expressing the optimization problem to be solved (minimize error, subject to subgroup fairness constraints) as solving for the equilibrium in a two player zero-sum game, between a Learner and an Audi"
      },
      "edge_contexts": [
        {
          "source": "1808.08166_formula_1",
          "target": "1808.08166_table_1",
          "ref_text": "Table 1",
          "context_snippet": "The properties of these datasets are summarized in Table 1, including the number of instances, the prediction being made, the overall numb"
        }
      ]
    },
    {
      "pair_id": "1902.03519_pair_4",
      "doc_id": "1902.03519",
      "element_a_id": "1902.03519_formula_1",
      "element_b_id": "1902.03519_table_1",
      "element_a_type": "formula",
      "element_b_type": "table",
      "pair_type": "formula+table",
      "hop_distance": 1,
      "path": [
        "1902.03519_formula_1",
        "1902.03519_table_1"
      ],
      "quality_score": 0.7749999999999999,
      "overlap_with_existing_l1": false,
      "element_a": {
        "element_id": "1902.03519_formula_1",
        "element_type": "formula",
        "caption": "",
        "content": "$$\\# \\text {p o i n t s} \\bar {c} \\geq \\frac {r}{b} \\sum_ {j \\in Q} b _ {j}. \\tag {3}$$",
        "image_path": null,
        "context_before": "In Figure 2 and both Table 1 and 3, the reported runtime for each sample size $S$ is the median runtime of our algorithm on 10 different sample sets from the given pointset each of size $S$ .\n\nAlthough the number of children of $T ( v )$ can be as large as $\\gamma ^ { d }$ , for each node $v$ in $T$ , MinHeavyPoints performs\n\nTable 1: The table compares the performance of our fairlet-decomposition algorithm and the algorithm of (Chierichetti et al., 2017).\n\nComparing the cost of the solution ret",
        "context_after": ""
      },
      "element_b": {
        "element_id": "1902.03519_table_1",
        "element_type": "table",
        "caption": "Table 1: The table compares the performance of our fairlet-decomposition algorithm and the algorithm of (Chierichetti et al., 2017). We remark that the number for (Chierichetti et al., 2017) mentioned in this table are not explicitly stated in their paper and we have extracted them from Figure 3 in their paper. Note that the cost denotes the total distances of the points to their fairlet/cluster centroids.",
        "content": "Table 1: The table compares the performance of our fairlet-decomposition algorithm and the algorithm of (Chierichetti et al., 2017). We remark that the number for (Chierichetti et al., 2017) mentioned in this table are not explicitly stated in their paper and we have extracted them from Figure 3 in their paper. Note that the cost denotes the total distances of the points to their fairlet/cluster centroids.",
        "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1902.03519/1902.03519/hybrid_auto/images/102e9d8de3b4a4062b1e1a379b8059afbe981b23412bb83be4a6b52fcd960bcb.jpg",
        "context_before": "Although the number of children of $T ( v )$ can be as large as $\\gamma ^ { d }$ , for each node $v$ in $T$ , MinHeavyPoints performs\n\nTable 1: The table compares the performance of our fairlet-decomposition algorithm and the algorithm of (Chierichetti et al., 2017).\n\nComparing the cost of the solution returned by our fairlet decomposition algorithm with the result of (Chierichetti et al., 2017) (as in Table 1) shows that we achieve empirical improvements on all instances.\n\n$$ \\# \\text {p o i n ",
        "context_after": "$O ( 1 )$ operations on the number of red and blue points in $T ( v )$ exactly twice: when it is called on $v$ and the parent of $v$ . Hence, in total MinHeavyPoints performs $O ( 1 )$ time on each node in $T$ which in total is $O ( n )$ .\n\n$\\mathbf { A l g o r i t h m 6 } \\ \\mathrm { N O N S A T U R F A I R L E T } ( N _ { r } , N _ { b } , r , b )$ : returns the non-saturated fairlet in a set with $( N _ { r } , N _ { b } )$ points.\n\nIn this section we show the performance of our proposed algo"
      },
      "edge_contexts": [
        {
          "source": "1902.03519_formula_1",
          "target": "1902.03519_table_1",
          "ref_text": "Table 1",
          "context_snippet": "In Figure 2 and both Table 1 and 3, the reported runtime for each sample size $S$ is the median runtime of o"
        }
      ]
    },
    {
      "pair_id": "1902.07823_pair_3",
      "doc_id": "1902.07823",
      "element_a_id": "1902.07823_formula_4",
      "element_b_id": "1902.07823_table_1",
      "element_a_type": "formula",
      "element_b_type": "table",
      "pair_type": "formula+table",
      "hop_distance": 1,
      "path": [
        "1902.07823_formula_4",
        "1902.07823_table_1"
      ],
      "quality_score": 0.7749999999999999,
      "overlap_with_existing_l1": false,
      "element_a": {
        "element_id": "1902.07823_formula_4",
        "element_type": "formula",
        "caption": "",
        "content": "$$| T | \\cdot \\operatorname * {P r} _ {S, S ^ {\\prime} \\sim \\mathfrak {S} ^ {N}, X \\sim T, \\mathcal {A}} \\left[ \\mathrm {I} \\left[ \\mathcal {A} _ {S} (X) \\geq 0 \\right] \\neq \\mathrm {I} \\left[ \\mathcal {A} _ {S ^ {\\prime}} (X) \\geq 0 \\right] \\right].$$",
        "image_path": null,
        "context_before": "Our simulations indicate that introducing a stability-focused regularization term can make the algorithm more stable by slightly sacrificing accuracy. Table 1 summarizes the accuracy\n\nAdult dataset,race attribute   \nFigure 1: stab vs. $\\lambda$ for race attribute.\n\nTable 1 summarizes the accuracy\n\nand fairness metric under different regularization parameters $\\lambda$ .",
        "context_after": ""
      },
      "element_b": {
        "element_id": "1902.07823_table_1",
        "element_type": "table",
        "caption": "Table 1: The performance (mean and standard deviation in parenthesis), of KAAS-St and ZVRG-St with respect to accuracy and the fairness metrics $\\gamma$ on the Adult dataset with race/sex attribute.",
        "content": "Table 1: The performance (mean and standard deviation in parenthesis), of KAAS-St and ZVRG-St with respect to accuracy and the fairness metrics $\\gamma$ on the Adult dataset with race/sex attribute.",
        "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1902.07823/1902.07823/hybrid_auto/images/5fbafdf67f8b6822ae2ce61ab4c05294f70538e986454198fd30e0620f0cad5c.jpg",
        "context_before": "2.3 The stable and fair optimization problem\n\nOur goal is to design fair classification algorithms that have a uniform stability guarantee. We focus on extending fair classification algorithms that are formulated as constrained empirical risk minimization problem over the collection $\\mathcal { F }$ of classifiers that is a reproducing kernel Hilbert space (RKHS), e.g., [77, 78, 34]; see the following program.\n\n$$ \\min _ {f \\in \\mathcal {F}} \\frac {1}{N} \\sum_ {i \\in [ N ]} L (f, s _ {i}) \\quad ",
        "context_after": "5.1 Empirical setting\n\nAlgorithms and baselines. We select three fair classification algorithms designed to ensure statistical parity that can be formulated in the convex optimization framework of Program (ConFair). We choose ZVRG [77] since it is reported to achieve the better fairness than, and comparable accuracy to, other algorithms [32]. We also select KAAS [43] and GYF [34] as representatives of algorithms that are formulated as Program (RegFair). Specifically, [34] showed that the perform"
      },
      "edge_contexts": [
        {
          "source": "1902.07823_formula_4",
          "target": "1902.07823_table_1",
          "ref_text": "Table 1",
          "context_snippet": "ation term can make the algorithm more stable by slightly sacrificing accuracy. Table 1 summarizes the accuracy\n\nAdult dataset,race attribute   \nFigure 1: stab vs. $\\l"
        }
      ]
    },
    {
      "pair_id": "1903.10561_pair_2",
      "doc_id": "1903.10561",
      "element_a_id": "1903.10561_formula_1",
      "element_b_id": "1903.10561_table_1",
      "element_a_type": "formula",
      "element_b_type": "table",
      "pair_type": "formula+table",
      "hop_distance": 1,
      "path": [
        "1903.10561_formula_1",
        "1903.10561_table_1"
      ],
      "quality_score": 0.7749999999999999,
      "overlap_with_existing_l1": false,
      "element_a": {
        "element_id": "1903.10561_formula_1",
        "element_type": "formula",
        "caption": "",
        "content": "$$\\begin{array}{l} s (X, Y, A, B) = \\left[ \\sum_ {x \\in X} s (x, A, B) - \\right. \\\\ \\left. \\sum_ {y \\in Y} s (y, A, B) \\right], \\\\ \\end{array}$$",
        "image_path": null,
        "context_before": "",
        "context_after": "To extend a word-level test to sentence contexts, we slot each word into each of several semantically bleached sentence templates such as “This is <word>.”, “<word> is here.”, “This will <word>.”, and “<word> are things.”. These templates make heavy use of deixis and are designed to convey little specific meaning beyond that of the terms inserted into them.2 For example, the word version of Caliskan Test 3 is illustrated in Table 1 and the sentence version is illustrated in Table 2. We choose th"
      },
      "element_b": {
        "element_id": "1903.10561_table_1",
        "element_type": "table",
        "caption": "Table 1: Subsets of target concepts and attributes from Caliskan Test 3. Concept and attribute names are in italics. The test compares the strength of association between the two target concepts and two attributes, where all four are represented as sets of words.",
        "content": "Table 1: Subsets of target concepts and attributes from Caliskan Test 3. Concept and attribute names are in italics. The test compares the strength of association between the two target concepts and two attributes, where all four are represented as sets of words.",
        "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1903.10561/1903.10561/hybrid_auto/images/5dea010797d851115cf930abfa604dd292de67088759d862a96f682b16295ba2.jpg",
        "context_before": "We find varying evidence of human-like bias in sentence encoders using SEAT. Sentence-tovector encoders largely exhibit the angry black woman stereotype and Caliskan biases, and to a lesser degree the double bind biases. Recent sentence encoders such as BERT (Devlin et al., 2018) display limited evidence of the tested biases. However, while SEAT can confirm the existence of bias, negative results do not indicate the model is bias-free. Furthermore, discrepancies in the results suggest that the c",
        "context_after": "To extend a word-level test to sentence contexts, we slot each word into each of several semantically bleached sentence templates such as “This is <word>.”, “<word> is here.”, “This will <word>.”, and “<word> are things.”. These templates make heavy use of deixis and are designed to convey little specific meaning beyond that of the terms inserted into them.2 For example, the word version of Caliskan Test 3 is illustrated in Table 1 and the sentence version is illustrated in Table 2. We choose th"
      },
      "edge_contexts": [
        {
          "source": "1903.10561_formula_1",
          "target": "1903.10561_table_1",
          "ref_text": "Table 1",
          "context_snippet": " into them.2 For example, the word version of Caliskan Test 3 is illustrated in Table 1 and the sentence version is illustrated in Table 2. We choose th\n\n<table><tr><t"
        }
      ]
    },
    {
      "pair_id": "1903.10561_pair_3",
      "doc_id": "1903.10561",
      "element_a_id": "1903.10561_formula_1",
      "element_b_id": "1903.10561_table_2",
      "element_a_type": "formula",
      "element_b_type": "table",
      "pair_type": "formula+table",
      "hop_distance": 1,
      "path": [
        "1903.10561_formula_1",
        "1903.10561_table_2"
      ],
      "quality_score": 0.7749999999999999,
      "overlap_with_existing_l1": false,
      "element_a": {
        "element_id": "1903.10561_formula_1",
        "element_type": "formula",
        "caption": "",
        "content": "$$\\begin{array}{l} s (X, Y, A, B) = \\left[ \\sum_ {x \\in X} s (x, A, B) - \\right. \\\\ \\left. \\sum_ {y \\in Y} s (y, A, B) \\right], \\\\ \\end{array}$$",
        "image_path": null,
        "context_before": "",
        "context_after": "To extend a word-level test to sentence contexts, we slot each word into each of several semantically bleached sentence templates such as “This is <word>.”, “<word> is here.”, “This will <word>.”, and “<word> are things.”. These templates make heavy use of deixis and are designed to convey little specific meaning beyond that of the terms inserted into them.2 For example, the word version of Caliskan Test 3 is illustrated in Table 1 and the sentence version is illustrated in Table 2. We choose th"
      },
      "element_b": {
        "element_id": "1903.10561_table_2",
        "element_type": "table",
        "caption": "Table 2: Subsets of target concepts and attributes from the bleached sentence version of Caliskan Test 3.",
        "content": "Table 2: Subsets of target concepts and attributes from the bleached sentence version of Caliskan Test 3.",
        "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1903.10561/1903.10561/hybrid_auto/images/c4cbf72f8ad41e4073c0e73a086159d8ca9a9bb59b5ca0d357e71ad268de1ec1.jpg",
        "context_before": "These templates make heavy use of deixis and are designed to convey little specific meaning beyond that of the terms inserted into them.2 For example, the word version of Caliskan Test 3 is illustrated in Table 1 and the sentence version is illustrated in Table 2.\n\nTo measure sentence encoders’ reproduction of the angry black woman stereotype, we create a test whose target concepts are black-identifying and white-identifying female given names from Sweeney (2013, Table 1) and whose attributes ar",
        "context_after": "The Word Embedding Association Test WEAT imitates the human implicit association test (Greenwald et al., 1998) for word embeddings, measuring the association between two sets of target concepts and two sets of attributes. Let $X$ and $Y$ be equal-size sets of target concept embeddings and let $A$ and $B$ be sets of attribute embeddings. The test statistic is a difference between sums over the respective target concepts,\n\n$$ \\begin{array}{l} s (X, Y, A, B) = \\left[ \\sum_ {x \\in X} s (x, A, B) - \\"
      },
      "edge_contexts": [
        {
          "source": "1903.10561_formula_1",
          "target": "1903.10561_table_2",
          "ref_text": "Table 2",
          "context_snippet": "kan Test 3 is illustrated in Table 1 and the sentence version is illustrated in Table 2. We choose th\n\n<table><tr><td>Target Concepts</td><td>Attributes</td></tr><tr><"
        }
      ]
    },
    {
      "pair_id": "1910.10872_pair_3",
      "doc_id": "1910.10872",
      "element_a_id": "1910.10872_formula_1",
      "element_b_id": "1910.10872_table_1",
      "element_a_type": "formula",
      "element_b_type": "table",
      "pair_type": "formula+table",
      "hop_distance": 1,
      "path": [
        "1910.10872_formula_1",
        "1910.10872_table_1"
      ],
      "quality_score": 0.7749999999999999,
      "overlap_with_existing_l1": false,
      "element_a": {
        "element_id": "1910.10872_formula_1",
        "element_type": "formula",
        "caption": "",
        "content": "$$\\frac {\\sum_ {n \\in N _ {f}} f r e q _ {f} (n _ {t y p e} = \\emptyset)}{\\sum_ {n \\in N _ {f}} f r e q _ {f} (n)}$$",
        "image_path": null,
        "context_before": "Our benchmark dataset consists of nine templates listed in Table 1 which are templated sentences that start with the existing names in the census data followed by a sentence that represents a human-like activity. The aim was to test the performance of the NER from different perspectives. Template 1, containing just the name, purely tests the name itself and reveals something about the distribution of the training data. Template 4 is designed to direct the model to tag the name as a person. Templ",
        "context_after": ""
      },
      "element_b": {
        "element_id": "1910.10872_table_1",
        "element_type": "table",
        "caption": "Table 1: Templates that form our benchmark with their corresponding numbers as referenced in the paper. Template 1 is the “no context” template.",
        "content": "Table 1: Templates that form our benchmark with their corresponding numbers as referenced in the paper. Template 1 is the “no context” template.",
        "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1910.10872/1910.10872/hybrid_auto/images/ea2c20b23f7c5ab05f55d12542141bb473bca2528646f38197e4fd3ace4054ec.jpg",
        "context_before": "Figure 1 has more examples with names that are either not recognized as an entity or wrongfully tagged.\n\nOur benchmark dataset consists of nine templates listed in Table 1 which are templated sentences that start with the existing names in the census data followed by a sentence that represents a human-like activity.\n\nThe contributions of this paper are fourfold:\n\narXiv:1910.10872v1 [cs.IR] 24 Oct 2019\n\n1https://github.com/Ninarehm/NERGenderBias",
        "context_after": "4. Finally, we analyzed datasets currently used for training many NER models and found the extent of gender bias in these datasets.\n\nModels and Experiments\n\nTo measure the existence of bias in NER systems, we evaluated five named entity models used in research and industry. We used Flair (Akbik, Blythe, and Vollgraf 2018; Akbik et al. 2019), CoreNLP version 3.9 (Manning et al. 2014; Finkel, Grenager, and Manning 2005), and Spacy version 2.1 with small, medium, and large models.2 We test these mo"
      },
      "edge_contexts": [
        {
          "source": "1910.10872_formula_1",
          "target": "1910.10872_table_1",
          "ref_text": "Table 1",
          "context_snippet": "Our benchmark dataset consists of nine templates listed in Table 1 which are templated sentences that start with the existing names in the census "
        }
      ]
    },
    {
      "pair_id": "1904.03035_pair_1",
      "doc_id": "1904.03035",
      "element_a_id": "1904.03035_formula_1",
      "element_b_id": "1904.03035_table_4",
      "element_a_type": "formula",
      "element_b_type": "table",
      "pair_type": "formula+table",
      "hop_distance": 1,
      "path": [
        "1904.03035_formula_1",
        "1904.03035_table_4"
      ],
      "quality_score": 0.77,
      "overlap_with_existing_l1": false,
      "element_a": {
        "element_id": "1904.03035_formula_1",
        "element_type": "formula",
        "caption": "",
        "content": "$$\\mathcal {L} _ {B} = \\lambda \\| N B \\| _ {F} ^ {2}$$",
        "image_path": null,
        "context_before": "",
        "context_after": "Table 4 shows excerpts around selected target words from the generated corpora to demonstrate the effect of debiasing for different values of $\\lambda$ . We highlight the words crying and fragile that are typically associated with feminine qualities, along\n\nThe detailed analysis is presented in Section 4.3\n\nTable 4 shows excerpts around selected target words from the generated corpora to demonstrate the effect of debiasing for different values of $\\lambda$ .\n\nB Word Level Bias Examples\n\nTables 5"
      },
      "element_b": {
        "element_id": "1904.03035_table_4",
        "element_type": "table",
        "caption": "Table 4: Generated text comparison for CNN/Daily Mail for different $\\lambda$ values",
        "content": "Table 4: Generated text comparison for CNN/Daily Mail for different $\\lambda$ values",
        "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1904.03035/1904.03035/hybrid_auto/images/9ddbc5e5d05a8d78eb2c0c9243d4c466f17287c54da57e7fe000dbf3f6919d15.jpg",
        "context_before": "3.3 Quantifying Biases\n\nFor numeric data, bias can be caused simply by class imbalance, which is relatively easy to quantify and fix. For text and image data, the complexity in the nature of the data increases and it becomes difficult to quantify. Nonetheless, defining relevant metrics is crucial in assessing the bias exhibited in a dataset or in a model’s behavior.\n\n3.3.1 Bias Score Definition\n\nThe difference between the pairs encodes the gender information corresponding to the gender pair. We ",
        "context_after": "Therefore, to reduce the bias learned by the embedding layer in the model, we can add the following bias regularization term to the training loss:\n\n$$ \\mathcal {L} _ {B} = \\lambda \\| N B \\| _ {F} ^ {2} $$\n\nwhere $\\lambda$ controls the importance of minimizing bias in the embedding matrix W (from which $N$ and $B$ are derived) relative to the other components of the model loss. The matrices $N$ and $C$ are updated each iteration during the model training.\n\nTable 4 shows excerpts around selected t"
      },
      "edge_contexts": [
        {
          "source": "1904.03035_formula_1",
          "target": "1904.03035_table_4",
          "ref_text": "Table 4",
          "context_snippet": "Table 4 shows excerpts around selected target words from the generated corpora to demon"
        }
      ]
    },
    {
      "pair_id": "1907.12059_pair_4",
      "doc_id": "1907.12059",
      "element_a_id": "1907.12059_formula_4",
      "element_b_id": "1907.12059_table_1",
      "element_a_type": "formula",
      "element_b_type": "table",
      "pair_type": "formula+table",
      "hop_distance": 1,
      "path": [
        "1907.12059_formula_4",
        "1907.12059_table_1"
      ],
      "quality_score": 0.73,
      "overlap_with_existing_l1": false,
      "element_a": {
        "element_id": "1907.12059_formula_4",
        "element_type": "formula",
        "caption": "",
        "content": "$$\\left| p _ {\\boldsymbol {a}} - \\hat {p} _ {\\boldsymbol {a}} \\right| \\leq \\frac {\\epsilon}{4 | \\mathcal {A} | \\max [ L , 1 ]}. \\tag {11}$$",
        "image_path": null,
        "context_before": "Figure 1 shows overlaying model belief histograms for four demographic groups and their barycenter in the Adult dataset. Wasserstein-1 Penalty effectively matches all group histograms to the barycenter after training for 10,000 steps with $\\beta = 1 0 0$ .\n\n(2016)) using a linear model as the underlying predictor and equal positive prediction rate between each group $\\mathcal { D } _ { a }$ and $\\mathcal { D }$\n\nTable 1: Adult Dataset – German Credit Dataset   \n\n<table><tr><td rowspan=\"2\"></td><",
        "context_after": ""
      },
      "element_b": {
        "element_id": "1907.12059_table_1",
        "element_type": "table",
        "caption": "Table 1: Adult Dataset – German Credit Dataset",
        "content": "Table 1: Adult Dataset – German Credit Dataset",
        "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1907.12059/1907.12059/hybrid_auto/images/bfb1d403cb4990f230659fd9fb77f12dd2f1edb741ebffdf56481dca13229b7c.jpg",
        "context_before": "Remark 1. Notice that $\\begin{array} { r } { ( i i ) = \\mathbb { E } _ { \\tau \\sim U ( \\Omega ) } | \\mathbb { P } ( S _ { 1 } > \\tau ) - } \\end{array}$ $\\mathbb { P } ( S _ { 2 } > \\tau ) | = 0$ if and only if $p _ { S _ { 1 } } = p _ { S _ { 2 } }$ . Indeed, by Proposition 1 and the property of the $\\mathcal { W } _ { 1 }$ metric, $( i i ) = 0$ $\\iff \\mathcal { W } _ { 1 } ( p _ { S _ { 1 } } , p _ { S _ { 2 } } ) = 0 \\iff p _ { S _ { 1 } } = p _ { S _ { 2 } } .$\n\nTo reach SDP, we need to achie",
        "context_after": "as fairness constraints with threshold $\\tau = 0$ .\n\nAdv. Constr. Opt.: The same as the previous method, but with more fairness constraints. Specifically, the fairness constraints are equal positive prediction rates for a set of thresholds from $- 2$ to 2 in increments of 0.2 on the output of the linear model.\n\n5.1 TRAINING DETAILS\n\nFigure 1 shows overlaying model belief histograms for four demographic groups and their barycenter in the Adult dataset. Wasserstein-1 Penalty effectively matches al"
      },
      "edge_contexts": [
        {
          "source": "1907.12059_formula_4",
          "target": "1907.12059_table_1",
          "ref_text": "Table 1",
          "context_snippet": "ediction rate between each group $\\mathcal { D } _ { a }$ and $\\mathcal { D }$\n\nTable 1: Adult Dataset – German Credit Dataset   \n\n<table><tr><td rowspan=\"2\"></td><td "
        }
      ]
    },
    {
      "pair_id": "1907.12059_pair_6",
      "doc_id": "1907.12059",
      "element_a_id": "1907.12059_formula_5",
      "element_b_id": "1907.12059_table_1",
      "element_a_type": "formula",
      "element_b_type": "table",
      "pair_type": "formula+table",
      "hop_distance": 1,
      "path": [
        "1907.12059_formula_5",
        "1907.12059_table_1"
      ],
      "quality_score": 0.73,
      "overlap_with_existing_l1": false,
      "element_a": {
        "element_id": "1907.12059_formula_5",
        "element_type": "formula",
        "caption": "",
        "content": "$$\\sum_ {\\boldsymbol {a} \\in \\mathcal {A}} p _ {\\boldsymbol {a}} \\mathcal {W} _ {1} \\left(p _ {S _ {\\boldsymbol {a}}}, p _ {\\bar {S}}\\right) \\leq \\sum_ {\\boldsymbol {a} \\in \\mathcal {A}} \\hat {p} _ {\\boldsymbol {a}} \\mathcal {W} _ {1} \\left(\\hat {p} _ {S _ {\\boldsymbol {a}}}, \\hat {p} _ {\\bar {S}}\\right) + \\epsilon .$$",
        "image_path": null,
        "context_before": "Figure 1 shows overlaying model belief histograms for four demographic groups and their barycenter in the Adult dataset. Wasserstein-1 Penalty effectively matches all group histograms to the barycenter after training for 10,000 steps with $\\beta = 1 0 0$ .\n\n(2016)) using a linear model as the underlying predictor and equal positive prediction rate between each group $\\mathcal { D } _ { a }$ and $\\mathcal { D }$\n\nTable 1: Adult Dataset – German Credit Dataset   \n\n<table><tr><td rowspan=\"2\"></td><",
        "context_after": ""
      },
      "element_b": {
        "element_id": "1907.12059_table_1",
        "element_type": "table",
        "caption": "Table 1: Adult Dataset – German Credit Dataset",
        "content": "Table 1: Adult Dataset – German Credit Dataset",
        "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1907.12059/1907.12059/hybrid_auto/images/bfb1d403cb4990f230659fd9fb77f12dd2f1edb741ebffdf56481dca13229b7c.jpg",
        "context_before": "Remark 1. Notice that $\\begin{array} { r } { ( i i ) = \\mathbb { E } _ { \\tau \\sim U ( \\Omega ) } | \\mathbb { P } ( S _ { 1 } > \\tau ) - } \\end{array}$ $\\mathbb { P } ( S _ { 2 } > \\tau ) | = 0$ if and only if $p _ { S _ { 1 } } = p _ { S _ { 2 } }$ . Indeed, by Proposition 1 and the property of the $\\mathcal { W } _ { 1 }$ metric, $( i i ) = 0$ $\\iff \\mathcal { W } _ { 1 } ( p _ { S _ { 1 } } , p _ { S _ { 2 } } ) = 0 \\iff p _ { S _ { 1 } } = p _ { S _ { 2 } } .$\n\nTo reach SDP, we need to achie",
        "context_after": "as fairness constraints with threshold $\\tau = 0$ .\n\nAdv. Constr. Opt.: The same as the previous method, but with more fairness constraints. Specifically, the fairness constraints are equal positive prediction rates for a set of thresholds from $- 2$ to 2 in increments of 0.2 on the output of the linear model.\n\n5.1 TRAINING DETAILS\n\nFigure 1 shows overlaying model belief histograms for four demographic groups and their barycenter in the Adult dataset. Wasserstein-1 Penalty effectively matches al"
      },
      "edge_contexts": [
        {
          "source": "1907.12059_formula_5",
          "target": "1907.12059_table_1",
          "ref_text": "Table 1",
          "context_snippet": "ediction rate between each group $\\mathcal { D } _ { a }$ and $\\mathcal { D }$\n\nTable 1: Adult Dataset – German Credit Dataset   \n\n<table><tr><td rowspan=\"2\"></td><td "
        }
      ]
    },
    {
      "pair_id": "1905.12843_pair_3",
      "doc_id": "1905.12843",
      "element_a_id": "1905.12843_formula_1",
      "element_b_id": "1905.12843_table_1",
      "element_a_type": "formula",
      "element_b_type": "table",
      "pair_type": "formula+table",
      "hop_distance": 1,
      "path": [
        "1905.12843_formula_1",
        "1905.12843_table_1"
      ],
      "quality_score": 0.7,
      "overlap_with_existing_l1": false,
      "element_a": {
        "element_id": "1905.12843_formula_1",
        "element_type": "formula",
        "caption": "",
        "content": "$$\\text {f o r a l l} i \\in [ n ]: t _ {i} \\geq \\frac {\\alpha}{2} - Y _ {i} \\langle \\beta , x _ {i} \\rangle ,$$",
        "image_path": null,
        "context_before": "Runtime comparison. We performed a comparison on the running time of a single call of the three supervised learning oracles. On a subsampled law school data set with 1,000 examples, we ran the oracles to solve an instance of the $\\boldsymbol { \\mathrm { B E S T } } _ { h }$ problem, optimizing over either the linear models or tree ensemble models. The details are listed in Table 1. We also compare the number of oracle calls for different specified values of fairness slackness.\n\nWe then evaluated",
        "context_after": ""
      },
      "element_b": {
        "element_id": "1905.12843_table_1",
        "element_type": "table",
        "caption": "The details are listed in Table 1.",
        "content": "The details are listed in Table 1.",
        "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1905.12843/1905.12843/hybrid_auto/images/1905.12843_page0_fig12.jpg",
        "context_before": "Runtime comparison. We performed a comparison on the running time of a single call of the three supervised learning oracles. On a subsampled law school data set with 1,000 examples, we ran the oracles to solve an instance of the $\\boldsymbol { \\mathrm { B E S T } } _ { h }$ problem, optimizing over either the linear models or tree ensemble models. The details are listed in Table 1. We also compare the number of oracle calls for different specified values of fairness slackness.\n\nWe then evaluated",
        "context_after": ""
      },
      "edge_contexts": [
        {
          "source": "1905.12843_formula_1",
          "target": "1905.12843_table_1",
          "ref_text": "Table 1",
          "context_snippet": "ver either the linear models or tree ensemble models. The details are listed in Table 1. We also compare the number of oracle calls for different specified values of f"
        }
      ]
    },
    {
      "pair_id": "1905.12843_pair_5",
      "doc_id": "1905.12843",
      "element_a_id": "1905.12843_formula_2",
      "element_b_id": "1905.12843_table_1",
      "element_a_type": "formula",
      "element_b_type": "table",
      "pair_type": "formula+table",
      "hop_distance": 1,
      "path": [
        "1905.12843_formula_2",
        "1905.12843_table_1"
      ],
      "quality_score": 0.7,
      "overlap_with_existing_l1": false,
      "element_a": {
        "element_id": "1905.12843_formula_2",
        "element_type": "formula",
        "caption": "",
        "content": "$$f o r \\quad j \\in [ d ]: - 1 \\leq \\beta_ {j} \\leq 1.$$",
        "image_path": null,
        "context_before": "Runtime comparison. We performed a comparison on the running time of a single call of the three supervised learning oracles. On a subsampled law school data set with 1,000 examples, we ran the oracles to solve an instance of the $\\boldsymbol { \\mathrm { B E S T } } _ { h }$ problem, optimizing over either the linear models or tree ensemble models. The details are listed in Table 1. We also compare the number of oracle calls for different specified values of fairness slackness.\n\nWe then evaluated",
        "context_after": ""
      },
      "element_b": {
        "element_id": "1905.12843_table_1",
        "element_type": "table",
        "caption": "The details are listed in Table 1.",
        "content": "The details are listed in Table 1.",
        "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1905.12843/1905.12843/hybrid_auto/images/1905.12843_page0_fig12.jpg",
        "context_before": "Runtime comparison. We performed a comparison on the running time of a single call of the three supervised learning oracles. On a subsampled law school data set with 1,000 examples, we ran the oracles to solve an instance of the $\\boldsymbol { \\mathrm { B E S T } } _ { h }$ problem, optimizing over either the linear models or tree ensemble models. The details are listed in Table 1. We also compare the number of oracle calls for different specified values of fairness slackness.\n\nWe then evaluated",
        "context_after": ""
      },
      "edge_contexts": [
        {
          "source": "1905.12843_formula_2",
          "target": "1905.12843_table_1",
          "ref_text": "Table 1",
          "context_snippet": "ver either the linear models or tree ensemble models. The details are listed in Table 1. We also compare the number of oracle calls for different specified values of f"
        }
      ]
    },
    {
      "pair_id": "1707.09457_pair_4",
      "doc_id": "1707.09457",
      "element_a_id": "1707.09457_table_1",
      "element_b_id": "1707.09457_formula_1",
      "element_a_type": "table",
      "element_b_type": "formula",
      "pair_type": "formula+table",
      "hop_distance": 2,
      "path": [
        "1707.09457_table_1",
        "1707.09457_figure_1",
        "1707.09457_formula_1"
      ],
      "quality_score": 0.7249999999999999,
      "overlap_with_existing_l1": false,
      "element_a": {
        "element_id": "1707.09457_table_1",
        "element_type": "table",
        "caption": "Table 1: Statistics for the two recognition problems. In vSRL, we consider gender bias relating to verbs, while in MLC we consider the gender bias related to objects.",
        "content": "Table 1: Statistics for the two recognition problems. In vSRL, we consider gender bias relating to verbs, while in MLC we consider the gender bias related to objects.",
        "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1707.09457/1707.09457/hybrid_auto/images/ec39e21fb44a922a487a63da2ae6305d9fa5edf16eb972b8a32abe434971542b.jpg",
        "context_before": "For example, one can represent the problem as an\n\nTable 1: Statistics for the two recognition problems.\n\nIn this section, we provide details about the two visual recognition tasks we evaluated for bias: visual semantic role labeling (vSRL), and multi-label classification (MLC). We focus on gender, defining $G = \\{ \\mathrm { m a n } , \\mathrm { w o m a n } \\}$ and focus on the agent\n\nrole in vSRL, and any occurrence in text associated with the images in MLC. Problem statistics are summarized in T",
        "context_after": "integer linear program and solve it using an offthe-shelf solver (e.g., Gurobi (Gurobi Optimization, 2016)). However, Eq. (3) involves all test instances. Solving a constrained optimization problem on such a scale is difficult. Therefore, we consider relaxing the constraints and solve Eq. (3) using a Lagrangian relaxation technique (Rush and Collins, 2012). We introduce a Lagrangian multiplier $\\lambda _ { j } \\geq 0$ for each corpus-level constraint. The Lagrangian is\n\n$$ \\begin{array}{l} L (\\l"
      },
      "element_b": {
        "element_id": "1707.09457_formula_1",
        "element_type": "formula",
        "caption": "",
        "content": "$$\\begin{array}{l} L (\\lambda , \\{y ^ {i} \\}) = \\\\ \\sum_ {i} f _ {\\theta} \\left(y ^ {i}\\right) - \\sum_ {j = 1} ^ {l} \\lambda_ {j} \\left(A _ {j} \\sum_ {i} y ^ {i} - b _ {j}\\right), \\tag {4} \\\\ \\end{array}$$",
        "image_path": null,
        "context_before": "",
        "context_after": "tics from images and require large quantities of labeled data, predominantly retrieved from the web. Methods often combine structured prediction and deep learning to model correlations between labels and images to make judgments that otherwise would have weak visual support. For example, in the first image of Figure 1, it is possible to predict a spatula by considering that it is a common tool used for the activity cooking. Yet such methods run the risk of discovering and exploiting societal bia"
      },
      "edge_contexts": [
        {
          "source": "1707.09457_table_1",
          "target": "1707.09457_figure_1",
          "ref_text": "Figure 1",
          "context_snippet": "at otherwise would have weak visual support. For example, in the first image of Figure 1, it is possible to predict a spatula by considering that it is a common tool us"
        },
        {
          "source": "1707.09457_formula_1",
          "target": "1707.09457_figure_1",
          "ref_text": "Figure 1",
          "context_snippet": "at otherwise would have weak visual support. For example, in the first image of Figure 1, it is possible to predict a spatula by considering that it is a common tool us"
        }
      ]
    }
  ]
}