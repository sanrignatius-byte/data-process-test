{
  "metadata": {
    "total_l1_entries": 974,
    "filtered_entries": 727,
    "unique_entities": 430,
    "cross_doc_entities": 56,
    "total_doc_pairs": 340,
    "output_pairs": 100
  },
  "pairs": [
    {
      "doc_a": "1412.3756",
      "doc_b": "1810.01943",
      "shared_entities": [
        "DI",
        "Disparate impact",
        "Fairness",
        "German Credit",
        "LR",
        "Logistic regression",
        "accuracy",
        "disparate impact",
        "logistic regression"
      ],
      "shared_entity_count": 9,
      "doc_a_query_id": "l1_1412.3756_1412.3756_fig_3_55",
      "doc_a_query": "Does the utility decay pattern visible across all scatter points support the expected tradeoff between fairness and utility?",
      "doc_a_answer": "Yes, the scatter points show declining utility (from ~1.0 to ~0.5-0.9) as disparate impact increases toward the green line at DI=1.0, demonstrating the expected decay over utility as fairness increases.",
      "doc_a_visual_anchor": "Dashed trend lines connecting scatter points showing downward slope from left to right",
      "doc_a_text_evidence": "The results, shown in Figure 3, demonstrate the expected decay over utility as fairness increases.",
      "doc_a_figure_id": "1412.3756_fig_3",
      "doc_a_figure_type": "plot",
      "doc_a_image_path": "data/mineru_output/1412.3756/1412.3756/hybrid_auto/images/1412.3756_page0_fig2.jpg",
      "doc_a_caption": "Fig. 3: Disparate impact (DI) vs. utility (1-BER) from our combinatorial and geometric partial repair processes using the SVM to classify on the Adult Income and German Credit data sets and the simple threshold classifier on the Ricci data set. Recall that only points with $\\mathsf { D } \\mathsf { I } \\ge \\tau = 0 . 8$ are legal. $\\mathsf { D } \\mathsf { I } = 1 . 0$ represents full fairness.",
      "doc_a_visual_score": 5,
      "doc_b_query_id": "l1_1810.01943_1810.01943_fig_10_578",
      "doc_b_query": "Do the green circles at disparate impact 0.8 in the top panel support that optimized pre-processing improves fairness?",
      "doc_b_answer": "Yes, the green circles (RF,Optimized pre-processing) show disparate impact closest to the ideal value of 1 in the top panel before mitigation, indicating that optimized pre-processing achieves better fairness metrics.",
      "doc_b_visual_anchor": "green circles at disparate impact ~0.8 in top panel",
      "doc_b_text_evidence": "The ideal fair value of disparate impact is 1, whereas for all other metrics it is 0.",
      "doc_b_figure_id": "1810.01943_fig_10",
      "doc_b_figure_type": "plot",
      "doc_b_image_path": "data/mineru_output/1810.01943/1810.01943/hybrid_auto/images/1810.01943_page0_fig9.jpg",
      "doc_b_caption": "Figure 5. Fairness vs. Balanced Accuracy before (top panel) and after (bottom panel) applying various bias mitigation algorithms. Four different fairness metrics are shown. In most cases two classifiers (Logistic regression - LR or Random forest classifier - RF) were used. The ideal fair value of disparate impact is 1, whereas for all other metrics it is 0. The circles indicate the mean value and bars indicate the extent of $\\pm 1$ standard deviation. Dataset: Adult, Protected attribute: race.",
      "doc_b_visual_score": 3,
      "score": 35.5
    },
    {
      "doc_a": "1810.01943",
      "doc_b": "2109.03952",
      "shared_entities": [
        "Accuracy",
        "Parity",
        "SPD",
        "Statistical Parity Difference",
        "accuracy",
        "parity"
      ],
      "shared_entity_count": 6,
      "doc_a_query_id": "l1_1810.01943_1810.01943_fig_10_578",
      "doc_a_query": "Do the green circles at disparate impact 0.8 in the top panel support that optimized pre-processing improves fairness?",
      "doc_a_answer": "Yes, the green circles (RF,Optimized pre-processing) show disparate impact closest to the ideal value of 1 in the top panel before mitigation, indicating that optimized pre-processing achieves better fairness metrics.",
      "doc_a_visual_anchor": "green circles at disparate impact ~0.8 in top panel",
      "doc_a_text_evidence": "The ideal fair value of disparate impact is 1, whereas for all other metrics it is 0.",
      "doc_a_figure_id": "1810.01943_fig_10",
      "doc_a_figure_type": "plot",
      "doc_a_image_path": "data/mineru_output/1810.01943/1810.01943/hybrid_auto/images/1810.01943_page0_fig9.jpg",
      "doc_a_caption": "Figure 5. Fairness vs. Balanced Accuracy before (top panel) and after (bottom panel) applying various bias mitigation algorithms. Four different fairness metrics are shown. In most cases two classifiers (Logistic regression - LR or Random forest classifier - RF) were used. The ideal fair value of disparate impact is 1, whereas for all other metrics it is 0. The circles indicate the mean value and bars indicate the extent of $\\pm 1$ standard deviation. Dataset: Adult, Protected attribute: race.",
      "doc_a_visual_score": 3,
      "doc_b_query_id": "l1_2109.03952_2109.03952_fig_1_948",
      "doc_b_query": "How does the orange Attention layer produce its output representation before the yellow Dense Layers?",
      "doc_b_answer": "The attention layer computes weights and takes a weighted combination of the input feature embeddings to produce a d^e-dimensional vector representation for the sample instance.",
      "doc_b_visual_anchor": "orange Attention layer box positioned between red embedding boxes and yellow Dense Layer boxes",
      "doc_b_text_evidence": "These vectors are passed to the attention layer. The Computation of attention weights and the final representation for a sample is",
      "doc_b_figure_id": "2109.03952_fig_1",
      "doc_b_figure_type": "architecture",
      "doc_b_image_path": "data/mineru_output/2109.03952/2109.03952/hybrid_auto/images/2109.03952_page0_fig0.jpg",
      "doc_b_caption": "Figure 1: (a) In general classification model, for each feature $f _ { k }$ a vector representation $e _ { k }$ of length $d ^ { e }$ is learned. This is passed to the attention layer which produces a $d ^ { e }$ - dimensional vector representation for the sample instance $i$ which is passed to two dense layers to get the final classification output. (b) The Attribution framework has the same architecture as the general model. One outcome is obtained through the original model and another through the model that has some attention weights zeroed. The observed difference in accuracy and fairness measures will indicate the effect of the zeroed out features on accuracy and fairness.",
      "doc_b_visual_score": 4,
      "score": 21.0
    },
    {
      "doc_a": "1511.00830",
      "doc_b": "1809.10083",
      "shared_entities": [
        "Extended Yale",
        "ID",
        "SNE",
        "t-SNE"
      ],
      "shared_entity_count": 4,
      "doc_a_query_id": "l1_1511.00830_1511.00830_fig_1_64",
      "doc_a_query": "Why does node S point directly to X rather than through Z in this unsupervised representation learning model?",
      "doc_a_answer": "In unsupervised representation learning, S represents noise or nuisance variables that directly affect the observed data X, independent of the informative latent factors Z. This direct connection models uninformative dimensions that are typically detrimental for the task under consideration.",
      "doc_a_visual_anchor": "arrow from node S (top-left) directly to node X (bottom-center)",
      "doc_a_text_evidence": "Uninformative dimensions are often called \"noise\" or \"nuisance variables\" while informative dimensions are usually called latent or hidden factors of variation.",
      "doc_a_figure_id": "1511.00830_fig_1",
      "doc_a_figure_type": "diagram",
      "doc_a_image_path": "data/mineru_output/1511.00830/1511.00830/hybrid_auto/images/1511.00830_page0_fig0.jpg",
      "doc_a_caption": "Figure 1: Unsupervised model",
      "doc_a_visual_score": 5,
      "doc_b_query_id": "l1_1809.10083_1809.10083_fig_5_550",
      "doc_b_query": "Does the separation between orange lower-right markers and cyan upper-right markers confirm that embedding e₁ captures lighting variations?",
      "doc_b_answer": "Yes, the clear spatial separation between different lighting conditions (lower-right vs upper-right) in the t-SNE visualization demonstrates that embedding e₁ successfully disentangles and represents lighting condition as a distinct factor.",
      "doc_b_visual_anchor": "orange plus signs (lower-right) and cyan stars (upper-right) spatially separated",
      "doc_b_text_evidence": "Figure 2 shows t-SNE [15] visualization of raw data and embeddings $e _ { 1 }$ and $e _ { 2 }$ for our model.",
      "doc_b_figure_id": "1809.10083_fig_5",
      "doc_b_figure_type": "plot",
      "doc_b_image_path": "data/mineru_output/1809.10083/1809.10083/hybrid_auto/images/1809.10083_page0_fig4.jpg",
      "doc_b_caption": "Figure 2: Extended Yale-B – t-SNE visualization of (a) raw data, (b) $e _ { 2 }$ labeled by lighting condition, (c) $e _ { 1 }$ labeled by lighting condition, and (d) $e _ { 1 }$ labeled by subject-ID (numerical markers, not colors).",
      "doc_b_visual_score": 4,
      "score": 18.0
    },
    {
      "doc_a": "1709.02012",
      "doc_b": "2109.03952",
      "shared_entities": [
        "Equalized Odds",
        "Parity",
        "parity"
      ],
      "shared_entity_count": 3,
      "doc_a_query_id": "l1_1709.02012_1709.02012_fig_3_267",
      "doc_a_query": "Do the calibrated classifier sets H₁* and H₂* both lie on the black diagonal boundary?",
      "doc_a_answer": "Yes, the sets H₁* and H₂* of calibrated classifiers for the two groups both lie on the black diagonal boundary, representing the Pareto frontier of achievable false-positive and false-negative trade-offs for each group.",
      "doc_a_visual_anchor": "Black diagonal line from top-left to bottom-right",
      "doc_a_text_evidence": "H₁*, H₂* are the set of cal. classifiers for the two groups",
      "doc_a_figure_id": "1709.02012_fig_3",
      "doc_a_figure_type": "plot",
      "doc_a_image_path": "data/mineru_output/1709.02012/1709.02012/hybrid_auto/images/1709.02012_page0_fig2.jpg",
      "doc_a_caption": "Figure 1: Calibration, trivial classifiers, and equal-cost constraints – plotted in the false-pos./false-neg. plane. $\\mathcal { H } _ { 1 } ^ { \\ast } , \\mathcal { H } _ { 2 } ^ { \\ast }$ are the set of cal. classifiers for the two groups, and $h ^ { \\mu _ { 1 } } , h ^ { \\mu _ { 2 } }$ are trivial classifiers.   ",
      "doc_a_visual_score": 6,
      "doc_b_query_id": "l1_2109.03952_2109.03952_fig_1_948",
      "doc_b_query": "How does the orange Attention layer produce its output representation before the yellow Dense Layers?",
      "doc_b_answer": "The attention layer computes weights and takes a weighted combination of the input feature embeddings to produce a d^e-dimensional vector representation for the sample instance.",
      "doc_b_visual_anchor": "orange Attention layer box positioned between red embedding boxes and yellow Dense Layer boxes",
      "doc_b_text_evidence": "These vectors are passed to the attention layer. The Computation of attention weights and the final representation for a sample is",
      "doc_b_figure_id": "2109.03952_fig_1",
      "doc_b_figure_type": "architecture",
      "doc_b_image_path": "data/mineru_output/2109.03952/2109.03952/hybrid_auto/images/2109.03952_page0_fig0.jpg",
      "doc_b_caption": "Figure 1: (a) In general classification model, for each feature $f _ { k }$ a vector representation $e _ { k }$ of length $d ^ { e }$ is learned. This is passed to the attention layer which produces a $d ^ { e }$ - dimensional vector representation for the sample instance $i$ which is passed to two dense layers to get the final classification output. (b) The Attribution framework has the same architecture as the general model. One outcome is obtained through the original model and another through the model that has some attention weights zeroed. The observed difference in accuracy and fairness measures will indicate the effect of the zeroed out features on accuracy and fairness.",
      "doc_b_visual_score": 4,
      "score": 16.5
    },
    {
      "doc_a": "1511.00830",
      "doc_b": "1805.09458",
      "shared_entities": [
        "SNE",
        "VFAE",
        "accuracy",
        "t-SNE"
      ],
      "shared_entity_count": 4,
      "doc_a_query_id": "l1_1511.00830_1511.00830_fig_1_64",
      "doc_a_query": "Why does node S point directly to X rather than through Z in this unsupervised representation learning model?",
      "doc_a_answer": "In unsupervised representation learning, S represents noise or nuisance variables that directly affect the observed data X, independent of the informative latent factors Z. This direct connection models uninformative dimensions that are typically detrimental for the task under consideration.",
      "doc_a_visual_anchor": "arrow from node S (top-left) directly to node X (bottom-center)",
      "doc_a_text_evidence": "Uninformative dimensions are often called \"noise\" or \"nuisance variables\" while informative dimensions are usually called latent or hidden factors of variation.",
      "doc_a_figure_id": "1511.00830_fig_1",
      "doc_a_figure_type": "diagram",
      "doc_a_image_path": "data/mineru_output/1511.00830/1511.00830/hybrid_auto/images/1511.00830_page0_fig0.jpg",
      "doc_a_caption": "Figure 1: Unsupervised model",
      "doc_a_visual_score": 5,
      "doc_b_query_id": "l1_1805.09458_1805.09458_fig_5_446",
      "doc_b_query": "How does using digit label as covariate enable the leftmost real 9 to generate all ten digits rightward?",
      "doc_b_answer": "Using digit label as covariate class c pushes all non-class stylistic information into latent space while removing information about the exact digit. This separation allows the decoder to recombine the extracted style with any specified digit label c′ to produce all ten digits.",
      "doc_b_visual_anchor": "Top row showing real digit 9 on left and generated sequence 0123456789 on right",
      "doc_b_text_evidence": "We use the digit label as the covariate class c, which pushes all non-class stylistic information into the latent space while attempting to remove information about the exact digit being written.",
      "doc_b_figure_id": "1805.09458_fig_5",
      "doc_b_figure_type": "example",
      "doc_b_image_path": "data/mineru_output/1805.09458/1805.09458/hybrid_auto/images/1805.09458_page0_fig4.jpg",
      "doc_b_caption": "Figure 3: We demonstrate the ability to generate stylistically similar images of varying classes using the MNIST dataset. The left column is mapped into $z$ that is invariant to its digit label c. We then can generate an image using $z$ and any other specified digit, $c ^ { \\prime }$ , as show on the right.",
      "doc_b_visual_score": 3,
      "score": 15.0
    },
    {
      "doc_a": "2005.07293",
      "doc_b": "2109.03952",
      "shared_entities": [
        "Accuracy",
        "Parity",
        "accuracy",
        "parity"
      ],
      "shared_entity_count": 4,
      "doc_a_query_id": "l1_2005.07293_2005.07293_fig_1_905",
      "doc_a_query": "How does the different-height box arrangement under the yellow figure in the right panel operationalize compensation for historically disadvantaged groups?",
      "doc_a_answer": "The taller box under the yellow figure represents resource distribution to overcome obstacles and raise opportunities for access, compensating historically disadvantaged groups so they can reach the apples like others.",
      "doc_a_visual_anchor": "Right panel 'Equity' showing yellow figure standing on tallest brown box",
      "doc_a_text_evidence": "Equity is the distribution of resources among groups to overcome obstacles and to raise their opportunities for access [Schement, 2001]. Thus, historically disadvantaged groups are compensated, and others get their fair share to reach their goals.",
      "doc_a_figure_id": "2005.07293_fig_1",
      "doc_a_figure_type": "diagram",
      "doc_a_image_path": "data/mineru_output/2005.07293/2005.07293/hybrid_auto/images/2005.07293_page0_fig0.jpg",
      "doc_a_caption": "Figure 1: Notion of equality in fairness is depicted and formalized along with our newly formalized notion of equity.",
      "doc_a_visual_score": 4,
      "doc_b_query_id": "l1_2109.03952_2109.03952_fig_1_948",
      "doc_b_query": "How does the orange Attention layer produce its output representation before the yellow Dense Layers?",
      "doc_b_answer": "The attention layer computes weights and takes a weighted combination of the input feature embeddings to produce a d^e-dimensional vector representation for the sample instance.",
      "doc_b_visual_anchor": "orange Attention layer box positioned between red embedding boxes and yellow Dense Layer boxes",
      "doc_b_text_evidence": "These vectors are passed to the attention layer. The Computation of attention weights and the final representation for a sample is",
      "doc_b_figure_id": "2109.03952_fig_1",
      "doc_b_figure_type": "architecture",
      "doc_b_image_path": "data/mineru_output/2109.03952/2109.03952/hybrid_auto/images/2109.03952_page0_fig0.jpg",
      "doc_b_caption": "Figure 1: (a) In general classification model, for each feature $f _ { k }$ a vector representation $e _ { k }$ of length $d ^ { e }$ is learned. This is passed to the attention layer which produces a $d ^ { e }$ - dimensional vector representation for the sample instance $i$ which is passed to two dense layers to get the final classification output. (b) The Attribution framework has the same architecture as the general model. One outcome is obtained through the original model and another through the model that has some attention weights zeroed. The observed difference in accuracy and fairness measures will indicate the effect of the zeroed out features on accuracy and fairness.",
      "doc_b_visual_score": 4,
      "score": 15.0
    },
    {
      "doc_a": "1610.02413",
      "doc_b": "2109.03952",
      "shared_entities": [
        "equalized odds",
        "parity"
      ],
      "shared_entity_count": 2,
      "doc_a_query_id": "l1_1610.02413_1610.02413_fig_4_136",
      "doc_a_query": "How does the horizontal gap indicated by the lower-right arrow relate to optimizing the equal opportunity predictor?",
      "doc_a_answer": "The horizontal gap between the ROC curve and the profit-maximizing tangent line is proportional to the cost for achieving a given true positive rate, which becomes a convex function that can be efficiently optimized using ternary search.",
      "doc_a_visual_anchor": "Black arrow pointing to horizontal gap between blue curve and tangent line in lower right",
      "doc_a_text_evidence": "within each group the cost for a given true positive rate is proportional to the horizontal gap between the ROC curve and the profit-maximizing tangent line (i.e., the two curves on the left plot), so it is a convex function of the true positive rate",
      "doc_a_figure_id": "1610.02413_fig_4",
      "doc_a_figure_type": "plot",
      "doc_a_image_path": "data/mineru_output/1610.02413/1610.02413/hybrid_auto/images/1610.02413_page0_fig3.jpg",
      "doc_a_caption": "Figure 2: Finding the optimal equalized odds threshold predictor (middle), and equal opportunity threshold predictor (right). For the equal opportunity predictor, within each group the cost for a given true positive rate is proportional to the horizontal gap between the ROC curve and the profit-maximizing tangent line (i.e., the two curves on the left plot), so it is a convex function of the true positive rate (right). This lets us optimize it efficiently with ternary search.",
      "doc_a_visual_score": 7,
      "doc_b_query_id": "l1_2109.03952_2109.03952_fig_1_948",
      "doc_b_query": "How does the orange Attention layer produce its output representation before the yellow Dense Layers?",
      "doc_b_answer": "The attention layer computes weights and takes a weighted combination of the input feature embeddings to produce a d^e-dimensional vector representation for the sample instance.",
      "doc_b_visual_anchor": "orange Attention layer box positioned between red embedding boxes and yellow Dense Layer boxes",
      "doc_b_text_evidence": "These vectors are passed to the attention layer. The Computation of attention weights and the final representation for a sample is",
      "doc_b_figure_id": "2109.03952_fig_1",
      "doc_b_figure_type": "architecture",
      "doc_b_image_path": "data/mineru_output/2109.03952/2109.03952/hybrid_auto/images/2109.03952_page0_fig0.jpg",
      "doc_b_caption": "Figure 1: (a) In general classification model, for each feature $f _ { k }$ a vector representation $e _ { k }$ of length $d ^ { e }$ is learned. This is passed to the attention layer which produces a $d ^ { e }$ - dimensional vector representation for the sample instance $i$ which is passed to two dense layers to get the final classification output. (b) The Attribution framework has the same architecture as the general model. One outcome is obtained through the original model and another through the model that has some attention weights zeroed. The observed difference in accuracy and fairness measures will indicate the effect of the zeroed out features on accuracy and fairness.",
      "doc_b_visual_score": 4,
      "score": 15.0
    },
    {
      "doc_a": "1810.01943",
      "doc_b": "2005.07293",
      "shared_entities": [
        "Accuracy",
        "Parity",
        "accuracy",
        "parity"
      ],
      "shared_entity_count": 4,
      "doc_a_query_id": "l1_1810.01943_1810.01943_fig_10_578",
      "doc_a_query": "Do the green circles at disparate impact 0.8 in the top panel support that optimized pre-processing improves fairness?",
      "doc_a_answer": "Yes, the green circles (RF,Optimized pre-processing) show disparate impact closest to the ideal value of 1 in the top panel before mitigation, indicating that optimized pre-processing achieves better fairness metrics.",
      "doc_a_visual_anchor": "green circles at disparate impact ~0.8 in top panel",
      "doc_a_text_evidence": "The ideal fair value of disparate impact is 1, whereas for all other metrics it is 0.",
      "doc_a_figure_id": "1810.01943_fig_10",
      "doc_a_figure_type": "plot",
      "doc_a_image_path": "data/mineru_output/1810.01943/1810.01943/hybrid_auto/images/1810.01943_page0_fig9.jpg",
      "doc_a_caption": "Figure 5. Fairness vs. Balanced Accuracy before (top panel) and after (bottom panel) applying various bias mitigation algorithms. Four different fairness metrics are shown. In most cases two classifiers (Logistic regression - LR or Random forest classifier - RF) were used. The ideal fair value of disparate impact is 1, whereas for all other metrics it is 0. The circles indicate the mean value and bars indicate the extent of $\\pm 1$ standard deviation. Dataset: Adult, Protected attribute: race.",
      "doc_a_visual_score": 3,
      "doc_b_query_id": "l1_2005.07293_2005.07293_fig_1_905",
      "doc_b_query": "How does the different-height box arrangement under the yellow figure in the right panel operationalize compensation for historically disadvantaged groups?",
      "doc_b_answer": "The taller box under the yellow figure represents resource distribution to overcome obstacles and raise opportunities for access, compensating historically disadvantaged groups so they can reach the apples like others.",
      "doc_b_visual_anchor": "Right panel 'Equity' showing yellow figure standing on tallest brown box",
      "doc_b_text_evidence": "Equity is the distribution of resources among groups to overcome obstacles and to raise their opportunities for access [Schement, 2001]. Thus, historically disadvantaged groups are compensated, and others get their fair share to reach their goals.",
      "doc_b_figure_id": "2005.07293_fig_1",
      "doc_b_figure_type": "diagram",
      "doc_b_image_path": "data/mineru_output/2005.07293/2005.07293/hybrid_auto/images/2005.07293_page0_fig0.jpg",
      "doc_b_caption": "Figure 1: Notion of equality in fairness is depicted and formalized along with our newly formalized notion of equity.",
      "doc_b_visual_score": 4,
      "score": 14.5
    },
    {
      "doc_a": "1810.08683",
      "doc_b": "2109.03952",
      "shared_entities": [
        "accuracy",
        "equalized odds"
      ],
      "shared_entity_count": 2,
      "doc_a_query_id": "l1_1810.08683_1810.08683_fig_4_640",
      "doc_a_query": "Does the inverse relationship between blue ACC and orange EOd curves in the bottom panel support varying λ conclusions?",
      "doc_a_answer": "Yes, as λ increases from 2e-10 to 1, ACC drops from ~91 to ~67 while EOd rises from ~0.05 to ~0.13, demonstrating the accuracy-fairness trade-off when varying λ as described in the analysis.",
      "doc_a_visual_anchor": "opposing trends of blue ACC (declining) and orange EOd (rising) curves from left to right in bottom panel",
      "doc_a_text_evidence": "This hypothesis is also supported by the results of Figure 5, in which we check how the accuracy and fairness, as measured with the EOd, varies by varying",
      "doc_a_figure_id": "1810.08683_fig_4",
      "doc_a_figure_type": "plot",
      "doc_a_image_path": "data/mineru_output/1810.08683/1810.08683/hybrid_auto/images/1810.08683_page0_fig3.jpg",
      "doc_a_caption": "Figure 5: Adult Dataset: ACC and EOd of MTL, when we fix $\\theta$ and $\\rho$ to be the best values found during the validation procedure and we vary  with $P { = } 0$ , $F { = } 1$ , and $S { = } 0$ .",
      "doc_a_visual_score": 6,
      "doc_b_query_id": "l1_2109.03952_2109.03952_fig_1_948",
      "doc_b_query": "How does the orange Attention layer produce its output representation before the yellow Dense Layers?",
      "doc_b_answer": "The attention layer computes weights and takes a weighted combination of the input feature embeddings to produce a d^e-dimensional vector representation for the sample instance.",
      "doc_b_visual_anchor": "orange Attention layer box positioned between red embedding boxes and yellow Dense Layer boxes",
      "doc_b_text_evidence": "These vectors are passed to the attention layer. The Computation of attention weights and the final representation for a sample is",
      "doc_b_figure_id": "2109.03952_fig_1",
      "doc_b_figure_type": "architecture",
      "doc_b_image_path": "data/mineru_output/2109.03952/2109.03952/hybrid_auto/images/2109.03952_page0_fig0.jpg",
      "doc_b_caption": "Figure 1: (a) In general classification model, for each feature $f _ { k }$ a vector representation $e _ { k }$ of length $d ^ { e }$ is learned. This is passed to the attention layer which produces a $d ^ { e }$ - dimensional vector representation for the sample instance $i$ which is passed to two dense layers to get the final classification output. (b) The Attribution framework has the same architecture as the general model. One outcome is obtained through the original model and another through the model that has some attention weights zeroed. The observed difference in accuracy and fairness measures will indicate the effect of the zeroed out features on accuracy and fairness.",
      "doc_b_visual_score": 4,
      "score": 14.5
    },
    {
      "doc_a": "1412.3756",
      "doc_b": "1706.02409",
      "shared_entities": [
        "Fairness",
        "accuracy",
        "logistic regression"
      ],
      "shared_entity_count": 3,
      "doc_a_query_id": "l1_1412.3756_1412.3756_fig_3_55",
      "doc_a_query": "Does the utility decay pattern visible across all scatter points support the expected tradeoff between fairness and utility?",
      "doc_a_answer": "Yes, the scatter points show declining utility (from ~1.0 to ~0.5-0.9) as disparate impact increases toward the green line at DI=1.0, demonstrating the expected decay over utility as fairness increases.",
      "doc_a_visual_anchor": "Dashed trend lines connecting scatter points showing downward slope from left to right",
      "doc_a_text_evidence": "The results, shown in Figure 3, demonstrate the expected decay over utility as fairness increases.",
      "doc_a_figure_id": "1412.3756_fig_3",
      "doc_a_figure_type": "plot",
      "doc_a_image_path": "data/mineru_output/1412.3756/1412.3756/hybrid_auto/images/1412.3756_page0_fig2.jpg",
      "doc_a_caption": "Fig. 3: Disparate impact (DI) vs. utility (1-BER) from our combinatorial and geometric partial repair processes using the SVM to classify on the Adult Income and German Credit data sets and the simple threshold classifier on the Ricci data set. Recall that only points with $\\mathsf { D } \\mathsf { I } \\ge \\tau = 0 . 8$ are legal. $\\mathsf { D } \\mathsf { I } = 1 . 0$ represents full fairness.",
      "doc_a_visual_score": 5,
      "doc_b_query_id": "l1_1706.02409_1706.02409_fig_10_235",
      "doc_b_query": "What explains the progressive height increase in magenta and yellow hybrid bars as alpha decreases from 0.5 to 0.01?",
      "doc_b_answer": "As alpha decreases (stronger regularization), the hybrid fairness regularizers show increasing Price of Fairness, indicating that stricter hybrid fairness constraints progressively sacrifice more model performance to achieve fairness goals.",
      "doc_b_visual_anchor": "magenta (Hybrid, separate) and yellow (Hybrid, single) bars increasing from left to right",
      "doc_b_text_evidence": "In Proceedings of the 16th International Conference on Data Mining, pages 1–10, 2016. [2] Julia Angwin, Jeff Larson, Surya Mattu, and Lauren Kirchner. Machine bias. Propublica, 2016.",
      "doc_b_figure_id": "1706.02409_fig_10",
      "doc_b_figure_type": "plot",
      "doc_b_image_path": "data/mineru_output/1706.02409/1706.02409/hybrid_auto/images/1706.02409_page0_fig9.jpg",
      "doc_b_caption": "Figure 2: The “Price of Fairness” across data sets, for each type of fairness regularizer, in both the single and separate model case.",
      "doc_b_visual_score": 3,
      "score": 13.5
    },
    {
      "doc_a": "1412.3756",
      "doc_b": "1805.09458",
      "shared_entities": [
        "accuracy",
        "logistic regression"
      ],
      "shared_entity_count": 2,
      "doc_a_query_id": "l1_1412.3756_1412.3756_fig_3_55",
      "doc_a_query": "Does the utility decay pattern visible across all scatter points support the expected tradeoff between fairness and utility?",
      "doc_a_answer": "Yes, the scatter points show declining utility (from ~1.0 to ~0.5-0.9) as disparate impact increases toward the green line at DI=1.0, demonstrating the expected decay over utility as fairness increases.",
      "doc_a_visual_anchor": "Dashed trend lines connecting scatter points showing downward slope from left to right",
      "doc_a_text_evidence": "The results, shown in Figure 3, demonstrate the expected decay over utility as fairness increases.",
      "doc_a_figure_id": "1412.3756_fig_3",
      "doc_a_figure_type": "plot",
      "doc_a_image_path": "data/mineru_output/1412.3756/1412.3756/hybrid_auto/images/1412.3756_page0_fig2.jpg",
      "doc_a_caption": "Fig. 3: Disparate impact (DI) vs. utility (1-BER) from our combinatorial and geometric partial repair processes using the SVM to classify on the Adult Income and German Credit data sets and the simple threshold classifier on the Ricci data set. Recall that only points with $\\mathsf { D } \\mathsf { I } \\ge \\tau = 0 . 8$ are legal. $\\mathsf { D } \\mathsf { I } = 1 . 0$ represents full fairness.",
      "doc_a_visual_score": 5,
      "doc_b_query_id": "l1_1805.09458_1805.09458_fig_5_446",
      "doc_b_query": "How does using digit label as covariate enable the leftmost real 9 to generate all ten digits rightward?",
      "doc_b_answer": "Using digit label as covariate class c pushes all non-class stylistic information into latent space while removing information about the exact digit. This separation allows the decoder to recombine the extracted style with any specified digit label c′ to produce all ten digits.",
      "doc_b_visual_anchor": "Top row showing real digit 9 on left and generated sequence 0123456789 on right",
      "doc_b_text_evidence": "We use the digit label as the covariate class c, which pushes all non-class stylistic information into the latent space while attempting to remove information about the exact digit being written.",
      "doc_b_figure_id": "1805.09458_fig_5",
      "doc_b_figure_type": "example",
      "doc_b_image_path": "data/mineru_output/1805.09458/1805.09458/hybrid_auto/images/1805.09458_page0_fig4.jpg",
      "doc_b_caption": "Figure 3: We demonstrate the ability to generate stylistically similar images of varying classes using the MNIST dataset. The left column is mapped into $z$ that is invariant to its digit label c. We then can generate an image using $z$ and any other specified digit, $c ^ { \\prime }$ , as show on the right.",
      "doc_b_visual_score": 3,
      "score": 13.5
    },
    {
      "doc_a": "1412.3756",
      "doc_b": "1805.11202",
      "shared_entities": [
        "Fairness",
        "disparate impact"
      ],
      "shared_entity_count": 2,
      "doc_a_query_id": "l1_1412.3756_1412.3756_fig_3_55",
      "doc_a_query": "Does the utility decay pattern visible across all scatter points support the expected tradeoff between fairness and utility?",
      "doc_a_answer": "Yes, the scatter points show declining utility (from ~1.0 to ~0.5-0.9) as disparate impact increases toward the green line at DI=1.0, demonstrating the expected decay over utility as fairness increases.",
      "doc_a_visual_anchor": "Dashed trend lines connecting scatter points showing downward slope from left to right",
      "doc_a_text_evidence": "The results, shown in Figure 3, demonstrate the expected decay over utility as fairness increases.",
      "doc_a_figure_id": "1412.3756_fig_3",
      "doc_a_figure_type": "plot",
      "doc_a_image_path": "data/mineru_output/1412.3756/1412.3756/hybrid_auto/images/1412.3756_page0_fig2.jpg",
      "doc_a_caption": "Fig. 3: Disparate impact (DI) vs. utility (1-BER) from our combinatorial and geometric partial repair processes using the SVM to classify on the Adult Income and German Credit data sets and the simple threshold classifier on the Ricci data set. Recall that only points with $\\mathsf { D } \\mathsf { I } \\ge \\tau = 0 . 8$ are legal. $\\mathsf { D } \\mathsf { I } = 1 . 0$ represents full fairness.",
      "doc_a_visual_score": 5,
      "doc_b_query_id": "l1_1805.11202_1805.11202_fig_5_454",
      "doc_b_query": "Why does the black curve reach approximately 0.10 at x≈3.5 while both conditional distributions peak below 0.05?",
      "doc_b_answer": "The black curve represents the overall data distribution P_data(x), which combines both conditional distributions P_data(x|s=1) and P_data(x|s=0), resulting in higher probability density than either individual conditional distribution alone.",
      "doc_b_visual_anchor": "Black solid curve peak at 0.10 versus green and red dashed curves peaking below 0.05",
      "doc_b_text_evidence": "shows the distributions P_data(x) (black), P_data(x|s=1) (green) and P_data(x|s=0) (red) of real data",
      "doc_b_figure_id": "1805.11202_fig_5",
      "doc_b_figure_type": "plot",
      "doc_b_image_path": "data/mineru_output/1805.11202/1805.11202/hybrid_auto/images/1805.11202_page0_fig4.jpg",
      "doc_b_caption": "Figure 3: Comparing FairGAN, NaïveFairGAN-I and NaïveFairGAN-II on a toy dataset. (a) shows the distributions $P _ { \\mathbf { d a t a } } ( x )$ (black), $P _ { \\bf d a t a } ( x | s = 1 )$ (green) and $P _ { \\bf d a t a } ( x | s = 0 )$ (red) of real data; (b), (c) and (d) are distributions $P _ { G } ( x ) , P _ { G } ( x | s = 1 )$ and $P _ { G } ( x | s = 0 )$ P x s P x s Pof synthetic datasets generated by NaïveFairGAN-I, NaïveFairGAN-II and FairGAN separately.",
      "doc_b_visual_score": 7,
      "score": 13.5
    },
    {
      "doc_a": "1412.3756",
      "doc_b": "1905.12843",
      "shared_entities": [
        "LR",
        "accuracy",
        "logistic regression"
      ],
      "shared_entity_count": 3,
      "doc_a_query_id": "l1_1412.3756_1412.3756_fig_3_55",
      "doc_a_query": "Does the utility decay pattern visible across all scatter points support the expected tradeoff between fairness and utility?",
      "doc_a_answer": "Yes, the scatter points show declining utility (from ~1.0 to ~0.5-0.9) as disparate impact increases toward the green line at DI=1.0, demonstrating the expected decay over utility as fairness increases.",
      "doc_a_visual_anchor": "Dashed trend lines connecting scatter points showing downward slope from left to right",
      "doc_a_text_evidence": "The results, shown in Figure 3, demonstrate the expected decay over utility as fairness increases.",
      "doc_a_figure_id": "1412.3756_fig_3",
      "doc_a_figure_type": "plot",
      "doc_a_image_path": "data/mineru_output/1412.3756/1412.3756/hybrid_auto/images/1412.3756_page0_fig2.jpg",
      "doc_a_caption": "Fig. 3: Disparate impact (DI) vs. utility (1-BER) from our combinatorial and geometric partial repair processes using the SVM to classify on the Adult Income and German Credit data sets and the simple threshold classifier on the Ricci data set. Recall that only points with $\\mathsf { D } \\mathsf { I } \\ge \\tau = 0 . 8$ are legal. $\\mathsf { D } \\mathsf { I } = 1 . 0$ represents full fairness.",
      "doc_a_visual_score": 5,
      "doc_b_query_id": "l1_1905.12843_1905.12843_fig_4_817",
      "doc_b_query": "Why does the solid black line in bottom plots perform better despite the claim of dominance on all datasets except adult?",
      "doc_b_answer": "The adult dataset is the exception where fair classification (solid black line) slightly outperforms the proposed method. On all other datasets, the proposed method dominates or matches baselines up to statistical uncertainty.",
      "doc_b_visual_anchor": "solid black line (fair class., oracle=LR, model=linear) in bottom plots",
      "doc_b_text_evidence": "Our method dominates or matches the baselines up to statistical uncertainty on all datasets except adult, where fair classification is slightly better.",
      "doc_b_figure_id": "1905.12843_fig_4",
      "doc_b_figure_type": "plot",
      "doc_b_image_path": "data/mineru_output/1905.12843/1905.12843/hybrid_auto/images/1905.12843_page0_fig3.jpg",
      "doc_b_caption": "Figure 1. Relative test loss versus the worst constraint violation with respect to SP. Relative losses are computed by subtracting the smallest baseline loss from the actual loss. For our algorithm and fair classification we plot the convex envelope of the predictors obtained on training data at various accuracy–fairness tradeoffs. We show $9 5 \\%$ confidence bands for the relative loss of our method and fair classification, and also show $9 5 \\%$ confidence intervals for constraint violation (the same for all methods). Our method dominates or matches the baselines up to statistical uncertainty on all datasets except adult, where fair classification is slightly better.",
      "doc_b_visual_score": 4,
      "score": 13.0
    },
    {
      "doc_a": "1511.00830",
      "doc_b": "1810.01943",
      "shared_entities": [
        "LR",
        "RF",
        "accuracy"
      ],
      "shared_entity_count": 3,
      "doc_a_query_id": "l1_1511.00830_1511.00830_fig_1_64",
      "doc_a_query": "Why does node S point directly to X rather than through Z in this unsupervised representation learning model?",
      "doc_a_answer": "In unsupervised representation learning, S represents noise or nuisance variables that directly affect the observed data X, independent of the informative latent factors Z. This direct connection models uninformative dimensions that are typically detrimental for the task under consideration.",
      "doc_a_visual_anchor": "arrow from node S (top-left) directly to node X (bottom-center)",
      "doc_a_text_evidence": "Uninformative dimensions are often called \"noise\" or \"nuisance variables\" while informative dimensions are usually called latent or hidden factors of variation.",
      "doc_a_figure_id": "1511.00830_fig_1",
      "doc_a_figure_type": "diagram",
      "doc_a_image_path": "data/mineru_output/1511.00830/1511.00830/hybrid_auto/images/1511.00830_page0_fig0.jpg",
      "doc_a_caption": "Figure 1: Unsupervised model",
      "doc_a_visual_score": 5,
      "doc_b_query_id": "l1_1810.01943_1810.01943_fig_10_578",
      "doc_b_query": "Do the green circles at disparate impact 0.8 in the top panel support that optimized pre-processing improves fairness?",
      "doc_b_answer": "Yes, the green circles (RF,Optimized pre-processing) show disparate impact closest to the ideal value of 1 in the top panel before mitigation, indicating that optimized pre-processing achieves better fairness metrics.",
      "doc_b_visual_anchor": "green circles at disparate impact ~0.8 in top panel",
      "doc_b_text_evidence": "The ideal fair value of disparate impact is 1, whereas for all other metrics it is 0.",
      "doc_b_figure_id": "1810.01943_fig_10",
      "doc_b_figure_type": "plot",
      "doc_b_image_path": "data/mineru_output/1810.01943/1810.01943/hybrid_auto/images/1810.01943_page0_fig9.jpg",
      "doc_b_caption": "Figure 5. Fairness vs. Balanced Accuracy before (top panel) and after (bottom panel) applying various bias mitigation algorithms. Four different fairness metrics are shown. In most cases two classifiers (Logistic regression - LR or Random forest classifier - RF) were used. The ideal fair value of disparate impact is 1, whereas for all other metrics it is 0. The circles indicate the mean value and bars indicate the extent of $\\pm 1$ standard deviation. Dataset: Adult, Protected attribute: race.",
      "doc_b_visual_score": 3,
      "score": 13.0
    },
    {
      "doc_a": "1805.09458",
      "doc_b": "1905.12843",
      "shared_entities": [
        "accuracy",
        "logistic regression"
      ],
      "shared_entity_count": 2,
      "doc_a_query_id": "l1_1805.09458_1805.09458_fig_5_446",
      "doc_a_query": "How does using digit label as covariate enable the leftmost real 9 to generate all ten digits rightward?",
      "doc_a_answer": "Using digit label as covariate class c pushes all non-class stylistic information into latent space while removing information about the exact digit. This separation allows the decoder to recombine the extracted style with any specified digit label c′ to produce all ten digits.",
      "doc_a_visual_anchor": "Top row showing real digit 9 on left and generated sequence 0123456789 on right",
      "doc_a_text_evidence": "We use the digit label as the covariate class c, which pushes all non-class stylistic information into the latent space while attempting to remove information about the exact digit being written.",
      "doc_a_figure_id": "1805.09458_fig_5",
      "doc_a_figure_type": "example",
      "doc_a_image_path": "data/mineru_output/1805.09458/1805.09458/hybrid_auto/images/1805.09458_page0_fig4.jpg",
      "doc_a_caption": "Figure 3: We demonstrate the ability to generate stylistically similar images of varying classes using the MNIST dataset. The left column is mapped into $z$ that is invariant to its digit label c. We then can generate an image using $z$ and any other specified digit, $c ^ { \\prime }$ , as show on the right.",
      "doc_a_visual_score": 3,
      "doc_b_query_id": "l1_1905.12843_1905.12843_fig_4_817",
      "doc_b_query": "Why does the solid black line in bottom plots perform better despite the claim of dominance on all datasets except adult?",
      "doc_b_answer": "The adult dataset is the exception where fair classification (solid black line) slightly outperforms the proposed method. On all other datasets, the proposed method dominates or matches baselines up to statistical uncertainty.",
      "doc_b_visual_anchor": "solid black line (fair class., oracle=LR, model=linear) in bottom plots",
      "doc_b_text_evidence": "Our method dominates or matches the baselines up to statistical uncertainty on all datasets except adult, where fair classification is slightly better.",
      "doc_b_figure_id": "1905.12843_fig_4",
      "doc_b_figure_type": "plot",
      "doc_b_image_path": "data/mineru_output/1905.12843/1905.12843/hybrid_auto/images/1905.12843_page0_fig3.jpg",
      "doc_b_caption": "Figure 1. Relative test loss versus the worst constraint violation with respect to SP. Relative losses are computed by subtracting the smallest baseline loss from the actual loss. For our algorithm and fair classification we plot the convex envelope of the predictors obtained on training data at various accuracy–fairness tradeoffs. We show $9 5 \\%$ confidence bands for the relative loss of our method and fair classification, and also show $9 5 \\%$ confidence intervals for constraint violation (the same for all methods). Our method dominates or matches the baselines up to statistical uncertainty on all datasets except adult, where fair classification is slightly better.",
      "doc_b_visual_score": 4,
      "score": 13.0
    },
    {
      "doc_a": "1412.3756",
      "doc_b": "1610.07524",
      "shared_entities": [
        "disparate impact",
        "logistic regression"
      ],
      "shared_entity_count": 2,
      "doc_a_query_id": "l1_1412.3756_1412.3756_fig_3_55",
      "doc_a_query": "Does the utility decay pattern visible across all scatter points support the expected tradeoff between fairness and utility?",
      "doc_a_answer": "Yes, the scatter points show declining utility (from ~1.0 to ~0.5-0.9) as disparate impact increases toward the green line at DI=1.0, demonstrating the expected decay over utility as fairness increases.",
      "doc_a_visual_anchor": "Dashed trend lines connecting scatter points showing downward slope from left to right",
      "doc_a_text_evidence": "The results, shown in Figure 3, demonstrate the expected decay over utility as fairness increases.",
      "doc_a_figure_id": "1412.3756_fig_3",
      "doc_a_figure_type": "plot",
      "doc_a_image_path": "data/mineru_output/1412.3756/1412.3756/hybrid_auto/images/1412.3756_page0_fig2.jpg",
      "doc_a_caption": "Fig. 3: Disparate impact (DI) vs. utility (1-BER) from our combinatorial and geometric partial repair processes using the SVM to classify on the Adult Income and German Credit data sets and the simple threshold classifier on the Ricci data set. Recall that only points with $\\mathsf { D } \\mathsf { I } \\ge \\tau = 0 . 8$ are legal. $\\mathsf { D } \\mathsf { I } = 1 . 0$ represents full fairness.",
      "doc_a_visual_score": 5,
      "doc_b_query_id": "l1_1610.07524_1610.07524_fig_2_170",
      "doc_b_query": "Why do differences persist between gray and orange bars within each prior category despite equal overall FPR claims?",
      "doc_b_answer": "Even if overall FPR is equal between Black and White defendants, differences in error rates can still appear within individual prior record score categories. This explains the persistent gap between gray and orange bars across all five groupings shown.",
      "doc_b_visual_anchor": "Consistent separation between gray and orange bars across all five x-axis categories (0, 1-3, 4-6, 7-10, >10)",
      "doc_b_text_evidence": "That is, even if $\\mathrm { F P R } _ { b } = \\mathrm { F P R } _ { w }$ , we may still see differences in error rates within prior record score categories (see e.g., Figure 2).",
      "doc_b_figure_id": "1610.07524_fig_2",
      "doc_b_figure_type": "plot",
      "doc_b_image_path": "data/mineru_output/1610.07524/1610.07524/hybrid_auto/images/1610.07524_page0_fig1.jpg",
      "doc_b_caption": "Figure 2: False positive rates across prior record count for defendants charged with a Misdemeanor offense. Plot is based on assessing a defendant as “high-risk” if their COMPAS decile score is $> 4$ . Error bars represent 95% confidence intervals.   ",
      "doc_b_visual_score": 3,
      "score": 13.0
    },
    {
      "doc_a": "1412.3756",
      "doc_b": "1711.05144",
      "shared_entities": [
        "accuracy",
        "disparate impact"
      ],
      "shared_entity_count": 2,
      "doc_a_query_id": "l1_1412.3756_1412.3756_fig_3_55",
      "doc_a_query": "Does the utility decay pattern visible across all scatter points support the expected tradeoff between fairness and utility?",
      "doc_a_answer": "Yes, the scatter points show declining utility (from ~1.0 to ~0.5-0.9) as disparate impact increases toward the green line at DI=1.0, demonstrating the expected decay over utility as fairness increases.",
      "doc_a_visual_anchor": "Dashed trend lines connecting scatter points showing downward slope from left to right",
      "doc_a_text_evidence": "The results, shown in Figure 3, demonstrate the expected decay over utility as fairness increases.",
      "doc_a_figure_id": "1412.3756_fig_3",
      "doc_a_figure_type": "plot",
      "doc_a_image_path": "data/mineru_output/1412.3756/1412.3756/hybrid_auto/images/1412.3756_page0_fig2.jpg",
      "doc_a_caption": "Fig. 3: Disparate impact (DI) vs. utility (1-BER) from our combinatorial and geometric partial repair processes using the SVM to classify on the Adult Income and German Credit data sets and the simple threshold classifier on the Ricci data set. Recall that only points with $\\mathsf { D } \\mathsf { I } \\ge \\tau = 0 . 8$ are legal. $\\mathsf { D } \\mathsf { I } = 1 . 0$ represents full fairness.",
      "doc_a_visual_score": 5,
      "doc_b_query_id": "l1_1711.05144_1711.05144_fig_3_299",
      "doc_b_query": "Why do the color-coded trajectories form distinct curves rather than a single frontier, given the reductions approach?",
      "doc_b_answer": "Each colored trajectory represents Pareto-optimal solutions for a specific gamma parameter value. The reductions approach by Agarwal et al. generates different error-unfairness trade-offs as gamma varies.",
      "doc_b_visual_anchor": "Multiple distinct colored curves (blue, red, green, purple, orange) descending from upper-left",
      "doc_b_text_evidence": "Pareto-optimal error-unfairness values, color coded by varying values of the input parameter γ",
      "doc_b_figure_id": "1711.05144_fig_3",
      "doc_b_figure_type": "plot",
      "doc_b_image_path": "data/mineru_output/1711.05144/1711.05144/hybrid_auto/images/1711.05144_page0_fig2.jpg",
      "doc_b_caption": "Figure 2: (a) Pareto-optimal error-unfairness values, color coded by varying values of the input parameter γ. (b) Aggregate Pareto frontier across all values of γ. Here the γ values cover the same range but are sampled more densely to get a smoother frontier. See text for details.",
      "doc_b_visual_score": 7,
      "score": 12.5
    },
    {
      "doc_a": "1706.02409",
      "doc_b": "1805.09458",
      "shared_entities": [
        "accuracy",
        "logistic regression"
      ],
      "shared_entity_count": 2,
      "doc_a_query_id": "l1_1706.02409_1706.02409_fig_10_235",
      "doc_a_query": "What explains the progressive height increase in magenta and yellow hybrid bars as alpha decreases from 0.5 to 0.01?",
      "doc_a_answer": "As alpha decreases (stronger regularization), the hybrid fairness regularizers show increasing Price of Fairness, indicating that stricter hybrid fairness constraints progressively sacrifice more model performance to achieve fairness goals.",
      "doc_a_visual_anchor": "magenta (Hybrid, separate) and yellow (Hybrid, single) bars increasing from left to right",
      "doc_a_text_evidence": "In Proceedings of the 16th International Conference on Data Mining, pages 1–10, 2016. [2] Julia Angwin, Jeff Larson, Surya Mattu, and Lauren Kirchner. Machine bias. Propublica, 2016.",
      "doc_a_figure_id": "1706.02409_fig_10",
      "doc_a_figure_type": "plot",
      "doc_a_image_path": "data/mineru_output/1706.02409/1706.02409/hybrid_auto/images/1706.02409_page0_fig9.jpg",
      "doc_a_caption": "Figure 2: The “Price of Fairness” across data sets, for each type of fairness regularizer, in both the single and separate model case.",
      "doc_a_visual_score": 3,
      "doc_b_query_id": "l1_1805.09458_1805.09458_fig_5_446",
      "doc_b_query": "How does using digit label as covariate enable the leftmost real 9 to generate all ten digits rightward?",
      "doc_b_answer": "Using digit label as covariate class c pushes all non-class stylistic information into latent space while removing information about the exact digit. This separation allows the decoder to recombine the extracted style with any specified digit label c′ to produce all ten digits.",
      "doc_b_visual_anchor": "Top row showing real digit 9 on left and generated sequence 0123456789 on right",
      "doc_b_text_evidence": "We use the digit label as the covariate class c, which pushes all non-class stylistic information into the latent space while attempting to remove information about the exact digit being written.",
      "doc_b_figure_id": "1805.09458_fig_5",
      "doc_b_figure_type": "example",
      "doc_b_image_path": "data/mineru_output/1805.09458/1805.09458/hybrid_auto/images/1805.09458_page0_fig4.jpg",
      "doc_b_caption": "Figure 3: We demonstrate the ability to generate stylistically similar images of varying classes using the MNIST dataset. The left column is mapped into $z$ that is invariant to its digit label c. We then can generate an image using $z$ and any other specified digit, $c ^ { \\prime }$ , as show on the right.",
      "doc_b_visual_score": 3,
      "score": 12.5
    },
    {
      "doc_a": "1706.02409",
      "doc_b": "1810.01943",
      "shared_entities": [
        "Fairness",
        "accuracy",
        "logistic regression"
      ],
      "shared_entity_count": 3,
      "doc_a_query_id": "l1_1706.02409_1706.02409_fig_10_235",
      "doc_a_query": "What explains the progressive height increase in magenta and yellow hybrid bars as alpha decreases from 0.5 to 0.01?",
      "doc_a_answer": "As alpha decreases (stronger regularization), the hybrid fairness regularizers show increasing Price of Fairness, indicating that stricter hybrid fairness constraints progressively sacrifice more model performance to achieve fairness goals.",
      "doc_a_visual_anchor": "magenta (Hybrid, separate) and yellow (Hybrid, single) bars increasing from left to right",
      "doc_a_text_evidence": "In Proceedings of the 16th International Conference on Data Mining, pages 1–10, 2016. [2] Julia Angwin, Jeff Larson, Surya Mattu, and Lauren Kirchner. Machine bias. Propublica, 2016.",
      "doc_a_figure_id": "1706.02409_fig_10",
      "doc_a_figure_type": "plot",
      "doc_a_image_path": "data/mineru_output/1706.02409/1706.02409/hybrid_auto/images/1706.02409_page0_fig9.jpg",
      "doc_a_caption": "Figure 2: The “Price of Fairness” across data sets, for each type of fairness regularizer, in both the single and separate model case.",
      "doc_a_visual_score": 3,
      "doc_b_query_id": "l1_1810.01943_1810.01943_fig_10_578",
      "doc_b_query": "Do the green circles at disparate impact 0.8 in the top panel support that optimized pre-processing improves fairness?",
      "doc_b_answer": "Yes, the green circles (RF,Optimized pre-processing) show disparate impact closest to the ideal value of 1 in the top panel before mitigation, indicating that optimized pre-processing achieves better fairness metrics.",
      "doc_b_visual_anchor": "green circles at disparate impact ~0.8 in top panel",
      "doc_b_text_evidence": "The ideal fair value of disparate impact is 1, whereas for all other metrics it is 0.",
      "doc_b_figure_id": "1810.01943_fig_10",
      "doc_b_figure_type": "plot",
      "doc_b_image_path": "data/mineru_output/1810.01943/1810.01943/hybrid_auto/images/1810.01943_page0_fig9.jpg",
      "doc_b_caption": "Figure 5. Fairness vs. Balanced Accuracy before (top panel) and after (bottom panel) applying various bias mitigation algorithms. Four different fairness metrics are shown. In most cases two classifiers (Logistic regression - LR or Random forest classifier - RF) were used. The ideal fair value of disparate impact is 1, whereas for all other metrics it is 0. The circles indicate the mean value and bars indicate the extent of $\\pm 1$ standard deviation. Dataset: Adult, Protected attribute: race.",
      "doc_b_visual_score": 3,
      "score": 12.5
    },
    {
      "doc_a": "1805.09458",
      "doc_b": "1810.01943",
      "shared_entities": [
        "accuracy",
        "logistic regression"
      ],
      "shared_entity_count": 2,
      "doc_a_query_id": "l1_1805.09458_1805.09458_fig_5_446",
      "doc_a_query": "How does using digit label as covariate enable the leftmost real 9 to generate all ten digits rightward?",
      "doc_a_answer": "Using digit label as covariate class c pushes all non-class stylistic information into latent space while removing information about the exact digit. This separation allows the decoder to recombine the extracted style with any specified digit label c′ to produce all ten digits.",
      "doc_a_visual_anchor": "Top row showing real digit 9 on left and generated sequence 0123456789 on right",
      "doc_a_text_evidence": "We use the digit label as the covariate class c, which pushes all non-class stylistic information into the latent space while attempting to remove information about the exact digit being written.",
      "doc_a_figure_id": "1805.09458_fig_5",
      "doc_a_figure_type": "example",
      "doc_a_image_path": "data/mineru_output/1805.09458/1805.09458/hybrid_auto/images/1805.09458_page0_fig4.jpg",
      "doc_a_caption": "Figure 3: We demonstrate the ability to generate stylistically similar images of varying classes using the MNIST dataset. The left column is mapped into $z$ that is invariant to its digit label c. We then can generate an image using $z$ and any other specified digit, $c ^ { \\prime }$ , as show on the right.",
      "doc_a_visual_score": 3,
      "doc_b_query_id": "l1_1810.01943_1810.01943_fig_10_578",
      "doc_b_query": "Do the green circles at disparate impact 0.8 in the top panel support that optimized pre-processing improves fairness?",
      "doc_b_answer": "Yes, the green circles (RF,Optimized pre-processing) show disparate impact closest to the ideal value of 1 in the top panel before mitigation, indicating that optimized pre-processing achieves better fairness metrics.",
      "doc_b_visual_anchor": "green circles at disparate impact ~0.8 in top panel",
      "doc_b_text_evidence": "The ideal fair value of disparate impact is 1, whereas for all other metrics it is 0.",
      "doc_b_figure_id": "1810.01943_fig_10",
      "doc_b_figure_type": "plot",
      "doc_b_image_path": "data/mineru_output/1810.01943/1810.01943/hybrid_auto/images/1810.01943_page0_fig9.jpg",
      "doc_b_caption": "Figure 5. Fairness vs. Balanced Accuracy before (top panel) and after (bottom panel) applying various bias mitigation algorithms. Four different fairness metrics are shown. In most cases two classifiers (Logistic regression - LR or Random forest classifier - RF) were used. The ideal fair value of disparate impact is 1, whereas for all other metrics it is 0. The circles indicate the mean value and bars indicate the extent of $\\pm 1$ standard deviation. Dataset: Adult, Protected attribute: race.",
      "doc_b_visual_score": 3,
      "score": 12.5
    },
    {
      "doc_a": "1805.11202",
      "doc_b": "1810.01943",
      "shared_entities": [
        "Fairness",
        "disparate impact"
      ],
      "shared_entity_count": 2,
      "doc_a_query_id": "l1_1805.11202_1805.11202_fig_5_454",
      "doc_a_query": "Why does the black curve reach approximately 0.10 at x≈3.5 while both conditional distributions peak below 0.05?",
      "doc_a_answer": "The black curve represents the overall data distribution P_data(x), which combines both conditional distributions P_data(x|s=1) and P_data(x|s=0), resulting in higher probability density than either individual conditional distribution alone.",
      "doc_a_visual_anchor": "Black solid curve peak at 0.10 versus green and red dashed curves peaking below 0.05",
      "doc_a_text_evidence": "shows the distributions P_data(x) (black), P_data(x|s=1) (green) and P_data(x|s=0) (red) of real data",
      "doc_a_figure_id": "1805.11202_fig_5",
      "doc_a_figure_type": "plot",
      "doc_a_image_path": "data/mineru_output/1805.11202/1805.11202/hybrid_auto/images/1805.11202_page0_fig4.jpg",
      "doc_a_caption": "Figure 3: Comparing FairGAN, NaïveFairGAN-I and NaïveFairGAN-II on a toy dataset. (a) shows the distributions $P _ { \\mathbf { d a t a } } ( x )$ (black), $P _ { \\bf d a t a } ( x | s = 1 )$ (green) and $P _ { \\bf d a t a } ( x | s = 0 )$ (red) of real data; (b), (c) and (d) are distributions $P _ { G } ( x ) , P _ { G } ( x | s = 1 )$ and $P _ { G } ( x | s = 0 )$ P x s P x s Pof synthetic datasets generated by NaïveFairGAN-I, NaïveFairGAN-II and FairGAN separately.",
      "doc_a_visual_score": 7,
      "doc_b_query_id": "l1_1810.01943_1810.01943_fig_10_578",
      "doc_b_query": "Do the green circles at disparate impact 0.8 in the top panel support that optimized pre-processing improves fairness?",
      "doc_b_answer": "Yes, the green circles (RF,Optimized pre-processing) show disparate impact closest to the ideal value of 1 in the top panel before mitigation, indicating that optimized pre-processing achieves better fairness metrics.",
      "doc_b_visual_anchor": "green circles at disparate impact ~0.8 in top panel",
      "doc_b_text_evidence": "The ideal fair value of disparate impact is 1, whereas for all other metrics it is 0.",
      "doc_b_figure_id": "1810.01943_fig_10",
      "doc_b_figure_type": "plot",
      "doc_b_image_path": "data/mineru_output/1810.01943/1810.01943/hybrid_auto/images/1810.01943_page0_fig9.jpg",
      "doc_b_caption": "Figure 5. Fairness vs. Balanced Accuracy before (top panel) and after (bottom panel) applying various bias mitigation algorithms. Four different fairness metrics are shown. In most cases two classifiers (Logistic regression - LR or Random forest classifier - RF) were used. The ideal fair value of disparate impact is 1, whereas for all other metrics it is 0. The circles indicate the mean value and bars indicate the extent of $\\pm 1$ standard deviation. Dataset: Adult, Protected attribute: race.",
      "doc_b_visual_score": 3,
      "score": 12.5
    },
    {
      "doc_a": "1805.09458",
      "doc_b": "1809.10083",
      "shared_entities": [
        "MNIST",
        "SNE",
        "t-SNE"
      ],
      "shared_entity_count": 3,
      "doc_a_query_id": "l1_1805.09458_1805.09458_fig_5_446",
      "doc_a_query": "How does using digit label as covariate enable the leftmost real 9 to generate all ten digits rightward?",
      "doc_a_answer": "Using digit label as covariate class c pushes all non-class stylistic information into latent space while removing information about the exact digit. This separation allows the decoder to recombine the extracted style with any specified digit label c′ to produce all ten digits.",
      "doc_a_visual_anchor": "Top row showing real digit 9 on left and generated sequence 0123456789 on right",
      "doc_a_text_evidence": "We use the digit label as the covariate class c, which pushes all non-class stylistic information into the latent space while attempting to remove information about the exact digit being written.",
      "doc_a_figure_id": "1805.09458_fig_5",
      "doc_a_figure_type": "example",
      "doc_a_image_path": "data/mineru_output/1805.09458/1805.09458/hybrid_auto/images/1805.09458_page0_fig4.jpg",
      "doc_a_caption": "Figure 3: We demonstrate the ability to generate stylistically similar images of varying classes using the MNIST dataset. The left column is mapped into $z$ that is invariant to its digit label c. We then can generate an image using $z$ and any other specified digit, $c ^ { \\prime }$ , as show on the right.",
      "doc_a_visual_score": 3,
      "doc_b_query_id": "l1_1809.10083_1809.10083_fig_5_550",
      "doc_b_query": "Does the separation between orange lower-right markers and cyan upper-right markers confirm that embedding e₁ captures lighting variations?",
      "doc_b_answer": "Yes, the clear spatial separation between different lighting conditions (lower-right vs upper-right) in the t-SNE visualization demonstrates that embedding e₁ successfully disentangles and represents lighting condition as a distinct factor.",
      "doc_b_visual_anchor": "orange plus signs (lower-right) and cyan stars (upper-right) spatially separated",
      "doc_b_text_evidence": "Figure 2 shows t-SNE [15] visualization of raw data and embeddings $e _ { 1 }$ and $e _ { 2 }$ for our model.",
      "doc_b_figure_id": "1809.10083_fig_5",
      "doc_b_figure_type": "plot",
      "doc_b_image_path": "data/mineru_output/1809.10083/1809.10083/hybrid_auto/images/1809.10083_page0_fig4.jpg",
      "doc_b_caption": "Figure 2: Extended Yale-B – t-SNE visualization of (a) raw data, (b) $e _ { 2 }$ labeled by lighting condition, (c) $e _ { 1 }$ labeled by lighting condition, and (d) $e _ { 1 }$ labeled by subject-ID (numerical markers, not colors).",
      "doc_b_visual_score": 4,
      "score": 12.5
    },
    {
      "doc_a": "1707.09457",
      "doc_b": "1903.08136",
      "shared_entities": [
        "In Figure"
      ],
      "shared_entity_count": 1,
      "doc_a_query_id": "l1_1707.09457_1707.09457_fig_7_264",
      "doc_a_query": "Why do green dots dominate the upper-right region while red dots cluster at lower training ratios?",
      "doc_a_answer": "RBA successfully reduces bias amplification across all initial training biases. Lower training ratios initially showed more violations, but after RBA application, most points (green) meet the margin constraint.",
      "doc_a_visual_anchor": "green dots in upper-right region versus red dots at lower x-axis values",
      "doc_a_text_evidence": "Across all initial training biases, RBA is able to reduce the bias amplification.",
      "doc_a_figure_id": "1707.09457_fig_7",
      "doc_a_figure_type": "plot",
      "doc_a_image_path": "data/mineru_output/1707.09457/1707.09457/hybrid_auto/images/1707.09457_page0_fig6.jpg",
      "doc_a_caption": "Figure 3: Results of reducing bias amplification using RBA on imSitu vSRL and MS-COCO MLC. Figures 3(a)-(d) show initial training set bias along the x-axis and development set bias along the yaxis. Dotted blue lines indicate the 0.05 margin used in RBA, with points violating the margin shown in red while points meeting the margin are shown in green. Across both settings adding RBA significantly reduces the number of violations, and reduces the bias amplification significantly. Figures 3(e)-(f) demonstrate bias amplification as a function of training bias, with and without RBA. Across all initial training biases, RBA is able to reduce the bias amplification.",
      "doc_a_visual_score": 6,
      "doc_b_query_id": "l1_1903.08136_1903.08136_fig_1_748",
      "doc_b_query": "Why does the gray sector contain only triangles and circles without showing how low-degree users get excluded?",
      "doc_b_answer": "The gray sector demonstrates a community where both opinion types (shapes) and variations (textures) coexist, but methods like CESNA and Louvain fail to assign low-degree nodes or assign them to trivially small communities that prevent inclusion in analysis.",
      "doc_b_visual_anchor": "gray quarter-circle sector with mixed blue triangles and red circles",
      "doc_b_text_evidence": "many community detection approaches either fail to assign low degree (or lowly-connected) users to communities, or assign them to trivially small communities that prevent them from being included in analysis",
      "doc_b_figure_id": "1903.08136_fig_1",
      "doc_b_figure_type": "diagram",
      "doc_b_image_path": "data/mineru_output/1903.08136/1903.08136/hybrid_auto/images/1903.08136_page0_fig0.jpg",
      "doc_b_caption": "Figure 1: Demonstration of versatility of relevant low-degree people in social networks who get ignored by biased community detection methods. Shapes (circle, triangle) represent opinion, and texture represents variations of the opinion. We provide real-world examples of how this manifests in subsequent sections.",
      "doc_b_visual_score": 4,
      "score": 12.5
    },
    {
      "doc_a": "1701.08230",
      "doc_b": "2005.07293",
      "shared_entities": [
        "COMPAS",
        "parity"
      ],
      "shared_entity_count": 2,
      "doc_a_query_id": "l1_1701.08230_1701.08230_fig_2_204",
      "doc_a_query": "Do the two distributions drawn from beta distributions with equal means in the right panel share the same peak location?",
      "doc_a_answer": "No, the blue curve peaks before 25% (near the dashed line) while the red curve peaks slightly after 25%, despite both distributions having equal means as stated.",
      "doc_a_visual_anchor": "Blue and red curves with vertical dashed line at 25% in top-right simulated panel",
      "doc_a_text_evidence": "simulated data drawn from two beta distributions with equal means (right)",
      "doc_a_figure_id": "1701.08230_fig_2",
      "doc_a_figure_type": "plot",
      "doc_a_image_path": "data/mineru_output/1701.08230/1701.08230/hybrid_auto/images/1701.08230_page0_fig1.jpg",
      "doc_a_caption": "Figure 1: Top: distribution of risk scores for Broward County data (le), and simulated data drawn from two beta distributions with equal means (right). Bottom: using a single threshold which detains $3 0 \\%$ of defendants in Broward County violates statistical parity (as measured by detention rate), predictive equality (false positive rate), and conditional statistical parity (detention rate conditional on number of prior arrests). We omit the last measure for the simulated data since that would require making additional assumptions about the relationship of priors and risk in the hypothetical populations.",
      "doc_a_visual_score": 7,
      "doc_b_query_id": "l1_2005.07293_2005.07293_fig_1_905",
      "doc_b_query": "How does the different-height box arrangement under the yellow figure in the right panel operationalize compensation for historically disadvantaged groups?",
      "doc_b_answer": "The taller box under the yellow figure represents resource distribution to overcome obstacles and raise opportunities for access, compensating historically disadvantaged groups so they can reach the apples like others.",
      "doc_b_visual_anchor": "Right panel 'Equity' showing yellow figure standing on tallest brown box",
      "doc_b_text_evidence": "Equity is the distribution of resources among groups to overcome obstacles and to raise their opportunities for access [Schement, 2001]. Thus, historically disadvantaged groups are compensated, and others get their fair share to reach their goals.",
      "doc_b_figure_id": "2005.07293_fig_1",
      "doc_b_figure_type": "diagram",
      "doc_b_image_path": "data/mineru_output/2005.07293/2005.07293/hybrid_auto/images/2005.07293_page0_fig0.jpg",
      "doc_b_caption": "Figure 1: Notion of equality in fairness is depicted and formalized along with our newly formalized notion of equity.",
      "doc_b_visual_score": 4,
      "score": 12.5
    },
    {
      "doc_a": "1610.07524",
      "doc_b": "2005.07293",
      "shared_entities": [
        "COM",
        "COMPAS",
        "PAS"
      ],
      "shared_entity_count": 3,
      "doc_a_query_id": "l1_1610.07524_1610.07524_fig_2_170",
      "doc_a_query": "Why do differences persist between gray and orange bars within each prior category despite equal overall FPR claims?",
      "doc_a_answer": "Even if overall FPR is equal between Black and White defendants, differences in error rates can still appear within individual prior record score categories. This explains the persistent gap between gray and orange bars across all five groupings shown.",
      "doc_a_visual_anchor": "Consistent separation between gray and orange bars across all five x-axis categories (0, 1-3, 4-6, 7-10, >10)",
      "doc_a_text_evidence": "That is, even if $\\mathrm { F P R } _ { b } = \\mathrm { F P R } _ { w }$ , we may still see differences in error rates within prior record score categories (see e.g., Figure 2).",
      "doc_a_figure_id": "1610.07524_fig_2",
      "doc_a_figure_type": "plot",
      "doc_a_image_path": "data/mineru_output/1610.07524/1610.07524/hybrid_auto/images/1610.07524_page0_fig1.jpg",
      "doc_a_caption": "Figure 2: False positive rates across prior record count for defendants charged with a Misdemeanor offense. Plot is based on assessing a defendant as “high-risk” if their COMPAS decile score is $> 4$ . Error bars represent 95% confidence intervals.   ",
      "doc_a_visual_score": 3,
      "doc_b_query_id": "l1_2005.07293_2005.07293_fig_1_905",
      "doc_b_query": "How does the different-height box arrangement under the yellow figure in the right panel operationalize compensation for historically disadvantaged groups?",
      "doc_b_answer": "The taller box under the yellow figure represents resource distribution to overcome obstacles and raise opportunities for access, compensating historically disadvantaged groups so they can reach the apples like others.",
      "doc_b_visual_anchor": "Right panel 'Equity' showing yellow figure standing on tallest brown box",
      "doc_b_text_evidence": "Equity is the distribution of resources among groups to overcome obstacles and to raise their opportunities for access [Schement, 2001]. Thus, historically disadvantaged groups are compensated, and others get their fair share to reach their goals.",
      "doc_b_figure_id": "2005.07293_fig_1",
      "doc_b_figure_type": "diagram",
      "doc_b_image_path": "data/mineru_output/2005.07293/2005.07293/hybrid_auto/images/2005.07293_page0_fig0.jpg",
      "doc_b_caption": "Figure 1: Notion of equality in fairness is depicted and formalized along with our newly formalized notion of equity.",
      "doc_b_visual_score": 4,
      "score": 12.5
    },
    {
      "doc_a": "1412.3756",
      "doc_b": "1511.00830",
      "shared_entities": [
        "LR",
        "accuracy"
      ],
      "shared_entity_count": 2,
      "doc_a_query_id": "l1_1412.3756_1412.3756_fig_3_55",
      "doc_a_query": "Does the utility decay pattern visible across all scatter points support the expected tradeoff between fairness and utility?",
      "doc_a_answer": "Yes, the scatter points show declining utility (from ~1.0 to ~0.5-0.9) as disparate impact increases toward the green line at DI=1.0, demonstrating the expected decay over utility as fairness increases.",
      "doc_a_visual_anchor": "Dashed trend lines connecting scatter points showing downward slope from left to right",
      "doc_a_text_evidence": "The results, shown in Figure 3, demonstrate the expected decay over utility as fairness increases.",
      "doc_a_figure_id": "1412.3756_fig_3",
      "doc_a_figure_type": "plot",
      "doc_a_image_path": "data/mineru_output/1412.3756/1412.3756/hybrid_auto/images/1412.3756_page0_fig2.jpg",
      "doc_a_caption": "Fig. 3: Disparate impact (DI) vs. utility (1-BER) from our combinatorial and geometric partial repair processes using the SVM to classify on the Adult Income and German Credit data sets and the simple threshold classifier on the Ricci data set. Recall that only points with $\\mathsf { D } \\mathsf { I } \\ge \\tau = 0 . 8$ are legal. $\\mathsf { D } \\mathsf { I } = 1 . 0$ represents full fairness.",
      "doc_a_visual_score": 5,
      "doc_b_query_id": "l1_1511.00830_1511.00830_fig_1_64",
      "doc_b_query": "Why does node S point directly to X rather than through Z in this unsupervised representation learning model?",
      "doc_b_answer": "In unsupervised representation learning, S represents noise or nuisance variables that directly affect the observed data X, independent of the informative latent factors Z. This direct connection models uninformative dimensions that are typically detrimental for the task under consideration.",
      "doc_b_visual_anchor": "arrow from node S (top-left) directly to node X (bottom-center)",
      "doc_b_text_evidence": "Uninformative dimensions are often called \"noise\" or \"nuisance variables\" while informative dimensions are usually called latent or hidden factors of variation.",
      "doc_b_figure_id": "1511.00830_fig_1",
      "doc_b_figure_type": "diagram",
      "doc_b_image_path": "data/mineru_output/1511.00830/1511.00830/hybrid_auto/images/1511.00830_page0_fig0.jpg",
      "doc_b_caption": "Figure 1: Unsupervised model",
      "doc_b_visual_score": 5,
      "score": 12.0
    },
    {
      "doc_a": "1810.01943",
      "doc_b": "1905.12843",
      "shared_entities": [
        "LR",
        "accuracy",
        "logistic regression"
      ],
      "shared_entity_count": 3,
      "doc_a_query_id": "l1_1810.01943_1810.01943_fig_10_578",
      "doc_a_query": "Do the green circles at disparate impact 0.8 in the top panel support that optimized pre-processing improves fairness?",
      "doc_a_answer": "Yes, the green circles (RF,Optimized pre-processing) show disparate impact closest to the ideal value of 1 in the top panel before mitigation, indicating that optimized pre-processing achieves better fairness metrics.",
      "doc_a_visual_anchor": "green circles at disparate impact ~0.8 in top panel",
      "doc_a_text_evidence": "The ideal fair value of disparate impact is 1, whereas for all other metrics it is 0.",
      "doc_a_figure_id": "1810.01943_fig_10",
      "doc_a_figure_type": "plot",
      "doc_a_image_path": "data/mineru_output/1810.01943/1810.01943/hybrid_auto/images/1810.01943_page0_fig9.jpg",
      "doc_a_caption": "Figure 5. Fairness vs. Balanced Accuracy before (top panel) and after (bottom panel) applying various bias mitigation algorithms. Four different fairness metrics are shown. In most cases two classifiers (Logistic regression - LR or Random forest classifier - RF) were used. The ideal fair value of disparate impact is 1, whereas for all other metrics it is 0. The circles indicate the mean value and bars indicate the extent of $\\pm 1$ standard deviation. Dataset: Adult, Protected attribute: race.",
      "doc_a_visual_score": 3,
      "doc_b_query_id": "l1_1905.12843_1905.12843_fig_4_817",
      "doc_b_query": "Why does the solid black line in bottom plots perform better despite the claim of dominance on all datasets except adult?",
      "doc_b_answer": "The adult dataset is the exception where fair classification (solid black line) slightly outperforms the proposed method. On all other datasets, the proposed method dominates or matches baselines up to statistical uncertainty.",
      "doc_b_visual_anchor": "solid black line (fair class., oracle=LR, model=linear) in bottom plots",
      "doc_b_text_evidence": "Our method dominates or matches the baselines up to statistical uncertainty on all datasets except adult, where fair classification is slightly better.",
      "doc_b_figure_id": "1905.12843_fig_4",
      "doc_b_figure_type": "plot",
      "doc_b_image_path": "data/mineru_output/1905.12843/1905.12843/hybrid_auto/images/1905.12843_page0_fig3.jpg",
      "doc_b_caption": "Figure 1. Relative test loss versus the worst constraint violation with respect to SP. Relative losses are computed by subtracting the smallest baseline loss from the actual loss. For our algorithm and fair classification we plot the convex envelope of the predictors obtained on training data at various accuracy–fairness tradeoffs. We show $9 5 \\%$ confidence bands for the relative loss of our method and fair classification, and also show $9 5 \\%$ confidence intervals for constraint violation (the same for all methods). Our method dominates or matches the baselines up to statistical uncertainty on all datasets except adult, where fair classification is slightly better.",
      "doc_b_visual_score": 4,
      "score": 12.0
    },
    {
      "doc_a": "1610.07524",
      "doc_b": "1810.01943",
      "shared_entities": [
        "disparate impact",
        "logistic regression"
      ],
      "shared_entity_count": 2,
      "doc_a_query_id": "l1_1610.07524_1610.07524_fig_2_170",
      "doc_a_query": "Why do differences persist between gray and orange bars within each prior category despite equal overall FPR claims?",
      "doc_a_answer": "Even if overall FPR is equal between Black and White defendants, differences in error rates can still appear within individual prior record score categories. This explains the persistent gap between gray and orange bars across all five groupings shown.",
      "doc_a_visual_anchor": "Consistent separation between gray and orange bars across all five x-axis categories (0, 1-3, 4-6, 7-10, >10)",
      "doc_a_text_evidence": "That is, even if $\\mathrm { F P R } _ { b } = \\mathrm { F P R } _ { w }$ , we may still see differences in error rates within prior record score categories (see e.g., Figure 2).",
      "doc_a_figure_id": "1610.07524_fig_2",
      "doc_a_figure_type": "plot",
      "doc_a_image_path": "data/mineru_output/1610.07524/1610.07524/hybrid_auto/images/1610.07524_page0_fig1.jpg",
      "doc_a_caption": "Figure 2: False positive rates across prior record count for defendants charged with a Misdemeanor offense. Plot is based on assessing a defendant as “high-risk” if their COMPAS decile score is $> 4$ . Error bars represent 95% confidence intervals.   ",
      "doc_a_visual_score": 3,
      "doc_b_query_id": "l1_1810.01943_1810.01943_fig_10_578",
      "doc_b_query": "Do the green circles at disparate impact 0.8 in the top panel support that optimized pre-processing improves fairness?",
      "doc_b_answer": "Yes, the green circles (RF,Optimized pre-processing) show disparate impact closest to the ideal value of 1 in the top panel before mitigation, indicating that optimized pre-processing achieves better fairness metrics.",
      "doc_b_visual_anchor": "green circles at disparate impact ~0.8 in top panel",
      "doc_b_text_evidence": "The ideal fair value of disparate impact is 1, whereas for all other metrics it is 0.",
      "doc_b_figure_id": "1810.01943_fig_10",
      "doc_b_figure_type": "plot",
      "doc_b_image_path": "data/mineru_output/1810.01943/1810.01943/hybrid_auto/images/1810.01943_page0_fig9.jpg",
      "doc_b_caption": "Figure 5. Fairness vs. Balanced Accuracy before (top panel) and after (bottom panel) applying various bias mitigation algorithms. Four different fairness metrics are shown. In most cases two classifiers (Logistic regression - LR or Random forest classifier - RF) were used. The ideal fair value of disparate impact is 1, whereas for all other metrics it is 0. The circles indicate the mean value and bars indicate the extent of $\\pm 1$ standard deviation. Dataset: Adult, Protected attribute: race.",
      "doc_b_visual_score": 3,
      "score": 12.0
    },
    {
      "doc_a": "1412.3756",
      "doc_b": "1802.08139",
      "shared_entities": [
        "German Credit"
      ],
      "shared_entity_count": 1,
      "doc_a_query_id": "l1_1412.3756_1412.3756_fig_3_55",
      "doc_a_query": "Does the utility decay pattern visible across all scatter points support the expected tradeoff between fairness and utility?",
      "doc_a_answer": "Yes, the scatter points show declining utility (from ~1.0 to ~0.5-0.9) as disparate impact increases toward the green line at DI=1.0, demonstrating the expected decay over utility as fairness increases.",
      "doc_a_visual_anchor": "Dashed trend lines connecting scatter points showing downward slope from left to right",
      "doc_a_text_evidence": "The results, shown in Figure 3, demonstrate the expected decay over utility as fairness increases.",
      "doc_a_figure_id": "1412.3756_fig_3",
      "doc_a_figure_type": "plot",
      "doc_a_image_path": "data/mineru_output/1412.3756/1412.3756/hybrid_auto/images/1412.3756_page0_fig2.jpg",
      "doc_a_caption": "Fig. 3: Disparate impact (DI) vs. utility (1-BER) from our combinatorial and geometric partial repair processes using the SVM to classify on the Adult Income and German Credit data sets and the simple threshold classifier on the Ricci data set. Recall that only points with $\\mathsf { D } \\mathsf { I } \\ge \\tau = 0 . 8$ are legal. $\\mathsf { D } \\mathsf { I } = 1 . 0$ represents full fairness.",
      "doc_a_visual_score": 5,
      "doc_b_query_id": "l1_1802.08139_1802.08139_fig_17_356",
      "doc_b_query": "Why does the gray node C in subfigure (a) become a red bidirected link in the ADMG representation?",
      "doc_b_answer": "The gray node C represents an unobserved confounder. In ADMG notation, unobserved common causes are indicated by red bidirected links between the affected variables, which is why C's influence is shown as a red bidirected edge in subfigure (b).",
      "doc_b_visual_anchor": "Gray node C in subfigure (a) and red bidirected edge in subfigure (b)",
      "doc_b_text_evidence": "An ADMG is a causal graph containing two kinds of links, directed links (either green or black depending on whether we are interested in the corresponding causal path), and red bidirected links, indicating the presence of an unobserved common cause.",
      "doc_b_figure_id": "1802.08139_fig_17",
      "doc_b_figure_type": "diagram",
      "doc_b_image_path": "data/mineru_output/1802.08139/1802.08139/hybrid_auto/images/1802.08139_page0_fig16.jpg",
      "doc_b_caption": "Figure 7. (a): GCM with an unobserved confounder $C$ indicated with a gray node. (b): ADMG corresponding to (a). The causal effect along the green path $A  Y$ cannot be identified by only using observed variables.",
      "doc_b_visual_score": 4,
      "score": 12.0
    },
    {
      "doc_a": "1801.04385",
      "doc_b": "1805.09458",
      "shared_entities": [
        "logistic regression"
      ],
      "shared_entity_count": 1,
      "doc_a_query_id": "l1_1801.04385_1801.04385_fig_5_326",
      "doc_a_query": "How does acceptance probability change from the blue diagonal band to the red upper region as both reputation and answer count increase?",
      "doc_a_answer": "Acceptance probability increases from near 0.0 (blue) to 0.7 (red) as reputation grows from 10^2 to 10^6 and answer count increases from 10^1 to 10^4, following the diagonal pattern.",
      "doc_a_visual_anchor": "Diagonal gradient transitioning from blue (lower-left) to red (upper-right)",
      "doc_a_text_evidence": "e rate of change of these probability masses with respect to Xp is",
      "doc_a_figure_id": "1801.04385_fig_5",
      "doc_a_figure_type": "plot",
      "doc_a_image_path": "data/mineru_output/1801.04385/1801.04385/hybrid_auto/images/1801.04385_page0_fig4.jpg",
      "doc_a_caption": "Figure 3: Analysis of the Simpson’s paradox Reputation – Number of Answers variable pair. (a) Average acceptance probability as a function of two variables. (b) e distribution of the number of data points contributing to the value of the outcome variable for each pair of variable values.",
      "doc_a_visual_score": 6,
      "doc_b_query_id": "l1_1805.09458_1805.09458_fig_5_446",
      "doc_b_query": "How does using digit label as covariate enable the leftmost real 9 to generate all ten digits rightward?",
      "doc_b_answer": "Using digit label as covariate class c pushes all non-class stylistic information into latent space while removing information about the exact digit. This separation allows the decoder to recombine the extracted style with any specified digit label c′ to produce all ten digits.",
      "doc_b_visual_anchor": "Top row showing real digit 9 on left and generated sequence 0123456789 on right",
      "doc_b_text_evidence": "We use the digit label as the covariate class c, which pushes all non-class stylistic information into the latent space while attempting to remove information about the exact digit being written.",
      "doc_b_figure_id": "1805.09458_fig_5",
      "doc_b_figure_type": "example",
      "doc_b_image_path": "data/mineru_output/1805.09458/1805.09458/hybrid_auto/images/1805.09458_page0_fig4.jpg",
      "doc_b_caption": "Figure 3: We demonstrate the ability to generate stylistically similar images of varying classes using the MNIST dataset. The left column is mapped into $z$ that is invariant to its digit label c. We then can generate an image using $z$ and any other specified digit, $c ^ { \\prime }$ , as show on the right.",
      "doc_b_visual_score": 3,
      "score": 12.0
    },
    {
      "doc_a": "1707.09457",
      "doc_b": "1905.03674",
      "shared_entities": [
        "In Figure"
      ],
      "shared_entity_count": 1,
      "doc_a_query_id": "l1_1707.09457_1707.09457_fig_7_264",
      "doc_a_query": "Why do green dots dominate the upper-right region while red dots cluster at lower training ratios?",
      "doc_a_answer": "RBA successfully reduces bias amplification across all initial training biases. Lower training ratios initially showed more violations, but after RBA application, most points (green) meet the margin constraint.",
      "doc_a_visual_anchor": "green dots in upper-right region versus red dots at lower x-axis values",
      "doc_a_text_evidence": "Across all initial training biases, RBA is able to reduce the bias amplification.",
      "doc_a_figure_id": "1707.09457_fig_7",
      "doc_a_figure_type": "plot",
      "doc_a_image_path": "data/mineru_output/1707.09457/1707.09457/hybrid_auto/images/1707.09457_page0_fig6.jpg",
      "doc_a_caption": "Figure 3: Results of reducing bias amplification using RBA on imSitu vSRL and MS-COCO MLC. Figures 3(a)-(d) show initial training set bias along the x-axis and development set bias along the yaxis. Dotted blue lines indicate the 0.05 margin used in RBA, with points violating the margin shown in red while points meeting the margin are shown in green. Across both settings adding RBA significantly reduces the number of violations, and reduces the bias amplification significantly. Figures 3(e)-(f) demonstrate bias amplification as a function of training bias, with and without RBA. Across all initial training biases, RBA is able to reduce the bias amplification.",
      "doc_a_visual_score": 6,
      "doc_b_query_id": "l1_1905.03674_1905.03674_fig_1_786",
      "doc_b_query": "Do the two red centers support the claim that groups are entitled to proportional shares of centers?",
      "doc_b_answer": "Yes, the two red centers are positioned to serve individuals outside the middle column, demonstrating that when groups N and M are equal in size, each receives a proportional share of the k=2 centers available.",
      "doc_b_visual_anchor": "two red dots positioned symmetrically on left and right sides",
      "doc_b_text_evidence": "Proportionality has many advantages as a notion of fairness in clustering, beyond the intuitive appeal of groups being entitled to a proportional share of centers.",
      "doc_b_figure_id": "1905.03674_fig_1",
      "doc_b_figure_type": "example",
      "doc_b_image_path": "data/mineru_output/1905.03674/1905.03674/hybrid_auto/images/1905.03674_page0_fig0.jpg",
      "doc_b_caption": "Figure 1: Proportionality Example",
      "doc_b_visual_score": 3,
      "score": 12.0
    },
    {
      "doc_a": "1709.02012",
      "doc_b": "2005.07293",
      "shared_entities": [
        "Parity",
        "parity"
      ],
      "shared_entity_count": 2,
      "doc_a_query_id": "l1_1709.02012_1709.02012_fig_3_267",
      "doc_a_query": "Do the calibrated classifier sets H₁* and H₂* both lie on the black diagonal boundary?",
      "doc_a_answer": "Yes, the sets H₁* and H₂* of calibrated classifiers for the two groups both lie on the black diagonal boundary, representing the Pareto frontier of achievable false-positive and false-negative trade-offs for each group.",
      "doc_a_visual_anchor": "Black diagonal line from top-left to bottom-right",
      "doc_a_text_evidence": "H₁*, H₂* are the set of cal. classifiers for the two groups",
      "doc_a_figure_id": "1709.02012_fig_3",
      "doc_a_figure_type": "plot",
      "doc_a_image_path": "data/mineru_output/1709.02012/1709.02012/hybrid_auto/images/1709.02012_page0_fig2.jpg",
      "doc_a_caption": "Figure 1: Calibration, trivial classifiers, and equal-cost constraints – plotted in the false-pos./false-neg. plane. $\\mathcal { H } _ { 1 } ^ { \\ast } , \\mathcal { H } _ { 2 } ^ { \\ast }$ are the set of cal. classifiers for the two groups, and $h ^ { \\mu _ { 1 } } , h ^ { \\mu _ { 2 } }$ are trivial classifiers.   ",
      "doc_a_visual_score": 6,
      "doc_b_query_id": "l1_2005.07293_2005.07293_fig_1_905",
      "doc_b_query": "How does the different-height box arrangement under the yellow figure in the right panel operationalize compensation for historically disadvantaged groups?",
      "doc_b_answer": "The taller box under the yellow figure represents resource distribution to overcome obstacles and raise opportunities for access, compensating historically disadvantaged groups so they can reach the apples like others.",
      "doc_b_visual_anchor": "Right panel 'Equity' showing yellow figure standing on tallest brown box",
      "doc_b_text_evidence": "Equity is the distribution of resources among groups to overcome obstacles and to raise their opportunities for access [Schement, 2001]. Thus, historically disadvantaged groups are compensated, and others get their fair share to reach their goals.",
      "doc_b_figure_id": "2005.07293_fig_1",
      "doc_b_figure_type": "diagram",
      "doc_b_image_path": "data/mineru_output/2005.07293/2005.07293/hybrid_auto/images/2005.07293_page0_fig0.jpg",
      "doc_b_caption": "Figure 1: Notion of equality in fairness is depicted and formalized along with our newly formalized notion of equity.",
      "doc_b_visual_score": 4,
      "score": 12.0
    },
    {
      "doc_a": "1611.07509",
      "doc_b": "1810.08683",
      "shared_entities": [
        "The Adult"
      ],
      "shared_entity_count": 1,
      "doc_a_query_id": "l1_1611.07509_1611.07509_fig_4_199",
      "doc_a_query": "How do the blue paths through marital_status differ from the green direct path given the binarization preprocessing?",
      "doc_a_answer": "The blue paths represent indirect causal effects where sex influences income through the mediating variable marital_status, while the green path shows direct influence. Binarization reduced domain sizes to two classes for computational feasibility, enabling the PC algorithm to discover both path types.",
      "doc_a_visual_anchor": "blue paths from sex through marital_status node versus green direct path to income",
      "doc_a_text_evidence": "we binarize each attribute's domain values into two classes to reduce the domain sizes",
      "doc_a_figure_id": "1611.07509_fig_4",
      "doc_a_figure_type": "diagram",
      "doc_a_image_path": "data/mineru_output/1611.07509/1611.07509/hybrid_auto/images/1611.07509_page0_fig3.jpg",
      "doc_a_caption": "Figure 3: Causal network for Adult dataset: the green path represents the direct path, and the blue paths represent the indirect paths passing through marital status.",
      "doc_a_visual_score": 3,
      "doc_b_query_id": "l1_1810.08683_1810.08683_fig_4_640",
      "doc_b_query": "Does the inverse relationship between blue ACC and orange EOd curves in the bottom panel support varying λ conclusions?",
      "doc_b_answer": "Yes, as λ increases from 2e-10 to 1, ACC drops from ~91 to ~67 while EOd rises from ~0.05 to ~0.13, demonstrating the accuracy-fairness trade-off when varying λ as described in the analysis.",
      "doc_b_visual_anchor": "opposing trends of blue ACC (declining) and orange EOd (rising) curves from left to right in bottom panel",
      "doc_b_text_evidence": "This hypothesis is also supported by the results of Figure 5, in which we check how the accuracy and fairness, as measured with the EOd, varies by varying",
      "doc_b_figure_id": "1810.08683_fig_4",
      "doc_b_figure_type": "plot",
      "doc_b_image_path": "data/mineru_output/1810.08683/1810.08683/hybrid_auto/images/1810.08683_page0_fig3.jpg",
      "doc_b_caption": "Figure 5: Adult Dataset: ACC and EOd of MTL, when we fix $\\theta$ and $\\rho$ to be the best values found during the validation procedure and we vary  with $P { = } 0$ , $F { = } 1$ , and $S { = } 0$ .",
      "doc_b_visual_score": 6,
      "score": 12.0
    },
    {
      "doc_a": "1511.00830",
      "doc_b": "1905.12843",
      "shared_entities": [
        "LR",
        "accuracy"
      ],
      "shared_entity_count": 2,
      "doc_a_query_id": "l1_1511.00830_1511.00830_fig_1_64",
      "doc_a_query": "Why does node S point directly to X rather than through Z in this unsupervised representation learning model?",
      "doc_a_answer": "In unsupervised representation learning, S represents noise or nuisance variables that directly affect the observed data X, independent of the informative latent factors Z. This direct connection models uninformative dimensions that are typically detrimental for the task under consideration.",
      "doc_a_visual_anchor": "arrow from node S (top-left) directly to node X (bottom-center)",
      "doc_a_text_evidence": "Uninformative dimensions are often called \"noise\" or \"nuisance variables\" while informative dimensions are usually called latent or hidden factors of variation.",
      "doc_a_figure_id": "1511.00830_fig_1",
      "doc_a_figure_type": "diagram",
      "doc_a_image_path": "data/mineru_output/1511.00830/1511.00830/hybrid_auto/images/1511.00830_page0_fig0.jpg",
      "doc_a_caption": "Figure 1: Unsupervised model",
      "doc_a_visual_score": 5,
      "doc_b_query_id": "l1_1905.12843_1905.12843_fig_4_817",
      "doc_b_query": "Why does the solid black line in bottom plots perform better despite the claim of dominance on all datasets except adult?",
      "doc_b_answer": "The adult dataset is the exception where fair classification (solid black line) slightly outperforms the proposed method. On all other datasets, the proposed method dominates or matches baselines up to statistical uncertainty.",
      "doc_b_visual_anchor": "solid black line (fair class., oracle=LR, model=linear) in bottom plots",
      "doc_b_text_evidence": "Our method dominates or matches the baselines up to statistical uncertainty on all datasets except adult, where fair classification is slightly better.",
      "doc_b_figure_id": "1905.12843_fig_4",
      "doc_b_figure_type": "plot",
      "doc_b_image_path": "data/mineru_output/1905.12843/1905.12843/hybrid_auto/images/1905.12843_page0_fig3.jpg",
      "doc_b_caption": "Figure 1. Relative test loss versus the worst constraint violation with respect to SP. Relative losses are computed by subtracting the smallest baseline loss from the actual loss. For our algorithm and fair classification we plot the convex envelope of the predictors obtained on training data at various accuracy–fairness tradeoffs. We show $9 5 \\%$ confidence bands for the relative loss of our method and fair classification, and also show $9 5 \\%$ confidence intervals for constraint violation (the same for all methods). Our method dominates or matches the baselines up to statistical uncertainty on all datasets except adult, where fair classification is slightly better.",
      "doc_b_visual_score": 4,
      "score": 11.5
    },
    {
      "doc_a": "1711.05144",
      "doc_b": "1810.01943",
      "shared_entities": [
        "accuracy",
        "disparate impact"
      ],
      "shared_entity_count": 2,
      "doc_a_query_id": "l1_1711.05144_1711.05144_fig_3_299",
      "doc_a_query": "Why do the color-coded trajectories form distinct curves rather than a single frontier, given the reductions approach?",
      "doc_a_answer": "Each colored trajectory represents Pareto-optimal solutions for a specific gamma parameter value. The reductions approach by Agarwal et al. generates different error-unfairness trade-offs as gamma varies.",
      "doc_a_visual_anchor": "Multiple distinct colored curves (blue, red, green, purple, orange) descending from upper-left",
      "doc_a_text_evidence": "Pareto-optimal error-unfairness values, color coded by varying values of the input parameter γ",
      "doc_a_figure_id": "1711.05144_fig_3",
      "doc_a_figure_type": "plot",
      "doc_a_image_path": "data/mineru_output/1711.05144/1711.05144/hybrid_auto/images/1711.05144_page0_fig2.jpg",
      "doc_a_caption": "Figure 2: (a) Pareto-optimal error-unfairness values, color coded by varying values of the input parameter γ. (b) Aggregate Pareto frontier across all values of γ. Here the γ values cover the same range but are sampled more densely to get a smoother frontier. See text for details.",
      "doc_a_visual_score": 7,
      "doc_b_query_id": "l1_1810.01943_1810.01943_fig_10_578",
      "doc_b_query": "Do the green circles at disparate impact 0.8 in the top panel support that optimized pre-processing improves fairness?",
      "doc_b_answer": "Yes, the green circles (RF,Optimized pre-processing) show disparate impact closest to the ideal value of 1 in the top panel before mitigation, indicating that optimized pre-processing achieves better fairness metrics.",
      "doc_b_visual_anchor": "green circles at disparate impact ~0.8 in top panel",
      "doc_b_text_evidence": "The ideal fair value of disparate impact is 1, whereas for all other metrics it is 0.",
      "doc_b_figure_id": "1810.01943_fig_10",
      "doc_b_figure_type": "plot",
      "doc_b_image_path": "data/mineru_output/1810.01943/1810.01943/hybrid_auto/images/1810.01943_page0_fig9.jpg",
      "doc_b_caption": "Figure 5. Fairness vs. Balanced Accuracy before (top panel) and after (bottom panel) applying various bias mitigation algorithms. Four different fairness metrics are shown. In most cases two classifiers (Logistic regression - LR or Random forest classifier - RF) were used. The ideal fair value of disparate impact is 1, whereas for all other metrics it is 0. The circles indicate the mean value and bars indicate the extent of $\\pm 1$ standard deviation. Dataset: Adult, Protected attribute: race.",
      "doc_b_visual_score": 3,
      "score": 11.5
    },
    {
      "doc_a": "1905.10674",
      "doc_b": "1906.02589",
      "shared_entities": [
        "MLP",
        "accuracy"
      ],
      "shared_entity_count": 2,
      "doc_a_query_id": "l1_1905.10674_1905.10674_fig_1_799",
      "doc_a_query": "Does the input graph's blue and red node structure support embedding generation that operates on social networks?",
      "doc_a_answer": "Yes, the bipartite-style input graph with blue and red nodes represents the type of graph data structure used in social networks and recommender systems that the embedding technique is designed to handle.",
      "doc_a_visual_anchor": "Input Graph box showing blue and red nodes connected in a network structure on the left side",
      "doc_a_text_evidence": "Learning high-quality node embeddings is an important building block for machine learning models that operate on graph data, such as social networks and recommender systems.",
      "doc_a_figure_id": "1905.10674_fig_1",
      "doc_a_figure_type": "diagram",
      "doc_a_image_path": "data/mineru_output/1905.10674/1905.10674/hybrid_auto/images/1905.10674_page0_fig0.jpg",
      "doc_a_caption": "Figure 1. Overview of our approach: Our goal is to generate graph embeddings that are invariant to particular sensitive attributes (e.g., age or gender). We train a set of “filters” to prevent adversarial discriminators from classifying the sensitive information from the filtered embeddings. After training, these filters can be composed together in different combinations, allowing the flexible generation of embeddings that are invariant w.r.t. any subset of the sensitive attributes.",
      "doc_a_visual_score": 4,
      "doc_b_query_id": "l1_1906.02589_1906.02589_fig_29_835",
      "doc_b_query": "Why does the solid blue TEXAS line maintain higher accuracy than dashed orange X-VAE across all Δ_CP values when predicting Heavy-Makeup?",
      "doc_b_answer": "FFVAE's structured latent code using multiple sensitive attributes (Chubby, Eyeglasses, Male) enables better disentanglement, aligning with subgroup fair machine learning goals more effectively than X-VAE's approach.",
      "doc_b_visual_anchor": "solid blue TEXAS line consistently above dashed orange X-VAE line from Δ_CP 0.00 to 0.40",
      "doc_b_text_evidence": "we discussed how disentangled representation learning aligns with the goals of subgroup fair machine learning, and presented a method for learning a structured latent code using multiple sensitive attributes",
      "doc_b_figure_id": "1906.02589_fig_29",
      "doc_b_figure_type": "plot",
      "doc_b_image_path": "data/mineru_output/1906.02589/1906.02589/hybrid_auto/images/1906.02589_page0_fig28.jpg",
      "doc_b_caption": "Figure 5. Celeb-A subgroup fair classification results. Sensitive attributes: Chubby (C), Eyeglasses (E), and Male (M). $y =$ Heavy-Makeup.",
      "doc_b_visual_score": 5,
      "score": 11.5
    },
    {
      "doc_a": "1906.02589",
      "doc_b": "2005.07293",
      "shared_entities": [
        "accuracy",
        "parity"
      ],
      "shared_entity_count": 2,
      "doc_a_query_id": "l1_1906.02589_1906.02589_fig_29_835",
      "doc_a_query": "Why does the solid blue TEXAS line maintain higher accuracy than dashed orange X-VAE across all Δ_CP values when predicting Heavy-Makeup?",
      "doc_a_answer": "FFVAE's structured latent code using multiple sensitive attributes (Chubby, Eyeglasses, Male) enables better disentanglement, aligning with subgroup fair machine learning goals more effectively than X-VAE's approach.",
      "doc_a_visual_anchor": "solid blue TEXAS line consistently above dashed orange X-VAE line from Δ_CP 0.00 to 0.40",
      "doc_a_text_evidence": "we discussed how disentangled representation learning aligns with the goals of subgroup fair machine learning, and presented a method for learning a structured latent code using multiple sensitive attributes",
      "doc_a_figure_id": "1906.02589_fig_29",
      "doc_a_figure_type": "plot",
      "doc_a_image_path": "data/mineru_output/1906.02589/1906.02589/hybrid_auto/images/1906.02589_page0_fig28.jpg",
      "doc_a_caption": "Figure 5. Celeb-A subgroup fair classification results. Sensitive attributes: Chubby (C), Eyeglasses (E), and Male (M). $y =$ Heavy-Makeup.",
      "doc_a_visual_score": 5,
      "doc_b_query_id": "l1_2005.07293_2005.07293_fig_1_905",
      "doc_b_query": "How does the different-height box arrangement under the yellow figure in the right panel operationalize compensation for historically disadvantaged groups?",
      "doc_b_answer": "The taller box under the yellow figure represents resource distribution to overcome obstacles and raise opportunities for access, compensating historically disadvantaged groups so they can reach the apples like others.",
      "doc_b_visual_anchor": "Right panel 'Equity' showing yellow figure standing on tallest brown box",
      "doc_b_text_evidence": "Equity is the distribution of resources among groups to overcome obstacles and to raise their opportunities for access [Schement, 2001]. Thus, historically disadvantaged groups are compensated, and others get their fair share to reach their goals.",
      "doc_b_figure_id": "2005.07293_fig_1",
      "doc_b_figure_type": "diagram",
      "doc_b_image_path": "data/mineru_output/2005.07293/2005.07293/hybrid_auto/images/2005.07293_page0_fig0.jpg",
      "doc_b_caption": "Figure 1: Notion of equality in fairness is depicted and formalized along with our newly formalized notion of equity.",
      "doc_b_visual_score": 4,
      "score": 11.5
    },
    {
      "doc_a": "1906.02589",
      "doc_b": "2109.03952",
      "shared_entities": [
        "accuracy",
        "parity"
      ],
      "shared_entity_count": 2,
      "doc_a_query_id": "l1_1906.02589_1906.02589_fig_29_835",
      "doc_a_query": "Why does the solid blue TEXAS line maintain higher accuracy than dashed orange X-VAE across all Δ_CP values when predicting Heavy-Makeup?",
      "doc_a_answer": "FFVAE's structured latent code using multiple sensitive attributes (Chubby, Eyeglasses, Male) enables better disentanglement, aligning with subgroup fair machine learning goals more effectively than X-VAE's approach.",
      "doc_a_visual_anchor": "solid blue TEXAS line consistently above dashed orange X-VAE line from Δ_CP 0.00 to 0.40",
      "doc_a_text_evidence": "we discussed how disentangled representation learning aligns with the goals of subgroup fair machine learning, and presented a method for learning a structured latent code using multiple sensitive attributes",
      "doc_a_figure_id": "1906.02589_fig_29",
      "doc_a_figure_type": "plot",
      "doc_a_image_path": "data/mineru_output/1906.02589/1906.02589/hybrid_auto/images/1906.02589_page0_fig28.jpg",
      "doc_a_caption": "Figure 5. Celeb-A subgroup fair classification results. Sensitive attributes: Chubby (C), Eyeglasses (E), and Male (M). $y =$ Heavy-Makeup.",
      "doc_a_visual_score": 5,
      "doc_b_query_id": "l1_2109.03952_2109.03952_fig_1_948",
      "doc_b_query": "How does the orange Attention layer produce its output representation before the yellow Dense Layers?",
      "doc_b_answer": "The attention layer computes weights and takes a weighted combination of the input feature embeddings to produce a d^e-dimensional vector representation for the sample instance.",
      "doc_b_visual_anchor": "orange Attention layer box positioned between red embedding boxes and yellow Dense Layer boxes",
      "doc_b_text_evidence": "These vectors are passed to the attention layer. The Computation of attention weights and the final representation for a sample is",
      "doc_b_figure_id": "2109.03952_fig_1",
      "doc_b_figure_type": "architecture",
      "doc_b_image_path": "data/mineru_output/2109.03952/2109.03952/hybrid_auto/images/2109.03952_page0_fig0.jpg",
      "doc_b_caption": "Figure 1: (a) In general classification model, for each feature $f _ { k }$ a vector representation $e _ { k }$ of length $d ^ { e }$ is learned. This is passed to the attention layer which produces a $d ^ { e }$ - dimensional vector representation for the sample instance $i$ which is passed to two dense layers to get the final classification output. (b) The Attribution framework has the same architecture as the general model. One outcome is obtained through the original model and another through the model that has some attention weights zeroed. The observed difference in accuracy and fairness measures will indicate the effect of the zeroed out features on accuracy and fairness.",
      "doc_b_visual_score": 4,
      "score": 11.5
    },
    {
      "doc_a": "1711.05144",
      "doc_b": "1805.11202",
      "shared_entities": [
        "disparate impact"
      ],
      "shared_entity_count": 1,
      "doc_a_query_id": "l1_1711.05144_1711.05144_fig_3_299",
      "doc_a_query": "Why do the color-coded trajectories form distinct curves rather than a single frontier, given the reductions approach?",
      "doc_a_answer": "Each colored trajectory represents Pareto-optimal solutions for a specific gamma parameter value. The reductions approach by Agarwal et al. generates different error-unfairness trade-offs as gamma varies.",
      "doc_a_visual_anchor": "Multiple distinct colored curves (blue, red, green, purple, orange) descending from upper-left",
      "doc_a_text_evidence": "Pareto-optimal error-unfairness values, color coded by varying values of the input parameter γ",
      "doc_a_figure_id": "1711.05144_fig_3",
      "doc_a_figure_type": "plot",
      "doc_a_image_path": "data/mineru_output/1711.05144/1711.05144/hybrid_auto/images/1711.05144_page0_fig2.jpg",
      "doc_a_caption": "Figure 2: (a) Pareto-optimal error-unfairness values, color coded by varying values of the input parameter γ. (b) Aggregate Pareto frontier across all values of γ. Here the γ values cover the same range but are sampled more densely to get a smoother frontier. See text for details.",
      "doc_a_visual_score": 7,
      "doc_b_query_id": "l1_1805.11202_1805.11202_fig_5_454",
      "doc_b_query": "Why does the black curve reach approximately 0.10 at x≈3.5 while both conditional distributions peak below 0.05?",
      "doc_b_answer": "The black curve represents the overall data distribution P_data(x), which combines both conditional distributions P_data(x|s=1) and P_data(x|s=0), resulting in higher probability density than either individual conditional distribution alone.",
      "doc_b_visual_anchor": "Black solid curve peak at 0.10 versus green and red dashed curves peaking below 0.05",
      "doc_b_text_evidence": "shows the distributions P_data(x) (black), P_data(x|s=1) (green) and P_data(x|s=0) (red) of real data",
      "doc_b_figure_id": "1805.11202_fig_5",
      "doc_b_figure_type": "plot",
      "doc_b_image_path": "data/mineru_output/1805.11202/1805.11202/hybrid_auto/images/1805.11202_page0_fig4.jpg",
      "doc_b_caption": "Figure 3: Comparing FairGAN, NaïveFairGAN-I and NaïveFairGAN-II on a toy dataset. (a) shows the distributions $P _ { \\mathbf { d a t a } } ( x )$ (black), $P _ { \\bf d a t a } ( x | s = 1 )$ (green) and $P _ { \\bf d a t a } ( x | s = 0 )$ (red) of real data; (b), (c) and (d) are distributions $P _ { G } ( x ) , P _ { G } ( x | s = 1 )$ and $P _ { G } ( x | s = 0 )$ P x s P x s Pof synthetic datasets generated by NaïveFairGAN-I, NaïveFairGAN-II and FairGAN separately.",
      "doc_b_visual_score": 7,
      "score": 11.5
    },
    {
      "doc_a": "1603.07025",
      "doc_b": "1903.08136",
      "shared_entities": [
        "In Figure"
      ],
      "shared_entity_count": 1,
      "doc_a_query_id": "l1_1603.07025_1603.07025_fig_2_98",
      "doc_a_query": "Why does the dashed teal line remain two orders of magnitude below the purple line throughout 2008-2014?",
      "doc_a_answer": "The dashed teal line represents cumulative subreddits while the purple line shows cumulative users. Reddit's structure allows many users to participate across relatively fewer subreddit communities.",
      "doc_a_visual_anchor": "dashed teal line staying around 10^5 while solid purple line reaches 10^7",
      "doc_a_text_evidence": "These datasets contain a total of 114 million submissions. An active user or subreddit is one that had at least one post (comment or submission) in the time bin we used—here, discretized by month",
      "doc_a_figure_id": "1603.07025_fig_2",
      "doc_a_figure_type": "plot",
      "doc_a_image_path": "data/mineru_output/1603.07025/1603.07025/hybrid_auto/images/1603.07025_page0_fig1.jpg",
      "doc_a_caption": "Figure 2: Figure (a) shows the cumulative growth of Reddit for users and subreddits. Figure (b) shows the number of active users and subreddits in Reddit over time. An active user or subreddit is one that had at least one post (comment or submission) in the time bin we used—here, discretized by month.",
      "doc_a_visual_score": 4,
      "doc_b_query_id": "l1_1903.08136_1903.08136_fig_1_748",
      "doc_b_query": "Why does the gray sector contain only triangles and circles without showing how low-degree users get excluded?",
      "doc_b_answer": "The gray sector demonstrates a community where both opinion types (shapes) and variations (textures) coexist, but methods like CESNA and Louvain fail to assign low-degree nodes or assign them to trivially small communities that prevent inclusion in analysis.",
      "doc_b_visual_anchor": "gray quarter-circle sector with mixed blue triangles and red circles",
      "doc_b_text_evidence": "many community detection approaches either fail to assign low degree (or lowly-connected) users to communities, or assign them to trivially small communities that prevent them from being included in analysis",
      "doc_b_figure_id": "1903.08136_fig_1",
      "doc_b_figure_type": "diagram",
      "doc_b_image_path": "data/mineru_output/1903.08136/1903.08136/hybrid_auto/images/1903.08136_page0_fig0.jpg",
      "doc_b_caption": "Figure 1: Demonstration of versatility of relevant low-degree people in social networks who get ignored by biased community detection methods. Shapes (circle, triangle) represent opinion, and texture represents variations of the opinion. We provide real-world examples of how this manifests in subsequent sections.",
      "doc_b_visual_score": 4,
      "score": 11.5
    },
    {
      "doc_a": "1810.03611",
      "doc_b": "1903.08136",
      "shared_entities": [
        "In Figure"
      ],
      "shared_entity_count": 1,
      "doc_a_query_id": "l1_1810.03611_1810.03611_fig_2_599",
      "doc_a_query": "Does the strong alignment of blue points along the red dashed diagonal support that approximation accuracy exceeds PPMI baseline?",
      "doc_a_answer": "The close alignment of data points along the diagonal trend line demonstrates high correlation between approximated and validated effect sizes, confirming the method's accuracy. This visual validation directly relates to the comparison with the PPMI baseline discussed in the subsequent section.",
      "doc_a_visual_anchor": "blue data points aligned with red dashed diagonal line",
      "doc_a_text_evidence": "We have shown that our method can be used to identify bias-influencing documents and accurately approximate the impact of their removal, but how does it compare to a more",
      "doc_a_figure_id": "1810.03611_fig_2",
      "doc_a_figure_type": "plot",
      "doc_a_image_path": "data/mineru_output/1810.03611/1810.03611/hybrid_auto/images/1810.03611_page0_fig1.jpg",
      "doc_a_caption": "Figure 2. Approximated and ground truth WEAT bias effect size due to the removal of various perturbation sets for our NYT corpus, considering WEAT1. Each point describes the mean effect size of one set; error bars depict one standard deviation; the baseline (unperturbed) mean is shown with a vertical dotted line.",
      "doc_a_visual_score": 4,
      "doc_b_query_id": "l1_1903.08136_1903.08136_fig_1_748",
      "doc_b_query": "Why does the gray sector contain only triangles and circles without showing how low-degree users get excluded?",
      "doc_b_answer": "The gray sector demonstrates a community where both opinion types (shapes) and variations (textures) coexist, but methods like CESNA and Louvain fail to assign low-degree nodes or assign them to trivially small communities that prevent inclusion in analysis.",
      "doc_b_visual_anchor": "gray quarter-circle sector with mixed blue triangles and red circles",
      "doc_b_text_evidence": "many community detection approaches either fail to assign low degree (or lowly-connected) users to communities, or assign them to trivially small communities that prevent them from being included in analysis",
      "doc_b_figure_id": "1903.08136_fig_1",
      "doc_b_figure_type": "diagram",
      "doc_b_image_path": "data/mineru_output/1903.08136/1903.08136/hybrid_auto/images/1903.08136_page0_fig0.jpg",
      "doc_b_caption": "Figure 1: Demonstration of versatility of relevant low-degree people in social networks who get ignored by biased community detection methods. Shapes (circle, triangle) represent opinion, and texture represents variations of the opinion. We provide real-world examples of how this manifests in subsequent sections.",
      "doc_b_visual_score": 4,
      "score": 11.5
    },
    {
      "doc_a": "1903.08136",
      "doc_b": "1905.12843",
      "shared_entities": [
        "In Figure"
      ],
      "shared_entity_count": 1,
      "doc_a_query_id": "l1_1903.08136_1903.08136_fig_1_748",
      "doc_a_query": "Why does the gray sector contain only triangles and circles without showing how low-degree users get excluded?",
      "doc_a_answer": "The gray sector demonstrates a community where both opinion types (shapes) and variations (textures) coexist, but methods like CESNA and Louvain fail to assign low-degree nodes or assign them to trivially small communities that prevent inclusion in analysis.",
      "doc_a_visual_anchor": "gray quarter-circle sector with mixed blue triangles and red circles",
      "doc_a_text_evidence": "many community detection approaches either fail to assign low degree (or lowly-connected) users to communities, or assign them to trivially small communities that prevent them from being included in analysis",
      "doc_a_figure_id": "1903.08136_fig_1",
      "doc_a_figure_type": "diagram",
      "doc_a_image_path": "data/mineru_output/1903.08136/1903.08136/hybrid_auto/images/1903.08136_page0_fig0.jpg",
      "doc_a_caption": "Figure 1: Demonstration of versatility of relevant low-degree people in social networks who get ignored by biased community detection methods. Shapes (circle, triangle) represent opinion, and texture represents variations of the opinion. We provide real-world examples of how this manifests in subsequent sections.",
      "doc_a_visual_score": 4,
      "doc_b_query_id": "l1_1905.12843_1905.12843_fig_4_817",
      "doc_b_query": "Why does the solid black line in bottom plots perform better despite the claim of dominance on all datasets except adult?",
      "doc_b_answer": "The adult dataset is the exception where fair classification (solid black line) slightly outperforms the proposed method. On all other datasets, the proposed method dominates or matches baselines up to statistical uncertainty.",
      "doc_b_visual_anchor": "solid black line (fair class., oracle=LR, model=linear) in bottom plots",
      "doc_b_text_evidence": "Our method dominates or matches the baselines up to statistical uncertainty on all datasets except adult, where fair classification is slightly better.",
      "doc_b_figure_id": "1905.12843_fig_4",
      "doc_b_figure_type": "plot",
      "doc_b_image_path": "data/mineru_output/1905.12843/1905.12843/hybrid_auto/images/1905.12843_page0_fig3.jpg",
      "doc_b_caption": "Figure 1. Relative test loss versus the worst constraint violation with respect to SP. Relative losses are computed by subtracting the smallest baseline loss from the actual loss. For our algorithm and fair classification we plot the convex envelope of the predictors obtained on training data at various accuracy–fairness tradeoffs. We show $9 5 \\%$ confidence bands for the relative loss of our method and fair classification, and also show $9 5 \\%$ confidence intervals for constraint violation (the same for all methods). Our method dominates or matches the baselines up to statistical uncertainty on all datasets except adult, where fair classification is slightly better.",
      "doc_b_visual_score": 4,
      "score": 11.5
    },
    {
      "doc_a": "1610.02413",
      "doc_b": "1805.05859",
      "shared_entities": [
        "X1",
        "X2"
      ],
      "shared_entity_count": 2,
      "doc_a_query_id": "l1_1610.02413_1610.02413_fig_4_136",
      "doc_a_query": "How does the horizontal gap indicated by the lower-right arrow relate to optimizing the equal opportunity predictor?",
      "doc_a_answer": "The horizontal gap between the ROC curve and the profit-maximizing tangent line is proportional to the cost for achieving a given true positive rate, which becomes a convex function that can be efficiently optimized using ternary search.",
      "doc_a_visual_anchor": "Black arrow pointing to horizontal gap between blue curve and tangent line in lower right",
      "doc_a_text_evidence": "within each group the cost for a given true positive rate is proportional to the horizontal gap between the ROC curve and the profit-maximizing tangent line (i.e., the two curves on the left plot), so it is a convex function of the true positive rate",
      "doc_a_figure_id": "1610.02413_fig_4",
      "doc_a_figure_type": "plot",
      "doc_a_image_path": "data/mineru_output/1610.02413/1610.02413/hybrid_auto/images/1610.02413_page0_fig3.jpg",
      "doc_a_caption": "Figure 2: Finding the optimal equalized odds threshold predictor (middle), and equal opportunity threshold predictor (right). For the equal opportunity predictor, within each group the cost for a given true positive rate is proportional to the horizontal gap between the ROC curve and the profit-maximizing tangent line (i.e., the two curves on the left plot), so it is a convex function of the true positive rate (right). This lets us optimize it efficiently with ternary search.",
      "doc_a_visual_score": 7,
      "doc_b_query_id": "l1_1805.05859_1805.05859_fig_1_433",
      "doc_b_query": "Does the arrow from A to Y in the graph represent a structural equation that gets overridden during intervention?",
      "doc_b_answer": "Yes, a perfect intervention on variable A at value v corresponds to overriding the structural equation f_i with V_i = v, which would affect the arrow from A to Y in the causal graph.",
      "doc_b_visual_anchor": "Arrow from node A to node Y",
      "doc_b_text_evidence": "This notion says that a perfect intervention on a variable V_i, at value v, corresponds to overriding f_i(·) with the equation V_i = v.",
      "doc_b_figure_id": "1805.05859_fig_1",
      "doc_b_figure_type": "diagram",
      "doc_b_image_path": "data/mineru_output/1805.05859/1805.05859/hybrid_auto/images/1805.05859_page0_fig0.jpg",
      "doc_b_caption": "Figure 1: (a) A causal graph for three observed variables $A , Y , Z$ . (b) A joint representation with explicit background variables, and two counterfactual alternatives where $A$ is intervened at two different levels. (c) Similar to (b), where the interventions take place on $Y$ .",
      "doc_b_visual_score": 2,
      "score": 11.5
    },
    {
      "doc_a": "1701.08230",
      "doc_b": "1705.10378",
      "shared_entities": [
        "COMPAS"
      ],
      "shared_entity_count": 1,
      "doc_a_query_id": "l1_1701.08230_1701.08230_fig_2_204",
      "doc_a_query": "Do the two distributions drawn from beta distributions with equal means in the right panel share the same peak location?",
      "doc_a_answer": "No, the blue curve peaks before 25% (near the dashed line) while the red curve peaks slightly after 25%, despite both distributions having equal means as stated.",
      "doc_a_visual_anchor": "Blue and red curves with vertical dashed line at 25% in top-right simulated panel",
      "doc_a_text_evidence": "simulated data drawn from two beta distributions with equal means (right)",
      "doc_a_figure_id": "1701.08230_fig_2",
      "doc_a_figure_type": "plot",
      "doc_a_image_path": "data/mineru_output/1701.08230/1701.08230/hybrid_auto/images/1701.08230_page0_fig1.jpg",
      "doc_a_caption": "Figure 1: Top: distribution of risk scores for Broward County data (le), and simulated data drawn from two beta distributions with equal means (right). Bottom: using a single threshold which detains $3 0 \\%$ of defendants in Broward County violates statistical parity (as measured by detention rate), predictive equality (false positive rate), and conditional statistical parity (detention rate conditional on number of prior arrests). We omit the last measure for the simulated data since that would require making additional assumptions about the relationship of priors and risk in the hypothetical populations.",
      "doc_a_visual_score": 7,
      "doc_b_query_id": "l1_1705.10378_1705.10378_fig_1_226",
      "doc_b_query": "Why does C point to both A and Y if causal models generalize Bayesian networks?",
      "doc_b_answer": "C pointing to both A and Y indicates C is a common cause (confounder) that creates conditional dependencies, which causal models encode beyond standard Bayesian network independence structures.",
      "doc_b_visual_anchor": "Node C at top-left with blue arrows pointing to both A (bottom-left) and Y (top-right)",
      "doc_b_text_evidence": "Such models generalize independence models on directed acyclic graphs, also known as Bayesian networks (Pearl 1988), to also encode conditional independence statements on counterfactual random variables",
      "doc_b_figure_id": "1705.10378_fig_1",
      "doc_b_figure_type": "diagram",
      "doc_b_image_path": "data/mineru_output/1705.10378/1705.10378/hybrid_auto/images/1705.10378_page0_fig0.jpg",
      "doc_b_caption": "Figure 1: (a) A causal graph with a single mediator. (b) A causal graph with two mediators, one confounded with the outcome via an unobserved common cause. (c) A causal graph with a single mediator where the natural direct effect is not identified.",
      "doc_b_visual_score": 6,
      "score": 11.5
    },
    {
      "doc_a": "1511.00830",
      "doc_b": "1711.05144",
      "shared_entities": [
        "accuracy"
      ],
      "shared_entity_count": 1,
      "doc_a_query_id": "l1_1511.00830_1511.00830_fig_1_64",
      "doc_a_query": "Why does node S point directly to X rather than through Z in this unsupervised representation learning model?",
      "doc_a_answer": "In unsupervised representation learning, S represents noise or nuisance variables that directly affect the observed data X, independent of the informative latent factors Z. This direct connection models uninformative dimensions that are typically detrimental for the task under consideration.",
      "doc_a_visual_anchor": "arrow from node S (top-left) directly to node X (bottom-center)",
      "doc_a_text_evidence": "Uninformative dimensions are often called \"noise\" or \"nuisance variables\" while informative dimensions are usually called latent or hidden factors of variation.",
      "doc_a_figure_id": "1511.00830_fig_1",
      "doc_a_figure_type": "diagram",
      "doc_a_image_path": "data/mineru_output/1511.00830/1511.00830/hybrid_auto/images/1511.00830_page0_fig0.jpg",
      "doc_a_caption": "Figure 1: Unsupervised model",
      "doc_a_visual_score": 5,
      "doc_b_query_id": "l1_1711.05144_1711.05144_fig_3_299",
      "doc_b_query": "Why do the color-coded trajectories form distinct curves rather than a single frontier, given the reductions approach?",
      "doc_b_answer": "Each colored trajectory represents Pareto-optimal solutions for a specific gamma parameter value. The reductions approach by Agarwal et al. generates different error-unfairness trade-offs as gamma varies.",
      "doc_b_visual_anchor": "Multiple distinct colored curves (blue, red, green, purple, orange) descending from upper-left",
      "doc_b_text_evidence": "Pareto-optimal error-unfairness values, color coded by varying values of the input parameter γ",
      "doc_b_figure_id": "1711.05144_fig_3",
      "doc_b_figure_type": "plot",
      "doc_b_image_path": "data/mineru_output/1711.05144/1711.05144/hybrid_auto/images/1711.05144_page0_fig2.jpg",
      "doc_b_caption": "Figure 2: (a) Pareto-optimal error-unfairness values, color coded by varying values of the input parameter γ. (b) Aggregate Pareto frontier across all values of γ. Here the γ values cover the same range but are sampled more densely to get a smoother frontier. See text for details.",
      "doc_b_visual_score": 7,
      "score": 11.0
    },
    {
      "doc_a": "1610.08452",
      "doc_b": "1808.08166",
      "shared_entities": [
        "Fairness",
        "accuracy"
      ],
      "shared_entity_count": 2,
      "doc_a_query_id": "l1_1610.08452_1610.08452_fig_9_182",
      "doc_a_query": "What causes the dashed boundary to separate more red circles from green ones compared to the solid boundary?",
      "doc_a_answer": "The fair constrained classifier shifts the decision boundary to equalize false positive and negative rates across groups, which the research compares against unconstrained classifiers to demonstrate performance trade-offs when avoiding disparate mistreatment.",
      "doc_a_visual_anchor": "dashed blue line positioned differently from solid cyan line relative to red and green markers",
      "doc_a_text_evidence": "Figure 4 summarizes the results by showing the decision boundaries for both the unconstrained classifiers (solid) and the fair constrained classifier (dashed) when controlling for disparate mistreatment",
      "doc_a_figure_id": "1610.08452_fig_9",
      "doc_a_figure_type": "plot",
      "doc_a_image_path": "data/mineru_output/1610.08452/1610.08452/hybrid_auto/images/1610.08452_page0_fig8.jpg",
      "doc_a_caption": "Figure 4: [Synthetic data] $D _ { F P R }$ and $D _ { F N R }$ have the same sign. Removing disparate mistreatment on FPR can potentially increase disparate mistreatment on FNR. Removing disparate mistreatment on both at the same time causes a larger drop in accuracy.",
      "doc_a_visual_score": 6,
      "doc_b_query_id": "l1_1808.08166_1808.08166_fig_11_472",
      "doc_b_query": "Does the blue points' downward trend from y=0.25 to y=0 demonstrate the MARGINAL algorithm's error-fairness pairs?",
      "doc_b_answer": "Yes, the blue points show the error and marginal fairness violation pairs achieved by the MARGINAL algorithm, displaying a clear trade-off as error increases from left to right.",
      "doc_b_visual_anchor": "blue points forming downward curve from upper-left (error ~0.16, fairness ~0.25) to lower-right (error ~0.23, fairness ~0)",
      "doc_b_text_evidence": "The error and marginal fairness violation for the MARGINAL algorithm across all four data sets.",
      "doc_b_figure_id": "1808.08166_fig_11",
      "doc_b_figure_type": "plot",
      "doc_b_image_path": "data/mineru_output/1808.08166/1808.08166/hybrid_auto/images/1808.08166_page0_fig10.jpg",
      "doc_b_caption": "Figure 3: Left column: The red points show the Pareto frontier of error (x axis) and subgroup fairness violation (y axis) for the SUBGROUP algorithm across all four data sets, while the blue points show the error and subgroup fairness violation for the models achieved by the MARGINAL algorithm. Right column: The error and marginal fairness violation for the MARGINAL algorithm across all four data sets. Ordering of datasets is Communities and Crime, Law School, Adult, and Student.",
      "doc_b_visual_score": 6,
      "score": 11.0
    },
    {
      "doc_a": "1805.09458",
      "doc_b": "1906.02589",
      "shared_entities": [
        "VAE",
        "accuracy"
      ],
      "shared_entity_count": 2,
      "doc_a_query_id": "l1_1805.09458_1805.09458_fig_5_446",
      "doc_a_query": "How does using digit label as covariate enable the leftmost real 9 to generate all ten digits rightward?",
      "doc_a_answer": "Using digit label as covariate class c pushes all non-class stylistic information into latent space while removing information about the exact digit. This separation allows the decoder to recombine the extracted style with any specified digit label c′ to produce all ten digits.",
      "doc_a_visual_anchor": "Top row showing real digit 9 on left and generated sequence 0123456789 on right",
      "doc_a_text_evidence": "We use the digit label as the covariate class c, which pushes all non-class stylistic information into the latent space while attempting to remove information about the exact digit being written.",
      "doc_a_figure_id": "1805.09458_fig_5",
      "doc_a_figure_type": "example",
      "doc_a_image_path": "data/mineru_output/1805.09458/1805.09458/hybrid_auto/images/1805.09458_page0_fig4.jpg",
      "doc_a_caption": "Figure 3: We demonstrate the ability to generate stylistically similar images of varying classes using the MNIST dataset. The left column is mapped into $z$ that is invariant to its digit label c. We then can generate an image using $z$ and any other specified digit, $c ^ { \\prime }$ , as show on the right.",
      "doc_a_visual_score": 3,
      "doc_b_query_id": "l1_1906.02589_1906.02589_fig_29_835",
      "doc_b_query": "Why does the solid blue TEXAS line maintain higher accuracy than dashed orange X-VAE across all Δ_CP values when predicting Heavy-Makeup?",
      "doc_b_answer": "FFVAE's structured latent code using multiple sensitive attributes (Chubby, Eyeglasses, Male) enables better disentanglement, aligning with subgroup fair machine learning goals more effectively than X-VAE's approach.",
      "doc_b_visual_anchor": "solid blue TEXAS line consistently above dashed orange X-VAE line from Δ_CP 0.00 to 0.40",
      "doc_b_text_evidence": "we discussed how disentangled representation learning aligns with the goals of subgroup fair machine learning, and presented a method for learning a structured latent code using multiple sensitive attributes",
      "doc_b_figure_id": "1906.02589_fig_29",
      "doc_b_figure_type": "plot",
      "doc_b_image_path": "data/mineru_output/1906.02589/1906.02589/hybrid_auto/images/1906.02589_page0_fig28.jpg",
      "doc_b_caption": "Figure 5. Celeb-A subgroup fair classification results. Sensitive attributes: Chubby (C), Eyeglasses (E), and Male (M). $y =$ Heavy-Makeup.",
      "doc_b_visual_score": 5,
      "score": 11.0
    },
    {
      "doc_a": "1810.01943",
      "doc_b": "1906.02589",
      "shared_entities": [
        "Fairness",
        "accuracy",
        "parity"
      ],
      "shared_entity_count": 3,
      "doc_a_query_id": "l1_1810.01943_1810.01943_fig_10_578",
      "doc_a_query": "Do the green circles at disparate impact 0.8 in the top panel support that optimized pre-processing improves fairness?",
      "doc_a_answer": "Yes, the green circles (RF,Optimized pre-processing) show disparate impact closest to the ideal value of 1 in the top panel before mitigation, indicating that optimized pre-processing achieves better fairness metrics.",
      "doc_a_visual_anchor": "green circles at disparate impact ~0.8 in top panel",
      "doc_a_text_evidence": "The ideal fair value of disparate impact is 1, whereas for all other metrics it is 0.",
      "doc_a_figure_id": "1810.01943_fig_10",
      "doc_a_figure_type": "plot",
      "doc_a_image_path": "data/mineru_output/1810.01943/1810.01943/hybrid_auto/images/1810.01943_page0_fig9.jpg",
      "doc_a_caption": "Figure 5. Fairness vs. Balanced Accuracy before (top panel) and after (bottom panel) applying various bias mitigation algorithms. Four different fairness metrics are shown. In most cases two classifiers (Logistic regression - LR or Random forest classifier - RF) were used. The ideal fair value of disparate impact is 1, whereas for all other metrics it is 0. The circles indicate the mean value and bars indicate the extent of $\\pm 1$ standard deviation. Dataset: Adult, Protected attribute: race.",
      "doc_a_visual_score": 3,
      "doc_b_query_id": "l1_1906.02589_1906.02589_fig_29_835",
      "doc_b_query": "Why does the solid blue TEXAS line maintain higher accuracy than dashed orange X-VAE across all Δ_CP values when predicting Heavy-Makeup?",
      "doc_b_answer": "FFVAE's structured latent code using multiple sensitive attributes (Chubby, Eyeglasses, Male) enables better disentanglement, aligning with subgroup fair machine learning goals more effectively than X-VAE's approach.",
      "doc_b_visual_anchor": "solid blue TEXAS line consistently above dashed orange X-VAE line from Δ_CP 0.00 to 0.40",
      "doc_b_text_evidence": "we discussed how disentangled representation learning aligns with the goals of subgroup fair machine learning, and presented a method for learning a structured latent code using multiple sensitive attributes",
      "doc_b_figure_id": "1906.02589_fig_29",
      "doc_b_figure_type": "plot",
      "doc_b_image_path": "data/mineru_output/1906.02589/1906.02589/hybrid_auto/images/1906.02589_page0_fig28.jpg",
      "doc_b_caption": "Figure 5. Celeb-A subgroup fair classification results. Sensitive attributes: Chubby (C), Eyeglasses (E), and Male (M). $y =$ Heavy-Makeup.",
      "doc_b_visual_score": 5,
      "score": 11.0
    },
    {
      "doc_a": "1802.08139",
      "doc_b": "1810.01943",
      "shared_entities": [
        "German Credit"
      ],
      "shared_entity_count": 1,
      "doc_a_query_id": "l1_1802.08139_1802.08139_fig_17_356",
      "doc_a_query": "Why does the gray node C in subfigure (a) become a red bidirected link in the ADMG representation?",
      "doc_a_answer": "The gray node C represents an unobserved confounder. In ADMG notation, unobserved common causes are indicated by red bidirected links between the affected variables, which is why C's influence is shown as a red bidirected edge in subfigure (b).",
      "doc_a_visual_anchor": "Gray node C in subfigure (a) and red bidirected edge in subfigure (b)",
      "doc_a_text_evidence": "An ADMG is a causal graph containing two kinds of links, directed links (either green or black depending on whether we are interested in the corresponding causal path), and red bidirected links, indicating the presence of an unobserved common cause.",
      "doc_a_figure_id": "1802.08139_fig_17",
      "doc_a_figure_type": "diagram",
      "doc_a_image_path": "data/mineru_output/1802.08139/1802.08139/hybrid_auto/images/1802.08139_page0_fig16.jpg",
      "doc_a_caption": "Figure 7. (a): GCM with an unobserved confounder $C$ indicated with a gray node. (b): ADMG corresponding to (a). The causal effect along the green path $A  Y$ cannot be identified by only using observed variables.",
      "doc_a_visual_score": 4,
      "doc_b_query_id": "l1_1810.01943_1810.01943_fig_10_578",
      "doc_b_query": "Do the green circles at disparate impact 0.8 in the top panel support that optimized pre-processing improves fairness?",
      "doc_b_answer": "Yes, the green circles (RF,Optimized pre-processing) show disparate impact closest to the ideal value of 1 in the top panel before mitigation, indicating that optimized pre-processing achieves better fairness metrics.",
      "doc_b_visual_anchor": "green circles at disparate impact ~0.8 in top panel",
      "doc_b_text_evidence": "The ideal fair value of disparate impact is 1, whereas for all other metrics it is 0.",
      "doc_b_figure_id": "1810.01943_fig_10",
      "doc_b_figure_type": "plot",
      "doc_b_image_path": "data/mineru_output/1810.01943/1810.01943/hybrid_auto/images/1810.01943_page0_fig9.jpg",
      "doc_b_caption": "Figure 5. Fairness vs. Balanced Accuracy before (top panel) and after (bottom panel) applying various bias mitigation algorithms. Four different fairness metrics are shown. In most cases two classifiers (Logistic regression - LR or Random forest classifier - RF) were used. The ideal fair value of disparate impact is 1, whereas for all other metrics it is 0. The circles indicate the mean value and bars indicate the extent of $\\pm 1$ standard deviation. Dataset: Adult, Protected attribute: race.",
      "doc_b_visual_score": 3,
      "score": 11.0
    },
    {
      "doc_a": "1603.07025",
      "doc_b": "1905.03674",
      "shared_entities": [
        "In Figure"
      ],
      "shared_entity_count": 1,
      "doc_a_query_id": "l1_1603.07025_1603.07025_fig_2_98",
      "doc_a_query": "Why does the dashed teal line remain two orders of magnitude below the purple line throughout 2008-2014?",
      "doc_a_answer": "The dashed teal line represents cumulative subreddits while the purple line shows cumulative users. Reddit's structure allows many users to participate across relatively fewer subreddit communities.",
      "doc_a_visual_anchor": "dashed teal line staying around 10^5 while solid purple line reaches 10^7",
      "doc_a_text_evidence": "These datasets contain a total of 114 million submissions. An active user or subreddit is one that had at least one post (comment or submission) in the time bin we used—here, discretized by month",
      "doc_a_figure_id": "1603.07025_fig_2",
      "doc_a_figure_type": "plot",
      "doc_a_image_path": "data/mineru_output/1603.07025/1603.07025/hybrid_auto/images/1603.07025_page0_fig1.jpg",
      "doc_a_caption": "Figure 2: Figure (a) shows the cumulative growth of Reddit for users and subreddits. Figure (b) shows the number of active users and subreddits in Reddit over time. An active user or subreddit is one that had at least one post (comment or submission) in the time bin we used—here, discretized by month.",
      "doc_a_visual_score": 4,
      "doc_b_query_id": "l1_1905.03674_1905.03674_fig_1_786",
      "doc_b_query": "Do the two red centers support the claim that groups are entitled to proportional shares of centers?",
      "doc_b_answer": "Yes, the two red centers are positioned to serve individuals outside the middle column, demonstrating that when groups N and M are equal in size, each receives a proportional share of the k=2 centers available.",
      "doc_b_visual_anchor": "two red dots positioned symmetrically on left and right sides",
      "doc_b_text_evidence": "Proportionality has many advantages as a notion of fairness in clustering, beyond the intuitive appeal of groups being entitled to a proportional share of centers.",
      "doc_b_figure_id": "1905.03674_fig_1",
      "doc_b_figure_type": "example",
      "doc_b_image_path": "data/mineru_output/1905.03674/1905.03674/hybrid_auto/images/1905.03674_page0_fig0.jpg",
      "doc_b_caption": "Figure 1: Proportionality Example",
      "doc_b_visual_score": 3,
      "score": 11.0
    },
    {
      "doc_a": "1810.03611",
      "doc_b": "1905.03674",
      "shared_entities": [
        "In Figure"
      ],
      "shared_entity_count": 1,
      "doc_a_query_id": "l1_1810.03611_1810.03611_fig_2_599",
      "doc_a_query": "Does the strong alignment of blue points along the red dashed diagonal support that approximation accuracy exceeds PPMI baseline?",
      "doc_a_answer": "The close alignment of data points along the diagonal trend line demonstrates high correlation between approximated and validated effect sizes, confirming the method's accuracy. This visual validation directly relates to the comparison with the PPMI baseline discussed in the subsequent section.",
      "doc_a_visual_anchor": "blue data points aligned with red dashed diagonal line",
      "doc_a_text_evidence": "We have shown that our method can be used to identify bias-influencing documents and accurately approximate the impact of their removal, but how does it compare to a more",
      "doc_a_figure_id": "1810.03611_fig_2",
      "doc_a_figure_type": "plot",
      "doc_a_image_path": "data/mineru_output/1810.03611/1810.03611/hybrid_auto/images/1810.03611_page0_fig1.jpg",
      "doc_a_caption": "Figure 2. Approximated and ground truth WEAT bias effect size due to the removal of various perturbation sets for our NYT corpus, considering WEAT1. Each point describes the mean effect size of one set; error bars depict one standard deviation; the baseline (unperturbed) mean is shown with a vertical dotted line.",
      "doc_a_visual_score": 4,
      "doc_b_query_id": "l1_1905.03674_1905.03674_fig_1_786",
      "doc_b_query": "Do the two red centers support the claim that groups are entitled to proportional shares of centers?",
      "doc_b_answer": "Yes, the two red centers are positioned to serve individuals outside the middle column, demonstrating that when groups N and M are equal in size, each receives a proportional share of the k=2 centers available.",
      "doc_b_visual_anchor": "two red dots positioned symmetrically on left and right sides",
      "doc_b_text_evidence": "Proportionality has many advantages as a notion of fairness in clustering, beyond the intuitive appeal of groups being entitled to a proportional share of centers.",
      "doc_b_figure_id": "1905.03674_fig_1",
      "doc_b_figure_type": "example",
      "doc_b_image_path": "data/mineru_output/1905.03674/1905.03674/hybrid_auto/images/1905.03674_page0_fig0.jpg",
      "doc_b_caption": "Figure 1: Proportionality Example",
      "doc_b_visual_score": 3,
      "score": 11.0
    },
    {
      "doc_a": "1903.08136",
      "doc_b": "1905.03674",
      "shared_entities": [
        "In Figure"
      ],
      "shared_entity_count": 1,
      "doc_a_query_id": "l1_1903.08136_1903.08136_fig_1_748",
      "doc_a_query": "Why does the gray sector contain only triangles and circles without showing how low-degree users get excluded?",
      "doc_a_answer": "The gray sector demonstrates a community where both opinion types (shapes) and variations (textures) coexist, but methods like CESNA and Louvain fail to assign low-degree nodes or assign them to trivially small communities that prevent inclusion in analysis.",
      "doc_a_visual_anchor": "gray quarter-circle sector with mixed blue triangles and red circles",
      "doc_a_text_evidence": "many community detection approaches either fail to assign low degree (or lowly-connected) users to communities, or assign them to trivially small communities that prevent them from being included in analysis",
      "doc_a_figure_id": "1903.08136_fig_1",
      "doc_a_figure_type": "diagram",
      "doc_a_image_path": "data/mineru_output/1903.08136/1903.08136/hybrid_auto/images/1903.08136_page0_fig0.jpg",
      "doc_a_caption": "Figure 1: Demonstration of versatility of relevant low-degree people in social networks who get ignored by biased community detection methods. Shapes (circle, triangle) represent opinion, and texture represents variations of the opinion. We provide real-world examples of how this manifests in subsequent sections.",
      "doc_a_visual_score": 4,
      "doc_b_query_id": "l1_1905.03674_1905.03674_fig_1_786",
      "doc_b_query": "Do the two red centers support the claim that groups are entitled to proportional shares of centers?",
      "doc_b_answer": "Yes, the two red centers are positioned to serve individuals outside the middle column, demonstrating that when groups N and M are equal in size, each receives a proportional share of the k=2 centers available.",
      "doc_b_visual_anchor": "two red dots positioned symmetrically on left and right sides",
      "doc_b_text_evidence": "Proportionality has many advantages as a notion of fairness in clustering, beyond the intuitive appeal of groups being entitled to a proportional share of centers.",
      "doc_b_figure_id": "1905.03674_fig_1",
      "doc_b_figure_type": "example",
      "doc_b_image_path": "data/mineru_output/1905.03674/1905.03674/hybrid_auto/images/1905.03674_page0_fig0.jpg",
      "doc_b_caption": "Figure 1: Proportionality Example",
      "doc_b_visual_score": 3,
      "score": 11.0
    },
    {
      "doc_a": "1905.03674",
      "doc_b": "1905.12843",
      "shared_entities": [
        "In Figure"
      ],
      "shared_entity_count": 1,
      "doc_a_query_id": "l1_1905.03674_1905.03674_fig_1_786",
      "doc_a_query": "Do the two red centers support the claim that groups are entitled to proportional shares of centers?",
      "doc_a_answer": "Yes, the two red centers are positioned to serve individuals outside the middle column, demonstrating that when groups N and M are equal in size, each receives a proportional share of the k=2 centers available.",
      "doc_a_visual_anchor": "two red dots positioned symmetrically on left and right sides",
      "doc_a_text_evidence": "Proportionality has many advantages as a notion of fairness in clustering, beyond the intuitive appeal of groups being entitled to a proportional share of centers.",
      "doc_a_figure_id": "1905.03674_fig_1",
      "doc_a_figure_type": "example",
      "doc_a_image_path": "data/mineru_output/1905.03674/1905.03674/hybrid_auto/images/1905.03674_page0_fig0.jpg",
      "doc_a_caption": "Figure 1: Proportionality Example",
      "doc_a_visual_score": 3,
      "doc_b_query_id": "l1_1905.12843_1905.12843_fig_4_817",
      "doc_b_query": "Why does the solid black line in bottom plots perform better despite the claim of dominance on all datasets except adult?",
      "doc_b_answer": "The adult dataset is the exception where fair classification (solid black line) slightly outperforms the proposed method. On all other datasets, the proposed method dominates or matches baselines up to statistical uncertainty.",
      "doc_b_visual_anchor": "solid black line (fair class., oracle=LR, model=linear) in bottom plots",
      "doc_b_text_evidence": "Our method dominates or matches the baselines up to statistical uncertainty on all datasets except adult, where fair classification is slightly better.",
      "doc_b_figure_id": "1905.12843_fig_4",
      "doc_b_figure_type": "plot",
      "doc_b_image_path": "data/mineru_output/1905.12843/1905.12843/hybrid_auto/images/1905.12843_page0_fig3.jpg",
      "doc_b_caption": "Figure 1. Relative test loss versus the worst constraint violation with respect to SP. Relative losses are computed by subtracting the smallest baseline loss from the actual loss. For our algorithm and fair classification we plot the convex envelope of the predictors obtained on training data at various accuracy–fairness tradeoffs. We show $9 5 \\%$ confidence bands for the relative loss of our method and fair classification, and also show $9 5 \\%$ confidence intervals for constraint violation (the same for all methods). Our method dominates or matches the baselines up to statistical uncertainty on all datasets except adult, where fair classification is slightly better.",
      "doc_b_visual_score": 4,
      "score": 11.0
    },
    {
      "doc_a": "1610.02413",
      "doc_b": "1810.08683",
      "shared_entities": [
        "equalized odds"
      ],
      "shared_entity_count": 1,
      "doc_a_query_id": "l1_1610.02413_1610.02413_fig_4_136",
      "doc_a_query": "How does the horizontal gap indicated by the lower-right arrow relate to optimizing the equal opportunity predictor?",
      "doc_a_answer": "The horizontal gap between the ROC curve and the profit-maximizing tangent line is proportional to the cost for achieving a given true positive rate, which becomes a convex function that can be efficiently optimized using ternary search.",
      "doc_a_visual_anchor": "Black arrow pointing to horizontal gap between blue curve and tangent line in lower right",
      "doc_a_text_evidence": "within each group the cost for a given true positive rate is proportional to the horizontal gap between the ROC curve and the profit-maximizing tangent line (i.e., the two curves on the left plot), so it is a convex function of the true positive rate",
      "doc_a_figure_id": "1610.02413_fig_4",
      "doc_a_figure_type": "plot",
      "doc_a_image_path": "data/mineru_output/1610.02413/1610.02413/hybrid_auto/images/1610.02413_page0_fig3.jpg",
      "doc_a_caption": "Figure 2: Finding the optimal equalized odds threshold predictor (middle), and equal opportunity threshold predictor (right). For the equal opportunity predictor, within each group the cost for a given true positive rate is proportional to the horizontal gap between the ROC curve and the profit-maximizing tangent line (i.e., the two curves on the left plot), so it is a convex function of the true positive rate (right). This lets us optimize it efficiently with ternary search.",
      "doc_a_visual_score": 7,
      "doc_b_query_id": "l1_1810.08683_1810.08683_fig_4_640",
      "doc_b_query": "Does the inverse relationship between blue ACC and orange EOd curves in the bottom panel support varying λ conclusions?",
      "doc_b_answer": "Yes, as λ increases from 2e-10 to 1, ACC drops from ~91 to ~67 while EOd rises from ~0.05 to ~0.13, demonstrating the accuracy-fairness trade-off when varying λ as described in the analysis.",
      "doc_b_visual_anchor": "opposing trends of blue ACC (declining) and orange EOd (rising) curves from left to right in bottom panel",
      "doc_b_text_evidence": "This hypothesis is also supported by the results of Figure 5, in which we check how the accuracy and fairness, as measured with the EOd, varies by varying",
      "doc_b_figure_id": "1810.08683_fig_4",
      "doc_b_figure_type": "plot",
      "doc_b_image_path": "data/mineru_output/1810.08683/1810.08683/hybrid_auto/images/1810.08683_page0_fig3.jpg",
      "doc_b_caption": "Figure 5: Adult Dataset: ACC and EOd of MTL, when we fix $\\theta$ and $\\rho$ to be the best values found during the validation procedure and we vary  with $P { = } 0$ , $F { = } 1$ , and $S { = } 0$ .",
      "doc_b_visual_score": 6,
      "score": 11.0
    },
    {
      "doc_a": "1610.02413",
      "doc_b": "1902.03519",
      "shared_entities": [
        "II"
      ],
      "shared_entity_count": 1,
      "doc_a_query_id": "l1_1610.02413_1610.02413_fig_4_136",
      "doc_a_query": "How does the horizontal gap indicated by the lower-right arrow relate to optimizing the equal opportunity predictor?",
      "doc_a_answer": "The horizontal gap between the ROC curve and the profit-maximizing tangent line is proportional to the cost for achieving a given true positive rate, which becomes a convex function that can be efficiently optimized using ternary search.",
      "doc_a_visual_anchor": "Black arrow pointing to horizontal gap between blue curve and tangent line in lower right",
      "doc_a_text_evidence": "within each group the cost for a given true positive rate is proportional to the horizontal gap between the ROC curve and the profit-maximizing tangent line (i.e., the two curves on the left plot), so it is a convex function of the true positive rate",
      "doc_a_figure_id": "1610.02413_fig_4",
      "doc_a_figure_type": "plot",
      "doc_a_image_path": "data/mineru_output/1610.02413/1610.02413/hybrid_auto/images/1610.02413_page0_fig3.jpg",
      "doc_a_caption": "Figure 2: Finding the optimal equalized odds threshold predictor (middle), and equal opportunity threshold predictor (right). For the equal opportunity predictor, within each group the cost for a given true positive rate is proportional to the horizontal gap between the ROC curve and the profit-maximizing tangent line (i.e., the two curves on the left plot), so it is a convex function of the true positive rate (right). This lets us optimize it efficiently with ternary search.",
      "doc_a_visual_score": 7,
      "doc_b_query_id": "l1_1902.03519_1902.03519_fig_4_736",
      "doc_b_query": "How does the top node containing 3 blue and 1 red point demonstrate the embedding into γ-HST?",
      "doc_b_answer": "The top node represents the root of the γ-HST embedding of input points, which is the first step in the fairlet decomposition algorithm after embedding the point set into a hierarchical tree structure.",
      "doc_b_visual_anchor": "top node with 3 blue circles and 1 red circle",
      "doc_b_text_evidence": "The first step in our algorithm is to embed the input point set into a γ-HST (for a value of γ to be determined later in this section). Once we build a γ-HST embedding T of the input points",
      "doc_b_figure_id": "1902.03519_fig_4",
      "doc_b_figure_type": "diagram",
      "doc_b_image_path": "data/mineru_output/1902.03519/1902.03519/hybrid_auto/images/1902.03519_page0_fig3.jpg",
      "doc_b_caption": "Figure 1: A run of our algorithm for (1,3)-fairlet decomposition on 8 blue points and 4 red points in $\\mathbb { R } ^ { 2 }$ . Steps (c)-(e) show the three stages of step 1 in FairletDecomposition.",
      "doc_b_visual_score": 5,
      "score": 11.0
    },
    {
      "doc_a": "1805.11202",
      "doc_b": "1902.03519",
      "shared_entities": [
        "II"
      ],
      "shared_entity_count": 1,
      "doc_a_query_id": "l1_1805.11202_1805.11202_fig_5_454",
      "doc_a_query": "Why does the black curve reach approximately 0.10 at x≈3.5 while both conditional distributions peak below 0.05?",
      "doc_a_answer": "The black curve represents the overall data distribution P_data(x), which combines both conditional distributions P_data(x|s=1) and P_data(x|s=0), resulting in higher probability density than either individual conditional distribution alone.",
      "doc_a_visual_anchor": "Black solid curve peak at 0.10 versus green and red dashed curves peaking below 0.05",
      "doc_a_text_evidence": "shows the distributions P_data(x) (black), P_data(x|s=1) (green) and P_data(x|s=0) (red) of real data",
      "doc_a_figure_id": "1805.11202_fig_5",
      "doc_a_figure_type": "plot",
      "doc_a_image_path": "data/mineru_output/1805.11202/1805.11202/hybrid_auto/images/1805.11202_page0_fig4.jpg",
      "doc_a_caption": "Figure 3: Comparing FairGAN, NaïveFairGAN-I and NaïveFairGAN-II on a toy dataset. (a) shows the distributions $P _ { \\mathbf { d a t a } } ( x )$ (black), $P _ { \\bf d a t a } ( x | s = 1 )$ (green) and $P _ { \\bf d a t a } ( x | s = 0 )$ (red) of real data; (b), (c) and (d) are distributions $P _ { G } ( x ) , P _ { G } ( x | s = 1 )$ and $P _ { G } ( x | s = 0 )$ P x s P x s Pof synthetic datasets generated by NaïveFairGAN-I, NaïveFairGAN-II and FairGAN separately.",
      "doc_a_visual_score": 7,
      "doc_b_query_id": "l1_1902.03519_1902.03519_fig_4_736",
      "doc_b_query": "How does the top node containing 3 blue and 1 red point demonstrate the embedding into γ-HST?",
      "doc_b_answer": "The top node represents the root of the γ-HST embedding of input points, which is the first step in the fairlet decomposition algorithm after embedding the point set into a hierarchical tree structure.",
      "doc_b_visual_anchor": "top node with 3 blue circles and 1 red circle",
      "doc_b_text_evidence": "The first step in our algorithm is to embed the input point set into a γ-HST (for a value of γ to be determined later in this section). Once we build a γ-HST embedding T of the input points",
      "doc_b_figure_id": "1902.03519_fig_4",
      "doc_b_figure_type": "diagram",
      "doc_b_image_path": "data/mineru_output/1902.03519/1902.03519/hybrid_auto/images/1902.03519_page0_fig3.jpg",
      "doc_b_caption": "Figure 1: A run of our algorithm for (1,3)-fairlet decomposition on 8 blue points and 4 red points in $\\mathbb { R } ^ { 2 }$ . Steps (c)-(e) show the three stages of step 1 in FairletDecomposition.",
      "doc_b_visual_score": 5,
      "score": 11.0
    },
    {
      "doc_a": "1809.02244",
      "doc_b": "1907.06430",
      "shared_entities": [
        "COMPAS",
        "PSE"
      ],
      "shared_entity_count": 2,
      "doc_a_query_id": "l1_1809.02244_1809.02244_fig_4_530",
      "doc_a_query": "How do the dashed and dotted blue curves demonstrate the formalization extended from Nabi and Shpitser 2018 for utility parameter above 2.5?",
      "doc_a_answer": "The gap between the dashed (optimal unrestricted) and dotted (optimal fair) blue curves shows the tradeoff between unconstrained policy optimization and fairness-constrained policy learning, illustrating how fairness constraints modify decision-making to induce more equitable outcomes.",
      "doc_a_visual_anchor": "Separation between blue dashed line (~1.0) and blue dotted line (~0.8) for utility parameter values above 2.5",
      "doc_a_text_evidence": "We have extended a formalization of algorithmic fairness from Nabi & Shpitser (2018) to the setting of learning optimal policies under fairness constraints. We show how to constrain a set of statistical models and learn a policy such that subsequent decision making given new observations from the \"unfair world\" induces high-quality outcomes while satisfying the specified fairness constraints",
      "doc_a_figure_id": "1809.02244_fig_4",
      "doc_a_figure_type": "plot",
      "doc_a_image_path": "data/mineru_output/1809.02244/1809.02244/hybrid_auto/images/1809.02244_page0_fig3.jpg",
      "doc_a_caption": "Figure 2. Group-level incarceration rates for the COMPAS data as a function of the utility parameter $\\theta$ .",
      "doc_a_visual_score": 4,
      "doc_b_query_id": "l1_1907.06430_1907.06430_fig_11_860",
      "doc_b_query": "Does the red solid arrow from A to Y represent the direct path whose unfairness is quantified by path-specific effect?",
      "doc_b_answer": "Yes, the red solid arrow from A to Y represents the direct path A → Y, whose unfairness is quantified by the path-specific effect PSE^aa when the path A → D → Y is considered fair.",
      "doc_b_visual_anchor": "red solid arrow from node A to node Y",
      "doc_b_text_evidence": "in which the path $A $ $D  Y$ is considered fair, unfairness can instead be quantified with the path-specific effect along the direct path $A  Y$ , PSE $^ { a a }$",
      "doc_b_figure_id": "1907.06430_fig_11",
      "doc_b_figure_type": "diagram",
      "doc_b_image_path": "data/mineru_output/1907.06430/1907.06430/hybrid_auto/images/1907.06430_page0_fig10.jpg",
      "doc_b_caption": "Fig. 7. CBN underlying a college admission scenario.",
      "doc_b_visual_score": 4,
      "score": 11.0
    },
    {
      "doc_a": "1802.08139",
      "doc_b": "1906.02589",
      "shared_entities": [
        "In Fig"
      ],
      "shared_entity_count": 1,
      "doc_a_query_id": "l1_1802.08139_1802.08139_fig_17_356",
      "doc_a_query": "Why does the gray node C in subfigure (a) become a red bidirected link in the ADMG representation?",
      "doc_a_answer": "The gray node C represents an unobserved confounder. In ADMG notation, unobserved common causes are indicated by red bidirected links between the affected variables, which is why C's influence is shown as a red bidirected edge in subfigure (b).",
      "doc_a_visual_anchor": "Gray node C in subfigure (a) and red bidirected edge in subfigure (b)",
      "doc_a_text_evidence": "An ADMG is a causal graph containing two kinds of links, directed links (either green or black depending on whether we are interested in the corresponding causal path), and red bidirected links, indicating the presence of an unobserved common cause.",
      "doc_a_figure_id": "1802.08139_fig_17",
      "doc_a_figure_type": "diagram",
      "doc_a_image_path": "data/mineru_output/1802.08139/1802.08139/hybrid_auto/images/1802.08139_page0_fig16.jpg",
      "doc_a_caption": "Figure 7. (a): GCM with an unobserved confounder $C$ indicated with a gray node. (b): ADMG corresponding to (a). The causal effect along the green path $A  Y$ cannot be identified by only using observed variables.",
      "doc_a_visual_score": 4,
      "doc_b_query_id": "l1_1906.02589_1906.02589_fig_29_835",
      "doc_b_query": "Why does the solid blue TEXAS line maintain higher accuracy than dashed orange X-VAE across all Δ_CP values when predicting Heavy-Makeup?",
      "doc_b_answer": "FFVAE's structured latent code using multiple sensitive attributes (Chubby, Eyeglasses, Male) enables better disentanglement, aligning with subgroup fair machine learning goals more effectively than X-VAE's approach.",
      "doc_b_visual_anchor": "solid blue TEXAS line consistently above dashed orange X-VAE line from Δ_CP 0.00 to 0.40",
      "doc_b_text_evidence": "we discussed how disentangled representation learning aligns with the goals of subgroup fair machine learning, and presented a method for learning a structured latent code using multiple sensitive attributes",
      "doc_b_figure_id": "1906.02589_fig_29",
      "doc_b_figure_type": "plot",
      "doc_b_image_path": "data/mineru_output/1906.02589/1906.02589/hybrid_auto/images/1906.02589_page0_fig28.jpg",
      "doc_b_caption": "Figure 5. Celeb-A subgroup fair classification results. Sensitive attributes: Chubby (C), Eyeglasses (E), and Male (M). $y =$ Heavy-Makeup.",
      "doc_b_visual_score": 5,
      "score": 11.0
    },
    {
      "doc_a": "1811.03654",
      "doc_b": "1903.08136",
      "shared_entities": [
        "Amazon Mechanical Turk"
      ],
      "shared_entity_count": 1,
      "doc_a_query_id": "l1_1811.03654_1811.03654_fig_3_659",
      "doc_a_query": "Does the Ratio condition's 6.00 rating in Treatment 2 support that participants consistently favored proportional division?",
      "doc_a_answer": "Yes, participants in Study 2 consistently gave most support to the decision to divide the $50,000 between the two individuals in proportion (Ratio condition), as shown by the highest ratings across all four treatments.",
      "doc_a_visual_anchor": "Ratio condition value of 6.00 with error bars in Treatment 2 upper-right panel",
      "doc_a_text_evidence": "Thus, participants in Study 2 consistently gave most support to the decision to divide the $50,000 between the two individuals in prop",
      "doc_a_figure_id": "1811.03654_fig_3",
      "doc_a_figure_type": "plot",
      "doc_a_image_path": "data/mineru_output/1811.03654/1811.03654/hybrid_auto/images/1811.03654_page0_fig2.jpg",
      "doc_a_caption": "Figure 3: Comparison of means (with $9 5 \\%$ CI) for Study 2 (when the individual with the higher loan repayment rate is black). Where * signifies p $< 0 . 0 5$ , $^ { * * } \\mathrm {  ~ p ~ } { < } 0 . 0 1$ , and $\\ast \\ast \\ast _ { \\mathrm { ~ p ~ } }$ ${ < } 0 . 0 0 1$ .",
      "doc_a_visual_score": 3,
      "doc_b_query_id": "l1_1903.08136_1903.08136_fig_1_748",
      "doc_b_query": "Why does the gray sector contain only triangles and circles without showing how low-degree users get excluded?",
      "doc_b_answer": "The gray sector demonstrates a community where both opinion types (shapes) and variations (textures) coexist, but methods like CESNA and Louvain fail to assign low-degree nodes or assign them to trivially small communities that prevent inclusion in analysis.",
      "doc_b_visual_anchor": "gray quarter-circle sector with mixed blue triangles and red circles",
      "doc_b_text_evidence": "many community detection approaches either fail to assign low degree (or lowly-connected) users to communities, or assign them to trivially small communities that prevent them from being included in analysis",
      "doc_b_figure_id": "1903.08136_fig_1",
      "doc_b_figure_type": "diagram",
      "doc_b_image_path": "data/mineru_output/1903.08136/1903.08136/hybrid_auto/images/1903.08136_page0_fig0.jpg",
      "doc_b_caption": "Figure 1: Demonstration of versatility of relevant low-degree people in social networks who get ignored by biased community detection methods. Shapes (circle, triangle) represent opinion, and texture represents variations of the opinion. We provide real-world examples of how this manifests in subsequent sections.",
      "doc_b_visual_score": 4,
      "score": 11.0
    },
    {
      "doc_a": "1306.5204",
      "doc_b": "1511.00830",
      "shared_entities": [
        "accuracy"
      ],
      "shared_entity_count": 1,
      "doc_a_query_id": "l1_1306.5204_1306.5204_fig_3_8",
      "doc_a_query": "How does the gap between red and blue lines relate to social media analysis reliability?",
      "doc_a_answer": "The persistent gap demonstrates that researchers' reliance on the Streaming API is problematic, as it consistently undersamples compared to the Firehose. This affects topic modeling, network analysis, and statistical content analysis accuracy.",
      "doc_a_visual_anchor": "Vertical gap between solid red Firehose line (higher) and dotted blue Streaming line (lower) throughout the time period",
      "doc_a_text_evidence": "It has been used to collect data for topic modeling (Hong and Davison 2010; Pozdnoukhov and Kaiser 2011), network analysis (Sofean and Smith 2012), and statistical analysis of content (Mathioudakis and Koudas 2010)",
      "doc_a_figure_id": "1306.5204_fig_3",
      "doc_a_figure_type": "plot",
      "doc_a_image_path": "data/mineru_output/1306.5204/1306.5204/hybrid_auto/images/1306.5204_page0_fig2.jpg",
      "doc_a_caption": "Figure 2: Raw tweet counts for each day from both the Streaming API and the Firehose.",
      "doc_a_visual_score": 6,
      "doc_b_query_id": "l1_1511.00830_1511.00830_fig_1_64",
      "doc_b_query": "Why does node S point directly to X rather than through Z in this unsupervised representation learning model?",
      "doc_b_answer": "In unsupervised representation learning, S represents noise or nuisance variables that directly affect the observed data X, independent of the informative latent factors Z. This direct connection models uninformative dimensions that are typically detrimental for the task under consideration.",
      "doc_b_visual_anchor": "arrow from node S (top-left) directly to node X (bottom-center)",
      "doc_b_text_evidence": "Uninformative dimensions are often called \"noise\" or \"nuisance variables\" while informative dimensions are usually called latent or hidden factors of variation.",
      "doc_b_figure_id": "1511.00830_fig_1",
      "doc_b_figure_type": "diagram",
      "doc_b_image_path": "data/mineru_output/1511.00830/1511.00830/hybrid_auto/images/1511.00830_page0_fig0.jpg",
      "doc_b_caption": "Figure 1: Unsupervised model",
      "doc_b_visual_score": 5,
      "score": 10.5
    },
    {
      "doc_a": "1412.3756",
      "doc_b": "1610.08452",
      "shared_entities": [
        "Fairness",
        "accuracy"
      ],
      "shared_entity_count": 2,
      "doc_a_query_id": "l1_1412.3756_1412.3756_fig_3_55",
      "doc_a_query": "Does the utility decay pattern visible across all scatter points support the expected tradeoff between fairness and utility?",
      "doc_a_answer": "Yes, the scatter points show declining utility (from ~1.0 to ~0.5-0.9) as disparate impact increases toward the green line at DI=1.0, demonstrating the expected decay over utility as fairness increases.",
      "doc_a_visual_anchor": "Dashed trend lines connecting scatter points showing downward slope from left to right",
      "doc_a_text_evidence": "The results, shown in Figure 3, demonstrate the expected decay over utility as fairness increases.",
      "doc_a_figure_id": "1412.3756_fig_3",
      "doc_a_figure_type": "plot",
      "doc_a_image_path": "data/mineru_output/1412.3756/1412.3756/hybrid_auto/images/1412.3756_page0_fig2.jpg",
      "doc_a_caption": "Fig. 3: Disparate impact (DI) vs. utility (1-BER) from our combinatorial and geometric partial repair processes using the SVM to classify on the Adult Income and German Credit data sets and the simple threshold classifier on the Ricci data set. Recall that only points with $\\mathsf { D } \\mathsf { I } \\ge \\tau = 0 . 8$ are legal. $\\mathsf { D } \\mathsf { I } = 1 . 0$ represents full fairness.",
      "doc_a_visual_score": 5,
      "doc_b_query_id": "l1_1610.08452_1610.08452_fig_9_182",
      "doc_b_query": "What causes the dashed boundary to separate more red circles from green ones compared to the solid boundary?",
      "doc_b_answer": "The fair constrained classifier shifts the decision boundary to equalize false positive and negative rates across groups, which the research compares against unconstrained classifiers to demonstrate performance trade-offs when avoiding disparate mistreatment.",
      "doc_b_visual_anchor": "dashed blue line positioned differently from solid cyan line relative to red and green markers",
      "doc_b_text_evidence": "Figure 4 summarizes the results by showing the decision boundaries for both the unconstrained classifiers (solid) and the fair constrained classifier (dashed) when controlling for disparate mistreatment",
      "doc_b_figure_id": "1610.08452_fig_9",
      "doc_b_figure_type": "plot",
      "doc_b_image_path": "data/mineru_output/1610.08452/1610.08452/hybrid_auto/images/1610.08452_page0_fig8.jpg",
      "doc_b_caption": "Figure 4: [Synthetic data] $D _ { F P R }$ and $D _ { F N R }$ have the same sign. Removing disparate mistreatment on FPR can potentially increase disparate mistreatment on FNR. Removing disparate mistreatment on both at the same time causes a larger drop in accuracy.",
      "doc_b_visual_score": 6,
      "score": 10.5
    },
    {
      "doc_a": "1412.3756",
      "doc_b": "1808.08166",
      "shared_entities": [
        "Fairness",
        "accuracy"
      ],
      "shared_entity_count": 2,
      "doc_a_query_id": "l1_1412.3756_1412.3756_fig_3_55",
      "doc_a_query": "Does the utility decay pattern visible across all scatter points support the expected tradeoff between fairness and utility?",
      "doc_a_answer": "Yes, the scatter points show declining utility (from ~1.0 to ~0.5-0.9) as disparate impact increases toward the green line at DI=1.0, demonstrating the expected decay over utility as fairness increases.",
      "doc_a_visual_anchor": "Dashed trend lines connecting scatter points showing downward slope from left to right",
      "doc_a_text_evidence": "The results, shown in Figure 3, demonstrate the expected decay over utility as fairness increases.",
      "doc_a_figure_id": "1412.3756_fig_3",
      "doc_a_figure_type": "plot",
      "doc_a_image_path": "data/mineru_output/1412.3756/1412.3756/hybrid_auto/images/1412.3756_page0_fig2.jpg",
      "doc_a_caption": "Fig. 3: Disparate impact (DI) vs. utility (1-BER) from our combinatorial and geometric partial repair processes using the SVM to classify on the Adult Income and German Credit data sets and the simple threshold classifier on the Ricci data set. Recall that only points with $\\mathsf { D } \\mathsf { I } \\ge \\tau = 0 . 8$ are legal. $\\mathsf { D } \\mathsf { I } = 1 . 0$ represents full fairness.",
      "doc_a_visual_score": 5,
      "doc_b_query_id": "l1_1808.08166_1808.08166_fig_11_472",
      "doc_b_query": "Does the blue points' downward trend from y=0.25 to y=0 demonstrate the MARGINAL algorithm's error-fairness pairs?",
      "doc_b_answer": "Yes, the blue points show the error and marginal fairness violation pairs achieved by the MARGINAL algorithm, displaying a clear trade-off as error increases from left to right.",
      "doc_b_visual_anchor": "blue points forming downward curve from upper-left (error ~0.16, fairness ~0.25) to lower-right (error ~0.23, fairness ~0)",
      "doc_b_text_evidence": "The error and marginal fairness violation for the MARGINAL algorithm across all four data sets.",
      "doc_b_figure_id": "1808.08166_fig_11",
      "doc_b_figure_type": "plot",
      "doc_b_image_path": "data/mineru_output/1808.08166/1808.08166/hybrid_auto/images/1808.08166_page0_fig10.jpg",
      "doc_b_caption": "Figure 3: Left column: The red points show the Pareto frontier of error (x axis) and subgroup fairness violation (y axis) for the SUBGROUP algorithm across all four data sets, while the blue points show the error and subgroup fairness violation for the models achieved by the MARGINAL algorithm. Right column: The error and marginal fairness violation for the MARGINAL algorithm across all four data sets. Ordering of datasets is Communities and Crime, Law School, Adult, and Student.",
      "doc_b_visual_score": 6,
      "score": 10.5
    },
    {
      "doc_a": "1511.00830",
      "doc_b": "1610.08452",
      "shared_entities": [
        "accuracy"
      ],
      "shared_entity_count": 1,
      "doc_a_query_id": "l1_1511.00830_1511.00830_fig_1_64",
      "doc_a_query": "Why does node S point directly to X rather than through Z in this unsupervised representation learning model?",
      "doc_a_answer": "In unsupervised representation learning, S represents noise or nuisance variables that directly affect the observed data X, independent of the informative latent factors Z. This direct connection models uninformative dimensions that are typically detrimental for the task under consideration.",
      "doc_a_visual_anchor": "arrow from node S (top-left) directly to node X (bottom-center)",
      "doc_a_text_evidence": "Uninformative dimensions are often called \"noise\" or \"nuisance variables\" while informative dimensions are usually called latent or hidden factors of variation.",
      "doc_a_figure_id": "1511.00830_fig_1",
      "doc_a_figure_type": "diagram",
      "doc_a_image_path": "data/mineru_output/1511.00830/1511.00830/hybrid_auto/images/1511.00830_page0_fig0.jpg",
      "doc_a_caption": "Figure 1: Unsupervised model",
      "doc_a_visual_score": 5,
      "doc_b_query_id": "l1_1610.08452_1610.08452_fig_9_182",
      "doc_b_query": "What causes the dashed boundary to separate more red circles from green ones compared to the solid boundary?",
      "doc_b_answer": "The fair constrained classifier shifts the decision boundary to equalize false positive and negative rates across groups, which the research compares against unconstrained classifiers to demonstrate performance trade-offs when avoiding disparate mistreatment.",
      "doc_b_visual_anchor": "dashed blue line positioned differently from solid cyan line relative to red and green markers",
      "doc_b_text_evidence": "Figure 4 summarizes the results by showing the decision boundaries for both the unconstrained classifiers (solid) and the fair constrained classifier (dashed) when controlling for disparate mistreatment",
      "doc_b_figure_id": "1610.08452_fig_9",
      "doc_b_figure_type": "plot",
      "doc_b_image_path": "data/mineru_output/1610.08452/1610.08452/hybrid_auto/images/1610.08452_page0_fig8.jpg",
      "doc_b_caption": "Figure 4: [Synthetic data] $D _ { F P R }$ and $D _ { F N R }$ have the same sign. Removing disparate mistreatment on FPR can potentially increase disparate mistreatment on FNR. Removing disparate mistreatment on both at the same time causes a larger drop in accuracy.",
      "doc_b_visual_score": 6,
      "score": 10.5
    },
    {
      "doc_a": "1511.00830",
      "doc_b": "1808.08166",
      "shared_entities": [
        "accuracy"
      ],
      "shared_entity_count": 1,
      "doc_a_query_id": "l1_1511.00830_1511.00830_fig_1_64",
      "doc_a_query": "Why does node S point directly to X rather than through Z in this unsupervised representation learning model?",
      "doc_a_answer": "In unsupervised representation learning, S represents noise or nuisance variables that directly affect the observed data X, independent of the informative latent factors Z. This direct connection models uninformative dimensions that are typically detrimental for the task under consideration.",
      "doc_a_visual_anchor": "arrow from node S (top-left) directly to node X (bottom-center)",
      "doc_a_text_evidence": "Uninformative dimensions are often called \"noise\" or \"nuisance variables\" while informative dimensions are usually called latent or hidden factors of variation.",
      "doc_a_figure_id": "1511.00830_fig_1",
      "doc_a_figure_type": "diagram",
      "doc_a_image_path": "data/mineru_output/1511.00830/1511.00830/hybrid_auto/images/1511.00830_page0_fig0.jpg",
      "doc_a_caption": "Figure 1: Unsupervised model",
      "doc_a_visual_score": 5,
      "doc_b_query_id": "l1_1808.08166_1808.08166_fig_11_472",
      "doc_b_query": "Does the blue points' downward trend from y=0.25 to y=0 demonstrate the MARGINAL algorithm's error-fairness pairs?",
      "doc_b_answer": "Yes, the blue points show the error and marginal fairness violation pairs achieved by the MARGINAL algorithm, displaying a clear trade-off as error increases from left to right.",
      "doc_b_visual_anchor": "blue points forming downward curve from upper-left (error ~0.16, fairness ~0.25) to lower-right (error ~0.23, fairness ~0)",
      "doc_b_text_evidence": "The error and marginal fairness violation for the MARGINAL algorithm across all four data sets.",
      "doc_b_figure_id": "1808.08166_fig_11",
      "doc_b_figure_type": "plot",
      "doc_b_image_path": "data/mineru_output/1808.08166/1808.08166/hybrid_auto/images/1808.08166_page0_fig10.jpg",
      "doc_b_caption": "Figure 3: Left column: The red points show the Pareto frontier of error (x axis) and subgroup fairness violation (y axis) for the SUBGROUP algorithm across all four data sets, while the blue points show the error and subgroup fairness violation for the models achieved by the MARGINAL algorithm. Right column: The error and marginal fairness violation for the MARGINAL algorithm across all four data sets. Ordering of datasets is Communities and Crime, Law School, Adult, and Student.",
      "doc_b_visual_score": 6,
      "score": 10.5
    },
    {
      "doc_a": "1511.00830",
      "doc_b": "1810.08683",
      "shared_entities": [
        "accuracy"
      ],
      "shared_entity_count": 1,
      "doc_a_query_id": "l1_1511.00830_1511.00830_fig_1_64",
      "doc_a_query": "Why does node S point directly to X rather than through Z in this unsupervised representation learning model?",
      "doc_a_answer": "In unsupervised representation learning, S represents noise or nuisance variables that directly affect the observed data X, independent of the informative latent factors Z. This direct connection models uninformative dimensions that are typically detrimental for the task under consideration.",
      "doc_a_visual_anchor": "arrow from node S (top-left) directly to node X (bottom-center)",
      "doc_a_text_evidence": "Uninformative dimensions are often called \"noise\" or \"nuisance variables\" while informative dimensions are usually called latent or hidden factors of variation.",
      "doc_a_figure_id": "1511.00830_fig_1",
      "doc_a_figure_type": "diagram",
      "doc_a_image_path": "data/mineru_output/1511.00830/1511.00830/hybrid_auto/images/1511.00830_page0_fig0.jpg",
      "doc_a_caption": "Figure 1: Unsupervised model",
      "doc_a_visual_score": 5,
      "doc_b_query_id": "l1_1810.08683_1810.08683_fig_4_640",
      "doc_b_query": "Does the inverse relationship between blue ACC and orange EOd curves in the bottom panel support varying λ conclusions?",
      "doc_b_answer": "Yes, as λ increases from 2e-10 to 1, ACC drops from ~91 to ~67 while EOd rises from ~0.05 to ~0.13, demonstrating the accuracy-fairness trade-off when varying λ as described in the analysis.",
      "doc_b_visual_anchor": "opposing trends of blue ACC (declining) and orange EOd (rising) curves from left to right in bottom panel",
      "doc_b_text_evidence": "This hypothesis is also supported by the results of Figure 5, in which we check how the accuracy and fairness, as measured with the EOd, varies by varying",
      "doc_b_figure_id": "1810.08683_fig_4",
      "doc_b_figure_type": "plot",
      "doc_b_image_path": "data/mineru_output/1810.08683/1810.08683/hybrid_auto/images/1810.08683_page0_fig3.jpg",
      "doc_b_caption": "Figure 5: Adult Dataset: ACC and EOd of MTL, when we fix $\\theta$ and $\\rho$ to be the best values found during the validation procedure and we vary  with $P { = } 0$ , $F { = } 1$ , and $S { = } 0$ .",
      "doc_b_visual_score": 6,
      "score": 10.5
    },
    {
      "doc_a": "1602.05352",
      "doc_b": "1905.10674",
      "shared_entities": [
        "RMSE",
        "accuracy"
      ],
      "shared_entity_count": 2,
      "doc_a_query_id": "l1_1602.05352_1602.05352_fig_5_88",
      "doc_a_query": "Why does the blue MF-IPS curve achieve lower MSE than red MF-Naive across all alpha values?",
      "doc_a_answer": "MF-IPS uses propensity weighting to correct for selection bias, while MF-Naive does not account for biased observations. This allows MF-IPS to achieve better prediction accuracy through improved risk estimation.",
      "doc_a_visual_anchor": "blue MF-IPS curve consistently below red MF-Naive curve across alpha from 0 to 1",
      "doc_a_text_evidence": "Using the same semi-synthetic ML100K dataset and observation model as above, we compare our matrix factorization MF-IPS with the traditional unweighted matrix factorization MF-Naive.",
      "doc_a_figure_id": "1602.05352_fig_5",
      "doc_a_figure_type": "plot",
      "doc_a_image_path": "data/mineru_output/1602.05352/1602.05352/hybrid_auto/images/1602.05352_page0_fig4.jpg",
      "doc_a_caption": "Figure 3. Prediction error (MSE) of matrix factorization methods as the observed ratings exhibit varying degrees of selection bias (left) and as propensity estimation quality degrades (right).",
      "doc_a_visual_score": 3,
      "doc_b_query_id": "l1_1905.10674_1905.10674_fig_1_799",
      "doc_b_query": "Does the input graph's blue and red node structure support embedding generation that operates on social networks?",
      "doc_b_answer": "Yes, the bipartite-style input graph with blue and red nodes represents the type of graph data structure used in social networks and recommender systems that the embedding technique is designed to handle.",
      "doc_b_visual_anchor": "Input Graph box showing blue and red nodes connected in a network structure on the left side",
      "doc_b_text_evidence": "Learning high-quality node embeddings is an important building block for machine learning models that operate on graph data, such as social networks and recommender systems.",
      "doc_b_figure_id": "1905.10674_fig_1",
      "doc_b_figure_type": "diagram",
      "doc_b_image_path": "data/mineru_output/1905.10674/1905.10674/hybrid_auto/images/1905.10674_page0_fig0.jpg",
      "doc_b_caption": "Figure 1. Overview of our approach: Our goal is to generate graph embeddings that are invariant to particular sensitive attributes (e.g., age or gender). We train a set of “filters” to prevent adversarial discriminators from classifying the sensitive information from the filtered embeddings. After training, these filters can be composed together in different combinations, allowing the flexible generation of embeddings that are invariant w.r.t. any subset of the sensitive attributes.",
      "doc_b_visual_score": 4,
      "score": 10.5
    },
    {
      "doc_a": "1610.08452",
      "doc_b": "1906.02589",
      "shared_entities": [
        "Fairness",
        "accuracy"
      ],
      "shared_entity_count": 2,
      "doc_a_query_id": "l1_1610.08452_1610.08452_fig_9_182",
      "doc_a_query": "What causes the dashed boundary to separate more red circles from green ones compared to the solid boundary?",
      "doc_a_answer": "The fair constrained classifier shifts the decision boundary to equalize false positive and negative rates across groups, which the research compares against unconstrained classifiers to demonstrate performance trade-offs when avoiding disparate mistreatment.",
      "doc_a_visual_anchor": "dashed blue line positioned differently from solid cyan line relative to red and green markers",
      "doc_a_text_evidence": "Figure 4 summarizes the results by showing the decision boundaries for both the unconstrained classifiers (solid) and the fair constrained classifier (dashed) when controlling for disparate mistreatment",
      "doc_a_figure_id": "1610.08452_fig_9",
      "doc_a_figure_type": "plot",
      "doc_a_image_path": "data/mineru_output/1610.08452/1610.08452/hybrid_auto/images/1610.08452_page0_fig8.jpg",
      "doc_a_caption": "Figure 4: [Synthetic data] $D _ { F P R }$ and $D _ { F N R }$ have the same sign. Removing disparate mistreatment on FPR can potentially increase disparate mistreatment on FNR. Removing disparate mistreatment on both at the same time causes a larger drop in accuracy.",
      "doc_a_visual_score": 6,
      "doc_b_query_id": "l1_1906.02589_1906.02589_fig_29_835",
      "doc_b_query": "Why does the solid blue TEXAS line maintain higher accuracy than dashed orange X-VAE across all Δ_CP values when predicting Heavy-Makeup?",
      "doc_b_answer": "FFVAE's structured latent code using multiple sensitive attributes (Chubby, Eyeglasses, Male) enables better disentanglement, aligning with subgroup fair machine learning goals more effectively than X-VAE's approach.",
      "doc_b_visual_anchor": "solid blue TEXAS line consistently above dashed orange X-VAE line from Δ_CP 0.00 to 0.40",
      "doc_b_text_evidence": "we discussed how disentangled representation learning aligns with the goals of subgroup fair machine learning, and presented a method for learning a structured latent code using multiple sensitive attributes",
      "doc_b_figure_id": "1906.02589_fig_29",
      "doc_b_figure_type": "plot",
      "doc_b_image_path": "data/mineru_output/1906.02589/1906.02589/hybrid_auto/images/1906.02589_page0_fig28.jpg",
      "doc_b_caption": "Figure 5. Celeb-A subgroup fair classification results. Sensitive attributes: Chubby (C), Eyeglasses (E), and Male (M). $y =$ Heavy-Makeup.",
      "doc_b_visual_score": 5,
      "score": 10.5
    },
    {
      "doc_a": "1711.05144",
      "doc_b": "1905.10674",
      "shared_entities": [
        "accuracy"
      ],
      "shared_entity_count": 1,
      "doc_a_query_id": "l1_1711.05144_1711.05144_fig_3_299",
      "doc_a_query": "Why do the color-coded trajectories form distinct curves rather than a single frontier, given the reductions approach?",
      "doc_a_answer": "Each colored trajectory represents Pareto-optimal solutions for a specific gamma parameter value. The reductions approach by Agarwal et al. generates different error-unfairness trade-offs as gamma varies.",
      "doc_a_visual_anchor": "Multiple distinct colored curves (blue, red, green, purple, orange) descending from upper-left",
      "doc_a_text_evidence": "Pareto-optimal error-unfairness values, color coded by varying values of the input parameter γ",
      "doc_a_figure_id": "1711.05144_fig_3",
      "doc_a_figure_type": "plot",
      "doc_a_image_path": "data/mineru_output/1711.05144/1711.05144/hybrid_auto/images/1711.05144_page0_fig2.jpg",
      "doc_a_caption": "Figure 2: (a) Pareto-optimal error-unfairness values, color coded by varying values of the input parameter γ. (b) Aggregate Pareto frontier across all values of γ. Here the γ values cover the same range but are sampled more densely to get a smoother frontier. See text for details.",
      "doc_a_visual_score": 7,
      "doc_b_query_id": "l1_1905.10674_1905.10674_fig_1_799",
      "doc_b_query": "Does the input graph's blue and red node structure support embedding generation that operates on social networks?",
      "doc_b_answer": "Yes, the bipartite-style input graph with blue and red nodes represents the type of graph data structure used in social networks and recommender systems that the embedding technique is designed to handle.",
      "doc_b_visual_anchor": "Input Graph box showing blue and red nodes connected in a network structure on the left side",
      "doc_b_text_evidence": "Learning high-quality node embeddings is an important building block for machine learning models that operate on graph data, such as social networks and recommender systems.",
      "doc_b_figure_id": "1905.10674_fig_1",
      "doc_b_figure_type": "diagram",
      "doc_b_image_path": "data/mineru_output/1905.10674/1905.10674/hybrid_auto/images/1905.10674_page0_fig0.jpg",
      "doc_b_caption": "Figure 1. Overview of our approach: Our goal is to generate graph embeddings that are invariant to particular sensitive attributes (e.g., age or gender). We train a set of “filters” to prevent adversarial discriminators from classifying the sensitive information from the filtered embeddings. After training, these filters can be composed together in different combinations, allowing the flexible generation of embeddings that are invariant w.r.t. any subset of the sensitive attributes.",
      "doc_b_visual_score": 4,
      "score": 10.5
    },
    {
      "doc_a": "1711.05144",
      "doc_b": "2005.07293",
      "shared_entities": [
        "accuracy"
      ],
      "shared_entity_count": 1,
      "doc_a_query_id": "l1_1711.05144_1711.05144_fig_3_299",
      "doc_a_query": "Why do the color-coded trajectories form distinct curves rather than a single frontier, given the reductions approach?",
      "doc_a_answer": "Each colored trajectory represents Pareto-optimal solutions for a specific gamma parameter value. The reductions approach by Agarwal et al. generates different error-unfairness trade-offs as gamma varies.",
      "doc_a_visual_anchor": "Multiple distinct colored curves (blue, red, green, purple, orange) descending from upper-left",
      "doc_a_text_evidence": "Pareto-optimal error-unfairness values, color coded by varying values of the input parameter γ",
      "doc_a_figure_id": "1711.05144_fig_3",
      "doc_a_figure_type": "plot",
      "doc_a_image_path": "data/mineru_output/1711.05144/1711.05144/hybrid_auto/images/1711.05144_page0_fig2.jpg",
      "doc_a_caption": "Figure 2: (a) Pareto-optimal error-unfairness values, color coded by varying values of the input parameter γ. (b) Aggregate Pareto frontier across all values of γ. Here the γ values cover the same range but are sampled more densely to get a smoother frontier. See text for details.",
      "doc_a_visual_score": 7,
      "doc_b_query_id": "l1_2005.07293_2005.07293_fig_1_905",
      "doc_b_query": "How does the different-height box arrangement under the yellow figure in the right panel operationalize compensation for historically disadvantaged groups?",
      "doc_b_answer": "The taller box under the yellow figure represents resource distribution to overcome obstacles and raise opportunities for access, compensating historically disadvantaged groups so they can reach the apples like others.",
      "doc_b_visual_anchor": "Right panel 'Equity' showing yellow figure standing on tallest brown box",
      "doc_b_text_evidence": "Equity is the distribution of resources among groups to overcome obstacles and to raise their opportunities for access [Schement, 2001]. Thus, historically disadvantaged groups are compensated, and others get their fair share to reach their goals.",
      "doc_b_figure_id": "2005.07293_fig_1",
      "doc_b_figure_type": "diagram",
      "doc_b_image_path": "data/mineru_output/2005.07293/2005.07293/hybrid_auto/images/2005.07293_page0_fig0.jpg",
      "doc_b_caption": "Figure 1: Notion of equality in fairness is depicted and formalized along with our newly formalized notion of equity.",
      "doc_b_visual_score": 4,
      "score": 10.5
    },
    {
      "doc_a": "1711.05144",
      "doc_b": "2109.03952",
      "shared_entities": [
        "accuracy"
      ],
      "shared_entity_count": 1,
      "doc_a_query_id": "l1_1711.05144_1711.05144_fig_3_299",
      "doc_a_query": "Why do the color-coded trajectories form distinct curves rather than a single frontier, given the reductions approach?",
      "doc_a_answer": "Each colored trajectory represents Pareto-optimal solutions for a specific gamma parameter value. The reductions approach by Agarwal et al. generates different error-unfairness trade-offs as gamma varies.",
      "doc_a_visual_anchor": "Multiple distinct colored curves (blue, red, green, purple, orange) descending from upper-left",
      "doc_a_text_evidence": "Pareto-optimal error-unfairness values, color coded by varying values of the input parameter γ",
      "doc_a_figure_id": "1711.05144_fig_3",
      "doc_a_figure_type": "plot",
      "doc_a_image_path": "data/mineru_output/1711.05144/1711.05144/hybrid_auto/images/1711.05144_page0_fig2.jpg",
      "doc_a_caption": "Figure 2: (a) Pareto-optimal error-unfairness values, color coded by varying values of the input parameter γ. (b) Aggregate Pareto frontier across all values of γ. Here the γ values cover the same range but are sampled more densely to get a smoother frontier. See text for details.",
      "doc_a_visual_score": 7,
      "doc_b_query_id": "l1_2109.03952_2109.03952_fig_1_948",
      "doc_b_query": "How does the orange Attention layer produce its output representation before the yellow Dense Layers?",
      "doc_b_answer": "The attention layer computes weights and takes a weighted combination of the input feature embeddings to produce a d^e-dimensional vector representation for the sample instance.",
      "doc_b_visual_anchor": "orange Attention layer box positioned between red embedding boxes and yellow Dense Layer boxes",
      "doc_b_text_evidence": "These vectors are passed to the attention layer. The Computation of attention weights and the final representation for a sample is",
      "doc_b_figure_id": "2109.03952_fig_1",
      "doc_b_figure_type": "architecture",
      "doc_b_image_path": "data/mineru_output/2109.03952/2109.03952/hybrid_auto/images/2109.03952_page0_fig0.jpg",
      "doc_b_caption": "Figure 1: (a) In general classification model, for each feature $f _ { k }$ a vector representation $e _ { k }$ of length $d ^ { e }$ is learned. This is passed to the attention layer which produces a $d ^ { e }$ - dimensional vector representation for the sample instance $i$ which is passed to two dense layers to get the final classification output. (b) The Attribution framework has the same architecture as the general model. One outcome is obtained through the original model and another through the model that has some attention weights zeroed. The observed difference in accuracy and fairness measures will indicate the effect of the zeroed out features on accuracy and fairness.",
      "doc_b_visual_score": 4,
      "score": 10.5
    },
    {
      "doc_a": "1808.08166",
      "doc_b": "1906.02589",
      "shared_entities": [
        "Fairness",
        "accuracy"
      ],
      "shared_entity_count": 2,
      "doc_a_query_id": "l1_1808.08166_1808.08166_fig_11_472",
      "doc_a_query": "Does the blue points' downward trend from y=0.25 to y=0 demonstrate the MARGINAL algorithm's error-fairness pairs?",
      "doc_a_answer": "Yes, the blue points show the error and marginal fairness violation pairs achieved by the MARGINAL algorithm, displaying a clear trade-off as error increases from left to right.",
      "doc_a_visual_anchor": "blue points forming downward curve from upper-left (error ~0.16, fairness ~0.25) to lower-right (error ~0.23, fairness ~0)",
      "doc_a_text_evidence": "The error and marginal fairness violation for the MARGINAL algorithm across all four data sets.",
      "doc_a_figure_id": "1808.08166_fig_11",
      "doc_a_figure_type": "plot",
      "doc_a_image_path": "data/mineru_output/1808.08166/1808.08166/hybrid_auto/images/1808.08166_page0_fig10.jpg",
      "doc_a_caption": "Figure 3: Left column: The red points show the Pareto frontier of error (x axis) and subgroup fairness violation (y axis) for the SUBGROUP algorithm across all four data sets, while the blue points show the error and subgroup fairness violation for the models achieved by the MARGINAL algorithm. Right column: The error and marginal fairness violation for the MARGINAL algorithm across all four data sets. Ordering of datasets is Communities and Crime, Law School, Adult, and Student.",
      "doc_a_visual_score": 6,
      "doc_b_query_id": "l1_1906.02589_1906.02589_fig_29_835",
      "doc_b_query": "Why does the solid blue TEXAS line maintain higher accuracy than dashed orange X-VAE across all Δ_CP values when predicting Heavy-Makeup?",
      "doc_b_answer": "FFVAE's structured latent code using multiple sensitive attributes (Chubby, Eyeglasses, Male) enables better disentanglement, aligning with subgroup fair machine learning goals more effectively than X-VAE's approach.",
      "doc_b_visual_anchor": "solid blue TEXAS line consistently above dashed orange X-VAE line from Δ_CP 0.00 to 0.40",
      "doc_b_text_evidence": "we discussed how disentangled representation learning aligns with the goals of subgroup fair machine learning, and presented a method for learning a structured latent code using multiple sensitive attributes",
      "doc_b_figure_id": "1906.02589_fig_29",
      "doc_b_figure_type": "plot",
      "doc_b_image_path": "data/mineru_output/1906.02589/1906.02589/hybrid_auto/images/1906.02589_page0_fig28.jpg",
      "doc_b_caption": "Figure 5. Celeb-A subgroup fair classification results. Sensitive attributes: Chubby (C), Eyeglasses (E), and Male (M). $y =$ Heavy-Makeup.",
      "doc_b_visual_score": 5,
      "score": 10.5
    },
    {
      "doc_a": "1810.03611",
      "doc_b": "1905.12843",
      "shared_entities": [
        "In Figure",
        "accuracy"
      ],
      "shared_entity_count": 2,
      "doc_a_query_id": "l1_1810.03611_1810.03611_fig_2_599",
      "doc_a_query": "Does the strong alignment of blue points along the red dashed diagonal support that approximation accuracy exceeds PPMI baseline?",
      "doc_a_answer": "The close alignment of data points along the diagonal trend line demonstrates high correlation between approximated and validated effect sizes, confirming the method's accuracy. This visual validation directly relates to the comparison with the PPMI baseline discussed in the subsequent section.",
      "doc_a_visual_anchor": "blue data points aligned with red dashed diagonal line",
      "doc_a_text_evidence": "We have shown that our method can be used to identify bias-influencing documents and accurately approximate the impact of their removal, but how does it compare to a more",
      "doc_a_figure_id": "1810.03611_fig_2",
      "doc_a_figure_type": "plot",
      "doc_a_image_path": "data/mineru_output/1810.03611/1810.03611/hybrid_auto/images/1810.03611_page0_fig1.jpg",
      "doc_a_caption": "Figure 2. Approximated and ground truth WEAT bias effect size due to the removal of various perturbation sets for our NYT corpus, considering WEAT1. Each point describes the mean effect size of one set; error bars depict one standard deviation; the baseline (unperturbed) mean is shown with a vertical dotted line.",
      "doc_a_visual_score": 4,
      "doc_b_query_id": "l1_1905.12843_1905.12843_fig_4_817",
      "doc_b_query": "Why does the solid black line in bottom plots perform better despite the claim of dominance on all datasets except adult?",
      "doc_b_answer": "The adult dataset is the exception where fair classification (solid black line) slightly outperforms the proposed method. On all other datasets, the proposed method dominates or matches baselines up to statistical uncertainty.",
      "doc_b_visual_anchor": "solid black line (fair class., oracle=LR, model=linear) in bottom plots",
      "doc_b_text_evidence": "Our method dominates or matches the baselines up to statistical uncertainty on all datasets except adult, where fair classification is slightly better.",
      "doc_b_figure_id": "1905.12843_fig_4",
      "doc_b_figure_type": "plot",
      "doc_b_image_path": "data/mineru_output/1905.12843/1905.12843/hybrid_auto/images/1905.12843_page0_fig3.jpg",
      "doc_b_caption": "Figure 1. Relative test loss versus the worst constraint violation with respect to SP. Relative losses are computed by subtracting the smallest baseline loss from the actual loss. For our algorithm and fair classification we plot the convex envelope of the predictors obtained on training data at various accuracy–fairness tradeoffs. We show $9 5 \\%$ confidence bands for the relative loss of our method and fair classification, and also show $9 5 \\%$ confidence intervals for constraint violation (the same for all methods). Our method dominates or matches the baselines up to statistical uncertainty on all datasets except adult, where fair classification is slightly better.",
      "doc_b_visual_score": 4,
      "score": 10.5
    },
    {
      "doc_a": "1610.07524",
      "doc_b": "1805.09458",
      "shared_entities": [
        "logistic regression"
      ],
      "shared_entity_count": 1,
      "doc_a_query_id": "l1_1610.07524_1610.07524_fig_2_170",
      "doc_a_query": "Why do differences persist between gray and orange bars within each prior category despite equal overall FPR claims?",
      "doc_a_answer": "Even if overall FPR is equal between Black and White defendants, differences in error rates can still appear within individual prior record score categories. This explains the persistent gap between gray and orange bars across all five groupings shown.",
      "doc_a_visual_anchor": "Consistent separation between gray and orange bars across all five x-axis categories (0, 1-3, 4-6, 7-10, >10)",
      "doc_a_text_evidence": "That is, even if $\\mathrm { F P R } _ { b } = \\mathrm { F P R } _ { w }$ , we may still see differences in error rates within prior record score categories (see e.g., Figure 2).",
      "doc_a_figure_id": "1610.07524_fig_2",
      "doc_a_figure_type": "plot",
      "doc_a_image_path": "data/mineru_output/1610.07524/1610.07524/hybrid_auto/images/1610.07524_page0_fig1.jpg",
      "doc_a_caption": "Figure 2: False positive rates across prior record count for defendants charged with a Misdemeanor offense. Plot is based on assessing a defendant as “high-risk” if their COMPAS decile score is $> 4$ . Error bars represent 95% confidence intervals.   ",
      "doc_a_visual_score": 3,
      "doc_b_query_id": "l1_1805.09458_1805.09458_fig_5_446",
      "doc_b_query": "How does using digit label as covariate enable the leftmost real 9 to generate all ten digits rightward?",
      "doc_b_answer": "Using digit label as covariate class c pushes all non-class stylistic information into latent space while removing information about the exact digit. This separation allows the decoder to recombine the extracted style with any specified digit label c′ to produce all ten digits.",
      "doc_b_visual_anchor": "Top row showing real digit 9 on left and generated sequence 0123456789 on right",
      "doc_b_text_evidence": "We use the digit label as the covariate class c, which pushes all non-class stylistic information into the latent space while attempting to remove information about the exact digit being written.",
      "doc_b_figure_id": "1805.09458_fig_5",
      "doc_b_figure_type": "example",
      "doc_b_image_path": "data/mineru_output/1805.09458/1805.09458/hybrid_auto/images/1805.09458_page0_fig4.jpg",
      "doc_b_caption": "Figure 3: We demonstrate the ability to generate stylistically similar images of varying classes using the MNIST dataset. The left column is mapped into $z$ that is invariant to its digit label c. We then can generate an image using $z$ and any other specified digit, $c ^ { \\prime }$ , as show on the right.",
      "doc_b_visual_score": 3,
      "score": 10.5
    },
    {
      "doc_a": "1610.02413",
      "doc_b": "2005.07293",
      "shared_entities": [
        "parity"
      ],
      "shared_entity_count": 1,
      "doc_a_query_id": "l1_1610.02413_1610.02413_fig_4_136",
      "doc_a_query": "How does the horizontal gap indicated by the lower-right arrow relate to optimizing the equal opportunity predictor?",
      "doc_a_answer": "The horizontal gap between the ROC curve and the profit-maximizing tangent line is proportional to the cost for achieving a given true positive rate, which becomes a convex function that can be efficiently optimized using ternary search.",
      "doc_a_visual_anchor": "Black arrow pointing to horizontal gap between blue curve and tangent line in lower right",
      "doc_a_text_evidence": "within each group the cost for a given true positive rate is proportional to the horizontal gap between the ROC curve and the profit-maximizing tangent line (i.e., the two curves on the left plot), so it is a convex function of the true positive rate",
      "doc_a_figure_id": "1610.02413_fig_4",
      "doc_a_figure_type": "plot",
      "doc_a_image_path": "data/mineru_output/1610.02413/1610.02413/hybrid_auto/images/1610.02413_page0_fig3.jpg",
      "doc_a_caption": "Figure 2: Finding the optimal equalized odds threshold predictor (middle), and equal opportunity threshold predictor (right). For the equal opportunity predictor, within each group the cost for a given true positive rate is proportional to the horizontal gap between the ROC curve and the profit-maximizing tangent line (i.e., the two curves on the left plot), so it is a convex function of the true positive rate (right). This lets us optimize it efficiently with ternary search.",
      "doc_a_visual_score": 7,
      "doc_b_query_id": "l1_2005.07293_2005.07293_fig_1_905",
      "doc_b_query": "How does the different-height box arrangement under the yellow figure in the right panel operationalize compensation for historically disadvantaged groups?",
      "doc_b_answer": "The taller box under the yellow figure represents resource distribution to overcome obstacles and raise opportunities for access, compensating historically disadvantaged groups so they can reach the apples like others.",
      "doc_b_visual_anchor": "Right panel 'Equity' showing yellow figure standing on tallest brown box",
      "doc_b_text_evidence": "Equity is the distribution of resources among groups to overcome obstacles and to raise their opportunities for access [Schement, 2001]. Thus, historically disadvantaged groups are compensated, and others get their fair share to reach their goals.",
      "doc_b_figure_id": "2005.07293_fig_1",
      "doc_b_figure_type": "diagram",
      "doc_b_image_path": "data/mineru_output/2005.07293/2005.07293/hybrid_auto/images/2005.07293_page0_fig0.jpg",
      "doc_b_caption": "Figure 1: Notion of equality in fairness is depicted and formalized along with our newly formalized notion of equity.",
      "doc_b_visual_score": 4,
      "score": 10.5
    },
    {
      "doc_a": "1701.08230",
      "doc_b": "2109.03952",
      "shared_entities": [
        "parity"
      ],
      "shared_entity_count": 1,
      "doc_a_query_id": "l1_1701.08230_1701.08230_fig_2_204",
      "doc_a_query": "Do the two distributions drawn from beta distributions with equal means in the right panel share the same peak location?",
      "doc_a_answer": "No, the blue curve peaks before 25% (near the dashed line) while the red curve peaks slightly after 25%, despite both distributions having equal means as stated.",
      "doc_a_visual_anchor": "Blue and red curves with vertical dashed line at 25% in top-right simulated panel",
      "doc_a_text_evidence": "simulated data drawn from two beta distributions with equal means (right)",
      "doc_a_figure_id": "1701.08230_fig_2",
      "doc_a_figure_type": "plot",
      "doc_a_image_path": "data/mineru_output/1701.08230/1701.08230/hybrid_auto/images/1701.08230_page0_fig1.jpg",
      "doc_a_caption": "Figure 1: Top: distribution of risk scores for Broward County data (le), and simulated data drawn from two beta distributions with equal means (right). Bottom: using a single threshold which detains $3 0 \\%$ of defendants in Broward County violates statistical parity (as measured by detention rate), predictive equality (false positive rate), and conditional statistical parity (detention rate conditional on number of prior arrests). We omit the last measure for the simulated data since that would require making additional assumptions about the relationship of priors and risk in the hypothetical populations.",
      "doc_a_visual_score": 7,
      "doc_b_query_id": "l1_2109.03952_2109.03952_fig_1_948",
      "doc_b_query": "How does the orange Attention layer produce its output representation before the yellow Dense Layers?",
      "doc_b_answer": "The attention layer computes weights and takes a weighted combination of the input feature embeddings to produce a d^e-dimensional vector representation for the sample instance.",
      "doc_b_visual_anchor": "orange Attention layer box positioned between red embedding boxes and yellow Dense Layer boxes",
      "doc_b_text_evidence": "These vectors are passed to the attention layer. The Computation of attention weights and the final representation for a sample is",
      "doc_b_figure_id": "2109.03952_fig_1",
      "doc_b_figure_type": "architecture",
      "doc_b_image_path": "data/mineru_output/2109.03952/2109.03952/hybrid_auto/images/2109.03952_page0_fig0.jpg",
      "doc_b_caption": "Figure 1: (a) In general classification model, for each feature $f _ { k }$ a vector representation $e _ { k }$ of length $d ^ { e }$ is learned. This is passed to the attention layer which produces a $d ^ { e }$ - dimensional vector representation for the sample instance $i$ which is passed to two dense layers to get the final classification output. (b) The Attribution framework has the same architecture as the general model. One outcome is obtained through the original model and another through the model that has some attention weights zeroed. The observed difference in accuracy and fairness measures will indicate the effect of the zeroed out features on accuracy and fairness.",
      "doc_b_visual_score": 4,
      "score": 10.5
    },
    {
      "doc_a": "1701.08230",
      "doc_b": "1907.06430",
      "shared_entities": [
        "COMPAS"
      ],
      "shared_entity_count": 1,
      "doc_a_query_id": "l1_1701.08230_1701.08230_fig_2_204",
      "doc_a_query": "Do the two distributions drawn from beta distributions with equal means in the right panel share the same peak location?",
      "doc_a_answer": "No, the blue curve peaks before 25% (near the dashed line) while the red curve peaks slightly after 25%, despite both distributions having equal means as stated.",
      "doc_a_visual_anchor": "Blue and red curves with vertical dashed line at 25% in top-right simulated panel",
      "doc_a_text_evidence": "simulated data drawn from two beta distributions with equal means (right)",
      "doc_a_figure_id": "1701.08230_fig_2",
      "doc_a_figure_type": "plot",
      "doc_a_image_path": "data/mineru_output/1701.08230/1701.08230/hybrid_auto/images/1701.08230_page0_fig1.jpg",
      "doc_a_caption": "Figure 1: Top: distribution of risk scores for Broward County data (le), and simulated data drawn from two beta distributions with equal means (right). Bottom: using a single threshold which detains $3 0 \\%$ of defendants in Broward County violates statistical parity (as measured by detention rate), predictive equality (false positive rate), and conditional statistical parity (detention rate conditional on number of prior arrests). We omit the last measure for the simulated data since that would require making additional assumptions about the relationship of priors and risk in the hypothetical populations.",
      "doc_a_visual_score": 7,
      "doc_b_query_id": "l1_1907.06430_1907.06430_fig_11_860",
      "doc_b_query": "Does the red solid arrow from A to Y represent the direct path whose unfairness is quantified by path-specific effect?",
      "doc_b_answer": "Yes, the red solid arrow from A to Y represents the direct path A → Y, whose unfairness is quantified by the path-specific effect PSE^aa when the path A → D → Y is considered fair.",
      "doc_b_visual_anchor": "red solid arrow from node A to node Y",
      "doc_b_text_evidence": "in which the path $A $ $D  Y$ is considered fair, unfairness can instead be quantified with the path-specific effect along the direct path $A  Y$ , PSE $^ { a a }$",
      "doc_b_figure_id": "1907.06430_fig_11",
      "doc_b_figure_type": "diagram",
      "doc_b_image_path": "data/mineru_output/1907.06430/1907.06430/hybrid_auto/images/1907.06430_page0_fig10.jpg",
      "doc_b_caption": "Fig. 7. CBN underlying a college admission scenario.",
      "doc_b_visual_score": 4,
      "score": 10.5
    },
    {
      "doc_a": "1306.5204",
      "doc_b": "1905.10674",
      "shared_entities": [
        "accuracy"
      ],
      "shared_entity_count": 1,
      "doc_a_query_id": "l1_1306.5204_1306.5204_fig_3_8",
      "doc_a_query": "How does the gap between red and blue lines relate to social media analysis reliability?",
      "doc_a_answer": "The persistent gap demonstrates that researchers' reliance on the Streaming API is problematic, as it consistently undersamples compared to the Firehose. This affects topic modeling, network analysis, and statistical content analysis accuracy.",
      "doc_a_visual_anchor": "Vertical gap between solid red Firehose line (higher) and dotted blue Streaming line (lower) throughout the time period",
      "doc_a_text_evidence": "It has been used to collect data for topic modeling (Hong and Davison 2010; Pozdnoukhov and Kaiser 2011), network analysis (Sofean and Smith 2012), and statistical analysis of content (Mathioudakis and Koudas 2010)",
      "doc_a_figure_id": "1306.5204_fig_3",
      "doc_a_figure_type": "plot",
      "doc_a_image_path": "data/mineru_output/1306.5204/1306.5204/hybrid_auto/images/1306.5204_page0_fig2.jpg",
      "doc_a_caption": "Figure 2: Raw tweet counts for each day from both the Streaming API and the Firehose.",
      "doc_a_visual_score": 6,
      "doc_b_query_id": "l1_1905.10674_1905.10674_fig_1_799",
      "doc_b_query": "Does the input graph's blue and red node structure support embedding generation that operates on social networks?",
      "doc_b_answer": "Yes, the bipartite-style input graph with blue and red nodes represents the type of graph data structure used in social networks and recommender systems that the embedding technique is designed to handle.",
      "doc_b_visual_anchor": "Input Graph box showing blue and red nodes connected in a network structure on the left side",
      "doc_b_text_evidence": "Learning high-quality node embeddings is an important building block for machine learning models that operate on graph data, such as social networks and recommender systems.",
      "doc_b_figure_id": "1905.10674_fig_1",
      "doc_b_figure_type": "diagram",
      "doc_b_image_path": "data/mineru_output/1905.10674/1905.10674/hybrid_auto/images/1905.10674_page0_fig0.jpg",
      "doc_b_caption": "Figure 1. Overview of our approach: Our goal is to generate graph embeddings that are invariant to particular sensitive attributes (e.g., age or gender). We train a set of “filters” to prevent adversarial discriminators from classifying the sensitive information from the filtered embeddings. After training, these filters can be composed together in different combinations, allowing the flexible generation of embeddings that are invariant w.r.t. any subset of the sensitive attributes.",
      "doc_b_visual_score": 4,
      "score": 10.0
    },
    {
      "doc_a": "1306.5204",
      "doc_b": "2005.07293",
      "shared_entities": [
        "accuracy"
      ],
      "shared_entity_count": 1,
      "doc_a_query_id": "l1_1306.5204_1306.5204_fig_3_8",
      "doc_a_query": "How does the gap between red and blue lines relate to social media analysis reliability?",
      "doc_a_answer": "The persistent gap demonstrates that researchers' reliance on the Streaming API is problematic, as it consistently undersamples compared to the Firehose. This affects topic modeling, network analysis, and statistical content analysis accuracy.",
      "doc_a_visual_anchor": "Vertical gap between solid red Firehose line (higher) and dotted blue Streaming line (lower) throughout the time period",
      "doc_a_text_evidence": "It has been used to collect data for topic modeling (Hong and Davison 2010; Pozdnoukhov and Kaiser 2011), network analysis (Sofean and Smith 2012), and statistical analysis of content (Mathioudakis and Koudas 2010)",
      "doc_a_figure_id": "1306.5204_fig_3",
      "doc_a_figure_type": "plot",
      "doc_a_image_path": "data/mineru_output/1306.5204/1306.5204/hybrid_auto/images/1306.5204_page0_fig2.jpg",
      "doc_a_caption": "Figure 2: Raw tweet counts for each day from both the Streaming API and the Firehose.",
      "doc_a_visual_score": 6,
      "doc_b_query_id": "l1_2005.07293_2005.07293_fig_1_905",
      "doc_b_query": "How does the different-height box arrangement under the yellow figure in the right panel operationalize compensation for historically disadvantaged groups?",
      "doc_b_answer": "The taller box under the yellow figure represents resource distribution to overcome obstacles and raise opportunities for access, compensating historically disadvantaged groups so they can reach the apples like others.",
      "doc_b_visual_anchor": "Right panel 'Equity' showing yellow figure standing on tallest brown box",
      "doc_b_text_evidence": "Equity is the distribution of resources among groups to overcome obstacles and to raise their opportunities for access [Schement, 2001]. Thus, historically disadvantaged groups are compensated, and others get their fair share to reach their goals.",
      "doc_b_figure_id": "2005.07293_fig_1",
      "doc_b_figure_type": "diagram",
      "doc_b_image_path": "data/mineru_output/2005.07293/2005.07293/hybrid_auto/images/2005.07293_page0_fig0.jpg",
      "doc_b_caption": "Figure 1: Notion of equality in fairness is depicted and formalized along with our newly formalized notion of equity.",
      "doc_b_visual_score": 4,
      "score": 10.0
    },
    {
      "doc_a": "1306.5204",
      "doc_b": "2109.03952",
      "shared_entities": [
        "accuracy"
      ],
      "shared_entity_count": 1,
      "doc_a_query_id": "l1_1306.5204_1306.5204_fig_3_8",
      "doc_a_query": "How does the gap between red and blue lines relate to social media analysis reliability?",
      "doc_a_answer": "The persistent gap demonstrates that researchers' reliance on the Streaming API is problematic, as it consistently undersamples compared to the Firehose. This affects topic modeling, network analysis, and statistical content analysis accuracy.",
      "doc_a_visual_anchor": "Vertical gap between solid red Firehose line (higher) and dotted blue Streaming line (lower) throughout the time period",
      "doc_a_text_evidence": "It has been used to collect data for topic modeling (Hong and Davison 2010; Pozdnoukhov and Kaiser 2011), network analysis (Sofean and Smith 2012), and statistical analysis of content (Mathioudakis and Koudas 2010)",
      "doc_a_figure_id": "1306.5204_fig_3",
      "doc_a_figure_type": "plot",
      "doc_a_image_path": "data/mineru_output/1306.5204/1306.5204/hybrid_auto/images/1306.5204_page0_fig2.jpg",
      "doc_a_caption": "Figure 2: Raw tweet counts for each day from both the Streaming API and the Firehose.",
      "doc_a_visual_score": 6,
      "doc_b_query_id": "l1_2109.03952_2109.03952_fig_1_948",
      "doc_b_query": "How does the orange Attention layer produce its output representation before the yellow Dense Layers?",
      "doc_b_answer": "The attention layer computes weights and takes a weighted combination of the input feature embeddings to produce a d^e-dimensional vector representation for the sample instance.",
      "doc_b_visual_anchor": "orange Attention layer box positioned between red embedding boxes and yellow Dense Layer boxes",
      "doc_b_text_evidence": "These vectors are passed to the attention layer. The Computation of attention weights and the final representation for a sample is",
      "doc_b_figure_id": "2109.03952_fig_1",
      "doc_b_figure_type": "architecture",
      "doc_b_image_path": "data/mineru_output/2109.03952/2109.03952/hybrid_auto/images/2109.03952_page0_fig0.jpg",
      "doc_b_caption": "Figure 1: (a) In general classification model, for each feature $f _ { k }$ a vector representation $e _ { k }$ of length $d ^ { e }$ is learned. This is passed to the attention layer which produces a $d ^ { e }$ - dimensional vector representation for the sample instance $i$ which is passed to two dense layers to get the final classification output. (b) The Attribution framework has the same architecture as the general model. One outcome is obtained through the original model and another through the model that has some attention weights zeroed. The observed difference in accuracy and fairness measures will indicate the effect of the zeroed out features on accuracy and fairness.",
      "doc_b_visual_score": 4,
      "score": 10.0
    },
    {
      "doc_a": "1409.0575",
      "doc_b": "1711.05144",
      "shared_entities": [
        "accuracy"
      ],
      "shared_entity_count": 1,
      "doc_a_query_id": "l1_1409.0575_1409.0575_fig_1_40",
      "doc_a_query": "How does the Object Scale row spanning from Candle to Spider Web relate to the 1000 object classes annotation goal?",
      "doc_a_answer": "The Object Scale row demonstrates the scale variability challenge that algorithms must handle across the 1000 object classes, showing that the dataset requires systems to recognize objects ranging from small items like candles to large structures like spider webs.",
      "doc_a_visual_anchor": "Top row labeled 'Object Scale' with Candle on left and Spider Web on right",
      "doc_a_text_evidence": "The image classification task tests the ability of an algorithm to name the objects present in the image, without necessarily localizing them.",
      "doc_a_figure_id": "1409.0575_fig_1",
      "doc_a_figure_type": "diagram",
      "doc_a_image_path": "data/mineru_output/1409.0575/1409.0575/hybrid_auto/images/1409.0575_page0_fig0.jpg",
      "doc_a_caption": "Figure 2. The synsets have remained consistent since year 2012. Appendix A provides the complete list of object categories used in ILSVRC2012-2014.",
      "doc_a_visual_score": 3,
      "doc_b_query_id": "l1_1711.05144_1711.05144_fig_3_299",
      "doc_b_query": "Why do the color-coded trajectories form distinct curves rather than a single frontier, given the reductions approach?",
      "doc_b_answer": "Each colored trajectory represents Pareto-optimal solutions for a specific gamma parameter value. The reductions approach by Agarwal et al. generates different error-unfairness trade-offs as gamma varies.",
      "doc_b_visual_anchor": "Multiple distinct colored curves (blue, red, green, purple, orange) descending from upper-left",
      "doc_b_text_evidence": "Pareto-optimal error-unfairness values, color coded by varying values of the input parameter γ",
      "doc_b_figure_id": "1711.05144_fig_3",
      "doc_b_figure_type": "plot",
      "doc_b_image_path": "data/mineru_output/1711.05144/1711.05144/hybrid_auto/images/1711.05144_page0_fig2.jpg",
      "doc_b_caption": "Figure 2: (a) Pareto-optimal error-unfairness values, color coded by varying values of the input parameter γ. (b) Aggregate Pareto frontier across all values of γ. Here the γ values cover the same range but are sampled more densely to get a smoother frontier. See text for details.",
      "doc_b_visual_score": 7,
      "score": 10.0
    },
    {
      "doc_a": "1412.3756",
      "doc_b": "1906.02589",
      "shared_entities": [
        "Fairness",
        "accuracy"
      ],
      "shared_entity_count": 2,
      "doc_a_query_id": "l1_1412.3756_1412.3756_fig_3_55",
      "doc_a_query": "Does the utility decay pattern visible across all scatter points support the expected tradeoff between fairness and utility?",
      "doc_a_answer": "Yes, the scatter points show declining utility (from ~1.0 to ~0.5-0.9) as disparate impact increases toward the green line at DI=1.0, demonstrating the expected decay over utility as fairness increases.",
      "doc_a_visual_anchor": "Dashed trend lines connecting scatter points showing downward slope from left to right",
      "doc_a_text_evidence": "The results, shown in Figure 3, demonstrate the expected decay over utility as fairness increases.",
      "doc_a_figure_id": "1412.3756_fig_3",
      "doc_a_figure_type": "plot",
      "doc_a_image_path": "data/mineru_output/1412.3756/1412.3756/hybrid_auto/images/1412.3756_page0_fig2.jpg",
      "doc_a_caption": "Fig. 3: Disparate impact (DI) vs. utility (1-BER) from our combinatorial and geometric partial repair processes using the SVM to classify on the Adult Income and German Credit data sets and the simple threshold classifier on the Ricci data set. Recall that only points with $\\mathsf { D } \\mathsf { I } \\ge \\tau = 0 . 8$ are legal. $\\mathsf { D } \\mathsf { I } = 1 . 0$ represents full fairness.",
      "doc_a_visual_score": 5,
      "doc_b_query_id": "l1_1906.02589_1906.02589_fig_29_835",
      "doc_b_query": "Why does the solid blue TEXAS line maintain higher accuracy than dashed orange X-VAE across all Δ_CP values when predicting Heavy-Makeup?",
      "doc_b_answer": "FFVAE's structured latent code using multiple sensitive attributes (Chubby, Eyeglasses, Male) enables better disentanglement, aligning with subgroup fair machine learning goals more effectively than X-VAE's approach.",
      "doc_b_visual_anchor": "solid blue TEXAS line consistently above dashed orange X-VAE line from Δ_CP 0.00 to 0.40",
      "doc_b_text_evidence": "we discussed how disentangled representation learning aligns with the goals of subgroup fair machine learning, and presented a method for learning a structured latent code using multiple sensitive attributes",
      "doc_b_figure_id": "1906.02589_fig_29",
      "doc_b_figure_type": "plot",
      "doc_b_image_path": "data/mineru_output/1906.02589/1906.02589/hybrid_auto/images/1906.02589_page0_fig28.jpg",
      "doc_b_caption": "Figure 5. Celeb-A subgroup fair classification results. Sensitive attributes: Chubby (C), Eyeglasses (E), and Male (M). $y =$ Heavy-Makeup.",
      "doc_b_visual_score": 5,
      "score": 10.0
    },
    {
      "doc_a": "1511.00830",
      "doc_b": "1906.02589",
      "shared_entities": [
        "accuracy"
      ],
      "shared_entity_count": 1,
      "doc_a_query_id": "l1_1511.00830_1511.00830_fig_1_64",
      "doc_a_query": "Why does node S point directly to X rather than through Z in this unsupervised representation learning model?",
      "doc_a_answer": "In unsupervised representation learning, S represents noise or nuisance variables that directly affect the observed data X, independent of the informative latent factors Z. This direct connection models uninformative dimensions that are typically detrimental for the task under consideration.",
      "doc_a_visual_anchor": "arrow from node S (top-left) directly to node X (bottom-center)",
      "doc_a_text_evidence": "Uninformative dimensions are often called \"noise\" or \"nuisance variables\" while informative dimensions are usually called latent or hidden factors of variation.",
      "doc_a_figure_id": "1511.00830_fig_1",
      "doc_a_figure_type": "diagram",
      "doc_a_image_path": "data/mineru_output/1511.00830/1511.00830/hybrid_auto/images/1511.00830_page0_fig0.jpg",
      "doc_a_caption": "Figure 1: Unsupervised model",
      "doc_a_visual_score": 5,
      "doc_b_query_id": "l1_1906.02589_1906.02589_fig_29_835",
      "doc_b_query": "Why does the solid blue TEXAS line maintain higher accuracy than dashed orange X-VAE across all Δ_CP values when predicting Heavy-Makeup?",
      "doc_b_answer": "FFVAE's structured latent code using multiple sensitive attributes (Chubby, Eyeglasses, Male) enables better disentanglement, aligning with subgroup fair machine learning goals more effectively than X-VAE's approach.",
      "doc_b_visual_anchor": "solid blue TEXAS line consistently above dashed orange X-VAE line from Δ_CP 0.00 to 0.40",
      "doc_b_text_evidence": "we discussed how disentangled representation learning aligns with the goals of subgroup fair machine learning, and presented a method for learning a structured latent code using multiple sensitive attributes",
      "doc_b_figure_id": "1906.02589_fig_29",
      "doc_b_figure_type": "plot",
      "doc_b_image_path": "data/mineru_output/1906.02589/1906.02589/hybrid_auto/images/1906.02589_page0_fig28.jpg",
      "doc_b_caption": "Figure 5. Celeb-A subgroup fair classification results. Sensitive attributes: Chubby (C), Eyeglasses (E), and Male (M). $y =$ Heavy-Makeup.",
      "doc_b_visual_score": 5,
      "score": 10.0
    },
    {
      "doc_a": "1610.08452",
      "doc_b": "1905.10674",
      "shared_entities": [
        "accuracy"
      ],
      "shared_entity_count": 1,
      "doc_a_query_id": "l1_1610.08452_1610.08452_fig_9_182",
      "doc_a_query": "What causes the dashed boundary to separate more red circles from green ones compared to the solid boundary?",
      "doc_a_answer": "The fair constrained classifier shifts the decision boundary to equalize false positive and negative rates across groups, which the research compares against unconstrained classifiers to demonstrate performance trade-offs when avoiding disparate mistreatment.",
      "doc_a_visual_anchor": "dashed blue line positioned differently from solid cyan line relative to red and green markers",
      "doc_a_text_evidence": "Figure 4 summarizes the results by showing the decision boundaries for both the unconstrained classifiers (solid) and the fair constrained classifier (dashed) when controlling for disparate mistreatment",
      "doc_a_figure_id": "1610.08452_fig_9",
      "doc_a_figure_type": "plot",
      "doc_a_image_path": "data/mineru_output/1610.08452/1610.08452/hybrid_auto/images/1610.08452_page0_fig8.jpg",
      "doc_a_caption": "Figure 4: [Synthetic data] $D _ { F P R }$ and $D _ { F N R }$ have the same sign. Removing disparate mistreatment on FPR can potentially increase disparate mistreatment on FNR. Removing disparate mistreatment on both at the same time causes a larger drop in accuracy.",
      "doc_a_visual_score": 6,
      "doc_b_query_id": "l1_1905.10674_1905.10674_fig_1_799",
      "doc_b_query": "Does the input graph's blue and red node structure support embedding generation that operates on social networks?",
      "doc_b_answer": "Yes, the bipartite-style input graph with blue and red nodes represents the type of graph data structure used in social networks and recommender systems that the embedding technique is designed to handle.",
      "doc_b_visual_anchor": "Input Graph box showing blue and red nodes connected in a network structure on the left side",
      "doc_b_text_evidence": "Learning high-quality node embeddings is an important building block for machine learning models that operate on graph data, such as social networks and recommender systems.",
      "doc_b_figure_id": "1905.10674_fig_1",
      "doc_b_figure_type": "diagram",
      "doc_b_image_path": "data/mineru_output/1905.10674/1905.10674/hybrid_auto/images/1905.10674_page0_fig0.jpg",
      "doc_b_caption": "Figure 1. Overview of our approach: Our goal is to generate graph embeddings that are invariant to particular sensitive attributes (e.g., age or gender). We train a set of “filters” to prevent adversarial discriminators from classifying the sensitive information from the filtered embeddings. After training, these filters can be composed together in different combinations, allowing the flexible generation of embeddings that are invariant w.r.t. any subset of the sensitive attributes.",
      "doc_b_visual_score": 4,
      "score": 10.0
    },
    {
      "doc_a": "1610.08452",
      "doc_b": "2005.07293",
      "shared_entities": [
        "accuracy"
      ],
      "shared_entity_count": 1,
      "doc_a_query_id": "l1_1610.08452_1610.08452_fig_9_182",
      "doc_a_query": "What causes the dashed boundary to separate more red circles from green ones compared to the solid boundary?",
      "doc_a_answer": "The fair constrained classifier shifts the decision boundary to equalize false positive and negative rates across groups, which the research compares against unconstrained classifiers to demonstrate performance trade-offs when avoiding disparate mistreatment.",
      "doc_a_visual_anchor": "dashed blue line positioned differently from solid cyan line relative to red and green markers",
      "doc_a_text_evidence": "Figure 4 summarizes the results by showing the decision boundaries for both the unconstrained classifiers (solid) and the fair constrained classifier (dashed) when controlling for disparate mistreatment",
      "doc_a_figure_id": "1610.08452_fig_9",
      "doc_a_figure_type": "plot",
      "doc_a_image_path": "data/mineru_output/1610.08452/1610.08452/hybrid_auto/images/1610.08452_page0_fig8.jpg",
      "doc_a_caption": "Figure 4: [Synthetic data] $D _ { F P R }$ and $D _ { F N R }$ have the same sign. Removing disparate mistreatment on FPR can potentially increase disparate mistreatment on FNR. Removing disparate mistreatment on both at the same time causes a larger drop in accuracy.",
      "doc_a_visual_score": 6,
      "doc_b_query_id": "l1_2005.07293_2005.07293_fig_1_905",
      "doc_b_query": "How does the different-height box arrangement under the yellow figure in the right panel operationalize compensation for historically disadvantaged groups?",
      "doc_b_answer": "The taller box under the yellow figure represents resource distribution to overcome obstacles and raise opportunities for access, compensating historically disadvantaged groups so they can reach the apples like others.",
      "doc_b_visual_anchor": "Right panel 'Equity' showing yellow figure standing on tallest brown box",
      "doc_b_text_evidence": "Equity is the distribution of resources among groups to overcome obstacles and to raise their opportunities for access [Schement, 2001]. Thus, historically disadvantaged groups are compensated, and others get their fair share to reach their goals.",
      "doc_b_figure_id": "2005.07293_fig_1",
      "doc_b_figure_type": "diagram",
      "doc_b_image_path": "data/mineru_output/2005.07293/2005.07293/hybrid_auto/images/2005.07293_page0_fig0.jpg",
      "doc_b_caption": "Figure 1: Notion of equality in fairness is depicted and formalized along with our newly formalized notion of equity.",
      "doc_b_visual_score": 4,
      "score": 10.0
    },
    {
      "doc_a": "1610.08452",
      "doc_b": "2109.03952",
      "shared_entities": [
        "accuracy"
      ],
      "shared_entity_count": 1,
      "doc_a_query_id": "l1_1610.08452_1610.08452_fig_9_182",
      "doc_a_query": "What causes the dashed boundary to separate more red circles from green ones compared to the solid boundary?",
      "doc_a_answer": "The fair constrained classifier shifts the decision boundary to equalize false positive and negative rates across groups, which the research compares against unconstrained classifiers to demonstrate performance trade-offs when avoiding disparate mistreatment.",
      "doc_a_visual_anchor": "dashed blue line positioned differently from solid cyan line relative to red and green markers",
      "doc_a_text_evidence": "Figure 4 summarizes the results by showing the decision boundaries for both the unconstrained classifiers (solid) and the fair constrained classifier (dashed) when controlling for disparate mistreatment",
      "doc_a_figure_id": "1610.08452_fig_9",
      "doc_a_figure_type": "plot",
      "doc_a_image_path": "data/mineru_output/1610.08452/1610.08452/hybrid_auto/images/1610.08452_page0_fig8.jpg",
      "doc_a_caption": "Figure 4: [Synthetic data] $D _ { F P R }$ and $D _ { F N R }$ have the same sign. Removing disparate mistreatment on FPR can potentially increase disparate mistreatment on FNR. Removing disparate mistreatment on both at the same time causes a larger drop in accuracy.",
      "doc_a_visual_score": 6,
      "doc_b_query_id": "l1_2109.03952_2109.03952_fig_1_948",
      "doc_b_query": "How does the orange Attention layer produce its output representation before the yellow Dense Layers?",
      "doc_b_answer": "The attention layer computes weights and takes a weighted combination of the input feature embeddings to produce a d^e-dimensional vector representation for the sample instance.",
      "doc_b_visual_anchor": "orange Attention layer box positioned between red embedding boxes and yellow Dense Layer boxes",
      "doc_b_text_evidence": "These vectors are passed to the attention layer. The Computation of attention weights and the final representation for a sample is",
      "doc_b_figure_id": "2109.03952_fig_1",
      "doc_b_figure_type": "architecture",
      "doc_b_image_path": "data/mineru_output/2109.03952/2109.03952/hybrid_auto/images/2109.03952_page0_fig0.jpg",
      "doc_b_caption": "Figure 1: (a) In general classification model, for each feature $f _ { k }$ a vector representation $e _ { k }$ of length $d ^ { e }$ is learned. This is passed to the attention layer which produces a $d ^ { e }$ - dimensional vector representation for the sample instance $i$ which is passed to two dense layers to get the final classification output. (b) The Attribution framework has the same architecture as the general model. One outcome is obtained through the original model and another through the model that has some attention weights zeroed. The observed difference in accuracy and fairness measures will indicate the effect of the zeroed out features on accuracy and fairness.",
      "doc_b_visual_score": 4,
      "score": 10.0
    },
    {
      "doc_a": "1706.02409",
      "doc_b": "1905.12843",
      "shared_entities": [
        "accuracy",
        "logistic regression"
      ],
      "shared_entity_count": 2,
      "doc_a_query_id": "l1_1706.02409_1706.02409_fig_10_235",
      "doc_a_query": "What explains the progressive height increase in magenta and yellow hybrid bars as alpha decreases from 0.5 to 0.01?",
      "doc_a_answer": "As alpha decreases (stronger regularization), the hybrid fairness regularizers show increasing Price of Fairness, indicating that stricter hybrid fairness constraints progressively sacrifice more model performance to achieve fairness goals.",
      "doc_a_visual_anchor": "magenta (Hybrid, separate) and yellow (Hybrid, single) bars increasing from left to right",
      "doc_a_text_evidence": "In Proceedings of the 16th International Conference on Data Mining, pages 1–10, 2016. [2] Julia Angwin, Jeff Larson, Surya Mattu, and Lauren Kirchner. Machine bias. Propublica, 2016.",
      "doc_a_figure_id": "1706.02409_fig_10",
      "doc_a_figure_type": "plot",
      "doc_a_image_path": "data/mineru_output/1706.02409/1706.02409/hybrid_auto/images/1706.02409_page0_fig9.jpg",
      "doc_a_caption": "Figure 2: The “Price of Fairness” across data sets, for each type of fairness regularizer, in both the single and separate model case.",
      "doc_a_visual_score": 3,
      "doc_b_query_id": "l1_1905.12843_1905.12843_fig_4_817",
      "doc_b_query": "Why does the solid black line in bottom plots perform better despite the claim of dominance on all datasets except adult?",
      "doc_b_answer": "The adult dataset is the exception where fair classification (solid black line) slightly outperforms the proposed method. On all other datasets, the proposed method dominates or matches baselines up to statistical uncertainty.",
      "doc_b_visual_anchor": "solid black line (fair class., oracle=LR, model=linear) in bottom plots",
      "doc_b_text_evidence": "Our method dominates or matches the baselines up to statistical uncertainty on all datasets except adult, where fair classification is slightly better.",
      "doc_b_figure_id": "1905.12843_fig_4",
      "doc_b_figure_type": "plot",
      "doc_b_image_path": "data/mineru_output/1905.12843/1905.12843/hybrid_auto/images/1905.12843_page0_fig3.jpg",
      "doc_b_caption": "Figure 1. Relative test loss versus the worst constraint violation with respect to SP. Relative losses are computed by subtracting the smallest baseline loss from the actual loss. For our algorithm and fair classification we plot the convex envelope of the predictors obtained on training data at various accuracy–fairness tradeoffs. We show $9 5 \\%$ confidence bands for the relative loss of our method and fair classification, and also show $9 5 \\%$ confidence intervals for constraint violation (the same for all methods). Our method dominates or matches the baselines up to statistical uncertainty on all datasets except adult, where fair classification is slightly better.",
      "doc_b_visual_score": 4,
      "score": 10.0
    },
    {
      "doc_a": "1711.05144",
      "doc_b": "1805.09458",
      "shared_entities": [
        "accuracy"
      ],
      "shared_entity_count": 1,
      "doc_a_query_id": "l1_1711.05144_1711.05144_fig_3_299",
      "doc_a_query": "Why do the color-coded trajectories form distinct curves rather than a single frontier, given the reductions approach?",
      "doc_a_answer": "Each colored trajectory represents Pareto-optimal solutions for a specific gamma parameter value. The reductions approach by Agarwal et al. generates different error-unfairness trade-offs as gamma varies.",
      "doc_a_visual_anchor": "Multiple distinct colored curves (blue, red, green, purple, orange) descending from upper-left",
      "doc_a_text_evidence": "Pareto-optimal error-unfairness values, color coded by varying values of the input parameter γ",
      "doc_a_figure_id": "1711.05144_fig_3",
      "doc_a_figure_type": "plot",
      "doc_a_image_path": "data/mineru_output/1711.05144/1711.05144/hybrid_auto/images/1711.05144_page0_fig2.jpg",
      "doc_a_caption": "Figure 2: (a) Pareto-optimal error-unfairness values, color coded by varying values of the input parameter γ. (b) Aggregate Pareto frontier across all values of γ. Here the γ values cover the same range but are sampled more densely to get a smoother frontier. See text for details.",
      "doc_a_visual_score": 7,
      "doc_b_query_id": "l1_1805.09458_1805.09458_fig_5_446",
      "doc_b_query": "How does using digit label as covariate enable the leftmost real 9 to generate all ten digits rightward?",
      "doc_b_answer": "Using digit label as covariate class c pushes all non-class stylistic information into latent space while removing information about the exact digit. This separation allows the decoder to recombine the extracted style with any specified digit label c′ to produce all ten digits.",
      "doc_b_visual_anchor": "Top row showing real digit 9 on left and generated sequence 0123456789 on right",
      "doc_b_text_evidence": "We use the digit label as the covariate class c, which pushes all non-class stylistic information into the latent space while attempting to remove information about the exact digit being written.",
      "doc_b_figure_id": "1805.09458_fig_5",
      "doc_b_figure_type": "example",
      "doc_b_image_path": "data/mineru_output/1805.09458/1805.09458/hybrid_auto/images/1805.09458_page0_fig4.jpg",
      "doc_b_caption": "Figure 3: We demonstrate the ability to generate stylistically similar images of varying classes using the MNIST dataset. The left column is mapped into $z$ that is invariant to its digit label c. We then can generate an image using $z$ and any other specified digit, $c ^ { \\prime }$ , as show on the right.",
      "doc_b_visual_score": 3,
      "score": 10.0
    },
    {
      "doc_a": "1808.08166",
      "doc_b": "1905.10674",
      "shared_entities": [
        "accuracy"
      ],
      "shared_entity_count": 1,
      "doc_a_query_id": "l1_1808.08166_1808.08166_fig_11_472",
      "doc_a_query": "Does the blue points' downward trend from y=0.25 to y=0 demonstrate the MARGINAL algorithm's error-fairness pairs?",
      "doc_a_answer": "Yes, the blue points show the error and marginal fairness violation pairs achieved by the MARGINAL algorithm, displaying a clear trade-off as error increases from left to right.",
      "doc_a_visual_anchor": "blue points forming downward curve from upper-left (error ~0.16, fairness ~0.25) to lower-right (error ~0.23, fairness ~0)",
      "doc_a_text_evidence": "The error and marginal fairness violation for the MARGINAL algorithm across all four data sets.",
      "doc_a_figure_id": "1808.08166_fig_11",
      "doc_a_figure_type": "plot",
      "doc_a_image_path": "data/mineru_output/1808.08166/1808.08166/hybrid_auto/images/1808.08166_page0_fig10.jpg",
      "doc_a_caption": "Figure 3: Left column: The red points show the Pareto frontier of error (x axis) and subgroup fairness violation (y axis) for the SUBGROUP algorithm across all four data sets, while the blue points show the error and subgroup fairness violation for the models achieved by the MARGINAL algorithm. Right column: The error and marginal fairness violation for the MARGINAL algorithm across all four data sets. Ordering of datasets is Communities and Crime, Law School, Adult, and Student.",
      "doc_a_visual_score": 6,
      "doc_b_query_id": "l1_1905.10674_1905.10674_fig_1_799",
      "doc_b_query": "Does the input graph's blue and red node structure support embedding generation that operates on social networks?",
      "doc_b_answer": "Yes, the bipartite-style input graph with blue and red nodes represents the type of graph data structure used in social networks and recommender systems that the embedding technique is designed to handle.",
      "doc_b_visual_anchor": "Input Graph box showing blue and red nodes connected in a network structure on the left side",
      "doc_b_text_evidence": "Learning high-quality node embeddings is an important building block for machine learning models that operate on graph data, such as social networks and recommender systems.",
      "doc_b_figure_id": "1905.10674_fig_1",
      "doc_b_figure_type": "diagram",
      "doc_b_image_path": "data/mineru_output/1905.10674/1905.10674/hybrid_auto/images/1905.10674_page0_fig0.jpg",
      "doc_b_caption": "Figure 1. Overview of our approach: Our goal is to generate graph embeddings that are invariant to particular sensitive attributes (e.g., age or gender). We train a set of “filters” to prevent adversarial discriminators from classifying the sensitive information from the filtered embeddings. After training, these filters can be composed together in different combinations, allowing the flexible generation of embeddings that are invariant w.r.t. any subset of the sensitive attributes.",
      "doc_b_visual_score": 4,
      "score": 10.0
    },
    {
      "doc_a": "1808.08166",
      "doc_b": "2005.07293",
      "shared_entities": [
        "accuracy"
      ],
      "shared_entity_count": 1,
      "doc_a_query_id": "l1_1808.08166_1808.08166_fig_11_472",
      "doc_a_query": "Does the blue points' downward trend from y=0.25 to y=0 demonstrate the MARGINAL algorithm's error-fairness pairs?",
      "doc_a_answer": "Yes, the blue points show the error and marginal fairness violation pairs achieved by the MARGINAL algorithm, displaying a clear trade-off as error increases from left to right.",
      "doc_a_visual_anchor": "blue points forming downward curve from upper-left (error ~0.16, fairness ~0.25) to lower-right (error ~0.23, fairness ~0)",
      "doc_a_text_evidence": "The error and marginal fairness violation for the MARGINAL algorithm across all four data sets.",
      "doc_a_figure_id": "1808.08166_fig_11",
      "doc_a_figure_type": "plot",
      "doc_a_image_path": "data/mineru_output/1808.08166/1808.08166/hybrid_auto/images/1808.08166_page0_fig10.jpg",
      "doc_a_caption": "Figure 3: Left column: The red points show the Pareto frontier of error (x axis) and subgroup fairness violation (y axis) for the SUBGROUP algorithm across all four data sets, while the blue points show the error and subgroup fairness violation for the models achieved by the MARGINAL algorithm. Right column: The error and marginal fairness violation for the MARGINAL algorithm across all four data sets. Ordering of datasets is Communities and Crime, Law School, Adult, and Student.",
      "doc_a_visual_score": 6,
      "doc_b_query_id": "l1_2005.07293_2005.07293_fig_1_905",
      "doc_b_query": "How does the different-height box arrangement under the yellow figure in the right panel operationalize compensation for historically disadvantaged groups?",
      "doc_b_answer": "The taller box under the yellow figure represents resource distribution to overcome obstacles and raise opportunities for access, compensating historically disadvantaged groups so they can reach the apples like others.",
      "doc_b_visual_anchor": "Right panel 'Equity' showing yellow figure standing on tallest brown box",
      "doc_b_text_evidence": "Equity is the distribution of resources among groups to overcome obstacles and to raise their opportunities for access [Schement, 2001]. Thus, historically disadvantaged groups are compensated, and others get their fair share to reach their goals.",
      "doc_b_figure_id": "2005.07293_fig_1",
      "doc_b_figure_type": "diagram",
      "doc_b_image_path": "data/mineru_output/2005.07293/2005.07293/hybrid_auto/images/2005.07293_page0_fig0.jpg",
      "doc_b_caption": "Figure 1: Notion of equality in fairness is depicted and formalized along with our newly formalized notion of equity.",
      "doc_b_visual_score": 4,
      "score": 10.0
    },
    {
      "doc_a": "1808.08166",
      "doc_b": "2109.03952",
      "shared_entities": [
        "accuracy"
      ],
      "shared_entity_count": 1,
      "doc_a_query_id": "l1_1808.08166_1808.08166_fig_11_472",
      "doc_a_query": "Does the blue points' downward trend from y=0.25 to y=0 demonstrate the MARGINAL algorithm's error-fairness pairs?",
      "doc_a_answer": "Yes, the blue points show the error and marginal fairness violation pairs achieved by the MARGINAL algorithm, displaying a clear trade-off as error increases from left to right.",
      "doc_a_visual_anchor": "blue points forming downward curve from upper-left (error ~0.16, fairness ~0.25) to lower-right (error ~0.23, fairness ~0)",
      "doc_a_text_evidence": "The error and marginal fairness violation for the MARGINAL algorithm across all four data sets.",
      "doc_a_figure_id": "1808.08166_fig_11",
      "doc_a_figure_type": "plot",
      "doc_a_image_path": "data/mineru_output/1808.08166/1808.08166/hybrid_auto/images/1808.08166_page0_fig10.jpg",
      "doc_a_caption": "Figure 3: Left column: The red points show the Pareto frontier of error (x axis) and subgroup fairness violation (y axis) for the SUBGROUP algorithm across all four data sets, while the blue points show the error and subgroup fairness violation for the models achieved by the MARGINAL algorithm. Right column: The error and marginal fairness violation for the MARGINAL algorithm across all four data sets. Ordering of datasets is Communities and Crime, Law School, Adult, and Student.",
      "doc_a_visual_score": 6,
      "doc_b_query_id": "l1_2109.03952_2109.03952_fig_1_948",
      "doc_b_query": "How does the orange Attention layer produce its output representation before the yellow Dense Layers?",
      "doc_b_answer": "The attention layer computes weights and takes a weighted combination of the input feature embeddings to produce a d^e-dimensional vector representation for the sample instance.",
      "doc_b_visual_anchor": "orange Attention layer box positioned between red embedding boxes and yellow Dense Layer boxes",
      "doc_b_text_evidence": "These vectors are passed to the attention layer. The Computation of attention weights and the final representation for a sample is",
      "doc_b_figure_id": "2109.03952_fig_1",
      "doc_b_figure_type": "architecture",
      "doc_b_image_path": "data/mineru_output/2109.03952/2109.03952/hybrid_auto/images/2109.03952_page0_fig0.jpg",
      "doc_b_caption": "Figure 1: (a) In general classification model, for each feature $f _ { k }$ a vector representation $e _ { k }$ of length $d ^ { e }$ is learned. This is passed to the attention layer which produces a $d ^ { e }$ - dimensional vector representation for the sample instance $i$ which is passed to two dense layers to get the final classification output. (b) The Attribution framework has the same architecture as the general model. One outcome is obtained through the original model and another through the model that has some attention weights zeroed. The observed difference in accuracy and fairness measures will indicate the effect of the zeroed out features on accuracy and fairness.",
      "doc_b_visual_score": 4,
      "score": 10.0
    },
    {
      "doc_a": "1810.08683",
      "doc_b": "1903.10598",
      "shared_entities": [
        "Adult Dataset",
        "accuracy"
      ],
      "shared_entity_count": 2,
      "doc_a_query_id": "l1_1810.08683_1810.08683_fig_4_640",
      "doc_a_query": "Does the inverse relationship between blue ACC and orange EOd curves in the bottom panel support varying λ conclusions?",
      "doc_a_answer": "Yes, as λ increases from 2e-10 to 1, ACC drops from ~91 to ~67 while EOd rises from ~0.05 to ~0.13, demonstrating the accuracy-fairness trade-off when varying λ as described in the analysis.",
      "doc_a_visual_anchor": "opposing trends of blue ACC (declining) and orange EOd (rising) curves from left to right in bottom panel",
      "doc_a_text_evidence": "This hypothesis is also supported by the results of Figure 5, in which we check how the accuracy and fairness, as measured with the EOd, varies by varying",
      "doc_a_figure_id": "1810.08683_fig_4",
      "doc_a_figure_type": "plot",
      "doc_a_image_path": "data/mineru_output/1810.08683/1810.08683/hybrid_auto/images/1810.08683_page0_fig3.jpg",
      "doc_a_caption": "Figure 5: Adult Dataset: ACC and EOd of MTL, when we fix $\\theta$ and $\\rho$ to be the best values found during the validation procedure and we vary  with $P { = } 0$ , $F { = } 1$ , and $S { = } 0$ .",
      "doc_a_visual_score": 6,
      "doc_b_query_id": "l1_1903.10598_1903.10598_fig_1_770",
      "doc_b_query": "What explains the steep accuracy drop from 78% to 72% in gray triangles as discrimination decreases from 0.4 to 0.8?",
      "doc_b_answer": "The DADT method shows this steep decline because it enforces fairness constraints more aggressively at higher discrimination values, trading off accuracy. This contrasts with the feature-based formulation where the jth feature values are constrained by possible levels in χ_j.",
      "doc_b_visual_anchor": "gray triangles (DADT) dropping from 78% at discrimination 0.4 to 72% at discrimination 0.8",
      "doc_b_text_evidence": "Denote with $\\mathbf { x } _ { i , j }$ the value attained by the $j$ th feature of the ith data point and for $j \\in \\mathcal { F } _ { \\mathrm { c } }$ , let $\\chi _ { j }$ collect the possible levels attainable by feature $j$",
      "doc_b_figure_id": "1903.10598_fig_1",
      "doc_b_figure_type": "plot",
      "doc_b_image_path": "data/mineru_output/1903.10598/1903.10598/hybrid_auto/images/1903.10598_page0_fig0.jpg",
      "doc_b_caption": "",
      "doc_b_visual_score": 1,
      "score": 10.0
    },
    {
      "doc_a": "1810.08683",
      "doc_b": "1905.10674",
      "shared_entities": [
        "accuracy"
      ],
      "shared_entity_count": 1,
      "doc_a_query_id": "l1_1810.08683_1810.08683_fig_4_640",
      "doc_a_query": "Does the inverse relationship between blue ACC and orange EOd curves in the bottom panel support varying λ conclusions?",
      "doc_a_answer": "Yes, as λ increases from 2e-10 to 1, ACC drops from ~91 to ~67 while EOd rises from ~0.05 to ~0.13, demonstrating the accuracy-fairness trade-off when varying λ as described in the analysis.",
      "doc_a_visual_anchor": "opposing trends of blue ACC (declining) and orange EOd (rising) curves from left to right in bottom panel",
      "doc_a_text_evidence": "This hypothesis is also supported by the results of Figure 5, in which we check how the accuracy and fairness, as measured with the EOd, varies by varying",
      "doc_a_figure_id": "1810.08683_fig_4",
      "doc_a_figure_type": "plot",
      "doc_a_image_path": "data/mineru_output/1810.08683/1810.08683/hybrid_auto/images/1810.08683_page0_fig3.jpg",
      "doc_a_caption": "Figure 5: Adult Dataset: ACC and EOd of MTL, when we fix $\\theta$ and $\\rho$ to be the best values found during the validation procedure and we vary  with $P { = } 0$ , $F { = } 1$ , and $S { = } 0$ .",
      "doc_a_visual_score": 6,
      "doc_b_query_id": "l1_1905.10674_1905.10674_fig_1_799",
      "doc_b_query": "Does the input graph's blue and red node structure support embedding generation that operates on social networks?",
      "doc_b_answer": "Yes, the bipartite-style input graph with blue and red nodes represents the type of graph data structure used in social networks and recommender systems that the embedding technique is designed to handle.",
      "doc_b_visual_anchor": "Input Graph box showing blue and red nodes connected in a network structure on the left side",
      "doc_b_text_evidence": "Learning high-quality node embeddings is an important building block for machine learning models that operate on graph data, such as social networks and recommender systems.",
      "doc_b_figure_id": "1905.10674_fig_1",
      "doc_b_figure_type": "diagram",
      "doc_b_image_path": "data/mineru_output/1905.10674/1905.10674/hybrid_auto/images/1905.10674_page0_fig0.jpg",
      "doc_b_caption": "Figure 1. Overview of our approach: Our goal is to generate graph embeddings that are invariant to particular sensitive attributes (e.g., age or gender). We train a set of “filters” to prevent adversarial discriminators from classifying the sensitive information from the filtered embeddings. After training, these filters can be composed together in different combinations, allowing the flexible generation of embeddings that are invariant w.r.t. any subset of the sensitive attributes.",
      "doc_b_visual_score": 4,
      "score": 10.0
    },
    {
      "doc_a": "1810.08683",
      "doc_b": "2005.07293",
      "shared_entities": [
        "accuracy"
      ],
      "shared_entity_count": 1,
      "doc_a_query_id": "l1_1810.08683_1810.08683_fig_4_640",
      "doc_a_query": "Does the inverse relationship between blue ACC and orange EOd curves in the bottom panel support varying λ conclusions?",
      "doc_a_answer": "Yes, as λ increases from 2e-10 to 1, ACC drops from ~91 to ~67 while EOd rises from ~0.05 to ~0.13, demonstrating the accuracy-fairness trade-off when varying λ as described in the analysis.",
      "doc_a_visual_anchor": "opposing trends of blue ACC (declining) and orange EOd (rising) curves from left to right in bottom panel",
      "doc_a_text_evidence": "This hypothesis is also supported by the results of Figure 5, in which we check how the accuracy and fairness, as measured with the EOd, varies by varying",
      "doc_a_figure_id": "1810.08683_fig_4",
      "doc_a_figure_type": "plot",
      "doc_a_image_path": "data/mineru_output/1810.08683/1810.08683/hybrid_auto/images/1810.08683_page0_fig3.jpg",
      "doc_a_caption": "Figure 5: Adult Dataset: ACC and EOd of MTL, when we fix $\\theta$ and $\\rho$ to be the best values found during the validation procedure and we vary  with $P { = } 0$ , $F { = } 1$ , and $S { = } 0$ .",
      "doc_a_visual_score": 6,
      "doc_b_query_id": "l1_2005.07293_2005.07293_fig_1_905",
      "doc_b_query": "How does the different-height box arrangement under the yellow figure in the right panel operationalize compensation for historically disadvantaged groups?",
      "doc_b_answer": "The taller box under the yellow figure represents resource distribution to overcome obstacles and raise opportunities for access, compensating historically disadvantaged groups so they can reach the apples like others.",
      "doc_b_visual_anchor": "Right panel 'Equity' showing yellow figure standing on tallest brown box",
      "doc_b_text_evidence": "Equity is the distribution of resources among groups to overcome obstacles and to raise their opportunities for access [Schement, 2001]. Thus, historically disadvantaged groups are compensated, and others get their fair share to reach their goals.",
      "doc_b_figure_id": "2005.07293_fig_1",
      "doc_b_figure_type": "diagram",
      "doc_b_image_path": "data/mineru_output/2005.07293/2005.07293/hybrid_auto/images/2005.07293_page0_fig0.jpg",
      "doc_b_caption": "Figure 1: Notion of equality in fairness is depicted and formalized along with our newly formalized notion of equity.",
      "doc_b_visual_score": 4,
      "score": 10.0
    },
    {
      "doc_a": "1412.3756",
      "doc_b": "1801.04385",
      "shared_entities": [
        "logistic regression"
      ],
      "shared_entity_count": 1,
      "doc_a_query_id": "l1_1412.3756_1412.3756_fig_3_55",
      "doc_a_query": "Does the utility decay pattern visible across all scatter points support the expected tradeoff between fairness and utility?",
      "doc_a_answer": "Yes, the scatter points show declining utility (from ~1.0 to ~0.5-0.9) as disparate impact increases toward the green line at DI=1.0, demonstrating the expected decay over utility as fairness increases.",
      "doc_a_visual_anchor": "Dashed trend lines connecting scatter points showing downward slope from left to right",
      "doc_a_text_evidence": "The results, shown in Figure 3, demonstrate the expected decay over utility as fairness increases.",
      "doc_a_figure_id": "1412.3756_fig_3",
      "doc_a_figure_type": "plot",
      "doc_a_image_path": "data/mineru_output/1412.3756/1412.3756/hybrid_auto/images/1412.3756_page0_fig2.jpg",
      "doc_a_caption": "Fig. 3: Disparate impact (DI) vs. utility (1-BER) from our combinatorial and geometric partial repair processes using the SVM to classify on the Adult Income and German Credit data sets and the simple threshold classifier on the Ricci data set. Recall that only points with $\\mathsf { D } \\mathsf { I } \\ge \\tau = 0 . 8$ are legal. $\\mathsf { D } \\mathsf { I } = 1 . 0$ represents full fairness.",
      "doc_a_visual_score": 5,
      "doc_b_query_id": "l1_1801.04385_1801.04385_fig_5_326",
      "doc_b_query": "How does acceptance probability change from the blue diagonal band to the red upper region as both reputation and answer count increase?",
      "doc_b_answer": "Acceptance probability increases from near 0.0 (blue) to 0.7 (red) as reputation grows from 10^2 to 10^6 and answer count increases from 10^1 to 10^4, following the diagonal pattern.",
      "doc_b_visual_anchor": "Diagonal gradient transitioning from blue (lower-left) to red (upper-right)",
      "doc_b_text_evidence": "e rate of change of these probability masses with respect to Xp is",
      "doc_b_figure_id": "1801.04385_fig_5",
      "doc_b_figure_type": "plot",
      "doc_b_image_path": "data/mineru_output/1801.04385/1801.04385/hybrid_auto/images/1801.04385_page0_fig4.jpg",
      "doc_b_caption": "Figure 3: Analysis of the Simpson’s paradox Reputation – Number of Answers variable pair. (a) Average acceptance probability as a function of two variables. (b) e distribution of the number of data points contributing to the value of the outcome variable for each pair of variable values.",
      "doc_b_visual_score": 6,
      "score": 10.0
    },
    {
      "doc_a": "1703.06856",
      "doc_b": "1805.03094",
      "shared_entities": [
        "map"
      ],
      "shared_entity_count": 1,
      "doc_a_query_id": "l1_1703.06856_1703.06856_fig_2_212",
      "doc_a_query": "How does node A in the DAG relate to the protected attribute in the insurance pricing scenario?",
      "doc_a_answer": "Node A represents the protected attribute (such as gender or race) in the causal model. In the insurance pricing scenario, the company must predict accident rate Y while considering fairness with respect to this protected attribute A.",
      "doc_a_visual_anchor": "Node A (white circle, top-left position) with directed edge to node X",
      "doc_a_text_evidence": "A car insurance company wishes to price insurance for car owners by predicting their accident rate $Y$.",
      "doc_a_figure_id": "1703.06856_fig_2",
      "doc_a_figure_type": "diagram",
      "doc_a_image_path": "data/mineru_output/1703.06856/1703.06856/hybrid_auto/images/1703.06856_page0_fig1.jpg",
      "doc_a_caption": "Figure 1: (a), (b) Two causal models for different real-world fair prediction scenarios. See Section 3.1 for discussion. (c) The graph corresponding to a causal model with $A$ being the protected attribute and $Y$ some outcome of interest, with background variables assumed to be independent. (d) Expanding the model to include an intermediate variable indicating whether the individual is employed with two (latent) background variables Prejudiced (if the person offering the job is prejudiced) and Qualifications (a measure of the individual’s qualifications). (e) A twin network representation of this system [28] under two different counterfactual levels for $A$ . This is created by copying nodes descending from $A$ , which inherit unaffected parents from the factual world.",
      "doc_a_visual_score": 5,
      "doc_b_query_id": "l1_1805.03094_1805.03094_fig_22_412",
      "doc_b_query": "Does the dark green diagonal band in the heatmap reflect users who answer all words correctly?",
      "doc_b_answer": "Yes, the dark green diagonal band represents high performance, specifically the probability to answer all words correctly in a lesson, as shown by the count scale reaching 10^4.",
      "doc_b_visual_anchor": "dark green diagonal band from lower-left to upper-right with count values ~10^3 to 10^4",
      "doc_b_text_evidence": "The heat map shows performance, i.e., probability to answer all the words correctly, conditioned on the number of distinct words in the lesson.",
      "doc_b_figure_id": "1805.03094_fig_22",
      "doc_b_figure_type": "plot",
      "doc_b_image_path": "data/mineru_output/1805.03094/1805.03094/hybrid_auto/images/1805.03094_page0_fig21.jpg",
      "doc_b_caption": "Figure 6: Disaggregation of Duolingo data showing performance as a function of lesson Correct (a) The heat map shows performance, i.e., probability to answer all the words correctly, conditioned on the number of distinct words in the lesson. (b) Number of data samples in each bin of the heat map. Trends in (c) the disaggregated data and in (d) aggregate data. Errors bars show $9 5 \\%$ confidence interval.",
      "doc_b_visual_score": 5,
      "score": 10.0
    },
    {
      "doc_a": "1610.02413",
      "doc_b": "1805.09458",
      "shared_entities": [
        "X1"
      ],
      "shared_entity_count": 1,
      "doc_a_query_id": "l1_1610.02413_1610.02413_fig_4_136",
      "doc_a_query": "How does the horizontal gap indicated by the lower-right arrow relate to optimizing the equal opportunity predictor?",
      "doc_a_answer": "The horizontal gap between the ROC curve and the profit-maximizing tangent line is proportional to the cost for achieving a given true positive rate, which becomes a convex function that can be efficiently optimized using ternary search.",
      "doc_a_visual_anchor": "Black arrow pointing to horizontal gap between blue curve and tangent line in lower right",
      "doc_a_text_evidence": "within each group the cost for a given true positive rate is proportional to the horizontal gap between the ROC curve and the profit-maximizing tangent line (i.e., the two curves on the left plot), so it is a convex function of the true positive rate",
      "doc_a_figure_id": "1610.02413_fig_4",
      "doc_a_figure_type": "plot",
      "doc_a_image_path": "data/mineru_output/1610.02413/1610.02413/hybrid_auto/images/1610.02413_page0_fig3.jpg",
      "doc_a_caption": "Figure 2: Finding the optimal equalized odds threshold predictor (middle), and equal opportunity threshold predictor (right). For the equal opportunity predictor, within each group the cost for a given true positive rate is proportional to the horizontal gap between the ROC curve and the profit-maximizing tangent line (i.e., the two curves on the left plot), so it is a convex function of the true positive rate (right). This lets us optimize it efficiently with ternary search.",
      "doc_a_visual_score": 7,
      "doc_b_query_id": "l1_1805.09458_1805.09458_fig_5_446",
      "doc_b_query": "How does using digit label as covariate enable the leftmost real 9 to generate all ten digits rightward?",
      "doc_b_answer": "Using digit label as covariate class c pushes all non-class stylistic information into latent space while removing information about the exact digit. This separation allows the decoder to recombine the extracted style with any specified digit label c′ to produce all ten digits.",
      "doc_b_visual_anchor": "Top row showing real digit 9 on left and generated sequence 0123456789 on right",
      "doc_b_text_evidence": "We use the digit label as the covariate class c, which pushes all non-class stylistic information into the latent space while attempting to remove information about the exact digit being written.",
      "doc_b_figure_id": "1805.09458_fig_5",
      "doc_b_figure_type": "example",
      "doc_b_image_path": "data/mineru_output/1805.09458/1805.09458/hybrid_auto/images/1805.09458_page0_fig4.jpg",
      "doc_b_caption": "Figure 3: We demonstrate the ability to generate stylistically similar images of varying classes using the MNIST dataset. The left column is mapped into $z$ that is invariant to its digit label c. We then can generate an image using $z$ and any other specified digit, $c ^ { \\prime }$ , as show on the right.",
      "doc_b_visual_score": 3,
      "score": 10.0
    },
    {
      "doc_a": "1705.10378",
      "doc_b": "1809.02244",
      "shared_entities": [
        "COMPAS"
      ],
      "shared_entity_count": 1,
      "doc_a_query_id": "l1_1705.10378_1705.10378_fig_1_226",
      "doc_a_query": "Why does C point to both A and Y if causal models generalize Bayesian networks?",
      "doc_a_answer": "C pointing to both A and Y indicates C is a common cause (confounder) that creates conditional dependencies, which causal models encode beyond standard Bayesian network independence structures.",
      "doc_a_visual_anchor": "Node C at top-left with blue arrows pointing to both A (bottom-left) and Y (top-right)",
      "doc_a_text_evidence": "Such models generalize independence models on directed acyclic graphs, also known as Bayesian networks (Pearl 1988), to also encode conditional independence statements on counterfactual random variables",
      "doc_a_figure_id": "1705.10378_fig_1",
      "doc_a_figure_type": "diagram",
      "doc_a_image_path": "data/mineru_output/1705.10378/1705.10378/hybrid_auto/images/1705.10378_page0_fig0.jpg",
      "doc_a_caption": "Figure 1: (a) A causal graph with a single mediator. (b) A causal graph with two mediators, one confounded with the outcome via an unobserved common cause. (c) A causal graph with a single mediator where the natural direct effect is not identified.",
      "doc_a_visual_score": 6,
      "doc_b_query_id": "l1_1809.02244_1809.02244_fig_4_530",
      "doc_b_query": "How do the dashed and dotted blue curves demonstrate the formalization extended from Nabi and Shpitser 2018 for utility parameter above 2.5?",
      "doc_b_answer": "The gap between the dashed (optimal unrestricted) and dotted (optimal fair) blue curves shows the tradeoff between unconstrained policy optimization and fairness-constrained policy learning, illustrating how fairness constraints modify decision-making to induce more equitable outcomes.",
      "doc_b_visual_anchor": "Separation between blue dashed line (~1.0) and blue dotted line (~0.8) for utility parameter values above 2.5",
      "doc_b_text_evidence": "We have extended a formalization of algorithmic fairness from Nabi & Shpitser (2018) to the setting of learning optimal policies under fairness constraints. We show how to constrain a set of statistical models and learn a policy such that subsequent decision making given new observations from the \"unfair world\" induces high-quality outcomes while satisfying the specified fairness constraints",
      "doc_b_figure_id": "1809.02244_fig_4",
      "doc_b_figure_type": "plot",
      "doc_b_image_path": "data/mineru_output/1809.02244/1809.02244/hybrid_auto/images/1809.02244_page0_fig3.jpg",
      "doc_b_caption": "Figure 2. Group-level incarceration rates for the COMPAS data as a function of the utility parameter $\\theta$ .",
      "doc_b_visual_score": 4,
      "score": 10.0
    },
    {
      "doc_a": "1705.10378",
      "doc_b": "2109.03952",
      "shared_entities": [
        "UCI"
      ],
      "shared_entity_count": 1,
      "doc_a_query_id": "l1_1705.10378_1705.10378_fig_1_226",
      "doc_a_query": "Why does C point to both A and Y if causal models generalize Bayesian networks?",
      "doc_a_answer": "C pointing to both A and Y indicates C is a common cause (confounder) that creates conditional dependencies, which causal models encode beyond standard Bayesian network independence structures.",
      "doc_a_visual_anchor": "Node C at top-left with blue arrows pointing to both A (bottom-left) and Y (top-right)",
      "doc_a_text_evidence": "Such models generalize independence models on directed acyclic graphs, also known as Bayesian networks (Pearl 1988), to also encode conditional independence statements on counterfactual random variables",
      "doc_a_figure_id": "1705.10378_fig_1",
      "doc_a_figure_type": "diagram",
      "doc_a_image_path": "data/mineru_output/1705.10378/1705.10378/hybrid_auto/images/1705.10378_page0_fig0.jpg",
      "doc_a_caption": "Figure 1: (a) A causal graph with a single mediator. (b) A causal graph with two mediators, one confounded with the outcome via an unobserved common cause. (c) A causal graph with a single mediator where the natural direct effect is not identified.",
      "doc_a_visual_score": 6,
      "doc_b_query_id": "l1_2109.03952_2109.03952_fig_1_948",
      "doc_b_query": "How does the orange Attention layer produce its output representation before the yellow Dense Layers?",
      "doc_b_answer": "The attention layer computes weights and takes a weighted combination of the input feature embeddings to produce a d^e-dimensional vector representation for the sample instance.",
      "doc_b_visual_anchor": "orange Attention layer box positioned between red embedding boxes and yellow Dense Layer boxes",
      "doc_b_text_evidence": "These vectors are passed to the attention layer. The Computation of attention weights and the final representation for a sample is",
      "doc_b_figure_id": "2109.03952_fig_1",
      "doc_b_figure_type": "architecture",
      "doc_b_image_path": "data/mineru_output/2109.03952/2109.03952/hybrid_auto/images/2109.03952_page0_fig0.jpg",
      "doc_b_caption": "Figure 1: (a) In general classification model, for each feature $f _ { k }$ a vector representation $e _ { k }$ of length $d ^ { e }$ is learned. This is passed to the attention layer which produces a $d ^ { e }$ - dimensional vector representation for the sample instance $i$ which is passed to two dense layers to get the final classification output. (b) The Attribution framework has the same architecture as the general model. One outcome is obtained through the original model and another through the model that has some attention weights zeroed. The observed difference in accuracy and fairness measures will indicate the effect of the zeroed out features on accuracy and fairness.",
      "doc_b_visual_score": 4,
      "score": 10.0
    },
    {
      "doc_a": "1711.08536",
      "doc_b": "1804.06876",
      "shared_entities": [
        "US"
      ],
      "shared_entity_count": 1,
      "doc_a_query_id": "l1_1711.08536_1711.08536_fig_4_310",
      "doc_a_query": "Why does the green rater curve peak at log-likelihood -6 while the blue standard test curve peaks near 0?",
      "doc_a_answer": "Hyderabad-located crowdsourced groom/bridegroom images are dramatically less likely to be recognized correctly by ImageNet-trained models compared to standard test set images, showing a 6-unit log-likelihood gap.",
      "doc_a_visual_anchor": "Green curve peak at -6 versus blue curve peak at 0",
      "doc_a_text_evidence": "the images provided by Hyderabad-located crowdsourcing are dramatically less likely to be recognized correctly by these models",
      "doc_a_figure_id": "1711.08536_fig_4",
      "doc_a_figure_type": "plot",
      "doc_a_image_path": "data/mineru_output/1711.08536/1711.08536/hybrid_auto/images/1711.08536_page0_fig3.jpg",
      "doc_a_caption": "Figure 3: Density plots of log-likelihood attributed for groom, bridegroom images crowdsourced by raters in Hyderabad, India, as scored by a model trained on ImageNet (left) and Open Images (center), as compared to images in the standard test sets. In both cases, the images provided by Hyderabad-located crowdsourcing are dramatically less likely to be recognized correctly by these models. The plot on right shows a similar trend for the woman class in OpenImages which has no corresponding class in ImageNet.",
      "doc_a_visual_score": 4,
      "doc_b_query_id": "l1_1804.06876_1804.06876_fig_3_384",
      "doc_b_query": "What makes the gender swap between solid blue 'him' and dashed orange 'her' irrelevant for coreference decisions?",
      "doc_b_answer": "The gender of the pronominal reference is explicitly designed to be irrelevant because the coreference decision should be based on contextual clues rather than stereotypical gender-occupation associations.",
      "doc_b_visual_anchor": "solid blue 'him' in top sentence versus dashed orange 'her' in bottom sentence",
      "doc_b_text_evidence": "For each example, the gender of the pronominal reference is irrelevant for the co-reference decision.",
      "doc_b_figure_id": "1804.06876_fig_3",
      "doc_b_figure_type": "example",
      "doc_b_image_path": "data/mineru_output/1804.06876/1804.06876/hybrid_auto/images/1804.06876_page0_fig2.jpg",
      "doc_b_caption": "Figure 1: Pairs of gender balanced co-reference tests in the WinoBias dataset. Male and female entities are marked in solid blue and dashed orange, respectively. For each example, the gender of the pronominal reference is irrelevant for the co-reference decision. Systems must be able to make correct linking predictions in pro-stereotypical scenarios (solid purple lines) and anti-stereotypical scenarios (dashed purple lines) equally well to pass the test. Importantly, stereotypical occupations are considered based on US Department of Labor statistics.",
      "doc_b_visual_score": 6,
      "score": 10.0
    },
    {
      "doc_a": "1801.04385",
      "doc_b": "1805.03094",
      "shared_entities": [
        "Stack Exchange"
      ],
      "shared_entity_count": 1,
      "doc_a_query_id": "l1_1801.04385_1801.04385_fig_5_326",
      "doc_a_query": "How does acceptance probability change from the blue diagonal band to the red upper region as both reputation and answer count increase?",
      "doc_a_answer": "Acceptance probability increases from near 0.0 (blue) to 0.7 (red) as reputation grows from 10^2 to 10^6 and answer count increases from 10^1 to 10^4, following the diagonal pattern.",
      "doc_a_visual_anchor": "Diagonal gradient transitioning from blue (lower-left) to red (upper-right)",
      "doc_a_text_evidence": "e rate of change of these probability masses with respect to Xp is",
      "doc_a_figure_id": "1801.04385_fig_5",
      "doc_a_figure_type": "plot",
      "doc_a_image_path": "data/mineru_output/1801.04385/1801.04385/hybrid_auto/images/1801.04385_page0_fig4.jpg",
      "doc_a_caption": "Figure 3: Analysis of the Simpson’s paradox Reputation – Number of Answers variable pair. (a) Average acceptance probability as a function of two variables. (b) e distribution of the number of data points contributing to the value of the outcome variable for each pair of variable values.",
      "doc_a_visual_score": 6,
      "doc_b_query_id": "l1_1805.03094_1805.03094_fig_22_412",
      "doc_b_query": "Does the dark green diagonal band in the heatmap reflect users who answer all words correctly?",
      "doc_b_answer": "Yes, the dark green diagonal band represents high performance, specifically the probability to answer all words correctly in a lesson, as shown by the count scale reaching 10^4.",
      "doc_b_visual_anchor": "dark green diagonal band from lower-left to upper-right with count values ~10^3 to 10^4",
      "doc_b_text_evidence": "The heat map shows performance, i.e., probability to answer all the words correctly, conditioned on the number of distinct words in the lesson.",
      "doc_b_figure_id": "1805.03094_fig_22",
      "doc_b_figure_type": "plot",
      "doc_b_image_path": "data/mineru_output/1805.03094/1805.03094/hybrid_auto/images/1805.03094_page0_fig21.jpg",
      "doc_b_caption": "Figure 6: Disaggregation of Duolingo data showing performance as a function of lesson Correct (a) The heat map shows performance, i.e., probability to answer all the words correctly, conditioned on the number of distinct words in the lesson. (b) Number of data samples in each bin of the heat map. Trends in (c) the disaggregated data and in (d) aggregate data. Errors bars show $9 5 \\%$ confidence interval.",
      "doc_b_visual_score": 5,
      "score": 10.0
    }
  ]
}